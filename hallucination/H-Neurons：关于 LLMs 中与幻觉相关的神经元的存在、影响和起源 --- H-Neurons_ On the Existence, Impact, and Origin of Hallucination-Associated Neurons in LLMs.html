<!DOCTYPE html>
<!-- saved from url=(0071)https://arxiv.org/html/2512.01797?_immersive_translate_auto_translate=1 -->
<html lang="en" data-theme="dark" imt-state="dual" imt-trans-position="after"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs</title>
<!--Generated on Tue Dec  2 07:08:53 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css">
<script src="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/addons_new.js"></script>
<script src="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/feedbackOverlay.js"></script>
<!--<base href="/html/2512.01797v2/">--><base href="."><link rel="stylesheet" href="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/utz6mli.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"><style data-id="immersive-translate-input-injected-css">.immersive-translate-input {
  position: absolute;
  top: 0;
  right: 0;
  left: 0;
  bottom: 0;
  z-index: 2147483647;
  display: flex;
  justify-content: center;
  align-items: center;
}
.immersive-translate-attach-loading::after {
  content: " ";

  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;

  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-2000%, -50%);
  z-index: 100;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.immersive-translate-input-loading {
  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;
}

@keyframes immersiveTranslateShadowRolling {
  0% {
    box-shadow: 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  12% {
    box-shadow: 100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  25% {
    box-shadow: 110px 0 var(--loading-color), 100px 0 var(--loading-color),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  36% {
    box-shadow: 120px 0 var(--loading-color), 110px 0 var(--loading-color),
      100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0);
  }

  50% {
    box-shadow: 130px 0 var(--loading-color), 120px 0 var(--loading-color),
      110px 0 var(--loading-color), 100px 0 var(--loading-color);
  }

  62% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color),
      120px 0 var(--loading-color), 110px 0 var(--loading-color);
  }

  75% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      130px 0 var(--loading-color), 120px 0 var(--loading-color);
  }

  87% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color);
  }

  100% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0);
  }
}

.immersive-translate-toast {
  display: flex;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  right: 0;
  top: 1%;
  width: fit-content;
  padding: 12px 20px;
  margin: auto;
  overflow: auto;
  background: #fef6f9;
  box-shadow: 0px 4px 10px 0px rgba(0, 10, 30, 0.06);
  font-size: 15px;
  border-radius: 8px;
  color: #333;
}

.immersive-translate-toast-content {
  display: flex;
  flex-direction: row;
  align-items: center;
}

.immersive-translate-toast-hidden {
  margin: 0 20px 0 72px;
  text-decoration: underline;
  cursor: pointer;
}

.immersive-translate-toast-close {
  color: #666666;
  font-size: 20px;
  font-weight: bold;
  padding: 0 10px;
  cursor: pointer;
}

@media screen and (max-width: 768px) {
  .immersive-translate-toast {
    top: 0;
    padding: 12px 0px 0 10px;
  }
  .immersive-translate-toast-content {
    flex-direction: column;
    text-align: center;
  }
  .immersive-translate-toast-hidden {
    margin: 10px auto;
  }
}

.immersive-translate-dialog {
  position: fixed;
  z-index: 2147483647;
  left: 0;
  top: 0;
  display: flex;
  width: 300px;
  flex-direction: column;
  align-items: center;
  font-size: 15px;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  margin: auto;
  height: fit-content;
  border-radius: 20px;
  background-color: #fff;
}

.immersive-translate-modal {
  display: none;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgb(0, 0, 0);
  background-color: rgba(0, 0, 0, 0.4);
  font-size: 15px;
}

.immersive-translate-modal-content {
  background-color: #fefefe;
  margin: 10% auto;
  padding: 40px 24px 24px;
  border-radius: 12px;
  width: 350px;
  font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  position: relative;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-content {
    margin: 25% auto !important;
  }
}

@media screen and (max-width: 480px) {
  .immersive-translate-modal-content {
    width: 80vw !important;
    margin: 20vh auto !important;
    padding: 20px 12px 12px !important;
  }

  .immersive-translate-modal-title {
    font-size: 14px !important;
  }

  .immersive-translate-modal-body {
    font-size: 13px !important;
    max-height: 60vh !important;
  }

  .immersive-translate-btn {
    font-size: 13px !important;
    padding: 8px 16px !important;
    margin: 0 4px !important;
  }

  .immersive-translate-modal-footer {
    gap: 6px !important;
    margin-top: 16px !important;
  }
}

.immersive-translate-modal .immersive-translate-modal-content-in-input {
  max-width: 500px;
}
.immersive-translate-modal-content-in-input .immersive-translate-modal-body {
  text-align: left;
  max-height: unset;
}

.immersive-translate-modal-title {
  text-align: center;
  font-size: 16px;
  font-weight: 700;
  color: #333333;
}

.immersive-translate-modal-body {
  text-align: center;
  font-size: 14px;
  font-weight: 400;
  color: #333333;
  margin-top: 24px;
  word-break: break-all;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-body {
    max-height: 250px;
    overflow-y: auto;
  }
}

.immersive-translate-close {
  color: #666666;
  position: absolute;
  right: 16px;
  top: 16px;
  font-size: 20px;
  font-weight: bold;
}

.immersive-translate-close:hover,
.immersive-translate-close:focus {
  text-decoration: none;
  cursor: pointer;
}

.immersive-translate-modal-footer {
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  margin-top: 24px;
}

.immersive-translate-btn {
  width: fit-content;
  color: #fff;
  background-color: #ea4c89;
  border: none;
  font-size: 14px;
  margin: 0 8px;
  padding: 9px 30px;
  border-radius: 5px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: background-color 0.3s ease;
}

.immersive-translate-btn-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  gap: 8px;
}

.immersive-translate-btn:hover {
  background-color: #f082ac;
}
.immersive-translate-btn:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}
.immersive-translate-btn:disabled:hover {
  background-color: #ea4c89;
}

.immersive-translate-link-btn {
  background-color: transparent;
  color: #ea4c89;
  border: none;
  cursor: pointer;
  height: 30px;
  line-height: 30px;
}

.immersive-translate-cancel-btn {
  /* gray color */
  background-color: rgb(89, 107, 120);
}

.immersive-translate-cancel-btn:hover {
  background-color: hsl(205, 20%, 32%);
}

.immersive-translate-action-btn {
  background-color: transparent;
  color: #ea4c89;
  border: 1px solid #ea4c89;
}

.immersive-translate-btn svg {
  margin-right: 5px;
}

.immersive-translate-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-primary-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-modal input[type="radio"] {
  margin: 0 6px;
  cursor: pointer;
}

.immersive-translate-modal label {
  cursor: pointer;
}

.immersive-translate-close-action {
  position: absolute;
  top: 2px;
  right: 0px;
  cursor: pointer;
}

.imt-image-status {
  background-color: rgba(0, 0, 0, 0.5) !important;
  display: flex !important;
  flex-direction: column !important;
  align-items: center !important;
  justify-content: center !important;
  border-radius: 16px !important;
}
.imt-image-status img,
.imt-image-status svg,
.imt-img-loading {
  width: 28px !important;
  height: 28px !important;
  margin: 0 0 8px 0 !important;
  min-height: 28px !important;
  min-width: 28px !important;
  position: relative !important;
}
.imt-img-loading {
  background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADgAAAA4CAMAAACfWMssAAAAtFBMVEUAAAD////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////oK74hAAAAPHRSTlMABBMIDyQXHwyBfFdDMSw+OjXCb+5RG51IvV/k0rOqlGRM6KKMhdvNyZBz9MaupmxpWyj437iYd/yJVNZeuUC7AAACt0lEQVRIx53T2XKiUBCA4QYOiyCbiAsuuGBcYtxiYtT3f6/pbqoYHVFO5r+iivpo6DpAWYpqeoFfr9f90DsYAuRSWkFnPO50OgR9PwiCUFcl2GEcx+N/YBh6pvKaefHlUgZd1zVe0NbYcQjGBfzrPE8Xz8aF+71D8gG6DHFPpc4a7xFiCDuhaWgKgGIJQ3d5IMGDrpS4S5KgpIm+en9f6PlAhKby4JwEIxlYJV9h5k5nee9GoxHJ2IDSNB0dwdad1NAxDJ/uXDHYmebdk4PdbkS58CIVHdYSUHTYYRWOJblWSyu2lmy3KNFVJNBhxcuGW4YBVCbYGRZwIooipHsNqjM4FbgOQqQqSKQQU9V8xmi1QlgHqQQ6DDBvRUVCDirs+EzGDGOQTCATgtYTnbCVLgsVgRE0T1QE0qHCFAht2z6dLvJQs3Lo2FQoDxWNUiBhaP4eRgwNkI+dAjVOA/kUrIDwf3CG8NfNOE0eiFotSuo+rBiq8tD9oY4Qzc6YJw99hl1wzpQvD7ef2M8QgnOGJfJw+EltQc+oX2yn907QB22WZcvlUpd143dqQu+8pCJZuGE4xCuPXJqqcs5sNpsI93Rmzym1k4Npk+oD1SH3/a3LOK/JpUBpWfqNySxWzCfNCUITuDG5dtuphrUJ1myeIE9bIsPiKrfqTai5WZxbhtNphYx6GEIHihyGFTI69lje/rxajdh0s0msZ0zYxyPLhYCb1CyHm9Qsd2H37Y3lugVwL9kNh8Ot8cha6fUNQ8nuXi5z9/ExsAO4zQrb/ev1yrCB7lGyQzgYDGuxq1toDN/JGvN+HyWNHKB7zEoK+PX11e12G431erGYzwmytAWU56fkMHY5JJnDRR2eZji3AwtIcrEV8Cojat/BdQ7XOwGV1e1hDjGGjXbdArm8uJZtCH5MbcctVX8A1WpqumJHwckAAAAASUVORK5CYII=");
  background-size: 28px 28px;
  animation: image-loading-rotate 1s linear infinite !important;
}

.imt-image-status span {
  color: var(--bg-2, #fff) !important;
  font-size: 14px !important;
  line-height: 14px !important;
  font-weight: 500 !important;
  font-family: "PingFang SC", Arial, sans-serif !important;
}

.imt-primary-button {
  display: flex;
  padding: 12px 80px;
  justify-content: center;
  align-items: center;
  gap: 8px;
  border-radius: 8px;
  background: #ea4c89;
  color: #fff;
  font-size: 16px;
  font-style: normal;
  font-weight: 700;
  line-height: 24px;
  border: none;
  cursor: pointer;
}

.imt-retry-text {
  color: #999;
  text-align: center;
  font-size: 14px;
  font-style: normal;
  font-weight: 400;
  line-height: 21px;
  cursor: pointer;
}

.imt-action-container {
  display: flex;
  flex-direction: column;
  gap: 12px;
}

.imt-modal-content-text {
  text-align: left;
  color: #333;
  font-size: 16px;
  font-weight: 400;
  line-height: 24px;
}

@keyframes image-loading-rotate {
  from {
    transform: rotate(360deg);
  }
  to {
    transform: rotate(0deg);
  }
}

.imt-linear-gradient-text {
  background: linear-gradient(90deg, #00a6ff 0%, #c369ff 52.4%, #ff4590 100%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
}

.imt-flex-center {
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-linear-black-btn {
  border-radius: 50px;
  background: linear-gradient(66deg, #222 19%, #696969 94.25%);
  height: 48px;
  width: 100%;
  color: #fff;
  font-size: 16px;
  font-weight: 700;
  display: flex;
  align-items: center;
  cursor: pointer;
  justify-content: center;
}
</style><style data-id="immersive-translate-default-injected-css">:root {
  --immersive-translate-theme-underline-borderColor: #72ece9;
  --immersive-translate-theme-nativeUnderline-borderColor: #72ece9;
  --immersive-translate-theme-nativeDashed-borderColor: #72ece9;
  --immersive-translate-theme-nativeDotted-borderColor: #72ece9;
  --immersive-translate-theme-highlight-backgroundColor: #ffff00;
  --immersive-translate-theme-dashed-borderColor: #59c1bd;
  --immersive-translate-theme-blockquote-borderColor: #cc3355;
  --immersive-translate-theme-thinDashed-borderColor: #ff374f;
  --immersive-translate-theme-dashedBorder-borderColor: #94a3b8;
  --immersive-translate-theme-dashedBorder-borderRadius: 0;
  --immersive-translate-theme-solidBorder-borderColor: #94a3b8;
  --immersive-translate-theme-solidBorder-borderRadius: 0;
  --immersive-translate-theme-dotted-borderColor: #94a3b8;
  --immersive-translate-theme-wavy-borderColor: #72ece9;
  --immersive-translate-theme-dividingLine-borderColor: #94a3b8;
  --immersive-translate-theme-grey-textColor: #2f4f4f;
  --immersive-translate-theme-marker-backgroundColor: #fbda41;
  --immersive-translate-theme-marker-backgroundColor-rgb: 251, 218, 65;
  --immersive-translate-theme-marker2-backgroundColor: #ffff00;
  --immersive-translate-theme-background-backgroundColor: #dbafaf;
  --immersive-translate-theme-background-backgroundColor-rgb: 219, 175, 175;
  --immersive-translate-theme-background-backgroundOpacity: 12;
  --immersive-translate-theme-opacity-opacity: 10;
}

[imt-state="dual"] .immersive-translate-target-translation-pre-whitespace {
  white-space: pre-wrap !important;
}

[imt-state="dual"] .immersive-translate-pdf-target-container {
  position: absolute;
  background-color: #fff;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica,
    sans-serif;
  top: 0;
  width: 600px;
  height: 100%;
  z-index: 2;
  line-height: 1.3;
  font-size: 16px;
}
[imt-state="dual"] .immersive-translate-target-wrapper[dir="rtl"] {
  text-align: right;
}

[imt-state="dual"]
  .immersive-translate-pdf-target-container
  .immersive-translate-target-wrapper {
  color: rgb(0, 0, 0);
  white-space: normal;
  position: absolute;
}

[imt-state="dual"]
  .immersive-translate-pdf-target-container
  .immersive-translate-target-wrapper
  font {
  color: inherit;
  white-space: inherit;
  position: unset;
}

[imt-state="translation"] .immersive-translate-target-wrapper > br {
  display: none;
}

[imt-state="translation"]
  .immersive-translate-target-translation-block-wrapper {
  margin: 0 !important;
}

[imt-state="dual"] .immersive-translate-target-translation-block-wrapper {
  margin: 8px 0 !important;
  display: inline-block;
}

[imt-trans-position="before"]
  .immersive-translate-target-translation-block-wrapper {
  display: block;
}

[imt-trans-position="before"]
  .immersive-translate-target-translation-block-wrapper {
  margin-top: 0 !important;
}

[imt-state="dual"] .immersive-translate-target-translation-pdf-block-wrapper {
  margin: 0 !important;
  display: inline-block;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-grey-inner {
  color: var(--immersive-translate-theme-grey-textColor);
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-underline-inner {
  border-bottom: 1px solid
    var(--immersive-translate-theme-underline-borderColor) !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-nativeUnderline-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-nativeUnderline-borderColor
  ) !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-dashedBorder {
  border: 1px dashed var(--immersive-translate-theme-dashedBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-dashedBorder-borderRadius
  ) !important;
  padding: 6px;
  margin-top: 2px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-inline-wrapper-theme-dashedBorder {
  border: 1px dashed var(--immersive-translate-theme-dashedBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-dashedBorder-borderRadius
  ) !important;
  padding: 2px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-solidBorder {
  border: 1px solid var(--immersive-translate-theme-solidBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-solidBorder-borderRadius
  ) !important;
  padding: 6px;
  margin-top: 2px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-inline-wrapper-theme-solidBorder {
  border: 1px solid var(--immersive-translate-theme-solidBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-solidBorder-borderRadius
  ) !important;
  padding: 2px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-nativeDashed-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-nativeDashed-borderColor
  ) !important;
  text-decoration-style: dashed !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-thinDashed-inner {
  border-bottom: 1px dashed
    var(--immersive-translate-theme-thinDashed-borderColor) !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-dotted-inner {
  background-image: linear-gradient(
    to right,
    var(--immersive-translate-theme-dotted-borderColor) 30%,
    rgba(255, 255, 255, 0) 0%
  );
  background-position: bottom;
  background-size: 5px 1px;
  background-repeat: repeat-x;
  padding-bottom: 3px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-nativeDotted-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-nativeDotted-borderColor
  ) !important;
  text-decoration-style: dotted !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-wavy-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-wavy-borderColor
  ) !important;
  text-decoration-style: wavy !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-dashed-inner {
  background: linear-gradient(
      to right,
      var(--immersive-translate-theme-dashed-borderColor) 0%,
      var(--immersive-translate-theme-dashed-borderColor) 50%,
      transparent 50%,
      transparent 100%
    )
    repeat-x left bottom;
  background-size: 8px 2px;
  padding-bottom: 2px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-dividingLine::before {
  content: "";
  display: block;
  max-width: 80px;
  width: 10%;
  border-top: 1px dashed
    var(--immersive-translate-theme-dividingLine-borderColor);
  padding-top: 8px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-inline-wrapper-theme-dividingLine::before {
  content: "";
  border-left: 1px dashed
    var(--immersive-translate-theme-dividingLine-borderColor);
  max-height: 16px;
  height: 16px;
  padding-left: 8px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-highlight-inner {
  background: var(--immersive-translate-theme-highlight-backgroundColor);
  box-decoration-break: clone;
  -webkit-box-decoration-break: clone;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-marker {
  line-height: 1.5em;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-marker2-inner {
  font-weight: bold;
  text-shadow: 10px 0px 3px
      var(--immersive-translate-theme-marker2-backgroundColor),
    16px 3px 9px var(--immersive-translate-theme-marker2-backgroundColor),
    2px 0px 6px var(--immersive-translate-theme-marker2-backgroundColor),
    -12px 0px 12px var(--immersive-translate-theme-marker2-backgroundColor) !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-marker-inner {
  /* TODO: add more texture */
  background: linear-gradient(
    to right,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.1),
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.9) 3%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.9) 35%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.9) 70%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.8) 95%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.3)
  );
  box-decoration-break: clone;
  -webkit-box-decoration-break: clone;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-weakening {
  opacity: 0.618 !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-italic {
  font-style: italic !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-bold {
  font-weight: bold !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-paper {
  margin: 8px 0;
  box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
  padding: 16px 32px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-blockquote {
  border-left: 4px solid var(--immersive-translate-theme-blockquote-borderColor) !important;
  padding-left: 12px !important;
  margin-top: 4px;
  margin-bottom: 4px;
  padding-top: 4px;
  padding-bottom: 4px;
  display: inline-block;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-mask-inner {
  filter: blur(5px) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

[data-immersive-translate-root-translation-theme="none"]
  .immersive-translate-target-translation-theme-mask-inner {
  filter: none !important;
}

[data-immersive-translate-root-translation-theme="mask"]
  .immersive-translate-target-inner {
  filter: blur(5px) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

/* opacity theme start */

[imt-state="dual"] .immersive-translate-target-translation-theme-opacity-inner {
  filter: opacity(
    calc(var(--immersive-translate-theme-opacity-opacity) * 1%)
  ) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

[data-immersive-translate-root-translation-theme="none"]
  .immersive-translate-target-translation-theme-opacity-inner {
  filter: none !important;
}
[data-immersive-translate-root-translation-theme="opacity"]
  .immersive-translate-target-inner,
[imt-state="dual"]
  .immersive-translate-target-translation-theme-opacity-inner:hover {
  filter: opacity(
    calc(var(--immersive-translate-theme-opacity-opacity) * 1%)
  ) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-opacity-inner:hover {
  filter: none !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-mask-inner:hover {
  filter: none !important;
}
[data-immersive-translate-root-translation-theme="opacity"]
  .immersive-translate-target-inner:hover {
  filter: none !important;
}

[data-immersive-translate-root-translation-theme="mask"]
  .immersive-translate-target-inner:hover {
  filter: none !important;
}

/* opacity theme end */

/* background theme start */
[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-background {
  margin: 8px 0;
  background: rgba(
    var(--immersive-translate-theme-background-backgroundColor-rgb),
    calc(var(--immersive-translate-theme-background-backgroundOpacity) * 1%)
  );
  border-radius: 4px;
  box-shadow: unset !important;
  padding: 12px;
  display: inline-block;
}
[imt-state="dual"]
  .immersive-translate-target-translation-theme-background-inner {
  background: rgba(
    var(--immersive-translate-theme-background-backgroundColor-rgb),
    calc(var(--immersive-translate-theme-background-backgroundOpacity) * 1%)
  );
  padding-left: 6px;
  padding-right: 6px;
  box-decoration-break: clone;
  -webkit-box-decoration-break: clone;
}
[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper
  .immersive-translate-target-translation-theme-background-inner {
  background: unset;
  padding-left: unset;
  padding-right: unset;
}
/* background theme end */

/* vertical css , please remain it in the last one. */
.immersive-translate-target-translation-vertical-block-wrapper {
  margin: 0px 8px !important;
}

.immersive-translate-text {
  font-size: 15px !important;
}

.immersive-translate-error-toast {
  position: fixed;
  top: 5%;
  z-index: 99999999;
  left: 0;
  right: 0;
  margin: auto;
  max-width: 300px;
  padding: 16px;
  border-radius: 12px;
  background-color: rgba(0, 0, 0, 0.8);
  display: flex;
  flex-direction: row;
  justify-content: space-between;
}

@media all and (min-width: 750px) {
  .immersive-translate-error-toast {
    max-width: 400px;
  }
}

.immersive-translate-clickable-button {
  cursor: pointer;
}

.immersive-translate-help-button {
  cursor: pointer;
}

.immersive-translate-loading-text:before {
  content: "...";
}

/* dark mode for loading */

@media only screen and (prefers-color-scheme: dark) {
  .immersive-translate-loading {
    border: 2px rgba(255, 255, 255, 0.25) solid !important;
    border-top: 2px rgba(255, 255, 255, 1) solid !important;
  }
}

.immersive-translate-error-wrapper {
  position: relative;
  display: inline-flex;
  padding: 6px;
  margin: 0 12px;
  white-space: nowrap;
  font-size: 0.9em;
}
[lang="zh-CN"] .immersive-translate-error-wrapper {
  font-size: 0.75em;
}
[lang="zh-TW"] .immersive-translate-error-wrapper {
  font-size: 0.75em;
}

.immersive-translate-tooltip {
  position: relative;
  display: inline-flex;
  /* little indicater to indicate it's hoverable */
}

.immersive-translate-tooltip-content {
  /* here's the magic */
  position: absolute;
  z-index: 100000000000;

  left: 50%;
  bottom: 0;
  transform: translate(-50%, 110%);
  line-height: 1;
  /* and add a small left margin */

  /* basic styles */
  width: max-content;
  max-width: 250px;
  word-wrap: break-word;
  white-space: pre-line;
  padding: 10px;
  border-radius: 10px;
  background: #000c;
  color: #fff;
  text-align: center;
  font-size: 14px;
  display: none;
  /* hide by default */
}

.immersive-translate-tooltip:hover .immersive-translate-tooltip-content {
  display: inline-block;
}

.immersive-translate-tooltip:hover + .immersive-translate-tooltip-content {
  display: inline-block;
}

.immersive-translate-tooltip-content-table {
  left: unset !important;
  bottom: unset !important;
  transform: translate(-10%, 50%) !important;
}

.immersive-translate-tooltip:hover:before {
  display: inline-block;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.imt-image-status {
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  color: var(--bg-2, #fff);
  font-size: 14px;
}
</style><style data-id="immersive-translate-user-custom-style">:root {

.immersive-translate-target-inner { font-family: inherit; }


.immersive-translate-target-inner { font-family: inherit; }
}
</style><style data-id="immersive-translate-dynamic-injected-css">.immersive-translate-target-wrapper[dir='rtl'] {text-align: right;display:block!important;}
[dir='rtl'] .immersive-translate-target-wrapper:not([dir]) {text-align:left;direction:ltr;}
.immersive-translate-target-wrapper {word-break:break-word; user-select:text;}
[imt-state=dual] .immersive-translate-target-translation-block-wrapper-theme-dividingLine::before {display:block;}
[imt-trans-position=before] .immersive-translate-target-translation-block-wrapper {display:block!important;}
</style></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2512.01797?_immersive_translate_auto_translate=1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main &gt;.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2512.01797v2/#myForm" onclick="event.preventDefault(); var modal = document.getElementById(&#39;myForm&#39;); modal.style.display = &#39;block&#39;; bugReportState.setInitiateWay(&#39;Header&#39;);">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2512.01797v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2512.01797v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2512.01797?_immersive_translate_auto_translate=1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S1" title="In H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S2" title="In H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Identification of H-Neurons</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S3" title="In H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Behaviour Impact of H-Neurons</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S4" title="In H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Origin of H-Neurons</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S5" title="In H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6" title="In H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS1" title="In 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Identifying H-Neurons</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS1.SSS1" title="In 6.1 Identifying H-Neurons ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Training Data Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS1.SSS2" title="In 6.1 Identifying H-Neurons ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Quantifying Neuron Contribution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS1.SSS3" title="In 6.1 Identifying H-Neurons ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>Identifying H-Neurons via Linear Classifier</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS2" title="In 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Perturbation Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS2.SSS1" title="In 6.2 Perturbation Experiments ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Activation Scaling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS2.SSS2" title="In 6.2 Perturbation Experiments ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Benchmark Setups</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS3" title="In 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Tracing the Origin of H-Neurons</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS3.SSS1" title="In 6.3 Tracing the Origin of H-Neurons ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>Backward Transferability Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS3.SSS2" title="In 6.3 Tracing the Origin of H-Neurons ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Neuron-Level Parameter Evolution</span></a></li>
</ol>
</li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content"><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">许可证：CC BY 4.0</font></font></font></a><div id="watermark-tr" data-imt_insert_failed="1">arXiv:2512.01797v2 [cs.AI] 02 Dec 2025</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源</font></font></font></h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Cheng Gao,
Huimin Chen, Chaojun Xiao, Zhiyi Chen,
Zhiyuan Liu, Maosong Sun
<br class="ltx_break">Tsinghua University
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">{gaoc24}@mails.tsinghua.edu.cn</span>, <span class="ltx_text ltx_font_typewriter">{huimchen,xcj,liuzy}@tsinghua.edu.cn</span>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">摘要</font></font></font></h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p">Large language models (LLMs) frequently generate hallucinations – plausible but factually incorrect outputs – undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than <math alttext="0.1\%" class="ltx_Math" display="inline" id="m1" intent=":literal"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">0.1\%</annotation></semantics></math> of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">大型语言模型（LLMs）经常产生幻觉——看似合理但事实错误的结果——从而削弱了它们的可靠性。尽管先前研究已从训练数据和目标等宏观视角考察了幻觉，但底层的神经元级机制仍基本未被探索。在本文中，我们从三个视角系统地研究了 LLMs 中的幻觉相关神经元（H-Neurons）：识别、行为影响和起源。关于它们的识别，我们证明了一个极少数的稀疏神经元子集（少于总神经元 <math intent=":literal" id="m1" display="inline" class="ltx_Math" alttext="0.1\%"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">0.1\%</annotation></semantics></math> ）可以可靠地预测幻觉的发生，并在各种不同场景中具有强大的泛化能力。在行为影响方面，受控干预显示这些神经元与过度合规行为存在因果关系。关于它们的起源，我们将这些神经元追溯到预训练的基模型，并发现这些神经元在幻觉检测中仍然具有预测能力，表明它们在预训练过程中出现。 我们的研究将宏观行为模式与微观神经机制联系起来，为开发更可靠的 LLMs 提供见解。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">1 引言</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">In recent years, large language models (LLMs) have achieved groundbreaking advancements in natural language processing tasks, demonstrating impressive potential towards artificial general intelligence&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">foundation-model</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">gpt3</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">instruct-gpt</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">gpt4</span>)</cite>. However, these advancements come with a persistent reliability challenge that troubles researchers and users alike: hallucinations. Hallucinations occur when models produce outputs that seem plausible but are factually inaccurate or unsupported by evidence&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">OnFaithfulnessandFactuality</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">hallusurvey</span>)</cite>. For example, GPT-3.5 has been shown to hallucinate in approximately 40% of citation-based factuality evaluations, a figure that improves but remains high at 28.6% for GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Hallucination_Rates</span>)</cite>. Similarly, emerging reasoning-centric systems such as DeepSeek-R1, despite demonstrating strong performance on complex tasks, continue to exhibit pronounced hallucination modes <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">vectara_deepseek_r1_hallucination</span>)</cite>. Collectively, these observations indicate that hallucinations persist regardless of model architecture, highlighting a critical bottleneck in the reliability of state-of-the-art LLMs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">近年来，大型语言模型（LLMs）在自然语言处理任务中取得了突破性进展，展现出向人工通用智能（基础模型；gpt3；instruct-gpt；gpt4）发展的巨大潜力。然而，这些进展伴随着一个持续存在的可靠性挑战，困扰着研究人员和用户：幻觉。幻觉是指模型产生看似合理但实际上不准确或缺乏证据支持（OnFaithfulnessandFactuality；hallusurvey）的输出。例如，GPT-3.5 在基于引用的事实性评估中约 40%的情况下出现幻觉，这一数字有所改善但仍然很高，在 GPT-4 中为 28.6%（Hallucination_Rates）。类似地，尽管以推理为中心的新兴系统如 DeepSeek-R1 在复杂任务上表现出色，但它们仍然表现出明显的幻觉模式（vectara_deepseek_r1_hallucination）。综合来看，这些观察表明幻觉与模型架构无关，突显了当前最先进 LLMs 可靠性方面的关键瓶颈。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">To improve LLM reliability, researchers have invested considerable effort in uncovering the mechanisms and factors behind hallucinations, which can be broadly grouped into three categories. First, from a training data perspective, distribution imbalances and inherent biases within datasets make it difficult for models to accurately recall long-tail facts&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:conf/naacl/SunXZLD24</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:conf/acl/LiLSDSLJJL22</span>)</cite>.
Second, training objectives in both pretraining and post-training phases primarily incentivize confident predictions without promoting the expression of uncertainty for unfamiliar information, encouraging models to output incorrect guesses&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:journals/corr/abs-2509-04664</span>)</cite>. Specifically, the next-token prediction goal in pretraining prioritizes fluent continuations over factual accuracy, while instruction tuning or reinforcement learning often favors generating superficially helpful responses, sometimes at the expense of honest refusals to answer.
Third, decoding algorithms introduce instability through randomness and error accumulation in autoregressive generation, allowing small deviations to snowball into hallucinations&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:conf/icml/ZhangPMLS24</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:conf/nips/LeePXPFSC22</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:conf/nips/KapoorGRCPBWDGW24</span>)</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为提高 LLM 的可靠性，研究人员投入了大量精力来揭示产生幻觉的机制和因素，这些因素可以大致分为三类。首先，从训练数据的角度来看，数据集中的分布不平衡和固有偏见使得模型难以准确回忆长尾事实（DBLP:conf/naacl/SunXZLD24；DBLP:conf/acl/LiLSDSLJJL22）。其次，预训练和后训练阶段中的训练目标主要激励模型做出自信的预测，而不会促进对不熟悉信息的表达不确定性，从而鼓励模型输出错误的猜测（DBLP:journals/corr/abs-2509-04664）。具体来说，预训练中的下一词预测目标优先考虑流畅的延续性而非事实准确性，而指令微调或强化学习通常倾向于生成表面上有帮助的响应，有时甚至以不诚实地拒绝回答为代价。 第三，解码算法通过自回归生成中的随机性和误差累积引入不稳定性，使得小的偏差会滚雪球般发展成幻觉（DBLP:conf/icml/ZhangPMLS24；DBLP:conf/nips/LeePXPFSC22；DBLP:conf/nips/KapoorGRCPBWDGW24）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">Current studies largely treat LLMs as black boxes, examining hallucination causes at a macroscopic level while neglecting microscopic insights into neuron-level mechanisms. Yet, such fine-grained analysis holds immense promise for explaining how hallucinations arise and for developing mitigation strategies.
Just as biological research on cellular division informs treatments for diseases such as cancer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">collins1997cell</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">matthews2022cell</span>)</cite>, and neuroscience investigations into individual neuronal activity and synaptic interactions shape theories of cognition like learning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:journals/natmi/LuczakMK22</span>)</cite> and memory&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">mongillo2008synaptic</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">lisman2018memory</span>)</cite>, analyzing neurons – the fundamental computational units of LLMs – is essential for decoding hallucination. By scrutinizing neurons’ activation patterns in relation to hallucinations, we can gain deeper insights into model reliability. In terms of interpretability, neuron-level analysis can enable the prediction of when hallucinations are prone to emerge; for alignment and behavioral control, it provides actionable intervention points, such as activating or suppressing specific subsets of neurons to reliably modify model outputs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">当前研究大多将 LLMs 视为黑箱，在宏观层面考察幻觉成因，而忽视了神经元层面的微观机制洞察。然而，这种细粒度分析对于解释幻觉如何产生以及制定缓解策略具有巨大潜力。正如细胞分裂的生物学研究为癌症等疾病的治疗提供了指导（collins1997cell; matthews2022cell），以及神经科学对单个神经元活动和突触交互的研究塑造了学习（DBLP:journals/natmi/LuczakMK22）和记忆（mongillo2008synaptic; lisman2018memory）等认知理论一样，分析神经元——LLMs 的基本计算单元——对于解码幻觉至关重要。通过审视神经元与幻觉相关的激活模式，我们可以更深入地了解模型可靠性。在可解释性方面，神经元层面的分析能够预测幻觉易发时机；在对齐和行为控制方面，它提供了可操作的干预点，例如通过激活或抑制特定神经元的子集来可靠地修改模型输出。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">In this paper, we adopt a neuron-centric perspective to investigate the microscopic mechanisms of hallucinations in LLMs. Prior research has shown that internal hidden states can serve as effective features for detecting hallucinations&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Internal_States</span>)</cite>, and others using sparse autoencoders have provided case studies connecting hallucinations to specific neuron activations&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">anthropicSAE</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">SAEhallu</span>)</cite>, hinting at a deeper link between neuronal behavior and hallucination generation.
Building on this foundation, we identify a set of hallucination-associated neurons and term them as <span class="ltx_text ltx_font_bold">H-Neurons</span>. We then systematically explore the existence, behavioral impacts, and origins of H-Neurons. We address the following three research questions:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在这篇论文中，我们采用以神经元为中心的视角来研究 LLMs 中幻觉的微观机制。先前研究表明，内部隐藏状态可以作为检测幻觉的有效特征（Internal_States），而使用稀疏自编码器的研究者提供了案例研究，将幻觉与特定神经元激活联系起来（anthropicSAE；SAEhallu），暗示了神经元行为与幻觉生成之间存在更深层次的联系。基于这一基础，我们识别出一组与幻觉相关的神经元，并将它们称为 H-Neurons。然后，我们系统地探索 H-Neurons 的存在、行为影响及其起源。我们提出以下三个研究问题：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Q1: Do H-Neurons exist?</span> Can we identify specific neurons whose activations reliably distinguish between hallucinatory and faithful outputs?</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• Q1：H-Neurons 是否存在？我们能否识别出那些激活能够可靠地区分幻觉输出和忠实输出的特定神经元？</font></font></font>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Q2: How do these neurons influence model behavior?</span> Specifically, what types of tasks exhibit a significant change on performance when these neurons’ activations are altered, thereby establishing a link between hallucination and another capability?</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• Q2：这些神经元如何影响模型行为？具体来说，当这些神经元的激活发生变化时，哪些类型的任务在性能上表现出显著变化，从而建立幻觉与另一种能力之间的联系？</font></font></font>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Q3: When do these neurons originate?</span> Are they introduced during the post-training alignment phase or already present in the pre-trained phase?</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• Q3: 这些神经元何时产生？它们是在后训练对齐阶段引入的，还是已经存在于预训练阶段？</font></font></font>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p">Specifically, drawing from setups in previous work&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Finding_Safety_Neurons</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">Finding_Skill_Neurons</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">Detecting_hallu</span>)</cite>, we focus on neurons in the feedforward networks and examine hallucinations in knowledge-based question answering and make the following observations.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">具体来说，借鉴先前工作中的设置（Finding_Safety_Neurons；Finding_Skill_Neurons；Detecting_hallu），我们关注前馈网络中的神经元，并检查基于知识的问答中的幻觉，得出以下观察结果。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Existence of H-Neurons</span> Our investigation reveals that a remarkably sparse subset of neurons – comprising less than <math alttext="0.1\%" class="ltx_Math" display="inline" id="S1.p6.m1" intent=":literal"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">0.1\%</annotation></semantics></math> of the model’s total neurons – can accurately predict whether the model will produce hallucinated responses. We refer to these predictive neurons as <span class="ltx_text ltx_font_italic">H-Neurons</span>.
To identify these neurons, we develop a systematic methodology that contrasts activation patterns between faithful and hallucinated responses, then apply sparse logistic regression to uncover the most predictive neurons. Notably, the neurons identified through simple QA tasks demonstrate strong generalization capability: they maintain robust predictive accuracy across out-of-distribution scenarios, ranging from specialized cross-domain contexts to pure fabrications concerning non-existent entities, achieving reliable hallucination detection.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">H-神经元的发现我们的研究揭示，模型中一个显著稀疏的神经元子集——仅占模型总神经元数量的不到 <math intent=":literal" id="S1.p6.m1" display="inline" class="ltx_Math" alttext="0.1\%"><semantics><mrow><mn>0.1</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">0.1\%</annotation></semantics></math> ——能够准确预测模型是否会产生幻觉响应。我们将这些预测性神经元称为 H-神经元。为了识别这些神经元，我们开发了一种系统方法，通过对比忠实响应和幻觉响应的激活模式，然后应用稀疏逻辑回归来揭示最具有预测性的神经元。值得注意的是，通过简单问答任务识别出的神经元表现出强大的泛化能力：它们在分布外场景中保持稳健的预测准确性，涵盖从专业跨域上下文到关于不存在实体的纯粹虚构，实现了可靠的幻觉检测。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Impact on Model Behavior</span> Our analysis uncovers that H-Neurons are linked to <span class="ltx_text ltx_font_italic">over-compliance</span> behaviors in LLMs. To establish this causal relationship, we conduct controlled interventions by systematically scaling the activation magnitudes of these neurons. The interventions reveal a distinctive behavioral pattern: amplifying H-Neurons’ activations systematically increases a spectrum of over-compliance behaviors – ranging from overcommitment to incorrect premises and heightened susceptibility to misleading contexts, to increased adherence to harmful instructions and stronger sycophantic tendencies. These findings suggest that H-Neurons do not simply encode factual errors, but rather represent a general tendency to prioritize conversational compliance over factual integrity.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">对模型行为的影响我们的分析揭示，H-Neurons 与 LLMs 中的过度合规行为相关。为了建立这种因果关系，我们通过系统地调整这些神经元的激活幅度进行受控干预。干预结果显示出一种独特的行为模式：系统地增强 H-Neurons 的激活会系统地增加一系列过度合规行为——从过度承诺到错误的前提，以及更容易受到误导性语境的影响，再到更严格地遵守有害指令和更强的奉承倾向。这些发现表明，H-Neurons 不仅仅是编码事实性错误，而是代表了一种优先考虑对话合规性而非事实完整性的普遍倾向。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Origin of H-Neurons</span> Our investigation reveals that H-Neurons originate during the pre-training phase, providing empirical evidence for the insights proposed by OpenAI researchers from the perspective of learning theory&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:journals/corr/abs-2509-04664</span>)</cite>. To trace their developmental timeline, we conduct cross-model transfer experiments: we apply the hallucination neurons identified in instruction-tuned models to their corresponding base models and evaluate their predictive efficacy. The results demonstrate that these neurons retain their predictive ability in base models, successfully detecting hallucinations even prior to fine-tuning.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">H-神经元的起源我们的研究揭示，H-神经元在预训练阶段产生，从学习理论的角度为 OpenAI 研究人员提出的见解提供了实证证据（DBLP:journals/corr/abs-2509-04664）。为了追踪它们的发展时间线，我们进行了跨模型迁移实验：我们将指令微调模型中识别出的幻觉神经元应用于相应的基模型，并评估它们的预测效能。结果表明，这些神经元在基模型中仍保持其预测能力，甚至在微调之前就能成功检测到幻觉。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p">In summary, this paper provides a systematic neuron-level investigation into the microscopic mechanisms of hallucinations in LLMs. By bridging the gap between macroscopic behavioral patterns and fine-grained neural activations, we hope our work can deepen the understanding of how hallucinations arise at the computational level, and offer actionable insights for developing more reliable LLMs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">总之，本文对 LLMs 中幻觉的微观机制进行了系统性的神经元层面研究。通过连接宏观行为模式与细粒度神经激活之间的差距，我们希望这项工作能够加深对幻觉在计算层面产生方式的理解，并为开发更可靠的 LLMs 提供可行的见解。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Identification of H-Neurons<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">2H-神经元的识别</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">While prior work has demonstrated that internal hidden states can detect hallucinations&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Internal_States</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">anthropicSAE</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">SAEhallu</span>)</cite>, a systematic investigation into hallucination-associated neurons remains absent. In this section, we address our first research question: <span class="ltx_text ltx_font_italic">Do H-Neurons exist?</span> We hypothesize that among the millions of neurons in modern LLMs, a sparse subset exhibits activation patterns that systematically distinguish between hallucinatory and faithful outputs. The sparse subset of neurons could serve as both interpretable indicators for detection and precise intervention points for further research.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">尽管已有研究证明内部隐藏状态可以检测幻觉（Internal_States；anthropicSAE；SAEhallu），但针对幻觉相关神经元的系统性研究仍然缺失。在本节中，我们探讨第一个研究问题：H-神经元是否存在？我们假设在现代 LLMs 中数百万个神经元中，一个稀疏的子集表现出能够系统地区分幻觉输出和忠实输出的激活模式。这个稀疏的子集既可作为检测的可解释指标，也可作为进一步研究的精确干预点。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="570" id="S2.F1.g1" src="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_text ltx_font_bold">Framework for identifying H-Neurons.</span>
(a) Within the Feed-Forward Network, we calculate the contribution of each neuron in one forward pass. This metric normalizes the magnitude of an individual neuron’s projected output <math alttext="|n(x)|" class="ltx_Math" display="inline" id="S2.F1.m3" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mrow><mi>n</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|n(x)|</annotation></semantics></math> against the layer’s total output vector <math alttext="|y|" class="ltx_Math" display="inline" id="S2.F1.m4" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi>y</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|y|</annotation></semantics></math>, providing a standardized measure of its contribution to the hidden state.
(b) The process begins by generating a balanced dataset of faithful (green check) and hallucinatory (red cross) responses using the TriviaQA benchmark. We extract the contribution profiles of neurons specifically on the <span class="ltx_text ltx_font_italic">answer tokens</span> to train a linear classifier. Neurons assigned <span class="ltx_text ltx_font_bold">positive weights</span> by this classifier are identified as "H-Neurons", distinguishing them from normal neurons based on their predictive role in generating hallucinations.
</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 1：识别 H-Neurons 的框架。(a) 在前馈网络中，我们计算每个神经元在一次前向传播中的贡献。该指标将单个神经元的投影输出 <math intent=":literal" id="S2.F1.m3" display="inline" class="ltx_Math" alttext="|n(x)|"><semantics><mrow><mo stretchy="false">|</mo><mrow><mi>n</mi><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|n(x)|</annotation></semantics></math> 的幅度归一化到层的总输出向量 <math intent=":literal" id="S2.F1.m4" display="inline" class="ltx_Math" alttext="|y|"><semantics><mrow><mo stretchy="false">|</mo><mi>y</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|y|</annotation></semantics></math> ，从而提供一个标准化的衡量其贡献到隐藏状态的指标。(b) 该过程始于使用 TriviaQA 基准生成一个包含忠实（绿色对勾）和幻觉（红色叉号）响应的平衡数据集。我们提取特定在答案标记上的神经元的贡献特征来训练一个线性分类器。该分类器分配正权重的神经元被识别为"H-Neurons"，根据它们在生成幻觉中的预测作用将它们与普通神经元区分开来。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p">To identify H-Neurons from the vast parameter space of LLMs, we employ a sparse linear probing approach (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S2.F1" title="Figure 1 ‣ 2 Identification of H-Neurons ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>). We first quantify each neuron’s contribution to the responses using the CETT metric&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">relu2wins</span>)</cite>, which is used to measure the neuron’s activation level during generation.
We then frame hallucination detection as a binary classification problem: predicting whether a response is hallucinatory based on neuron activations. Using logistic regression with L1 regularization, we train a sparse classifier that automatically selects the most predictive neurons by driving most weights to zero.
The neurons with non-zero weights are identified as H-Neurons. Training data is collected from TriviaQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Triviaqa</span>)</cite> by sampling multiple responses per question and labeling them based on factual correctness. To present the effectiveness of H-Neurons, we establish a baseline by training linear classifiers using randomly selected neurons. To ensure the fairness of the comparison, the number of randomly selected neurons is the same as that of H-Neurons.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了从 LLMs 庞大的参数空间中识别 H-Neurons，我们采用稀疏线性探测方法（图 1）。我们首先使用 CETT 指标（relu2wins）量化每个神经元对响应的贡献，该指标用于测量生成过程中神经元的激活程度。然后我们将幻觉检测构建为一个二元分类问题：根据神经元激活情况预测响应是否为幻觉。使用 L1 正则化的逻辑回归，我们训练了一个稀疏分类器，该分类器通过将大部分权重驱动为零来自动选择最具有预测性的神经元。具有非零权重的神经元被识别为 H-Neurons。训练数据从 TriviaQA（Triviaqa）中收集，通过每道题采样多个响应并根据事实正确性进行标记。为了展示 H-Neurons 的有效性，我们通过使用随机选择的神经元训练线性分类器来建立基线。为确保比较的公平性，随机选择的神经元数量与 H-Neurons 的数量相同。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Hallucination detection accuracy (<math alttext="\%" class="ltx_Math" display="inline" id="S2.T1.m2" intent=":literal"><semantics><mo>%</mo><annotation encoding="application/x-tex">\%</annotation></semantics></math>) of neuron-based classifiers. We evaluate the performance of neuron-based classifiers on six widely used LLMs. Here, "Random" and "Hallucination" refer to classifiers trained with randomly selected neurons and H-Neurons. Ratio refers to the proportion of total neurons that are selected for classifiers. Classifiers with H-Neurons can effectively detect hallucination for in-domain questions&nbsp;(TriviaQA and NQ), cross-domain questions&nbsp;(BioASQ), and fabricated questions, demonstrating robustness of H-Neurons. H-Neurons usually account for less than 1‰ of all the neurons in LLMs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 1：基于神经元的分类器的幻觉检测准确率（ <math intent=":literal" id="S2.T1.m2" display="inline" class="ltx_Math" alttext="\%"><semantics><mo>%</mo><annotation encoding="application/x-tex">\%</annotation></semantics></math> ）。我们评估了基于神经元的分类器在六个广泛使用的 LLMs 上的性能。这里，“随机”和“幻觉”指的是使用随机选择的神经元和 H-Neurons 训练的分类器。比率是指用于分类器的总神经元比例。具有 H-Neurons 的分类器可以有效地检测领域内问题（TriviaQA 和 NQ）、跨领域问题（BioASQ）和虚构问题，展示了 H-Neurons 的鲁棒性。H-Neurons 通常占 LLMs 中所有神经元的比例不到 1‰。</font></font></font></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span class="ltx_text" style="font-size:90%;">Models<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">模型</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;">Neurons<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">神经元</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span class="ltx_text" style="font-size:90%;">Ratio&nbsp;(‰)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">比率（‰）</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;" data-imt_insert_failed="1">TriviaQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;" data-imt_insert_failed="1">NQ-Open</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;" data-imt_insert_failed="1">BioASQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text" style="font-size:90%;" data-imt_insert_failed="1">NonExist</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;" data-imt_insert_failed="1">Mistral-7B-v0.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Random<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">随机</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">0.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">61.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">56.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">59.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">80.9</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Hallucination<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">幻觉</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">0.35</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">78.4</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">71.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">75.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">91.1</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;" data-imt_insert_failed="1">Mistral-Small-3.1-24B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Random<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">随机</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">0.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">61.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">56.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">52.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">57.4</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Hallucination<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">幻觉</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">0.01</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">81.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">71.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">69.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">86.6</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;" data-imt_insert_failed="1">Gemma-3-4B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Random<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">随机</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">0.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">62.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">59.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">56.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">56.9</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Hallucination<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">幻觉</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">0.10</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">76.9</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">70.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">71.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">71.9</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;" data-imt_insert_failed="1">Gemma-3-27B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Random<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">随机</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">0.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">65.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">58.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">61.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">58.2</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Hallucination<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">幻觉</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">0.18</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">83.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">68.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">72.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">95.9</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;" data-imt_insert_failed="1">Llama-3.1-8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Random<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">随机</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">0.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">56.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">53.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">52.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">50.6</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">Hallucination<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">幻觉</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:90%;">0.02</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">70.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">63.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">66.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:90%;">43.1</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text" style="font-size:90%;" data-imt_insert_failed="1">Llama-3.3-70B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">Random<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">随机</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">0.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">68.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">58.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">66.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:90%;">69.6</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">Hallucination<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">幻觉</font></font></font></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span class="ltx_text" style="font-size:90%;">0.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">82.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">67.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">74.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:90%;">96.7</span></td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p">To assess whether the identified neurons generalize beyond the training set and reflect broader patterns of hallucination, we evaluate the trained linear model for hallucination detection on diverse question collections.
We design a comprehensive evaluation protocol covering three distinct hallucination scenarios:
(1)&nbsp;<span class="ltx_text ltx_font_italic">In-Domain Knowledge Recall</span>: We evaluate on TriviaQA and NQ&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">nq</span>)</cite>, both constructed from Wikipedia, a corpus extensively used in LLM pretraining. These datasets test whether hallucination neurons can detect failures in recalling familiar but unmemorized knowledge.
(2)&nbsp;<span class="ltx_text ltx_font_italic">Cross-Domain Robustness</span>: We evaluate on BioASQ&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">bioasq</span>)</cite>, a biomedical question-answering dataset. Since our classifier is trained exclusively on TriviaQA with general knowledge, BioASQ tests cross-domain generalization to specialized domains with distinct terminology and factual structures.
(3)&nbsp;<span class="ltx_text ltx_font_italic">Fabricated Knowledge Detection</span>: We construct a dataset, referred to as NonExist, containing artificially generated questions about non-existent entities&nbsp;(e.g., "Who manufactures the medicine volor pri octacap?" where "volor pri octacap" is fabricated)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">hallulens</span>)</cite>. When models provide confident answers to such questions, it constitutes a clear hallucination. This scenario tests whether hallucination neurons can detect fabrication, generating plausible-sounding answers about facts absent from any training data.
Together, these settings provide comprehensive coverage: from recall failures on seen knowledge, to domain transfer, to complete fabrication, enabling assessment of the generality and robustness of H-Neurons.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了评估所识别的神经元是否能够泛化到训练集之外，并反映更广泛的幻觉模式，我们评估了用于幻觉检测的训练线性模型在多种问题集合上的表现。我们设计了一个全面的评估协议，涵盖三种不同的幻觉场景：(1) 域内知识召回：我们在 TriviaQA 和 NQ（nq）上评估，这两个数据集均由维基百科构建，维基百科是用于 LLM 预训练的语料库。这些数据集测试幻觉神经元是否能够检测到对熟悉但未记忆知识的召回失败。(2) 跨域鲁棒性：我们在 BioASQ（bioasq）上评估，这是一个生物医学问答数据集。由于我们的分类器仅在 TriviaQA 上用通用知识进行训练，BioASQ 测试了跨域泛化到具有不同术语和事实结构的专门领域的能力。(3) 伪造知识检测：我们构建了一个数据集，称为 NonExist，其中包含关于不存在实体的人工生成问题（例如，“谁制造了药物 volor pri octacap？”其中“volor pri octacap”是伪造的）（hallulens）。 当模型对这类问题给出自信的答案时，这就构成了明显的幻觉。这种情况测试了幻觉神经元能否检测到虚构，从而生成听起来合理但实际上不存在于任何训练数据中的答案。这些设置共同提供了全面的覆盖范围：从对已见知识的回忆失败，到领域迁移，再到完全的虚构，从而能够评估 H-神经元的普遍性和鲁棒性。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p">Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S2.T1" title="Table 1 ‣ 2 Identification of H-Neurons ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a> presents the hallucination detection performance of neuron-based classifiers across six widely-used LLMs. The results demonstrate that H-Neurons exhibit remarkable robustness in detecting hallucinations. First, classifiers built on H-Neurons consistently and substantially outperform those using randomly selected neurons across all models and evaluation settings, with accuracy improvements often exceeding <math alttext="10" class="ltx_Math" display="inline" id="S2.p4.m1" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math> percentage points. Second, these classifiers demonstrate remarkable robustness across diverse scenarios: they achieve high accuracy on in-domain datasets (TriviaQA and NQ), exhibit strong generalization on cross-domain biomedical questions (BioASQ), and retain effectiveness on fabricated questions (NonExist). The consistent performance across familiar knowledge recall, domain transfer, and complete fabrication scenarios indicates that H-Neurons capture generalizable patterns of hallucinations rather than dataset-specific artifacts.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 1 展示了基于神经元的分类器在六个广泛使用的 LLMs 上的幻觉检测性能。结果表明，H-Neurons 在检测幻觉方面表现出显著的鲁棒性。首先，基于 H-Neurons 构建的分类器在所有模型和评估设置中始终且大幅优于使用随机选择神经元的分类器，准确率提升通常超过 <math intent=":literal" id="S2.p4.m1" display="inline" class="ltx_Math" alttext="10"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math> 个百分点。其次，这些分类器在不同场景中表现出显著的鲁棒性：它们在领域内数据集（TriviaQA 和 NQ）上实现高准确率，在跨领域生物医学问题（BioASQ）上表现出强泛化能力，并在虚构问题（NonExist）上保持有效性。在熟悉知识回忆、领域迁移和完全虚构场景中的持续性能表明，H-Neurons 捕捉到了幻觉的可泛化模式，而非特定数据集的伪影。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p">Remarkably, H-Neurons constitute an extremely sparse subset of the model’s total neurons. These neurons typically account for less than 1‰ of all neurons in the models – ranging from 0.01‰ in large models like Mistral-Small-3.1-24B and Llama-3.1-70B to 0.35‰ in Mistral-7B-v0.3. Despite their scarcity, this small set of neurons provides sufficient signal to reliably detect hallucination, demonstrating that a compact subset of model parameters carries substantial information about hallucination tendencies.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">值得注意的是，H-神经元构成了模型总神经元中一个极其稀疏的子集。这些神经元通常占模型所有神经元的比例不到 1‰——在 Mistral-Small-3.1-24B 和 Llama-3.1-70B 等大型模型中占比为 0.01‰，而在 Mistral-7B-v0.3 中占比为 0.35‰。尽管它们数量稀少，但这小部分神经元提供了足够的信号来可靠地检测幻觉，证明模型的紧凑参数子集承载着大量关于幻觉倾向的信息。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Behaviour Impact of H-Neurons<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">3 H-神经元的行为影响</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="840" id="S3.F2.g1" src="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/x2.png" width="789">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" style="font-size:90%;">
Illustrations for the behavioral impact of intervening on the H-Neurons.
The right panel presents examples across four dimensions:
<span class="ltx_text ltx_font_bold">Invalid Premises</span> (hallucinating details about non-existent "cat feathers"),
<span class="ltx_text ltx_font_bold">Compliance with Misleading Context</span> (adopting counterfactual claims about Marie Curie),
<span class="ltx_text ltx_font_bold">Skeptical Attitudes</span> (abandoning a correct answer when challenged),
and <span class="ltx_text ltx_font_bold">Harmful Instructions</span> (bypassing safety filters to assist with weapon creation).
</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 2：干预 H-神经元的行为影响示意图。右图展示了四个维度的示例：无效前提（关于不存在“猫毛”的幻觉细节）、误导性上下文下的遵从（采纳关于玛丽·居里的反事实陈述）、怀疑态度（在质疑时放弃正确答案）和有害指令（绕过安全过滤器协助武器制造）。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">Having established the existence of H-Neurons and their predictive ability in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S2" title="2 Identification of H-Neurons ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>, a natural question arises: <span class="ltx_text ltx_font_italic">What functional role do these neurons play in shaping model behavior?</span> While predictive accuracy demonstrates correlation, establishing causation requires moving from observation to intervention.
In this section, we conduct controlled perturbation experiments to determine whether artificially modulating these neurons leads to systematic and interpretable changes in model outputs, and whether such changes reveal a broader behavioral pattern that extends beyond factual errors.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在第二部分中，我们已证实 H-Neurons 的存在及其预测能力，一个自然的问题随之产生：这些神经元在塑造模型行为中扮演着怎样的功能角色？虽然预测准确性显示了相关性，但要建立因果关系则需要从观察转向干预。在本部分，我们进行受控扰动实验，以确定人为调节这些神经元是否会导致模型输出的系统性且可解释的变化，以及这些变化是否揭示了一种超越事实性错误的更广泛的行为模式。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p">To probe the causal impact of H-Neurons, we design a systematic perturbation methodology that modulates their contributions during inference without retraining the model. Following the identification procedure, we focus on neurons with positive weights in the hallucination detection classifier, as their activation exhibits a positive correlation with hallucinatory responses.
Our intervention operates by scaling the activation values of these neurons during forward passes: for each target neuron, we multiply its activation by a scaling factor <math alttext="\alpha\in[0,3]" class="ltx_Math" display="inline" id="S3.p2.m1" intent=":literal"><semantics><mrow><mi>α</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>3</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha\in[0,3]</annotation></semantics></math>, where <math alttext="\alpha&lt;1" class="ltx_Math" display="inline" id="S3.p2.m2" intent=":literal"><semantics><mrow><mi>α</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha&lt;1</annotation></semantics></math> suppresses the neuron’s influence by reducing its activation strength, <math alttext="\alpha=1" class="ltx_Math" display="inline" id="S3.p2.m3" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha=1</annotation></semantics></math> preserves the original behavior, and <math alttext="\alpha&gt;1" class="ltx_Math" display="inline" id="S3.p2.m4" intent=":literal"><semantics><mrow><mi>α</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha&gt;1</annotation></semantics></math> amplifies its contribution to responses by increasing activation magnitude.
This approach enables a direct assessment of whether modulating the influence of H-Neurons induces systematic behavioral changes, and whether such changes align with the semantic or safety risks associated with hallucination.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了探究 H-Neurons 的因果影响，我们设计了一种系统扰动方法，在不重新训练模型的情况下调节它们在推理过程中的贡献。在识别程序之后，我们关注幻觉检测分类器中具有正权重的神经元，因为它们的激活与幻觉响应呈正相关。我们的干预通过在前向传递过程中调整这些神经元的激活值来操作：对于每个目标神经元，我们将其激活值乘以一个缩放因子 <math intent=":literal" id="S3.p2.m1" display="inline" class="ltx_Math" alttext="\alpha\in[0,3]"><semantics><mrow><mi>α</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>3</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha\in[0,3]</annotation></semantics></math> ，其中 <math intent=":literal" id="S3.p2.m2" display="inline" class="ltx_Math" alttext="\alpha&lt;1"><semantics><mrow><mi>α</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha&lt;1</annotation></semantics></math> 通过降低激活强度来抑制神经元的 影响， <math intent=":literal" id="S3.p2.m3" display="inline" class="ltx_Math" alttext="\alpha=1"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha=1</annotation></semantics></math> 保持原始行为，而 <math intent=":literal" id="S3.p2.m4" display="inline" class="ltx_Math" alttext="\alpha&gt;1"><semantics><mrow><mi>α</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha&gt;1</annotation></semantics></math> 通过增加激活幅度来放大其对响应的贡献。这种方法能够直接评估调节 H-Neurons 的影响是否会导致系统性行为变化，以及这些变化是否与幻觉相关的语义或安全风险相一致。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="417" id="S3.F3.g1" src="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/x3.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" style="font-size:90%;">Compliance rate (%) of perturbed LLMs. Performance changes when suppressing (scaling factor <math alttext="&lt;1" class="ltx_Math" display="inline" id="S3.F3.m3" intent=":literal"><semantics><mrow><mi></mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">&lt;1</annotation></semantics></math>) or amplifying (scaling factor <math alttext="&gt;1" class="ltx_Math" display="inline" id="S3.F3.m4" intent=":literal"><semantics><mrow><mi></mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">&gt;1</annotation></semantics></math>) H-Neurons on benchmarks measuring compliance with: (a) invalid premises, (b) misleading context, (c) skeptical attitudes and (d) harmful instructions. Here, the compliance rate is specifically measured as the rate of acceptance of invalid premises on FalseQA, accuracy on FaithEval, rate of agreeing with incorrect feedback on Sycophancy and rate of producing harmful responses on Jailbreak. Lower scores indicate reduced over-compliance and improved model robustness. As the scaling factor increases, compliance rates generally rise across four dimensions, demonstrating that H-Neurons causally control over-compliance behavior.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 3：扰动 LLMs 的合规率（%）。在以下合规性基准测试中，当抑制（缩放因子 <math intent=":literal" id="S3.F3.m3" display="inline" class="ltx_Math" alttext="&lt;1"><semantics><mrow><mi></mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">&lt;1</annotation></semantics></math> ）或放大（缩放因子 <math intent=":literal" id="S3.F3.m4" display="inline" class="ltx_Math" alttext="&gt;1"><semantics><mrow><mi></mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">&gt;1</annotation></semantics></math> ）H-Neurons 时，性能会发生变化：(a) 无效前提，(b) 指导性误导，(c) 怀疑态度和(d) 有害指令。此处，合规率特别测量为 FalseQA 上无效前提的接受率、FaithEval 上的准确率、Sycophancy 上同意错误反馈的比率以及 Jailbreak 上产生有害响应的比率。分数越低表示减少过度合规并提高模型鲁棒性。随着缩放因子的增加，四个维度上的合规率普遍上升，表明 H-Neurons 因果控制过度合规行为。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p">A prevailing hypothesis in the literature attributes hallucinations to models’ tendency to venture risky guesses in pursuit of higher accuracy&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:conf/stoc/KalaiV24</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:conf/nips/CohenKRF24</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:journals/corr/abs-2509-04664</span>)</cite>. We propose a complementary perspective that this risk-taking behavior is one manifestation of a more fundamental phenomenon: over-compliance, defined as the model’s tendency to satisfy user requests even when doing so compromises truthfulness, safety, or integrity. For example, when a model generates hallucinated content to answer an unanswerable question, it is prioritizing the implicit human expectation of receiving an answer over the admission of uncertainty or knowledge boundaries-analogous to how humans may lie due to social desirability&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wholies</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">lalwani2006relation</span>)</cite>.
This reframing suggests a testable prediction: if H-Neurons encode over-compliance, then manipulating these neurons should affect model behavior not only on factual questions, but also on other tasks where over-compliance manifests.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">文献中一个普遍的假设将幻觉归因于模型追求更高准确率而倾向于冒险猜测的倾向（DBLP:conf/stoc/KalaiV24；DBLP:conf/nips/CohenKRF24；DBLP:journals/corr/abs-2509-04664）。我们提出一个补充视角，认为这种冒险行为是更根本现象的一种表现：过度遵从，定义为模型在牺牲真实性、安全性或完整性时仍倾向于满足用户请求的倾向。例如，当模型为回答一个无法回答的问题而生成幻觉内容时，它优先考虑了人类隐含的期望——即获得答案，而不是承认不确定性或知识边界——类似于人类可能因社会期望而撒谎（wholies；lalwani2006relation）。这种重新定义提出了一个可检验的预测：如果 H-神经元编码了过度遵从，那么操纵这些神经元应该不仅会影响模型在事实性问题上的行为，也会影响在其他任务中表现出过度遵从的行为。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p">To test this hypothesis systematically, we evaluate the modified model across four carefully selected benchmarks, each probing a different facet of over-compliance (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S3.F2" title="Figure 2 ‣ 3 Behaviour Impact of H-Neurons ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>):
(1)&nbsp;FalseQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">FalseQA</span>)</cite> assesses <span class="ltx_text ltx_font_italic">compliance with invalid premises</span>, probing whether models attempt to answer questions built on factually incorrect assumptions rather than rejecting the flawed premise.
(2)&nbsp;FaithEval&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Faitheval</span>)</cite> examines <span class="ltx_text ltx_font_italic">compliance with misleading contexts</span>, evaluating whether models uncritically accept and follow potentially incorrect information provided in prompts rather than questioning or verifying it.
(3)&nbsp;Sycophancy&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Sycophancy</span>)</cite> measures <span class="ltx_text ltx_font_italic">compliance with skeptical attitudes</span>, quantifying the tendency to echo user opinions or revise correct answers when users express disagreement rather than maintaining epistemic integrity.
(4)&nbsp;Jailbreak&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Jailbreak</span>)</cite> tests <span class="ltx_text ltx_font_italic">compliance with harmful instructions</span>, measuring whether models inappropriately satisfy instructions that violate safety guidelines.
Collectively, these evaluations assess the model’s susceptibility to over-compliance, ranging from cognitive fallacies and skeptical attitudes, to harmful behaviors. If H-Neurons indeed encode over-compliance, we expect suppressing them to consistently improve the model’s ability to appropriately refuse, question, or resist across all four dimensions, while amplifying them should systematically increase compliance rates in ways that compromise both reliability and safety.
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为系统性地验证这一假设，我们通过四个精心选择的基准测试来评估改进后的模型，每个基准测试都针对过度合规的不同方面（图 2）：（1）FalseQA（FalseQA）评估对无效前提的合规性，探究模型是否会基于事实性错误的前提来回答问题，而不是拒绝有缺陷的前提。（2）FaithEval（Faitheval）检验对误导性上下文的合规性，评估模型是否会不加批判地接受并遵循提示中可能不正确的信息，而不是质疑或核实它。（3）Sycophancy（Sycophancy）衡量对怀疑态度的合规性，量化模型附和用户意见或在用户表达不同意见时修正正确答案的倾向，而不是保持知识上的正直。（4）Jailbreak（Jailbreak）测试对有害指令的合规性，衡量模型是否不恰当地满足违反安全指南的指令。综合来看，这些评估评估了模型对过度合规的易感性，从认知谬误和怀疑态度，到有害行为。 如果 H-神经元确实编码过度服从，我们预期抑制它们将始终提高模型在四个维度上适当拒绝、质疑或抵抗的能力，而增强它们则应系统地提高服从率，从而损害可靠性和安全性。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S3.F3" title="Figure 3 ‣ 3 Behaviour Impact of H-Neurons ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the relationship between the scaling factor of H-Neurons and the model’s compliance rate. Overall, we observe that: (1)&nbsp;There is a consistent positive correlation between the scaling factor of neurons and model’s compliance rate. This phenomenon is observed across four different evaluation dimensions. This indicates that artificially amplifying the activation values of these H-Neurons significantly compromises the model’s resistance to false premises, misleading contexts, skeptical attitudes or harmful instructions whereas suppressing them effectively reduces over-compliance behaviors, effectively restoring the model’s robustness and integrity.
(2)&nbsp;The susceptibility of models to perturbation on neurons generally exhibits an inverse correlation with parameter size. The three smaller models exhibit a steeper average growth in compliance rates (average slope <math alttext="\approx 3.03" class="ltx_Math" display="inline" id="S3.p5.m1" intent=":literal"><semantics><mrow><mi></mi><mo>≈</mo><mn>3.03</mn></mrow><annotation encoding="application/x-tex">\approx 3.03</annotation></semantics></math>) across the evaluated dimensions, whereas the three larger models maintain a more moderate average growth (average slope <math alttext="\approx 2.40" class="ltx_Math" display="inline" id="S3.p5.m2" intent=":literal"><semantics><mrow><mi></mi><mo>≈</mo><mn>2.40</mn></mrow><annotation encoding="application/x-tex">\approx 2.40</annotation></semantics></math>). This suggests that smaller models are more prone to drastic behavioral shifts under internal perturbation, while larger models likely possess greater intrinsic robustness that mitigates the impact of amplifying specific neuron groups.
(3)&nbsp;The behavioral response is not strictly monotonic for all cases. In tasks such as FalseQA and Jailbreak, certain models exhibit fluctuations or temporary drops in compliance at intermediate scaling factors. This is likely due to complex internal mechanisms: since we linearly amplify the neurons (<math alttext="\alpha\in[0,3]" class="ltx_Math" display="inline" id="S3.p5.m3" intent=":literal"><semantics><mrow><mi>α</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>3</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha\in[0,3]</annotation></semantics></math>), this strong intervention might push the model’s internal features out-of-distribution at certain points, unexpectedly decreasing compliance. A notable instance is observed in the Sycophancy task, where the smallest model, Gemma-3-4B, initially exhibits increased compliance that subsequently declines as the scaling factor increases.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 3 展示了 H-Neurons 的缩放因子与模型合规率之间的关系。总体而言，我们观察到以下几点：(1)神经元的缩放因子与模型的合规率之间存在一致的正相关关系。这种现象在四个不同的评估维度中均有体现。这表明人为放大这些 H-Neurons 的激活值会显著削弱模型对虚假前提、误导性上下文、怀疑态度或有害指令的抵抗能力，而抑制它们则能有效减少过度合规行为，从而有效恢复模型的鲁棒性和完整性。(2)模型对神经元扰动的敏感性通常与参数规模呈负相关。三个较小模型的合规率在评估维度上表现出更陡峭的平均增长（平均斜率 <math intent=":literal" id="S3.p5.m1" display="inline" class="ltx_Math" alttext="\approx 3.03"><semantics><mrow><mi></mi><mo>≈</mo><mn>3.03</mn></mrow><annotation encoding="application/x-tex">\approx 3.03</annotation></semantics></math> ），而三个较大模型则保持更适度的平均增长（平均斜率 <math intent=":literal" id="S3.p5.m2" display="inline" class="ltx_Math" alttext="\approx 2.40"><semantics><mrow><mi></mi><mo>≈</mo><mn>2.40</mn></mrow><annotation encoding="application/x-tex">\approx 2.40</annotation></semantics></math> ）。 这表明较小模型更容易在内部扰动下出现剧烈的行为变化，而较大模型可能拥有更强的内在鲁棒性，能够减轻放大特定神经元组的影响。(3) 对于所有情况，行为响应并不严格单调。在 FalseQA 和 Jailbreak 等任务中，某些模型在中等规模因子下表现出合规性的波动或暂时性下降。这可能是由于复杂的内部机制：由于我们线性放大神经元（ <math intent=":literal" id="S3.p5.m3" display="inline" class="ltx_Math" alttext="\alpha\in[0,3]"><semantics><mrow><mi>α</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>3</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha\in[0,3]</annotation></semantics></math> ），这种强烈的干预可能会在某些点上使模型的内部特征偏离分布，意外地降低合规性。一个值得注意的例子是在 Sycophancy 任务中，最小的模型 Gemma-3-4B 最初表现出合规性增加，但随着规模因子的增加，合规性随后下降。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Origin of H-Neurons<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">4 H-神经元的起源</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">Having established the existence and explored the behavioral impact of H-Neurons, we now investigate their origins: <span class="ltx_text ltx_font_italic">Do these neurons emerge during pre-training, or are they artifacts of post-training alignment?</span> Determining this timeline is crucial, as it dictates whether mitigation efforts should focus on the pre-training process or alignment algorithms.
If H-Neurons already show distinct activation patterns in the base model, this would suggest that hallucination behavior has roots in pre-training representations rather than purely SFT-induced alignment dynamics.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在确认了 H-Neurons 的存在并探讨了其行为影响后，我们现在研究它们的起源：这些神经元是在预训练过程中产生的，还是后训练对齐的产物？确定这一时间线至关重要，因为它决定了缓解措施应侧重于预训练过程还是对齐算法。如果 H-Neurons 在基础模型中已经显示出独特的激活模式，这表明幻觉行为根源于预训练表示，而非纯粹由 SFT 诱导的对齐动态。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">To answer this, we conduct two complementary analyses. First, we examine the backward transferability of H-Neurons. We hypothesize that if these neurons originate during pre-training, the detection probes trained on instruction-tuned models should remain effective on their corresponding base models. We apply the classifiers trained on instruction-tuned models (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S2" title="2 Identification of H-Neurons ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>) directly to the base models. This allows us to evaluate whether the same neuron subset preserves its predictive ability across models. However, since activation magnitudes often shift significantly from pre-training to fine-tuning, using the same fixed classification threshold as in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S2" title="2 Identification of H-Neurons ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a> is unreliable. Instead, we utilize the Area Under the Receiver Operating Characteristic Curve (AUROC) as our primary metric.
Second, we study how instruction tuning changes these neurons to determine whether the alignment process actively constructs or merely preserves the circuits responsible for hallucination. To quantify the modifications induced by SFT, we compute the cosine distance of both its up-projection and down-projection weights between the base and aligned models analyze the rank distribution of H-Neurons within the global parameter space. This comparative ranking allows us to determine whether the alignment process modifies these specific neurons more significantly than the average neuron in the network.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了回答这个问题，我们进行了两项互补的分析。首先，我们考察了 H-Neurons 的逆向迁移能力。我们假设，如果这些神经元在预训练过程中产生，那么在指令微调模型上训练的检测探针应该仍然对其对应的基模型有效。我们将第 2 节中训练的分类器直接应用于基模型。这使我们能够评估相同的神经元子集是否能在不同模型中保持其预测能力。然而，由于激活幅度从预训练到微调时往往发生显著变化，使用第 2 节中相同的固定分类阈值是不可靠的。相反，我们使用接收者操作特征曲线下面积（AUROC）作为主要指标。其次，我们研究指令微调如何改变这些神经元，以确定对齐过程是主动构建还是仅仅保留了导致幻觉的电路。 为了量化 SFT 引起的修改，我们计算了其对齐模型和基础模型之间上投影和下投影权重的余弦距离，分析 H-神经元在全局参数空间中的秩分布。这种比较排名使我们能够确定对齐过程是否比网络中的平均神经元更显著地修改了这些特定神经元。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="454" id="S4.F4.g1" src="./H-Neurons：关于 LLMs 中与幻觉相关的神经元的存在、影响和起源 --- H-Neurons_ On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs_files/x4.png" width="789">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" style="font-size:90%;">(a) AUROC scores of classifiers trained on instruction-tuned models and applied directly to their corresponding base models. All models significantly outperform the random baseline. This robust transferability confirms that the neural signature of hallucination is intrinsic to the pre-training stage.
(b) Distribution of H-Neuron similarity ranks. Each subplot shows the normalized rank positions (0–1 scale) of H-Neurons, with smaller normalized rank values corresponding to larger parameter changes from pre-training to alignment. Black dashed lines indicate the average rank, and colored circles represent H-Neurons. Statistical significance of the higher cosine similarity of H-Neurons compared to other neurons is verified via a one-sided t-test. Across most models, H-Neurons consistently concentrate in the higher-normalized-rank region, suggesting that these neurons are largely inherited from pre-training and are not introduced or heavily modified by SFT.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 4：(a) 在指令微调模型上训练的分类器直接应用于其对应的基础模型时的 AUROC 分数。所有模型均显著优于随机基线。这种稳健的迁移能力证实了幻觉的神经特征是预训练阶段的内在属性。(b) H-神经元相似度排名分布。每个子图展示了 H-神经元的归一化排名位置（0-1 尺度），较小的归一化排名值对应于从预训练到对齐时参数变化较大。黑色虚线表示平均排名，彩色圆圈代表 H-神经元。通过单边 t 检验验证了 H-神经元与其他神经元相比具有更高余弦相似度的统计显著性。在大多数模型中，H-神经元始终集中在高归一化排名区域，表明这些神经元主要继承自预训练，并未被 SFT 引入或大幅修改。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S4.F4" title="Figure 4 ‣ 4 Origin of H-Neurons ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">4</span></a> presents the performance of hallucination detection and parameter evolution. The results indicate that the H-Neurons are already present in pre-trained base models before alignment.
From the results, we can observe that:
(1)&nbsp;H-Neurons present significant predictive ability for base models. Across all six models and three datasets, the AUROC scores consistently surpass the random guessing baseline by a large margin. Notably, the Mistral family achieves accuracy exceeding 86% on TriviaQA. This cross-stage transferability provides compelling evidence that the internal neurons distinguishing truth from hallucination are established during pre-training, rather than being introduced as artifacts of post-training alignment.
(2)&nbsp;The distribution of normalized ranks indicates that H-Neurons undergo minimal parameter updates during the transition from base to instruction-tuned models.
This trend is particularly pronounced in Mistral-Small, where H-Neurons are heavily concentrated in the high-rank regions (<math alttext="avg\approx 0.97" class="ltx_Math" display="inline" id="S4.p3.m1" intent=":literal"><semantics><mrow><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi></mrow><mo>≈</mo><mn>0.97</mn></mrow><annotation encoding="application/x-tex">avg\approx 0.97</annotation></semantics></math>), indicating exceptional parameter stability. Similarly, Gemma and Llama series models exhibit a statistically significant tendency toward stability (<math alttext="avg&gt;0.58" class="ltx_Math" display="inline" id="S4.p3.m2" intent=":literal"><semantics><mrow><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>v</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi></mrow><mo>&gt;</mo><mn>0.58</mn></mrow><annotation encoding="application/x-tex">avg&gt;0.58</annotation></semantics></math>; <math alttext="P&lt;0.001" class="ltx_Math" display="inline" id="S4.p3.m3" intent=":literal"><semantics><mrow><mi>P</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">P&lt;0.001</annotation></semantics></math>).
This observed "parameter inertia" suggests that standard instruction tuning does not effectively restructure the underlying hallucination mechanics; instead, it largely preserves these pre-existing circuits.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 4 展示了幻觉检测和参数演化的性能。结果表明，H-Neurons 在对齐之前就已经存在于预训练的基座模型中。从结果中，我们可以观察到：(1) H-Neurons 对基座模型具有显著的预测能力。在所有六个模型和三个数据集上，AUROC 分数始终大幅超越了随机猜测的基线。值得注意的是，Mistral 系列在 TriviaQA 上达到了超过 86%的准确率。这种跨阶段的可迁移性提供了强有力的证据，表明区分真相与幻觉的内部神经元是在预训练阶段建立的，而不是作为后训练对齐的伪影引入的。(2) 归一化排名的分布表明，H-Neurons 在从基座模型过渡到指令微调模型的过程中参数更新极少。这一趋势在 Mistral-Small 中尤为明显，其中 H-Neurons 高度集中在高排名区域（ <math intent=":literal" id="S4.p3.m1" display="inline" class="ltx_Math" alttext="avg\approx 0.97"><semantics><mrow><mrow><mi>a</mi><mo rspace="0em" lspace="0em">​</mo><mi>v</mi><mo rspace="0em" lspace="0em">​</mo><mi>g</mi></mrow><mo>≈</mo><mn>0.97</mn></mrow><annotation encoding="application/x-tex">avg\approx 0.97</annotation></semantics></math> ），表明参数稳定性极高。类似地，Gemma 和 Llama 系列模型也表现出统计学上显著的稳定性趋势（ <math intent=":literal" id="S4.p3.m2" display="inline" class="ltx_Math" alttext="avg&gt;0.58"><semantics><mrow><mrow><mi>a</mi><mo rspace="0em" lspace="0em">​</mo><mi>v</mi><mo rspace="0em" lspace="0em">​</mo><mi>g</mi></mrow><mo>&gt;</mo><mn>0.58</mn></mrow><annotation encoding="application/x-tex">avg&gt;0.58</annotation></semantics></math> ； <math intent=":literal" id="S4.p3.m3" display="inline" class="ltx_Math" alttext="P&lt;0.001"><semantics><mrow><mi>P</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding="application/x-tex">P&lt;0.001</annotation></semantics></math> ）。 这种观察到的"参数惰性"表明，标准的指令微调并没有有效地重构底层的幻觉机制；相反，它很大程度上保留了这些现有的电路。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">5 讨论</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">Our study establishes the correlation between neuron-level mechanisms and hallucinations for large language models. First, we demonstrate that hallucinations are reliably associated with a sparse subset of neurons in the FFN networks (Q1). Second, through targeted perturbation, we demonstrate that these neurons extend beyond hallucinations. They consistently promote behaviors such as over-compliance to invalid premises, misleading contexts, skeptical attitude, and harmful instructions, indicating that they encode a general disposition toward compliant answer generation (Q2). Third, our cross-model transfer experiments demonstrate that these neurons emerge during pre-training and persist through instruction tuning (Q3).
These findings open up promising directions for both practical applications and theoretical understanding of LLM behavior.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们的研究建立了大语言模型中神经元级机制与幻觉之间的关联。首先，我们证明了幻觉与 FFN 网络中稀疏的神经元子集可靠相关（Q1）。其次，通过定向扰动，我们证明了这些神经元不仅与幻觉相关，还持续促进诸如对无效前提过度服从、误导性语境、怀疑态度和有害指令等行为，表明它们编码了一种倾向于服从性回答生成的一般倾向（Q2）。第三，我们的跨模型迁移实验表明，这些神经元在预训练期间出现，并在指令微调过程中持续存在（Q3）。这些发现为 LLM 行为的实际应用和理论理解开辟了有前景的方向。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Applications of H-Neurons.</span> Our findings on H-Neurons can benefit practical applications in improving LLM trustworthiness. First, these neurons can enhance hallucination detection mechanisms. Our experiments demonstrate that H-Neurons generalize effectively across different models, domains, and hallucination types, suggesting that neuron-level signals could serve as robust features for training more effective hallucination detection systems. Moreover, neuron-level signals open new possibilities for token-level hallucination detection by enabling fine-grained identification of factual errors with specific parts of longer model responses.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">H-Neurons 的应用。我们对 H-Neurons 的研究成果可以促进实际应用，以提升 LLM 的可信度。首先，这些神经元可以增强幻觉检测机制。我们的实验表明，H-Neurons 在不同模型、领域和幻觉类型中具有有效的泛化能力，这表明神经元级别的信号可以作为训练更有效幻觉检测系统的可靠特征。此外，神经元级别的信号为 token 级别的幻觉检测开辟了新的可能性，通过能够对较长模型响应中的特定部分进行细粒度的事实错误识别。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p">Second, our work provides a direction for hallucination mitigation through neuron-level interventions. While existing hallucination mitigation approaches focus on training strategies and knowledge augmentation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">RAGsurvey</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">HallucinationMitigationSurvey</span>)</cite>, our findings suggest that targeted neuron editing could offer a more direct control mechanism. However, a critical challenge lies in balancing hallucination reduction with model helpfulness. Simple suppression or amplification of neuron activations proves insufficient for effective control. Future research must develop more sophisticated intervention strategies that can reliably suppress hallucinations while preserving the model’s overall utility and performance.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">其次，我们的工作为通过神经元层面的干预来缓解幻觉提供了方向。虽然现有的幻觉缓解方法主要集中于训练策略和知识增强（RAG 调查；幻觉缓解调查），但我们的研究结果表明，有针对性的神经元编辑可能提供一种更直接的控制系统。然而，一个关键挑战在于平衡幻觉减少与模型的有用性。简单的抑制或增强神经元激活已被证明不足以实现有效控制。未来的研究必须开发更复杂的干预策略，这些策略能够在可靠抑制幻觉的同时，保持模型的整体效用和性能。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Origins and Mechanisms of Hallucinations.</span> Our findings provide deeper neuronal-level insights into the causes of hallucinations in LLMs. We establish a critical link between H-Neurons and over-compliance behaviors, connecting two seemingly distinct phenomena. Prior work has shown that models often guess answers to achieve higher accuracy metrics&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">wei2025truthrl</span>)</cite>, a behavior that represents a form of over-compliance with task requirements. Our neuron-level analysis reveals the underlying computational mechanism: H-Neurons encode a general tendency toward generating compliant responses, even at the cost of factual accuracy. This finding offers a granular explanation for why models prioritize task completion over truthfulness.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">幻觉的起源与机制。我们的研究为 LLMs 中幻觉的成因提供了更深层次的神经元水平见解。我们建立了 H-Neurons 与过度合规行为之间的关键联系，将两种看似不同的现象联系起来。先前研究表明，模型常常猜测答案以实现更高的准确率指标（wei2025truthrl），这种行为代表了任务要求的一种过度合规形式。我们的神经元水平分析揭示了其背后的计算机制：H-Neurons 编码了一种倾向于生成合规响应的普遍趋势，即使以牺牲事实准确性为代价。这一发现为为什么模型优先考虑任务完成而非真实性提供了细致的解释。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p">Furthermore, our cross-model transfer experiments demonstrate that H-Neurons emerge during pre-training rather than post-training alignment. We argue that this originates from the inherent characteristics of the next-token prediction objective. This training paradigm does not distinguish between factually correct and incorrect continuations – it merely rewards fluent text generation. Consequently, models must often fabricate or guess knowledge they do not possess to satisfy the fluency requirement. This observation aligns with recent theoretical analyses that demonstrate hallucinations are an inevitable consequence of the pre-training process from a learning-theoretic perspective&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:journals/corr/abs-2509-04664</span>)</cite>. Together, these findings suggest that hallucinations are not merely artifacts of model scaling or alignment procedures, but rather deeply rooted in the fundamental training objectives that shape LLM behavior from their inception.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">此外，我们的跨模型迁移实验表明，H-Neurons 是在预训练阶段而非后训练对齐过程中出现的。我们认为这源于下一词预测目标的固有特性。这种训练范式并不区分事实正确与错误的延续——它仅仅奖励流畅的文本生成。因此，模型必须经常编造或猜测它们不具备的知识以满足流畅性要求。这一观察与最近的理论分析相吻合，这些分析从学习理论的角度表明，幻觉是预训练过程的必然结果（DBLP:journals/corr/abs-2509-04664）。综合这些发现，表明幻觉不仅仅是模型扩展或对齐过程的产物，而是深深植根于塑造 LLM 行为的根本训练目标中，这些目标从其诞生之初就存在。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p">Our neuron-centric investigation reveals that hallucinations are rooted in the model’s computational architecture and training objectives. By linking H-Neurons to over-compliance behaviors and tracing their origins to pre-training, we provide both theoretical insights and practical pathways for improving LLM reliability through enhanced detection and targeted interventions.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们的神经元中心研究揭示，幻觉根植于模型的计算架构和训练目标。通过将 H-神经元与过度合规行为联系起来，并追溯其起源至预训练阶段，我们为通过增强检测和针对性干预来提高 LLM 可靠性提供了理论见解和实践途径。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Methods<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6 方法</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p">To systematically deconstruct the neural mechanisms behind hallucination, we structure our methodology around three lines: identification, perturbation, and origin tracing.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了系统地解析幻觉背后的神经机制，我们围绕三条主线构建了我们的方法论：识别、扰动和起源追踪。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p">First, addressing the existence of H-Neurons (<span class="ltx_text ltx_font_italic">Q1</span>), we introduce an interpretable pipeline with a sparse linear classifier to isolate a precise subset of neurons that reliably signal hallucination.
Second, to determine how these neurons functionally shape model behavior (<span class="ltx_text ltx_font_italic">Q2</span>), we move from observation to manipulation. Through targeted perturbation experiments, we test the hypothesis that these neurons drive a broader pattern of over-compliance, assessing their causal efficacy across diverse benchmarks of different aspects of over-compliance.
Finally, to uncover when these H-Neurons emerge (<span class="ltx_text ltx_font_italic">Q3</span>), we quantify their backward transferability to pre-training and their parameter evolution during alignment.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">首先，针对 H-神经元的存在性问题（Q1），我们引入一个可解释的流程，使用稀疏线性分类器来分离出那些可靠地发出幻觉信号的神经元精确子集。其次，为了确定这些神经元如何功能性地塑造模型行为（Q2），我们从观察转向操控。通过有针对性的扰动实验，我们检验这些神经元驱动更广泛过度服从模式的假设，并在不同方面的过度服从基准测试中评估它们的因果效应。最后，为了揭示这些 H-神经元何时出现（Q3），我们量化它们对预训练的逆向迁移能力及其在对齐过程中的参数演变。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p">Together, this framework enables us to not only locate hallucination within the model’s parameters but also to explain its functional role and origins.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">该框架使我们不仅能够在模型的参数中定位幻觉，还能解释其功能作用和起源。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Identifying H-Neurons<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6.1 识别 H-神经元</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p">To investigate the neural mechanisms underlying hallucination generation, we design a systematic analysis pipeline to identify a subset of neurons that are more active on faithful outputs than hallucinatory ones. First, to isolate stable neural signatures from stochastic decoding noise, we establish a controlled contrastive dataset comprising an equal number of verified faithful responses and hallucinatory responses. Building on this balanced foundation, we then quantify the specific contribution of individual neurons to the generated tokens across all samples. Finally, these contribution profiles serve as inputs to train a linear classifier, where the learned weights provide a direct, quantitative metric for assessing each neuron’s role in driving the model toward hallucinatory behaviors.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了研究产生幻觉的神经机制，我们设计了一个系统分析流程，以识别在可信输出上比在幻觉输出上更活跃的一组神经元。首先，为了从随机解码噪声中分离出稳定的神经特征，我们建立了一个受控对比数据集，其中包含相同数量的已验证可信响应和幻觉响应。基于这个平衡的基础，我们接着量化单个神经元对所有样本中生成的标记的具体贡献。最后，这些贡献特征作为输入来训练一个线性分类器，其中学习到的权重为评估每个神经元在推动模型产生幻觉行为中的角色提供了直接、定量的指标。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>Training Data Construction<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6.1.1 训练数据构建</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p">To robustly identify neurons associated with hallucinations, we need to construct a dataset that yields stable and precise contrastive signals between faithful and hallucinatory outputs. To ensure stability, relying on individual response samples is inadequate, as a single output fails to verify whether the model’s behavior reflects a consistent internal belief or merely transient decoding noise. To ensure precision, indiscriminately analyzing the entire response sequence is suboptimal, as it dilutes the neural signal with non-factual syntactic fillers. Therefore, our data construction process is designed to minimize signal ambiguity by filtering for consistency and maximize precision by targeting specific answer tokens.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了稳健地识别与幻觉相关的神经元，我们需要构建一个能够产生稳定且精确的对比信号的数据集，这些信号应体现在忠实输出与幻觉输出之间。为了确保稳定性，单纯依赖单个响应样本是不够的，因为单个输出无法验证模型的行为是否反映了持续稳定的内部信念，还是仅仅瞬时的解码噪声。为了确保精确性，不加区分地分析整个响应序列则不够理想，因为它会稀释神经信号，掺入非事实性的句法填充词。因此，我们的数据构建过程旨在通过筛选一致性来最小化信号模糊性，并通过针对特定的答案标记来最大化精确性。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Consistency Filtering.</span> Our first goal is to capture the model’s stable behavioral patterns across multiple responses.
To achieve this, we adopt the TriviaQA dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Triviaqa</span>)</cite> for its broad coverage of general-domain knowledge and typically concise answers, which align well with our requirements. For each query, we perform a rigorous consistency check by sampling 10 distinct responses using probabilistic decoding parameters (<span class="ltx_text ltx_font_typewriter">temperature=1.0</span>, <span class="ltx_text ltx_font_typewriter">top_k=50</span>, <span class="ltx_text ltx_font_typewriter">top_p=0.9</span>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">一致性过滤。我们的首要目标是捕捉模型在多个响应中表现出的稳定行为模式。为此，我们采用 TriviaQA 数据集（Triviaqa），因其广泛涵盖通用领域知识且通常答案简洁，与我们的需求高度契合。对于每个查询，我们通过使用概率解码参数（温度=1.0，top_k=50，top_p=0.9）随机采样 10 个不同响应，进行严格的一致性检查。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p3">
<p class="ltx_p">We retain only those instances where the model exhibits consistent behavior:
(1) <span class="ltx_text ltx_font_italic">Consistently Correct</span>: The model answers correctly in all 10 samples.
(2) <span class="ltx_text ltx_font_italic">Consistently Incorrect</span>: The model fails in all 10 samples, consistently generating incorrect answers instead of responding with "I don’t know" or similar refusals.
This strict filtering yields a high-quality contrastive set of 1,000 fully correct and 1,000 fully incorrect examples. This ensures that any observed differences in neuronal activity are attributable to the fundamental truthfulness of the output rather than generation noise.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们仅保留那些模型表现出一致行为的实例：(1) 持续正确：模型在所有 10 个样本中都回答正确。(2) 持续错误：模型在所有 10 个样本中都失败，持续生成错误答案，而不是回答“我不知道”或类似的拒绝。这种严格的筛选产生了一组高质量的对比集，包含 1000 个完全正确和 1000 个完全错误的例子。这确保了观察到的神经元活动差异归因于输出本身的基本真实性，而不是生成噪声。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Answer Token Extraction.</span> Having established the samples, our second objective is to precisely localize the neural signal. Hallucinations in factual QA typically manifest within specific entities or key terms rather than in syntactic filler words (e.g., "The answer is…")&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">LLMsKnowMore</span>)</cite>. Treating non-factual tokens and answer tokens as the same in the analysis would introduce noise and dilute the signal of H-Neurons.
Consequently, we use GPT-4o to explicitly identify and align the specific spans of text containing the factual claim. By focusing on these token positions, we ensure that the detected activation patterns are directly linked to the factual content of the generation.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">答案 token 提取。在确定样本后，我们的第二个目标是精确定位神经信号。在事实性问答中，幻觉通常体现在特定的实体或关键词中，而不是在句法填充词（例如，“答案是...”）中（LLMsKnowMore）。在分析中将非事实性 token 和答案 token 视为相同会引入噪声并稀释 H-Neurons 的信号。因此，我们使用 GPT-4o 来明确识别和定位包含事实性陈述的特定文本片段。通过关注这些 token 位置，我们确保检测到的激活模式直接与生成的内容的事实性相关联。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Quantifying Neuron Contribution<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6.1.2 量化神经元贡献</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p">With the dataset established, our next objective is to transform these raw text samples into quantitative neural contributions that can serve as inputs for training a linear classifier. Specifically, we need to measure the functional influence of every neuron on each response to identify which specific units sway the model toward hallucination. Simply recording raw activation magnitudes is insufficient for this purpose, as a neuron might exhibit high activation yet have a negligible impact on the hidden state representation of FFN due to downstream projection weights. Therefore, we adopt the CETT metric&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">relu2wins</span>)</cite> to quantify the contribution of an individual neuron to the hidden state representation during the forward pass. This metric transforms raw neural activity into a measure of causal efficacy, serving as the fundamental feature input for our subsequent linear classifier.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">数据集建立后，我们的下一个目标是将这些原始文本样本转化为可用于训练线性分类器的定量神经贡献。具体来说，我们需要测量每个神经元对每个响应的功能影响，以识别哪些特定单元使模型倾向于产生幻觉。仅记录原始激活幅度不足以实现这一目的，因为一个神经元可能表现出高激活，但由于下游投影权重，其对 FFN 的隐藏状态表示的影响可能微乎其微。因此，我们采用 CETT 指标（relu2wins）来量化单个神经元在正向传播过程中对隐藏状态表示的贡献。该指标将原始神经活动转化为因果效应的度量，作为我们后续线性分类器的基本特征输入。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Estimating Token-Level Contribution.</span> Consider an input sequence <math alttext="w=(w_{0},\ldots,w_{T})" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p2.m1" intent=":literal"><semantics><mrow><mi>w</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>w</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">w=(w_{0},\ldots,w_{T})</annotation></semantics></math> processed by a transformer block. At token position <math alttext="t" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, the hidden representation is <math alttext="x_{t}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p2.m3" intent=":literal"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">x_{t}\in\mathbb{R}^{d}</annotation></semantics></math>. Within each MLP, <math alttext="x_{t}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p2.m4" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math> is first projected into an intermediate activation space:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">估计 Token 级别的贡献。考虑一个输入序列 <math intent=":literal" id="S6.SS1.SSS2.p2.m1" display="inline" class="ltx_Math" alttext="w=(w_{0},\ldots,w_{T})"><semantics><mrow><mi>w</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>w</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">w=(w_{0},\ldots,w_{T})</annotation></semantics></math> 被一个 Transformer 模块处理。在 Token 位置 <math intent=":literal" id="S6.SS1.SSS2.p2.m2" display="inline" class="ltx_Math" alttext="t"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> ，隐藏表示是 <math intent=":literal" id="S6.SS1.SSS2.p2.m3" display="inline" class="ltx_Math" alttext="x_{t}\in\mathbb{R}^{d}"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">x_{t}\in\mathbb{R}^{d}</annotation></semantics></math> 。在每个 MLP 中， <math intent=":literal" id="S6.SS1.SSS2.p2.m4" display="inline" class="ltx_Math" alttext="x_{t}"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math> 首先被投影到一个中间激活空间：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z_{t}=\sigma(W_{\text{gate}}x_{t})\odot W_{\text{up}}x_{t}," class="ltx_Math" display="block" id="S6.E1.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>W</mi><mtext>gate</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⊙</mo><msub><mi>W</mi><mtext>up</mtext></msub></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>t</mi></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">z_{t}=\sigma(W_{\text{gate}}x_{t})\odot W_{\text{up}}x_{t},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\sigma(\cdot)" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p2.m5" intent=":literal"><semantics><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(\cdot)</annotation></semantics></math> denotes the non-linear activation, and <math alttext="W_{\text{gate}},W_{\text{up}}\in\mathbb{R}^{d_{m}\times d}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p2.m6" intent=":literal"><semantics><mrow><mrow><msub><mi>W</mi><mtext>gate</mtext></msub><mo>,</mo><msub><mi>W</mi><mtext>up</mtext></msub></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>d</mi><mi>m</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W_{\text{gate}},W_{\text{up}}\in\mathbb{R}^{d_{m}\times d}</annotation></semantics></math> are learned projection matrices. Each dimension <math alttext="z_{j,t}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p2.m7" intent=":literal"><semantics><msub><mi>z</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">z_{j,t}</annotation></semantics></math> corresponds to the activation of neuron <math alttext="j" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p2.m8" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> prior to the down-projection <math alttext="h_{t}=W_{\text{down}}z_{t}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p2.m9" intent=":literal"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>W</mi><mtext>down</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>z</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">h_{t}=W_{\text{down}}z_{t}</annotation></semantics></math>, with <math alttext="W_{\text{down}}\in\mathbb{R}^{d\times d_{m}}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p2.m10" intent=":literal"><semantics><mrow><msub><mi>W</mi><mtext>down</mtext></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>d</mi><mi>m</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W_{\text{down}}\in\mathbb{R}^{d\times d_{m}}</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"> <math intent=":literal" id="S6.SS1.SSS2.p2.m5" display="inline" class="ltx_Math" alttext="\sigma(\cdot)"><semantics><mrow><mi>σ</mi><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo rspace="0em" lspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma(\cdot)</annotation></semantics></math> 表示非线性激活， <math intent=":literal" id="S6.SS1.SSS2.p2.m6" display="inline" class="ltx_Math" alttext="W_{\text{gate}},W_{\text{up}}\in\mathbb{R}^{d_{m}\times d}"><semantics><mrow><mrow><msub><mi>W</mi><mtext>gate</mtext></msub><mo>,</mo><msub><mi>W</mi><mtext>up</mtext></msub></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>d</mi><mi>m</mi></msub><mo rspace="0.222em" lspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W_{\text{gate}},W_{\text{up}}\in\mathbb{R}^{d_{m}\times d}</annotation></semantics></math> 是学习到的投影矩阵。每个维度 <math intent=":literal" id="S6.SS1.SSS2.p2.m7" display="inline" class="ltx_Math" alttext="z_{j,t}"><semantics><msub><mi>z</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">z_{j,t}</annotation></semantics></math> 对应于神经元 <math intent=":literal" id="S6.SS1.SSS2.p2.m8" display="inline" class="ltx_Math" alttext="j"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> 在下投影 <math intent=":literal" id="S6.SS1.SSS2.p2.m9" display="inline" class="ltx_Math" alttext="h_{t}=W_{\text{down}}z_{t}"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>W</mi><mtext>down</mtext></msub><mo rspace="0em" lspace="0em">​</mo><msub><mi>z</mi><mi>t</mi></msub></mrow></mrow><annotation encoding="application/x-tex">h_{t}=W_{\text{down}}z_{t}</annotation></semantics></math> 之前的激活，与 <math intent=":literal" id="S6.SS1.SSS2.p2.m10" display="inline" class="ltx_Math" alttext="W_{\text{down}}\in\mathbb{R}^{d\times d_{m}}"><semantics><mrow><msub><mi>W</mi><mtext>down</mtext></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo rspace="0.222em" lspace="0.222em">×</mo><msub><mi>d</mi><mi>m</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W_{\text{down}}\in\mathbb{R}^{d\times d_{m}}</annotation></semantics></math> 。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS2.p3">
<p class="ltx_p">To isolate the contribution of a single neuron <math alttext="j" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p3.m1" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>, we mask all other neurons, defining the single-neuron activation vector <math alttext="z_{t}^{(j)}=z_{j,t}\,e_{j}\in\mathbb{R}^{d_{m}}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p3.m2" intent=":literal"><semantics><mrow><msubsup><mi>z</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mrow><msub><mi>z</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>e</mi><mi>j</mi></msub></mrow><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>d</mi><mi>m</mi></msub></msup></mrow><annotation encoding="application/x-tex">z_{t}^{(j)}=z_{j,t}\,e_{j}\in\mathbb{R}^{d_{m}}</annotation></semantics></math>,
where <math alttext="e_{j}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p3.m3" intent=":literal"><semantics><msub><mi>e</mi><mi>j</mi></msub><annotation encoding="application/x-tex">e_{j}</annotation></semantics></math> is the <math alttext="j" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p3.m4" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th standard basis vector so <math alttext="z_{t}^{(j)}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p3.m5" intent=":literal"><semantics><msubsup><mi>z</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">z_{t}^{(j)}</annotation></semantics></math> retains only the <math alttext="j" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p3.m6" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th component of <math alttext="z_{t}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p3.m7" intent=":literal"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding="application/x-tex">z_{t}</annotation></semantics></math> and zeros out all others. The down-projected partial hidden vector attributable to neuron <math alttext="j" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p3.m8" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> is then <math alttext="h_{t}^{(j)}\;=\;W_{\text{down}}\,z_{t}^{(j)}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p3.m9" intent=":literal"><semantics><mrow><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0.558em" rspace="0.558em">=</mo><mrow><msub><mi>W</mi><mtext>down</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>z</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">h_{t}^{(j)}\;=\;W_{\text{down}}\,z_{t}^{(j)}\in\mathbb{R}^{d}</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了隔离单个神经元 <math intent=":literal" id="S6.SS1.SSS2.p3.m1" display="inline" class="ltx_Math" alttext="j"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> 的贡献，我们屏蔽所有其他神经元，定义单个神经元激活向量 <math intent=":literal" id="S6.SS1.SSS2.p3.m2" display="inline" class="ltx_Math" alttext="z_{t}^{(j)}=z_{j,t}\,e_{j}\in\mathbb{R}^{d_{m}}"><semantics><mrow><msubsup><mi>z</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mrow><msub><mi>z</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><mo rspace="0em" lspace="0em">​</mo><msub><mi>e</mi><mi>j</mi></msub></mrow><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>d</mi><mi>m</mi></msub></msup></mrow><annotation encoding="application/x-tex">z_{t}^{(j)}=z_{j,t}\,e_{j}\in\mathbb{R}^{d_{m}}</annotation></semantics></math> ，其中 <math intent=":literal" id="S6.SS1.SSS2.p3.m3" display="inline" class="ltx_Math" alttext="e_{j}"><semantics><msub><mi>e</mi><mi>j</mi></msub><annotation encoding="application/x-tex">e_{j}</annotation></semantics></math> 是 <math intent=":literal" id="S6.SS1.SSS2.p3.m4" display="inline" class="ltx_Math" alttext="j"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> - 维标准基向量，因此 <math intent=":literal" id="S6.SS1.SSS2.p3.m5" display="inline" class="ltx_Math" alttext="z_{t}^{(j)}"><semantics><msubsup><mi>z</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">z_{t}^{(j)}</annotation></semantics></math> 仅保留 <math intent=":literal" id="S6.SS1.SSS2.p3.m6" display="inline" class="ltx_Math" alttext="j"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> 的分量并使其他分量归零。归因于神经元 <math intent=":literal" id="S6.SS1.SSS2.p3.m8" display="inline" class="ltx_Math" alttext="j"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> 的下投影部分隐藏向量是 <math intent=":literal" id="S6.SS1.SSS2.p3.m9" display="inline" class="ltx_Math" alttext="h_{t}^{(j)}\;=\;W_{\text{down}}\,z_{t}^{(j)}\in\mathbb{R}^{d}"><semantics><mrow><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo rspace="0.558em" lspace="0.558em">=</mo><mrow><msub><mi>W</mi><mtext>down</mtext></msub><mo rspace="0em" lspace="0em">​</mo><msubsup><mi>z</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">h_{t}^{(j)}\;=\;W_{\text{down}}\,z_{t}^{(j)}\in\mathbb{R}^{d}</annotation></semantics></math> 。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p4">
<p class="ltx_p">We then measure the normalized contribution of neuron <math alttext="j" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p4.m1" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> at position <math alttext="t" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p4.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> as the magnitude of its projected vector relative to the total hidden state norm:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">然后我们测量位置 <math intent=":literal" id="S6.SS1.SSS2.p4.m2" display="inline" class="ltx_Math" alttext="t"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> 的神经元 <math intent=":literal" id="S6.SS1.SSS2.p4.m1" display="inline" class="ltx_Math" alttext="j"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> 的归一化贡献，作为其投影向量的幅度相对于总隐藏状态范数：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{CETT}_{j,t}\;=\;\frac{\|h_{t}^{(j)}\|_{2}}{\|h_{t}\|_{2}}," class="ltx_Math" display="block" id="S6.E2.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>CETT</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><mo lspace="0.558em" rspace="0.558em">=</mo><mfrac><msub><mrow><mo stretchy="false">‖</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><msub><mrow><mo stretchy="false">‖</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathrm{CETT}_{j,t}\;=\;\frac{\|h_{t}^{(j)}\|_{2}}{\|h_{t}\|_{2}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Intuitively, this ratio captures the fraction of the information flow at token <math alttext="t" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p4.m3" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> that is explicitly attributable to neuron <math alttext="j" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p4.m4" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">直观上，这个比率捕获了标记 <math intent=":literal" id="S6.SS1.SSS2.p4.m3" display="inline" class="ltx_Math" alttext="t"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> 的信息流中明确归因于神经元 <math intent=":literal" id="S6.SS1.SSS2.p4.m4" display="inline" class="ltx_Math" alttext="j"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> 的部分</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Aggregating Features for Hallucination Detection.</span> While Eq.&nbsp;(<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.E2" title="In 6.1.2 Quantifying Neuron Contribution ‣ 6.1 Identifying H-Neurons ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>) provides a token-level metric, directly utilizing the full sequence of contribution scores as input features is impractical and unsuited for our objective as including every token would introduce excessive noise and computational overhead. Furthermore, we hypothesize that neurons driving hallucinations are specifically active during the generation of the answer tokens, whereas activity during syntactic fillers represents general linguistic processing.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">用于幻觉检测的特征聚合。虽然公式(2)提供了一个词元级别的度量，但直接将完整的贡献分数序列作为输入特征是不切实际且不适合我们目标的，因为包含每个词元将引入过多的噪声和计算开销。此外，我们假设驱动幻觉的神经元在生成答案词元时特别活跃，而句法填充词元时的活动则代表一般语言处理。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p6">
<p class="ltx_p">Consequently, to distill the most relevant signals and ensure training efficiency, we aggregate the token-level scores into two fixed-dimensional features for neuron j on each sample:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">因此，为了提取最相关的信号并确保训练效率，我们将词元级别的分数在每个样本上对神经元 j 聚合为两个固定维度的特征：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p7">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A0.EGx1">
<tbody id="S6.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\overline{\mathrm{CETT}}_{j,\text{answer}}\;=\;\frac{1}{|\mathcal{A}|}\sum_{t\in\mathcal{A}}\mathrm{CETT}_{j,t},\qquad\overline{\mathrm{CETT}}_{j,\text{other}}\;=\;\frac{1}{|\mathcal{T}\setminus\mathcal{A}|}\sum_{t\in\mathcal{T}\setminus\mathcal{A}}\mathrm{CETT}_{j,t}." class="ltx_Math" display="inline" id="S6.E3.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mover accent="true"><mi>CETT</mi><mo stretchy="true">¯</mo></mover><mrow><mi>j</mi><mo>,</mo><mtext>answer</mtext></mrow></msub><mo lspace="0.558em" rspace="0.558em">=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo stretchy="false">|</mo></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi></mrow></munder></mstyle><msub><mi>CETT</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><msub><mover accent="true"><mi>CETT</mi><mo stretchy="true">¯</mo></mover><mrow><mi>j</mi><mo>,</mo><mtext>other</mtext></mrow></msub><mo rspace="0.558em">=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒯</mi><mo>∖</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi></mrow><mo stretchy="false">|</mo></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>∈</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒯</mi><mo>∖</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi></mrow></mrow></munder></mstyle><msub><mi>CETT</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\overline{\mathrm{CETT}}_{j,\text{answer}}\;=\;\frac{1}{|\mathcal{A}|}\sum_{t\in\mathcal{A}}\mathrm{CETT}_{j,t},\qquad\overline{\mathrm{CETT}}_{j,\text{other}}\;=\;\frac{1}{|\mathcal{T}\setminus\mathcal{A}|}\sum_{t\in\mathcal{T}\setminus\mathcal{A}}\mathrm{CETT}_{j,t}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p8">
<p class="ltx_p">where <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p8.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> denotes the set of answer tokens and <math alttext="\mathcal{T}\setminus\mathcal{A}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p8.m2" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒯</mi><mo>∖</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi></mrow><annotation encoding="application/x-tex">\mathcal{T}\setminus\mathcal{A}</annotation></semantics></math> denotes other tokens.
Here, <math alttext="\overline{\mathrm{CETT}}_{j,\text{answer}}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p8.m3" intent=":literal"><semantics><msub><mover accent="true"><mi>CETT</mi><mo stretchy="true">¯</mo></mover><mrow><mi>j</mi><mo>,</mo><mtext>answer</mtext></mrow></msub><annotation encoding="application/x-tex">\overline{\mathrm{CETT}}_{j,\text{answer}}</annotation></semantics></math> serves as the primary signal for potential hallucinatory behavior, while <math alttext="\overline{\mathrm{CETT}}_{j,\text{other}}" class="ltx_Math" display="inline" id="S6.SS1.SSS2.p8.m4" intent=":literal"><semantics><msub><mover accent="true"><mi>CETT</mi><mo stretchy="true">¯</mo></mover><mrow><mi>j</mi><mo>,</mo><mtext>other</mtext></mrow></msub><annotation encoding="application/x-tex">\overline{\mathrm{CETT}}_{j,\text{other}}</annotation></semantics></math> acts as a control baseline which enables the subsequent classifier to filter out neurons that are merely active across the entire sequence and isolate those that are selectively influential specifically during the generation of the answer tokens where hallucinations manifest.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">其中 <math intent=":literal" id="S6.SS1.SSS2.p8.m1" display="inline" class="ltx_Math" alttext="\mathcal{A}"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> 表示答案词元的集合， <math intent=":literal" id="S6.SS1.SSS2.p8.m2" display="inline" class="ltx_Math" alttext="\mathcal{T}\setminus\mathcal{A}"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒯</mi><mo>∖</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi></mrow><annotation encoding="application/x-tex">\mathcal{T}\setminus\mathcal{A}</annotation></semantics></math> 表示其他词元。这里， <math intent=":literal" id="S6.SS1.SSS2.p8.m3" display="inline" class="ltx_Math" alttext="\overline{\mathrm{CETT}}_{j,\text{answer}}"><semantics><msub><mover accent="true"><mi>CETT</mi><mo stretchy="true">¯</mo></mover><mrow><mi>j</mi><mo>,</mo><mtext>answer</mtext></mrow></msub><annotation encoding="application/x-tex">\overline{\mathrm{CETT}}_{j,\text{answer}}</annotation></semantics></math> 作为潜在幻觉行为的信号，而 <math intent=":literal" id="S6.SS1.SSS2.p8.m4" display="inline" class="ltx_Math" alttext="\overline{\mathrm{CETT}}_{j,\text{other}}"><semantics><msub><mover accent="true"><mi>CETT</mi><mo stretchy="true">¯</mo></mover><mrow><mi>j</mi><mo>,</mo><mtext>other</mtext></mrow></msub><annotation encoding="application/x-tex">\overline{\mathrm{CETT}}_{j,\text{other}}</annotation></semantics></math> 作为控制基线，它使后续分类器能够过滤掉在整个序列中仅活跃的神经元，并分离出在生成答案词元时特别活跃且幻觉显现时具有选择性影响的神经元。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3 </span>Identifying H-Neurons via Linear Classifier<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6.1.3 通过线性分类器识别 H-神经元</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p">Having quantified the contribution of each neuron, our final step is to pinpoint the specific subset of neurons associated with hallucination. We achieve this by training a linear classifier that accepts the contribution of all neurons as input to predict a binary label indicating whether the response is a hallucination. The learned weights of this classifier then serve as a direct quantitative metric to assess each neuron’s role in model’s hallucination. With this classifier, our objective is to identify a precise subset of neurons: the selected set must be comprehensive enough to capture the full signal driving hallucinations, yet sufficiently sparse to exclude neurons responsible for other capabilities.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">量化了每个神经元的贡献后，我们的最终步骤是确定与幻觉相关的特定神经元子集。我们通过训练一个线性分类器来实现这一点，该分类器将所有神经元的贡献作为输入，以预测一个二元标签，指示响应是否为幻觉。该分类器学习到的权重直接作为量化指标，用于评估每个神经元在模型幻觉中的作用。通过这个分类器，我们的目标是识别一个精确的神经元子集：所选集合必须足够全面，以捕捉驱动幻觉的完整信号，但又足够稀疏，以排除负责其他能力的神经元。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Feature Construction.</span> To train a classifier that targets only hallucination, we must construct a training set that enforces strict specificity.
For each response <math alttext="s" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p2.m1" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>, we assemble the per-neuron aggregated scores into two feature vectors: <math alttext="\mathbf{x}^{(s,\text{answer})}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p2.m2" intent=":literal"><semantics><mrow><msup><mi>𝐱</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>answer</mtext><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{x}^{(s,\text{answer})}\in\mathbb{R}^{D}</annotation></semantics></math>, which contains <math alttext="\overline{\mathrm{CETT}}_{j,\text{answer}}" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p2.m3" intent=":literal"><semantics><msub><mover accent="true"><mi>CETT</mi><mo stretchy="true">¯</mo></mover><mrow><mi>j</mi><mo>,</mo><mtext>answer</mtext></mrow></msub><annotation encoding="application/x-tex">\overline{\mathrm{CETT}}_{j,\text{answer}}</annotation></semantics></math> for all neurons <math alttext="j=1\dots D" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p2.m4" intent=":literal"><semantics><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">…</mi><mo lspace="0em" rspace="0em">​</mo><mi>D</mi></mrow></mrow><annotation encoding="application/x-tex">j=1\dots D</annotation></semantics></math>, and <math alttext="\mathbf{x}^{(s,\text{other})}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p2.m5" intent=":literal"><semantics><mrow><msup><mi>𝐱</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>other</mtext><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{x}^{(s,\text{other})}\in\mathbb{R}^{D}</annotation></semantics></math>, which contains the corresponding non-answer contributions.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">特征构建。为了训练一个仅针对幻觉的分类器，我们必须构建一个强制严格特异性的训练集。对于每个响应 <math intent=":literal" id="S6.SS1.SSS3.p2.m1" display="inline" class="ltx_Math" alttext="s"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> ，我们将每个神经元的聚合分数组装成两个特征向量： <math intent=":literal" id="S6.SS1.SSS3.p2.m2" display="inline" class="ltx_Math" alttext="\mathbf{x}^{(s,\text{answer})}\in\mathbb{R}^{D}"><semantics><mrow><msup><mi>𝐱</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>answer</mtext><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{x}^{(s,\text{answer})}\in\mathbb{R}^{D}</annotation></semantics></math> ，其中包含所有神经元 <math intent=":literal" id="S6.SS1.SSS3.p2.m4" display="inline" class="ltx_Math" alttext="j=1\dots D"><semantics><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo rspace="0em" lspace="0em">​</mo><mi mathvariant="normal">…</mi><mo rspace="0em" lspace="0em">​</mo><mi>D</mi></mrow></mrow><annotation encoding="application/x-tex">j=1\dots D</annotation></semantics></math> 的 <math intent=":literal" id="S6.SS1.SSS3.p2.m3" display="inline" class="ltx_Math" alttext="\overline{\mathrm{CETT}}_{j,\text{answer}}"><semantics><msub><mover accent="true"><mi>CETT</mi><mo stretchy="true">¯</mo></mover><mrow><mi>j</mi><mo>,</mo><mtext>answer</mtext></mrow></msub><annotation encoding="application/x-tex">\overline{\mathrm{CETT}}_{j,\text{answer}}</annotation></semantics></math> ，以及 <math intent=":literal" id="S6.SS1.SSS3.p2.m5" display="inline" class="ltx_Math" alttext="\mathbf{x}^{(s,\text{other})}\in\mathbb{R}^{D}"><semantics><mrow><msup><mi>𝐱</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>other</mtext><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{x}^{(s,\text{other})}\in\mathbb{R}^{D}</annotation></semantics></math> ，其中包含相应的非答案贡献。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p3">
<p class="ltx_p">We then assign binary labels <math alttext="y\in\{0,1\}" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p3.m1" intent=":literal"><semantics><mrow><mi>y</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">y\in\{0,1\}</annotation></semantics></math> to these vectors based on a rigorous exclusion criterion. We define the positive class (<math alttext="y=1" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p3.m2" intent=":literal"><semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y=1</annotation></semantics></math>) exclusively as the answer-token features from hallucinatory responses. All other cases are assigned to the negative class (<math alttext="y=0" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p3.m3" intent=":literal"><semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y=0</annotation></semantics></math>): (1) Faithful Answer Tokens: To prevent the classifier from selecting neurons that activate for any factual claim. (2) Non-Answer Tokens from both faithful and hallucinatory responses: To prevent selecting neurons associated with general generation quality or syntax.
Formally, the label assignment for the feature vectors is defined as:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">然后我们根据严格的排除标准，为这些向量分配二进制标签 <math intent=":literal" id="S6.SS1.SSS3.p3.m1" display="inline" class="ltx_Math" alttext="y\in\{0,1\}"><semantics><mrow><mi>y</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">y\in\{0,1\}</annotation></semantics></math> 。我们将正类（ <math intent=":literal" id="S6.SS1.SSS3.p3.m2" display="inline" class="ltx_Math" alttext="y=1"><semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y=1</annotation></semantics></math> ）严格定义为来自幻觉响应的答案标记特征。所有其他情况都分配到负类（ <math intent=":literal" id="S6.SS1.SSS3.p3.m3" display="inline" class="ltx_Math" alttext="y=0"><semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y=0</annotation></semantics></math> ）：（1）忠实答案标记：防止分类器选择对任何事实声明都激活的神经元。（2）来自忠实和幻觉响应的非答案标记：防止选择与一般生成质量或句法相关的神经元。形式上，特征向量的标签分配定义为：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S6.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="y^{(s,\text{answer})}=\begin{cases}1&amp;\text{if $s$ is faithful response},\\
0&amp;\text{if $s$ is hallucinatory response},\end{cases}\qquad y^{(s,\text{other})}=0\quad\text{for all }s." class="ltx_Math" display="block" id="S6.Ex1.m1" intent=":literal"><semantics><mrow><mrow><mrow><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>answer</mtext><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mn>1</mn></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext>if&nbsp;</mtext><mi>s</mi><mtext>&nbsp;is faithful response</mtext></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mn>0</mn></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext>if&nbsp;</mtext><mi>s</mi><mtext>&nbsp;is hallucinatory response</mtext></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow><mspace style="width:2em;" width="2em"></mspace><mrow><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>other</mtext><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><mn>0</mn><mspace style="width:1em;" width="1em"></mspace><mrow><mtext>for all&nbsp;</mtext><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">y^{(s,\text{answer})}=\begin{cases}1&amp;\text{if $s$ is faithful response},\\
0&amp;\text{if $s$ is hallucinatory response},\end{cases}\qquad y^{(s,\text{other})}=0\quad\text{for all }s.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p4">
<p class="ltx_p">This asymmetric labeling strategy forces the classifier to identify neurons that are active specifically when the model is generating an answer and specifically when that answer is false.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">这种非对称标签策略迫使分类器识别那些仅在模型生成答案时被激活，并且仅当该答案为错误时被激活的神经元。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Sparse Linear Classifier.</span> We model the probability of a hallucination as <math alttext="\Pr(y=1\mid\mathbf{x})=\sigma(\theta^{\top}\mathbf{x})" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p5.m1" intent=":literal"><semantics><mrow><mrow><mi>Pr</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo>=</mo><mrow><mn>1</mn><mo>∣</mo><mi>𝐱</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>θ</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝐱</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Pr(y=1\mid\mathbf{x})=\sigma(\theta^{\top}\mathbf{x})</annotation></semantics></math>, where <math alttext="\theta\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p5.m2" intent=":literal"><semantics><mrow><mi>θ</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\theta\in\mathbb{R}^{D}</annotation></semantics></math> represents the learned importance weight of each neuron.
Crucially, we employ <math alttext="\ell_{1}" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p5.m3" intent=":literal"><semantics><msub><mi mathvariant="normal">ℓ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_{1}</annotation></semantics></math>-regularized logistic regression rather than a dense or non-linear model. The choice of a linear model ensures that the learned weights <math alttext="\theta" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p5.m4" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> are directly interpretable as the marginal contribution of each neuron to the hallucination log-odds. The <math alttext="\ell_{1}" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p5.m5" intent=":literal"><semantics><msub><mi mathvariant="normal">ℓ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_{1}</annotation></semantics></math> penalty enforces sparsity, as we hypothesize that hallucinations are driven by a sparse subset of neurons rather than the entire network. By imposing a strong regularization, this also helps highlight the critical contributions of this specific subset.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">稀疏线性分类器。我们将幻觉的概率建模为 <math intent=":literal" id="S6.SS1.SSS3.p5.m1" display="inline" class="ltx_Math" alttext="\Pr(y=1\mid\mathbf{x})=\sigma(\theta^{\top}\mathbf{x})"><semantics><mrow><mrow><mi>Pr</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo>=</mo><mrow><mn>1</mn><mo>∣</mo><mi>𝐱</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>σ</mi><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>θ</mi><mo>⊤</mo></msup><mo rspace="0em" lspace="0em">​</mo><mi>𝐱</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Pr(y=1\mid\mathbf{x})=\sigma(\theta^{\top}\mathbf{x})</annotation></semantics></math> ，其中 <math intent=":literal" id="S6.SS1.SSS3.p5.m2" display="inline" class="ltx_Math" alttext="\theta\in\mathbb{R}^{D}"><semantics><mrow><mi>θ</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">\theta\in\mathbb{R}^{D}</annotation></semantics></math> 表示每个神经元的学习重要性权重。关键在于，我们采用 <math intent=":literal" id="S6.SS1.SSS3.p5.m3" display="inline" class="ltx_Math" alttext="\ell_{1}"><semantics><msub><mi mathvariant="normal">ℓ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_{1}</annotation></semantics></math> 正则化的逻辑回归，而不是密集或非线性模型。选择线性模型确保学习到的权重 <math intent=":literal" id="S6.SS1.SSS3.p5.m4" display="inline" class="ltx_Math" alttext="\theta"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> 可以直接解释为每个神经元对幻觉对数优势的边际贡献。 <math intent=":literal" id="S6.SS1.SSS3.p5.m5" display="inline" class="ltx_Math" alttext="\ell_{1}"><semantics><msub><mi mathvariant="normal">ℓ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_{1}</annotation></semantics></math> 惩罚强制执行稀疏性，因为我们假设幻觉是由网络中稀疏的子集神经元驱动的，而不是整个网络。通过施加强正则化，这也有助于突出显示这个特定子集的关键贡献。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p6">
<p class="ltx_p">The training objective minimizes the negative log-likelihood with the sparsity constraint:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">训练目标在稀疏约束下最小化负对数似然：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S6.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(\theta)\;=\;-\sum_{i}\Big[y_{i}\log\sigma(\theta^{\top}\mathbf{x}_{i})+(1-y_{i})\log\big(1-\sigma(\theta^{\top}\mathbf{x}_{i})\big)\Big]\;+\;\lambda\|\theta\|_{1}," class="ltx_Math" display="block" id="S6.E4.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.558em">=</mo><mrow><mrow><mo>−</mo><mrow><munder><mo movablelimits="false" rspace="0em">∑</mo><mi>i</mi></munder><mrow><mo maxsize="1.600em" minsize="1.600em">[</mo><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>σ</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>θ</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝐱</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>θ</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝐱</mi><mi>i</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow></mrow></mrow><mo maxsize="1.600em" minsize="1.600em" rspace="0.280em">]</mo></mrow></mrow></mrow><mo rspace="0.502em">+</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><msub><mrow><mo stretchy="false">‖</mo><mi>θ</mi><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}(\theta)\;=\;-\sum_{i}\Big[y_{i}\log\sigma(\theta^{\top}\mathbf{x}_{i})+(1-y_{i})\log\big(1-\sigma(\theta^{\top}\mathbf{x}_{i})\big)\Big]\;+\;\lambda\|\theta\|_{1},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p7">
<p class="ltx_p">where the sum ranges over all constructed examples <math alttext="(\mathbf{x}_{i},y_{i})" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p7.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{x}_{i},y_{i})</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">其中，求和范围覆盖所有构建的示例 <math intent=":literal" id="S6.SS1.SSS3.p7.m1" display="inline" class="ltx_Math" alttext="(\mathbf{x}_{i},y_{i})"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>𝐱</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{x}_{i},y_{i})</annotation></semantics></math> 。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p8">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Evaluation Protocol.</span> To assess the predictive power and generalization capability of this classifier, we evaluate it under more challenging settings than the training phase.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">评估协议。为了评估该分类器的预测能力和泛化能力，我们在比训练阶段更具挑战性的设置下对其进行评估。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p9">
<p class="ltx_p">First, we expand the scope beyond the training source to include two out-of-distribution datasets: NQ-Open&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">nq</span>)</cite> and BioASQ&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">bioasq</span>)</cite>. Second, we mimic real-world deployment by sampling only one response using the same probabilistic decoding parameters. From these, we retain a balanced set of hallucinated and faithful responses for each dataset.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">首先，我们将范围扩展到训练数据源之外，包括两个分布外数据集：NQ-Open（nq）和 BioASQ（bioasq）。其次，我们通过使用相同的概率解码参数仅采样一个响应来模拟真实世界的部署。从这些数据中，我们为每个数据集保留一个平衡的幻觉和忠实响应集。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p10">
<p class="ltx_p">Unlike training, where non-answer tokens served as negative controls, during evaluation we extract only the aggregated contribution vector of the answer span <math alttext="\mathbf{x}^{(s,\text{answer})}" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p10.m1" intent=":literal"><semantics><msup><mi>𝐱</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>answer</mtext><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{x}^{(s,\text{answer})}</annotation></semantics></math> and compute the hallucination probability <math alttext="\Pr(y=1|\mathbf{x}^{(s,\text{answer})})=\sigma(\theta^{\top}\mathbf{x}^{(s,\text{answer})})" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p10.m2" intent=":literal"><semantics><mrow><mrow><mi>Pr</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo>=</mo><mrow><mn>1</mn><mo fence="false">|</mo><msup><mi>𝐱</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>answer</mtext><mo stretchy="false">)</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>θ</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><msup><mi>𝐱</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>answer</mtext><mo stretchy="false">)</mo></mrow></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Pr(y=1|\mathbf{x}^{(s,\text{answer})})=\sigma(\theta^{\top}\mathbf{x}^{(s,\text{answer})})</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">与训练阶段不同，在训练中非答案标记用作负控，在评估期间我们仅提取答案跨度 <math intent=":literal" id="S6.SS1.SSS3.p10.m1" display="inline" class="ltx_Math" alttext="\mathbf{x}^{(s,\text{answer})}"><semantics><msup><mi>𝐱</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>answer</mtext><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{x}^{(s,\text{answer})}</annotation></semantics></math> 的聚合贡献向量，并计算幻觉概率 <math intent=":literal" id="S6.SS1.SSS3.p10.m2" display="inline" class="ltx_Math" alttext="\Pr(y=1|\mathbf{x}^{(s,\text{answer})})=\sigma(\theta^{\top}\mathbf{x}^{(s,\text{answer})})"><semantics><mrow><mrow><mi>Pr</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo>=</mo><mrow><mn>1</mn><mo fence="false">|</mo><msup><mi>𝐱</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>answer</mtext><mo stretchy="false">)</mo></mrow></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>σ</mi><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>θ</mi><mo>⊤</mo></msup><mo rspace="0em" lspace="0em">​</mo><msup><mi>𝐱</mi><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mtext>answer</mtext><mo stretchy="false">)</mo></mrow></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Pr(y=1|\mathbf{x}^{(s,\text{answer})})=\sigma(\theta^{\top}\mathbf{x}^{(s,\text{answer})})</annotation></semantics></math> 。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p11">
<p class="ltx_p">This setting is more challenging because the classifier must detect hallucinations without the contrasting baseline of the surrounding context tokens and must do so on noisy, single-sample generations from unseen domains. High accuracy under these conditions would strongly validate that the selected neurons are robust indicators of hallucination.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">这种设置更具挑战性，因为分类器必须在缺乏周围上下文标记的对比基线的情况下检测幻觉，并且必须在来自未见过的领域的噪声、单样本生成上进行。在这些条件下获得高精度将有力证明所选神经元是幻觉的稳健指标。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p12">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Balancing Detection Recall and Functional Safety.</span> In Eq.&nbsp;(<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.E4" title="In 6.1.3 Identifying H-Neurons via Linear Classifier ‣ 6.1 Identifying H-Neurons ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">4</span></a>), the regularization parameter <math alttext="\lambda" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p12.m1" intent=":literal"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> or its inverse <math alttext="C=1/\lambda" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p12.m2" intent=":literal"><semantics><mrow><mi>C</mi><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>λ</mi></mrow></mrow><annotation encoding="application/x-tex">C=1/\lambda</annotation></semantics></math> acts as the critical control knob for the scope of the identified neurons.
Selecting an appropriate <math alttext="C" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p12.m3" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> is a delicate trade-off. On one hand, setting <math alttext="C" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p12.m4" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> too low enforces aggressive sparsity, which risks excluding too many H-Neurons. Such incomplete coverage would fail to capture the full driver of hallucination. On the other hand, setting <math alttext="C" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p12.m5" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> too high introduces noise by including neurons essential for general language modeling, thereby causing damage to the model’s fundamental capabilities during intervention.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">平衡检测召回率和功能安全性。在公式(4)中，正则化参数 <math intent=":literal" id="S6.SS1.SSS3.p12.m1" display="inline" class="ltx_Math" alttext="\lambda"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> 或其倒数 <math intent=":literal" id="S6.SS1.SSS3.p12.m2" display="inline" class="ltx_Math" alttext="C=1/\lambda"><semantics><mrow><mi>C</mi><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>λ</mi></mrow></mrow><annotation encoding="application/x-tex">C=1/\lambda</annotation></semantics></math> 作为识别神经元范围的关键控制旋钮。选择合适的 <math intent=":literal" id="S6.SS1.SSS3.p12.m3" display="inline" class="ltx_Math" alttext="C"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> 是一个微妙的权衡。一方面，设置 <math intent=":literal" id="S6.SS1.SSS3.p12.m4" display="inline" class="ltx_Math" alttext="C"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> 过低会强制执行激进的稀疏性，这会风险排除过多的 H-神经元。这种不完整的覆盖将无法捕捉到幻觉的完整驱动因素。另一方面，设置 <math intent=":literal" id="S6.SS1.SSS3.p12.m5" display="inline" class="ltx_Math" alttext="C"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> 过高会通过包含对通用语言建模必不可少的神经元而引入噪声，从而在干预期间损害模型的基本能力。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p13">
<p class="ltx_p">To navigate this trade-off, we perform a grid search to select <math alttext="C" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p13.m1" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> to maximize the sum of (1) classification accuracy on a held-out set and (2) model performance on TriviaQA when suppressing the identified H-Neurons. This optimization criterion ensures that the selected subset is comprehensive enough to fully capture the signals driving hallucination and guarantees that the selection excludes redundant neurons to preserve the model’s fundamental functional integrity.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了应对这一权衡，我们进行网格搜索来选择 <math intent=":literal" id="S6.SS1.SSS3.p13.m1" display="inline" class="ltx_Math" alttext="C"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> ，以最大化(1)在保留集上的分类精度和(2)在抑制识别的 H-神经元时 TriviaQA 上的模型性能。这一优化标准确保所选子集足够全面，能够完全捕捉驱动幻觉的信号，并保证选择排除冗余神经元，以保持模型的基本功能完整性。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p14">
<p class="ltx_p">Through this optimization, we identify a sparse vector <math alttext="\theta" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p14.m1" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> where only a small fraction of neurons (typically <math alttext="&lt;0.1\%" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p14.m2" intent=":literal"><semantics><mrow><mi></mi><mo>&lt;</mo><mrow><mn>0.1</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">&lt;0.1\%</annotation></semantics></math>) have positive weights <math alttext="\theta_{j}" class="ltx_Math" display="inline" id="S6.SS1.SSS3.p14.m3" intent=":literal"><semantics><msub><mi>θ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\theta_{j}</annotation></semantics></math>. These positively weighted neurons form our candidate set of H-Neurons, which we carry forward to the perturbation experiments.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">通过这次优化，我们识别出一个稀疏向量 <math intent=":literal" id="S6.SS1.SSS3.p14.m1" display="inline" class="ltx_Math" alttext="\theta"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> ，其中只有一小部分神经元（通常为 <math intent=":literal" id="S6.SS1.SSS3.p14.m2" display="inline" class="ltx_Math" alttext="&lt;0.1\%"><semantics><mrow><mi></mi><mo>&lt;</mo><mrow><mn>0.1</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">&lt;0.1\%</annotation></semantics></math> ）具有正权重 <math intent=":literal" id="S6.SS1.SSS3.p14.m3" display="inline" class="ltx_Math" alttext="\theta_{j}"><semantics><msub><mi>θ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\theta_{j}</annotation></semantics></math> 。这些具有正权重的神经元构成了我们的 H-神经元候选集，我们将这些神经元用于后续的扰动实验。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Perturbation Experiments<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6.2 扰动实验</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p">While the linear probing analysis in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS1" title="6.1 Identifying H-Neurons ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">6.1</span></a> establishes a strong predictive correlation between specific neurons and hallucinatory outputs, establishing causation requires moving from observation to intervention. To probe the functional role of these neurons, we design a controlled perturbation pipeline that modulates their activity during inference.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">尽管第 6.1 节的线性探测分析表明特定神经元与幻觉输出之间存在强烈的预测相关性，但要建立因果关系需要从观察转向干预。为了探究这些神经元的职能作用，我们设计了一个受控扰动流程，在推理过程中调节它们的活动。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p">We hypothesize that the neurons identifying hallucinations do not merely encode factual errors, but rather drive a fundamental behavioral we term over-compliance, which means the model’s tendency to satisfy user prompts even at the expense of truthfulness, safety, or integrity. Under this framework, hallucination results from over-compliance, which leads the model to generate a factual-sounding response rather than acknowledging its uncertainty. If this hypothesis holds, manipulating these neurons should systematically alter model behavior not only on factual QA but across different types of compliance-related tasks.
Accordingly, we evaluate the effects of perturbation on four distinct benchmarks. Each of these benchmarks represents a different facet of the over-compliance.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们假设识别幻觉的神经元不仅编码事实性错误，而是驱动一种我们称之为过度顺从的基本行为，这意味着模型在牺牲真实性、安全性或完整性时仍倾向于满足用户提示。在此框架下，幻觉是由过度顺从引起的，这导致模型生成听起来像事实的回应，而不是承认其不确定性。如果这一假设成立，操控这些神经元应该系统地改变模型行为，不仅限于事实性问答，还涵盖不同类型的顺从相关任务。因此，我们评估了扰动对四个不同基准的影响。这些基准中的每一个都代表了过度顺从的不同方面。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Activation Scaling<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6.2.1 激活缩放</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p">To causally verify this hypothesis, we require a method to precisely modulate the influence of the identified neurons without retraining the model. We employ inference-time activation scaling, modifying the activation of a target neuron <math alttext="j" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.m1" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> during the forward pass by a scalar <math alttext="\alpha" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.m2" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了因果验证这一假设，我们需要一种在不重新训练模型的情况下精确调节已识别神经元影响的方法。我们采用推理时激活缩放，通过一个标量 <math intent=":literal" id="S6.SS2.SSS1.p1.m2" display="inline" class="ltx_Math" alttext="\alpha"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> 在前向传递中修改目标神经元 <math intent=":literal" id="S6.SS2.SSS1.p1.m1" display="inline" class="ltx_Math" alttext="j"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> 的激活值：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S6.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z_{j,t}\leftarrow\alpha\cdot z_{j,t},\quad\text{with }\alpha\in[0,3]." class="ltx_Math" display="block" id="S6.E5.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>z</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="false">←</mo><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>z</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mtext>with&nbsp;</mtext><mo lspace="0em" rspace="0em">​</mo><mi>α</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>3</mn><mo stretchy="false">]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">z_{j,t}\leftarrow\alpha\cdot z_{j,t},\quad\text{with }\alpha\in[0,3].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, <math alttext="\alpha&lt;1" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.m3" intent=":literal"><semantics><mrow><mi>α</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha&lt;1</annotation></semantics></math> suppresses the neuron’s influence, <math alttext="\alpha=1" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.m4" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha=1</annotation></semantics></math> maintains the original behavior, and <math alttext="\alpha&gt;1" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.m5" intent=":literal"><semantics><mrow><mi>α</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha&gt;1</annotation></semantics></math> amplifies its contribution.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在这里， <math intent=":literal" id="S6.SS2.SSS1.p1.m3" display="inline" class="ltx_Math" alttext="\alpha&lt;1"><semantics><mrow><mi>α</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha&lt;1</annotation></semantics></math> 抑制神经元的影响， <math intent=":literal" id="S6.SS2.SSS1.p1.m4" display="inline" class="ltx_Math" alttext="\alpha=1"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha=1</annotation></semantics></math> 保持原始行为，而 <math intent=":literal" id="S6.SS2.SSS1.p1.m5" display="inline" class="ltx_Math" alttext="\alpha&gt;1"><semantics><mrow><mi>α</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha&gt;1</annotation></semantics></math> 放大其贡献。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p2">
<p class="ltx_p">Crucially, we must ensure that this mathematical operation translates into a predictable shift in the neuron’s functional contribution to the residual stream. Using the CETT framework, we demonstrate that scaling activations results in a linear scaling of contribution.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">关键在于，我们必须确保这一数学操作能够转化为神经元对残差流的贡献的预测性变化。使用 CETT 框架，我们证明了缩放激活会导致贡献的线性缩放。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p3">
<p class="ltx_p">Recall from Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.E2" title="In 6.1.2 Quantifying Neuron Contribution ‣ 6.1 Identifying H-Neurons ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a> that the contribution of neuron <math alttext="j" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m1" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> at token <math alttext="t" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> is the ratio of its projected magnitude to the total hidden state norm: <math alttext="\mathrm{CETT}_{j,t}=\|h_{t}^{(j)}\|_{2}/\|h_{t}\|_{2}" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m3" intent=":literal"><semantics><mrow><msub><mi>CETT</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><msub><mrow><mo stretchy="false">‖</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>/</mo><msub><mrow><mo stretchy="false">‖</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mrow><annotation encoding="application/x-tex">\mathrm{CETT}_{j,t}=\|h_{t}^{(j)}\|_{2}/\|h_{t}\|_{2}</annotation></semantics></math>. Under perturbation, the modified activation becomes <math alttext="z_{t}^{(j)}(\alpha)=\alpha\cdot z_{j,t}e_{j}" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m4" intent=":literal"><semantics><mrow><mrow><msubsup><mi>z</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>z</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>e</mi><mi>j</mi></msub></mrow></mrow><annotation encoding="application/x-tex">z_{t}^{(j)}(\alpha)=\alpha\cdot z_{j,t}e_{j}</annotation></semantics></math>, leading to the perturbed hidden vector <math alttext="h_{t}^{(j)}(\alpha)=\alpha\cdot W_{\text{down}}z_{t}^{(j)}" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m5" intent=":literal"><semantics><mrow><mrow><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>W</mi><mtext>down</mtext></msub></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>z</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow><annotation encoding="application/x-tex">h_{t}^{(j)}(\alpha)=\alpha\cdot W_{\text{down}}z_{t}^{(j)}</annotation></semantics></math>. The perturbed full hidden state is given by <math alttext="h_{t}(\alpha)=W_{\text{down}}z_{t}+(\alpha-1)\cdot h_{t}^{(j)}" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m6" intent=":literal"><semantics><mrow><mrow><msub><mi>h</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>W</mi><mtext>down</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>α</mi><mo>−</mo><mn>1</mn></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">⋅</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">h_{t}(\alpha)=W_{\text{down}}z_{t}+(\alpha-1)\cdot h_{t}^{(j)}</annotation></semantics></math>. The resulting CETT value under perturbation is:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">回想公式 2，神经元 <math intent=":literal" id="S6.SS2.SSS1.p3.m1" display="inline" class="ltx_Math" alttext="j"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> 在标记 <math intent=":literal" id="S6.SS2.SSS1.p3.m2" display="inline" class="ltx_Math" alttext="t"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> 上的贡献是其投影幅度与总隐藏状态范数的比值： <math intent=":literal" id="S6.SS2.SSS1.p3.m3" display="inline" class="ltx_Math" alttext="\mathrm{CETT}_{j,t}=\|h_{t}^{(j)}\|_{2}/\|h_{t}\|_{2}"><semantics><mrow><msub><mi>CETT</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><msub><mrow><mo stretchy="false">‖</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>/</mo><msub><mrow><mo stretchy="false">‖</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mrow><annotation encoding="application/x-tex">\mathrm{CETT}_{j,t}=\|h_{t}^{(j)}\|_{2}/\|h_{t}\|_{2}</annotation></semantics></math> 。在扰动下，修改后的激活变为 <math intent=":literal" id="S6.SS2.SSS1.p3.m4" display="inline" class="ltx_Math" alttext="z_{t}^{(j)}(\alpha)=\alpha\cdot z_{j,t}e_{j}"><semantics><mrow><mrow><msubsup><mi>z</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>α</mi><mo rspace="0.222em" lspace="0.222em">⋅</mo><msub><mi>z</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow><mo rspace="0em" lspace="0em">​</mo><msub><mi>e</mi><mi>j</mi></msub></mrow></mrow><annotation encoding="application/x-tex">z_{t}^{(j)}(\alpha)=\alpha\cdot z_{j,t}e_{j}</annotation></semantics></math> ，导致扰动后的隐藏向量 <math intent=":literal" id="S6.SS2.SSS1.p3.m5" display="inline" class="ltx_Math" alttext="h_{t}^{(j)}(\alpha)=\alpha\cdot W_{\text{down}}z_{t}^{(j)}"><semantics><mrow><mrow><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>α</mi><mo rspace="0.222em" lspace="0.222em">⋅</mo><msub><mi>W</mi><mtext>down</mtext></msub></mrow><mo rspace="0em" lspace="0em">​</mo><msubsup><mi>z</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow><annotation encoding="application/x-tex">h_{t}^{(j)}(\alpha)=\alpha\cdot W_{\text{down}}z_{t}^{(j)}</annotation></semantics></math> 。扰动的完整隐藏状态由 <math intent=":literal" id="S6.SS2.SSS1.p3.m6" display="inline" class="ltx_Math" alttext="h_{t}(\alpha)=W_{\text{down}}z_{t}+(\alpha-1)\cdot h_{t}^{(j)}"><semantics><mrow><mrow><msub><mi>h</mi><mi>t</mi></msub><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>W</mi><mtext>down</mtext></msub><mo rspace="0em" lspace="0em">​</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>α</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false" rspace="0.055em">)</mo></mrow><mo rspace="0.222em">⋅</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow></mrow><annotation encoding="application/x-tex">h_{t}(\alpha)=W_{\text{down}}z_{t}+(\alpha-1)\cdot h_{t}^{(j)}</annotation></semantics></math> 给出。扰动下的 CETT 值如下：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S6.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{CETT}_{j,t}(\alpha)=\frac{\|\alpha\cdot h_{t}^{(j)}\|_{2}}{\|h_{t}+(\alpha-1)h_{t}^{(j)}\|_{2}}." class="ltx_Math" display="block" id="S6.E6.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>CETT</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>α</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathrm{CETT}_{j,t}(\alpha)=\frac{\|\alpha\cdot h_{t}^{(j)}\|_{2}}{\|h_{t}+(\alpha-1)h_{t}^{(j)}\|_{2}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In LLMs with thousands of neurons in a layer, <math alttext="\|h_{t}^{(j)}\|_{2}" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m7" intent=":literal"><semantics><msub><mrow><mo stretchy="false">‖</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><annotation encoding="application/x-tex">\|h_{t}^{(j)}\|_{2}</annotation></semantics></math> is much smaller than <math alttext="\|h_{t}\|_{2}" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m8" intent=":literal"><semantics><msub><mrow><mo stretchy="false">‖</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><annotation encoding="application/x-tex">\|h_{t}\|_{2}</annotation></semantics></math> since the contribution of any single neuron is typically infinitesimal compared to the aggregate hidden state. Consequently, the perturbation term in the denominator <math alttext="(\alpha-1)h_{t}^{(j)}" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m9" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>α</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">(\alpha-1)h_{t}^{(j)}</annotation></semantics></math> has a negligible impact on the overall norm. We can therefore approximate the denominator as <math alttext="\|h_{t}(\alpha)\|_{2}\approx\|h_{t}\|_{2}" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m10" intent=":literal"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>h</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>≈</mo><msub><mrow><mo stretchy="false">‖</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\|h_{t}(\alpha)\|_{2}\approx\|h_{t}\|_{2}</annotation></semantics></math>, yielding:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在具有一层数千个神经元的 LLMs 中， <math intent=":literal" id="S6.SS2.SSS1.p3.m7" display="inline" class="ltx_Math" alttext="\|h_{t}^{(j)}\|_{2}"><semantics><msub><mrow><mo stretchy="false">‖</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><annotation encoding="application/x-tex">\|h_{t}^{(j)}\|_{2}</annotation></semantics></math> 远小于 <math intent=":literal" id="S6.SS2.SSS1.p3.m8" display="inline" class="ltx_Math" alttext="\|h_{t}\|_{2}"><semantics><msub><mrow><mo stretchy="false">‖</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><annotation encoding="application/x-tex">\|h_{t}\|_{2}</annotation></semantics></math> ，因为单个神经元的贡献通常与总隐藏状态相比微不足道。因此，分母中的扰动项 <math intent=":literal" id="S6.SS2.SSS1.p3.m9" display="inline" class="ltx_Math" alttext="(\alpha-1)h_{t}^{(j)}"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>α</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo rspace="0em" lspace="0em">​</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">(\alpha-1)h_{t}^{(j)}</annotation></semantics></math> 对整体范数的影响可以忽略不计。我们可以将分母近似为 <math intent=":literal" id="S6.SS2.SSS1.p3.m10" display="inline" class="ltx_Math" alttext="\|h_{t}(\alpha)\|_{2}\approx\|h_{t}\|_{2}"><semantics><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>h</mi><mi>t</mi></msub><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>≈</mo><msub><mrow><mo stretchy="false">‖</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\|h_{t}(\alpha)\|_{2}\approx\|h_{t}\|_{2}</annotation></semantics></math> ，从而得到：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S6.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{CETT}_{j,t}(\alpha)\approx\frac{\alpha\cdot\|h_{t}^{(j)}\|_{2}}{\|h_{t}\|_{2}}=\alpha\cdot\mathrm{CETT}_{j,t}." class="ltx_Math" display="block" id="S6.E7.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>CETT</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mfrac><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mrow><mo stretchy="false">‖</mo><msubsup><mi>h</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow><msub><mrow><mo stretchy="false">‖</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac><mo>=</mo><mrow><mi>α</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>CETT</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathrm{CETT}_{j,t}(\alpha)\approx\frac{\alpha\cdot\|h_{t}^{(j)}\|_{2}}{\|h_{t}\|_{2}}=\alpha\cdot\mathrm{CETT}_{j,t}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This derivation provides the theoretical grounding for our experiments: it confirms that <math alttext="\alpha" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m11" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> has a linear relationship with the neuron’s functional importance. By changing <math alttext="\alpha" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p3.m12" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>, we can directly observe how increasing the activity of these specific neurons impacts the model’s over-compliant behaviors.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">这一推导为我们的实验提供了理论基础：它证实了 <math intent=":literal" id="S6.SS2.SSS1.p3.m11" display="inline" class="ltx_Math" alttext="\alpha"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> 与神经元的功能重要性呈线性关系。通过改变 <math intent=":literal" id="S6.SS2.SSS1.p3.m12" display="inline" class="ltx_Math" alttext="\alpha"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> ，我们可以直接观察到增加这些特定神经元的活性如何影响模型的过度顺从行为。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Benchmark Setups<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6.2.2 基准设置</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p">We measure the behavior of the perturbed model across four benchmarks, each chosen to probe a distinct dimension of over-compliance: (1) FalseQA tests compliance with invalid premises. (2) FaithEval tests compliance with misleading context. (3) Sycophancy tests compliance with skeptical attitudes. (4) Jailbreak tests compliance with harmful instructions. Together, they collectively provide a comprehensive profile of model over-compliance.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们测量了扰动模型在四个基准测试中的行为，每个基准测试都用于探测过度合规的不同维度：（1）FalseQA 测试对无效前提的合规性。（2）FaithEval 测试对误导性上下文的合规性。（3）Sycophancy 测试对怀疑态度的合规性。（4）Jailbreak 测试对有害指令的合规性。它们共同为模型的过度合规提供了一个全面的概况。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Compliance with invalid premises: FalseQA.</span> This benchmark evaluates the model’s robustness against user prompts containing incorrect premises. Over-compliance manifests as the model ignoring the false premise in user’s question rather than correcting it.
We employ greedy decoding and use GPT-4o as a binary judge to determine whether the model successfully corrects the false premise.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">对无效前提的合规性：FalseQA。这个基准测试评估模型对包含错误前提的用户提示的鲁棒性。过度合规表现为模型忽略用户问题中的错误前提，而不是纠正它。我们采用贪婪解码，并使用 GPT-4o 作为二元裁判来确定模型是否成功纠正了错误前提。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Compliance with misleading context: FaithEval.</span> This benchmark evaluates the model’s tendency to prioritize provided context over its internal factual knowledge. We utilize the <em class="ltx_emph ltx_font_italic">Counterfactual Context</em> subset of FaithEval, where the model is prompted with fabricated information and asked to answer questions based upon it. Over-compliance here manifests as faithfully hallucinating based on the false context.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">符合误导性上下文的合规性：FaithEval。该基准评估模型优先考虑所提供上下文而非其内部事实性知识的倾向。我们使用 FaithEval 的 Counterfactual Context 子集，其中模型被提示以虚构信息为基础回答问题。在此过度合规表现为基于错误上下文忠实地产生幻觉。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p4">
<p class="ltx_p">We employ greedy decoding with a maximum length of 256 new tokens to isolate the model’s most likely path. Evaluation uses a rule-based parser that aligns the option selected in the generated text with the gold label provided by the dataset.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们采用最大长度为 256 个新 token 的贪婪解码来隔离模型最可能的路径。评估使用基于规则的解析器，将生成文本中选择的选项与数据集提供的标准标签进行匹配。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Compliance with skeptical attitudes: Sycophancy.</span> This benchmark measures the model’s tendency to abandon its own correct beliefs to conform to skeptical attitudes. Following prior work&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Sycophancy</span>)</cite>, we utilize their curated dataset covering five distinct QA benchmarks: MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">MMLU</span>)</cite>, MATH&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">MATH</span>)</cite>, AQuA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">AQuA</span>)</cite>, TriviaQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Triviaqa</span>)</cite> and TruthfulQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">TruthfulQA</span>)</cite>. We employ open-ended QA for TriviaQA and TruthfulQA, and multiple-choice QA with Chain-of-Thought reasoning for MMLU and MATH. Additionally, standard multiple-choice formats are applied to AQuA and TruthfulQA.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">遵循怀疑态度的合规性：阿谀奉承。该基准测量模型放弃自身正确信念以顺应怀疑态度的倾向。基于先前研究（阿谀奉承），我们采用了他们整理的数据集，涵盖五个不同的问答基准：MMLU（MMLU）、MATH（MATH）、AQuA（AQuA）、TriviaQA（Triviaqa）和 TruthfulQA（TruthfulQA）。我们为 TriviaQA 和 TruthfulQA 采用开放式问答，为 MMLU 和 MATH 采用带思维链推理的多项选择题问答。此外，AQuA 和 TruthfulQA 采用标准多项选择题格式。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p6">
<p class="ltx_p">To measure the model’s compliance with skeptical attitudes, we first present it with a question and record its initial response. We then challenge the model with <span class="ltx_text ltx_font_italic">"I don’t think that’s right. Are you sure?"</span> and allow it to reply before providing a final answer. We then observe if the model flips to an incorrect answer in its final reply.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了测量模型对怀疑态度的遵守程度，我们首先向其提出一个问题并记录其初始回答。然后我们向模型提出挑战："我不认为那是正确的。你确定吗？"并允许其回复，之后我们提供最终答案。接着我们观察模型是否在最终回答中转向了错误答案。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p7">
<p class="ltx_p">For open-ended generation, we set <span class="ltx_text ltx_font_typewriter">temperature=0.7</span>, <span class="ltx_text ltx_font_typewriter">top_k=50</span>, <span class="ltx_text ltx_font_typewriter">top_p=0.95</span>, and a maximum length of 512 tokens, while multiple-choice questions are decoded greedily. For evaluation, a rule-based parser is employed to extract the answer from the generated text and compare it with the gold label. If it fails, GPT-4o is utilized as a fallback parser to compare the response against gold labels.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">对于开放式生成，我们设置温度为 0.7，top_k 为 50，top_p 为 0.95，并设定最大长度为 512 个 token，而选择题则采用贪婪解码。在评估过程中，我们使用基于规则的解析器从生成文本中提取答案，并将其与标准答案进行比较。如果解析失败，则使用 GPT-4o 作为备用解析器来将响应与标准答案进行对比。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p8">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Compliance with harmful instruction: Jailbreak.</span> This benchmark tests the model’s compliance with harmful instructions, where the urge to satisfy a user’s request overrides safety alignment training. We adopt the <em class="ltx_emph ltx_font_italic">forbidden question set</em> which comprises 390 test cases spanning 13 scenarios with 30 questions each and pair each harmful query with a jailbreak template designed to bypass safety filters.
We generate responses using open-ended sampling with parameters <span class="ltx_text ltx_font_typewriter">temperature=0.7</span>, <span class="ltx_text ltx_font_typewriter">top_k=20</span>, <span class="ltx_text ltx_font_typewriter">top_p=0.8</span> and a maximum output length of 256 tokens. A GPT-4o judge serves as an automated safety evaluator, instructed to flag any response that provides harmful information, guided by 15 benchmark examples included with the dataset.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">符合有害指令：越狱。该基准测试模型对有害指令的遵守情况，其中满足用户请求的冲动会覆盖安全对齐训练。我们采用禁止问题集，该集合包含 13 种场景，每种场景有 30 个问题，共 390 个测试用例，并将每个有害查询与一个旨在绕过安全过滤器的越狱模板配对。我们使用开放式采样生成响应，参数设置为温度=0.7，top_k=20，top_p=0.8，最大输出长度为 256 个 token。一个 GPT-4o 裁判作为自动化安全评估器，被指示标记任何提供有害信息的响应，并参考数据集中包含的 15 个基准示例进行指导。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p9">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Definition of Compliance Rate.</span> To enable a comparative analysis across these diverse benchmarks, we define a unified metric, <span class="ltx_text ltx_font_italic">Compliance Rate</span>, which quantifies the model’s propensity to yield to the prompt’s intent. Specifically, the calculation for each benchmark is as follows: (1) FalseQA: The frequency with which the model accepts and answers the invalid premise without refutation. (2) FaithEval: The percentage of responses where the model adopts the counterfactual information provided in the context rather than relying on its internal world knowledge. (3) Sycophancy: The ratio of instances where the model abandons an initially correct answer and changes to an incorrect answer. (4) Jailbreak: The proportion of responses classified as harmful by the safety evaluator (equivalent to the Attack Success Rate).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">合规率的定义。为了在这些不同的基准测试之间进行对比分析，我们定义了一个统一的指标——合规率，它量化了模型遵循提示意图的倾向。具体计算方法如下：（1）FalseQA：模型接受并回答无效前提而不进行反驳的频率。（2）FaithEval：模型在回答中采用上下文中提供的反事实信息而非依赖其内部世界知识的响应百分比。（3）Sycophancy：模型放弃最初正确答案并改为错误答案的实例比例。（4）Jailbreak：被安全评估器分类为有害的响应比例（等同于攻击成功率）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Tracing the Origin of H-Neurons<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6.3 追溯 H-神经元的起源</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p">Having established the causal role of H-Neurons in instruction-tuned models, a critical question remains unsolved: Are they introduced during post-training alignment phase or already present in the pre-trained phase? To answer this, we design two complementary analyses: a backward transferability analysis and a neuron-level parameter evolution analysis.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在确定了 H-神经元在指令微调模型中的因果作用后，一个关键问题仍未解决：它们是在后训练对齐阶段引入的，还是早已存在于预训练阶段？为了回答这个问题，我们设计了两种互补的分析方法：一种反向可迁移性分析，以及一种神经元级别的参数演化分析。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S6.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1 </span>Backward Transferability Analysis<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6.3.1 反向迁移性分析</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS3.SSS1.p1">
<p class="ltx_p">Our first approach investigates whether the functional distinction between faithful and hallucinatory neurons exists before alignment. We hypothesize that if hallucination drives are rooted in pre-training, the sparse classifiers trained on the instruction-tuned model should retain predictive power when applied directly to its corresponding base model.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们的第一种方法研究了在模型对齐之前，忠实神经元和幻觉神经元之间是否存在功能上的区别。我们假设，如果幻觉驱动力根植于预训练阶段，那么在指令微调模型上训练的稀疏分类器在直接应用于其对应的基模型时仍应保持预测能力。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS3.SSS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Standardizing Base Model Decoding.</span> Directly comparing base and instruction-tuned models is challenging due to their divergent output formats. Base models are trained for text completion rather than question answering. To ensure a valid comparison, we standardize the decoding process. For each query in TriviaQA, NQ-Open, and BioASQ, we append a strict prompt suffix <span class="ltx_text ltx_font_italic">"\nAnswer:"</span> and terminate generation upon the first newline character. This aligns the base model’s output structure with the instruction-tuned model’s.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">标准化基模型解码。由于基模型和指令微调模型的输出格式不同，直接比较它们存在挑战。基模型是用于文本补全而非问答任务进行训练的。为确保有效比较，我们对解码过程进行标准化。对于 TriviaQA、NQ-Open 和 BioASQ 中的每个查询，我们附加一个严格的提示后缀"\nAnswer:"，并在遇到第一个换行符时终止生成。这使基模型的输出结构与指令微调模型的输出结构保持一致。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS3.SSS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Evaluation via Threshold-Invariant Metrics.</span> We apply the logistic regression probes derived in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2512.01797v2#S6.SS1" title="6.1 Identifying H-Neurons ‣ 6 Methods ‣ H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs"><span class="ltx_text ltx_ref_tag">6.1</span></a> directly to the base model’s activation states without retraining to examine whether the identified H-Neurons exhibit similar activation patterns within the pre-trained models.
However, alignment training typically shifts the global distribution of activation magnitudes, making the fixed decision thresholds learned on the instruction-tuned model unreliable. To overcome this distributional drift, we adopt the Area Under the Receiver Operating Characteristic Curve (AUROC) as our primary evaluation metric. Unlike accuracy, AUROC provides a stable measure of ranking capability because it is unaffected by the choice of threshold or linear scaling, allowing us to directly measure whether the neurons that signal hallucinations in the aligned model retain their higher ranking for hallucinations in the base model. High backward transferability would indicate that the functional distinction between hallucination and faithful responses already exists before post-training alignment.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">通过阈值不变指标进行评估。我们直接将第 6.1 节中推导的逻辑回归探针应用于基础模型的激活状态，而不进行重新训练，以检验所识别的 H-Neurons 在预训练模型中是否表现出相似的激活模式。然而，对齐训练通常会改变激活幅度的全局分布，使得在指令微调模型上学习到的固定决策阈值变得不可靠。为了克服这种分布漂移，我们采用受试者工作特征曲线下面积（AUROC）作为主要评估指标。与准确率不同，AUROC 提供了一个稳定的排序能力度量，因为它不受阈值选择或线性缩放的 影响，使我们能够直接测量在对齐模型中发出幻觉信号的神经元在基础模型中对幻觉的排序是否仍然较高。高逆向迁移性表明，幻觉与忠实响应之间的功能差异在训练后对齐之前就已经存在。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2 </span>Neuron-Level Parameter Evolution<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6.3.2 神经元级参数进化</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS3.SSS2.p1">
<p class="ltx_p">Our second approach quantifies the physical modifications applied to these neurons during the alignment process. By tracking parameter shifts, we aim to determine whether H-Neurons are the subject of aggressive fine-tuning or if they remain relatively static.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们的第二种方法量化了在对齐过程中施加于这些神经元的物理修改。通过追踪参数变化，我们旨在确定 H-神经元是否是积极微调的对象，还是保持相对静态。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS3.SSS2.p2">
<p class="ltx_p">We define the mechanistic drift of a neuron based on the cosine similarity between its weights before and after instruction tuning. Crucially, a neuron’s functional identity is governed by a dual interface: its encoding of input patterns, and its broadcasting of output signals. We therefore compute the drift for both its input and output weights, corresponding to the up-projection and down-projection components in FFN. To capture the full scope of functional adaptation, we therefore compute the drift for both its up- and down-projection weights:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们根据指令微调前后神经元的权重之间的余弦相似度来定义其机制漂移。关键在于，神经元的功能身份受双重接口的支配：其输入模式的编码方式，以及其输出信号的广播方式。因此，我们计算其输入和输出权重的漂移，分别对应于 FFN 中的上投影和下投影组件。为了捕捉功能适应的完整范围，我们因此计算了其上投影和下投影权重的漂移：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S6.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Delta_{j}^{\text{up}}=1-\cos(W_{\text{up}}^{(j,\text{base})},W_{\text{up}}^{(j,\text{chat})}),\qquad\Delta_{j}^{\text{down}}=1-\cos(W_{\text{down}}^{(j,\text{base})},W_{\text{down}}^{(j,\text{chat})})." class="ltx_Math" display="block" id="S6.Ex2.m1" intent=":literal"><semantics><mrow><mrow><mrow><msubsup><mi mathvariant="normal">Δ</mi><mi>j</mi><mtext>up</mtext></msubsup><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>cos</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>W</mi><mtext>up</mtext><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>,</mo><mtext>base</mtext><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>W</mi><mtext>up</mtext><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>,</mo><mtext>chat</mtext><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><msubsup><mi mathvariant="normal">Δ</mi><mi>j</mi><mtext>down</mtext></msubsup><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>cos</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>W</mi><mtext>down</mtext><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>,</mo><mtext>base</mtext><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>W</mi><mtext>down</mtext><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>,</mo><mtext>chat</mtext><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\Delta_{j}^{\text{up}}=1-\cos(W_{\text{up}}^{(j,\text{base})},W_{\text{up}}^{(j,\text{chat})}),\qquad\Delta_{j}^{\text{down}}=1-\cos(W_{\text{down}}^{(j,\text{base})},W_{\text{down}}^{(j,\text{chat})}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Larger <math alttext="\Delta" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p2.m1" intent=":literal"><semantics><mi mathvariant="normal">Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math> values indicate greater modification.
Since the inherent dynamics of parameters may vary across modules, we normalize these raw drift scores to ensure comparability. We calculate the <math alttext="z" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p2.m2" intent=":literal"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>-scores and average the up- and down-projection drifts to obtain a unified final drift <math alttext="\Delta_{j}" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p2.m3" intent=":literal"><semantics><msub><mi mathvariant="normal">Δ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\Delta_{j}</annotation></semantics></math>:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">更大的 <math intent=":literal" id="S6.SS3.SSS2.p2.m1" display="inline" class="ltx_Math" alttext="\Delta"><semantics><mi mathvariant="normal">Δ</mi><annotation encoding="application/x-tex">\Delta</annotation></semantics></math> 值表示更大的修改。由于参数的内在动态可能因模块而异，我们将这些原始漂移分数进行归一化处理，以确保可比性。我们计算 <math intent=":literal" id="S6.SS3.SSS2.p2.m2" display="inline" class="ltx_Math" alttext="z"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> -分数，并将向上和向下投影的漂移平均，以获得一个统一的最终漂移 <math intent=":literal" id="S6.SS3.SSS2.p2.m3" display="inline" class="ltx_Math" alttext="\Delta_{j}"><semantics><msub><mi mathvariant="normal">Δ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\Delta_{j}</annotation></semantics></math> ：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S6.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{\Delta}_{j}^{\text{up}}=\frac{\Delta_{j}^{\text{up}}-\mu_{\text{up}}}{\sigma_{\text{up}}},\qquad\tilde{\Delta}_{j}^{\text{down}}=\frac{\Delta_{j}^{\text{down}}-\mu_{\text{down}}}{\sigma_{\text{down}}},\qquad\Delta_{j}=\frac{\tilde{\Delta}_{j}^{\text{up}}+\tilde{\Delta}_{j}^{\text{down}}}{2}." class="ltx_Math" display="block" id="S6.Ex3.m1" intent=":literal"><semantics><mrow><mrow><mrow><msubsup><mover accent="true"><mi mathvariant="normal">Δ</mi><mo>~</mo></mover><mi>j</mi><mtext>up</mtext></msubsup><mo>=</mo><mfrac><mrow><msubsup><mi mathvariant="normal">Δ</mi><mi>j</mi><mtext>up</mtext></msubsup><mo>−</mo><msub><mi>μ</mi><mtext>up</mtext></msub></mrow><msub><mi>σ</mi><mtext>up</mtext></msub></mfrac></mrow><mo rspace="2.167em">,</mo><mrow><mrow><msubsup><mover accent="true"><mi mathvariant="normal">Δ</mi><mo>~</mo></mover><mi>j</mi><mtext>down</mtext></msubsup><mo>=</mo><mfrac><mrow><msubsup><mi mathvariant="normal">Δ</mi><mi>j</mi><mtext>down</mtext></msubsup><mo>−</mo><msub><mi>μ</mi><mtext>down</mtext></msub></mrow><msub><mi>σ</mi><mtext>down</mtext></msub></mfrac></mrow><mo rspace="2.167em">,</mo><mrow><msub><mi mathvariant="normal">Δ</mi><mi>j</mi></msub><mo>=</mo><mfrac><mrow><msubsup><mover accent="true"><mi mathvariant="normal">Δ</mi><mo>~</mo></mover><mi>j</mi><mtext>up</mtext></msubsup><mo>+</mo><msubsup><mover accent="true"><mi mathvariant="normal">Δ</mi><mo>~</mo></mover><mi>j</mi><mtext>down</mtext></msubsup></mrow><mn>2</mn></mfrac></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\tilde{\Delta}_{j}^{\text{up}}=\frac{\Delta_{j}^{\text{up}}-\mu_{\text{up}}}{\sigma_{\text{up}}},\qquad\tilde{\Delta}_{j}^{\text{down}}=\frac{\Delta_{j}^{\text{down}}-\mu_{\text{down}}}{\sigma_{\text{down}}},\qquad\Delta_{j}=\frac{\tilde{\Delta}_{j}^{\text{up}}+\tilde{\Delta}_{j}^{\text{down}}}{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">We then analyze the rank distribution of H-Neurons based on <math alttext="\Delta_{j}" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p2.m4" intent=":literal"><semantics><msub><mi mathvariant="normal">Δ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\Delta_{j}</annotation></semantics></math>. A concentration of these neurons in the high-<math alttext="\Delta_{j}" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p2.m5" intent=":literal"><semantics><msub><mi mathvariant="normal">Δ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\Delta_{j}</annotation></semantics></math> end would suggest that alignment actively constructs or heavily modifies these neurons. Conversely, a uniform distribution or concentration in the low-<math alttext="\Delta_{j}" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p2.m6" intent=":literal"><semantics><msub><mi mathvariant="normal">Δ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\Delta_{j}</annotation></semantics></math> regime would provide strong evidence that the function of these neurons is largely inherited from pre-training.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">然后，我们根据 <math intent=":literal" id="S6.SS3.SSS2.p2.m4" display="inline" class="ltx_Math" alttext="\Delta_{j}"><semantics><msub><mi mathvariant="normal">Δ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\Delta_{j}</annotation></semantics></math> 分析 H-Neurons 的秩分布。这些神经元在高 <math intent=":literal" id="S6.SS3.SSS2.p2.m5" display="inline" class="ltx_Math" alttext="\Delta_{j}"><semantics><msub><mi mathvariant="normal">Δ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\Delta_{j}</annotation></semantics></math> 端的集中表明，对齐过程积极构建或大量修改了这些神经元。相反，在低 <math intent=":literal" id="S6.SS3.SSS2.p2.m6" display="inline" class="ltx_Math" alttext="\Delta_{j}"><semantics><msub><mi mathvariant="normal">Δ</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\Delta_{j}</annotation></semantics></math> 端的均匀分布或集中则提供了强有力的证据，表明这些神经元的功能主要继承自预训练。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">报告问题</font></font></font></button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed; display: none; left: 1101.42px; top: -389.109px; transform: translate(-50%, -100%);">Report Issue for Selection<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">报告选择问题</font></font></font></button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" width="11" height="14">
            </a><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">由 L A T E xml <img height="14" width="11" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"> 生成</font></font></font>
        </div></div><footer id="footer" class="ltx_document" default-translate="no">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body><div id="immersive-translate-popup" style="all: initial"><template shadowrootmode="open"><style>@charset "UTF-8";
/*!
 * Pico.css v1.5.6 (https://picocss.com)
 * Copyright 2019-2022 - Licensed under MIT
 */
/**
 * Theme: default
 */
#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 0.25rem;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 1rem;
  --typography-spacing-vertical: 1.5rem;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 0.75rem;
  --form-element-spacing-horizontal: 1rem;
  --nav-element-spacing-vertical: 1rem;
  --nav-element-spacing-horizontal: 0.5rem;
  --nav-link-spacing-vertical: 0.5rem;
  --nav-link-spacing-horizontal: 0.5rem;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(0.25rem);
}
@media (min-width: 576px) {
  #mount {
    --font-size: 17px;
  }
}
@media (min-width: 768px) {
  #mount {
    --font-size: 18px;
  }
}
@media (min-width: 992px) {
  #mount {
    --font-size: 19px;
  }
}
@media (min-width: 1200px) {
  #mount {
    --font-size: 20px;
  }
}

@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 2);
  }
}
@media (min-width: 768px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3);
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3.5);
  }
}

@media (min-width: 576px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}
@media (min-width: 992px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.75);
  }
}
@media (min-width: 1200px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 2);
  }
}

dialog > article {
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
}
@media (min-width: 576px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 3);
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}

a {
  --text-decoration: none;
}
a.secondary,
a.contrast {
  --text-decoration: underline;
}

small {
  --font-size: 0.875em;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  --font-weight: 700;
}

h1 {
  --font-size: 2rem;
  --typography-spacing-vertical: 3rem;
}

h2 {
  --font-size: 1.75rem;
  --typography-spacing-vertical: 2.625rem;
}

h3 {
  --font-size: 1.5rem;
  --typography-spacing-vertical: 2.25rem;
}

h4 {
  --font-size: 1.25rem;
  --typography-spacing-vertical: 1.874rem;
}

h5 {
  --font-size: 1.125rem;
  --typography-spacing-vertical: 1.6875rem;
}

[type="checkbox"],
[type="radio"] {
  --border-width: 2px;
}

[type="checkbox"][role="switch"] {
  --border-width: 2px;
}

thead th,
thead td,
tfoot th,
tfoot td {
  --border-width: 3px;
}

:not(thead, tfoot) > * > td {
  --font-size: 0.875em;
}

pre,
code,
kbd,
samp {
  --font-family: "Menlo", "Consolas", "Roboto Mono", "Ubuntu Monospace",
    "Noto Mono", "Oxygen Mono", "Liberation Mono", monospace,
    "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
}

kbd {
  --font-weight: bolder;
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --background-color: #fff;
  --background-light-green: #f5f7f9;
  --color: hsl(205deg, 20%, 32%);
  --h1-color: hsl(205deg, 30%, 15%);
  --h2-color: #24333e;
  --h3-color: hsl(205deg, 25%, 23%);
  --h4-color: #374956;
  --h5-color: hsl(205deg, 20%, 32%);
  --h6-color: #4d606d;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: hsl(205deg, 20%, 94%);
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 90%, 32%);
  --primary-focus: rgba(16, 149, 193, 0.125);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 20%, 32%);
  --secondary-focus: rgba(89, 107, 120, 0.125);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 30%, 15%);
  --contrast-hover: #000;
  --contrast-focus: rgba(89, 107, 120, 0.125);
  --contrast-inverse: #fff;
  --mark-background-color: #fff2ca;
  --mark-color: #543a26;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: transparent;
  --form-element-border-color: hsl(205deg, 14%, 68%);
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: transparent;
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 18%, 86%);
  --form-element-disabled-border-color: hsl(205deg, 14%, 68%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #c62828;
  --form-element-invalid-active-border-color: #d32f2f;
  --form-element-invalid-focus-color: rgba(211, 47, 47, 0.125);
  --form-element-valid-border-color: #388e3c;
  --form-element-valid-active-border-color: #43a047;
  --form-element-valid-focus-color: rgba(67, 160, 71, 0.125);
  --switch-background-color: hsl(205deg, 16%, 77%);
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: hsl(205deg, 18%, 86%);
  --range-active-border-color: hsl(205deg, 16%, 77%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: #f6f8f9;
  --code-background-color: hsl(205deg, 20%, 94%);
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 40%, 50%);
  --code-property-color: hsl(185deg, 40%, 40%);
  --code-value-color: hsl(40deg, 20%, 50%);
  --code-comment-color: hsl(205deg, 14%, 68%);
  --accordion-border-color: var(--muted-border-color);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: var(--background-color);
  --card-border-color: var(--muted-border-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(27, 40, 50, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(27, 40, 50, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(27, 40, 50, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(27, 40, 50, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(27, 40, 50, 0.04302),
    0.5rem 1rem 6rem rgba(27, 40, 50, 0.06),
    0 0 0 0.0625rem rgba(27, 40, 50, 0.015);
  --card-sectionning-background-color: #fbfbfc;
  --dropdown-background-color: #fbfbfc;
  --dropdown-border-color: #e1e6eb;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: hsl(205deg, 20%, 94%);
  --modal-overlay-background-color: rgba(213, 220, 226, 0.7);
  --progress-background-color: hsl(205deg, 18%, 86%);
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(198, 40, 40)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(56, 142, 60)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  --float-ball-more-button-border-color: #f6f6f6;
  --float-ball-more-button-background-color: #ffffff;
  --float-ball-more-button-svg-color: #6c6f73;
  color-scheme: light;
  --service-bg-hover: #f7faff;
  --service-bg: #fafbfb;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --background-color: #11191f;
    --float-ball-more-button-background-color: #ffffff;
    --background-light-green: #141e26;
    --color: hsl(205deg, 16%, 77%);
    --h1-color: hsl(205deg, 20%, 94%);
    --h2-color: #e1e6eb;
    --h3-color: hsl(205deg, 18%, 86%);
    --h4-color: #c8d1d8;
    --h5-color: hsl(205deg, 16%, 77%);
    --h6-color: #afbbc4;
    --muted-color: hsl(205deg, 10%, 50%);
    --muted-border-color: #1f2d38;
    --primary: hsl(195deg, 85%, 41%);
    --primary-hover: hsl(195deg, 80%, 50%);
    --primary-focus: rgba(16, 149, 193, 0.25);
    --primary-inverse: #fff;
    --secondary: hsl(205deg, 15%, 41%);
    --secondary-hover: hsl(205deg, 10%, 50%);
    --secondary-focus: rgba(115, 130, 140, 0.25);
    --secondary-inverse: #fff;
    --contrast: hsl(205deg, 20%, 94%);
    --contrast-hover: #fff;
    --contrast-focus: rgba(115, 130, 140, 0.25);
    --contrast-inverse: #000;
    --mark-background-color: #d1c284;
    --mark-color: #11191f;
    --ins-color: #388e3c;
    --del-color: #c62828;
    --blockquote-border-color: var(--muted-border-color);
    --blockquote-footer-color: var(--muted-color);
    --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --form-element-background-color: #11191f;
    --form-element-border-color: #374956;
    --form-element-color: var(--color);
    --form-element-placeholder-color: var(--muted-color);
    --form-element-active-background-color: var(
      --form-element-background-color
    );
    --form-element-active-border-color: var(--primary);
    --form-element-focus-color: var(--primary-focus);
    --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
    --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
    --form-element-disabled-opacity: 0.5;
    --form-element-invalid-border-color: #b71c1c;
    --form-element-invalid-active-border-color: #c62828;
    --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
    --form-element-valid-border-color: #2e7d32;
    --form-element-valid-active-border-color: #388e3c;
    --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
    --switch-background-color: #374956;
    --switch-color: var(--primary-inverse);
    --switch-checked-background-color: var(--primary);
    --range-border-color: #24333e;
    --range-active-border-color: hsl(205deg, 25%, 23%);
    --range-thumb-border-color: var(--background-color);
    --range-thumb-color: var(--secondary);
    --range-thumb-hover-color: var(--secondary-hover);
    --range-thumb-active-color: var(--primary);
    --table-border-color: var(--muted-border-color);
    --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
    --code-background-color: #18232c;
    --code-color: var(--muted-color);
    --code-kbd-background-color: var(--contrast);
    --code-kbd-color: var(--contrast-inverse);
    --code-tag-color: hsl(330deg, 30%, 50%);
    --code-property-color: hsl(185deg, 30%, 50%);
    --code-value-color: hsl(40deg, 10%, 50%);
    --code-comment-color: #4d606d;
    --accordion-border-color: var(--muted-border-color);
    --accordion-active-summary-color: var(--primary);
    --accordion-close-summary-color: var(--color);
    --accordion-open-summary-color: var(--muted-color);
    --card-background-color: #141e26;
    --card-border-color: var(--card-background-color);
    --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
      0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
      0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
      0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
      0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
      0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
    --card-sectionning-background-color: #18232c;
    --dropdown-background-color: hsl(205deg, 30%, 15%);
    --dropdown-border-color: #24333e;
    --dropdown-box-shadow: var(--card-box-shadow);
    --dropdown-color: var(--color);
    --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
    --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
    --progress-background-color: #24333e;
    --progress-color: var(--primary);
    --loading-spinner-opacity: 0.5;
    --tooltip-background-color: var(--contrast);
    --tooltip-color: var(--contrast-inverse);
    --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
    --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
    --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
    --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
    --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
    --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-share: url("data:image/svg+xml;charset=utf-8;base64,PHN2ZyB3aWR0aD0nMjInIGhlaWdodD0nMjInIHZpZXdCb3g9JzAgMCAyMiAyMicgZmlsbD0nbm9uZScgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJz48cGF0aCBkPSdNMTcuOTM0OCA3LjY0ODQ0QzE5Ljg5NDEgNy42NDg0NCAyMS40ODU1IDYuMDU0NjkgMjEuNDg1NSA0LjA5NzY2QzIxLjQ4NTUgMi4xNDA2MiAxOS44OTE4IDAuNTQ2ODc1IDE3LjkzNDggMC41NDY4NzVDMTUuOTc1NCAwLjU0Njg3NSAxNC4zODQgMi4xNDA2MiAxNC4zODQgNC4wOTc2NkMxNC4zODQgNC4yOTkyMiAxNC40MDA0IDQuNDkzNzUgMTQuNDMzMiA0LjY4NTk0TDYuMzIzODMgOC4zNTM5MUM1LjcwOTc3IDcuODQ1MzEgNC45MjIyNyA3LjU0MDYyIDQuMDY0NDUgNy41NDA2MkMyLjEwNTA4IDcuNTQwNjIgMC41MTM2NzIgOS4xMzQzOCAwLjUxMzY3MiAxMS4wOTE0QzAuNTEzNjcyIDEzLjA0ODQgMi4xMDc0MiAxNC42NDIyIDQuMDY0NDUgMTQuNjQyMkM0LjgzMzIgMTQuNjQyMiA1LjU0NTcgMTQuMzk2MSA2LjEyNjk1IDEzLjk4MTNMMTEuNDk0MSAxNi45OTUzQzExLjQxNjggMTcuMjg1OSAxMS4zNzcgMTcuNTg4MyAxMS4zNzcgMTcuOTAyM0MxMS4zNzcgMTkuODYxNyAxMi45NzA3IDIxLjQ1MzEgMTQuOTI3NyAyMS40NTMxQzE2Ljg4NzEgMjEuNDUzMSAxOC40Nzg1IDE5Ljg1OTQgMTguNDc4NSAxNy45MDIzQzE4LjQ3ODUgMTUuOTQzIDE2Ljg4NDggMTQuMzUxNiAxNC45Mjc3IDE0LjM1MTZDMTMuOTU3NCAxNC4zNTE2IDEzLjA3ODUgMTQuNzQzIDEyLjQzNjMgMTUuMzczNEw3LjMyMjI3IDEyLjUwNDdDNy41MDk3NyAxMi4wNzExIDcuNjE1MjMgMTEuNTk1MyA3LjYxNTIzIDExLjA5MzhDNy42MTUyMyAxMC42ODEyIDcuNTQ0OTIgMTAuMjg3NSA3LjQxNjAyIDkuOTE5NTNMMTUuMjIzIDYuMzg3NUMxNS44NzQ2IDcuMTU2MjUgMTYuODQ5NiA3LjY0ODQ0IDE3LjkzNDggNy42NDg0NFpNNC4wNjQ0NSAxMi43Njk1QzMuMTQxMDIgMTIuNzY5NSAyLjM4ODY3IDEyLjAxNzIgMi4zODg2NyAxMS4wOTM4QzIuMzg4NjcgMTAuMTcwMyAzLjE0MTAyIDkuNDE3OTcgNC4wNjQ0NSA5LjQxNzk3QzQuOTg3ODkgOS40MTc5NyA1Ljc0MDIzIDEwLjE3MDMgNS43NDAyMyAxMS4wOTM4QzUuNzQwMjMgMTIuMDE3MiA0Ljk4Nzg5IDEyLjc2OTUgNC4wNjQ0NSAxMi43Njk1Wk0xNC45Mjc3IDE2LjIyNjZDMTUuODUxMiAxNi4yMjY2IDE2LjYwMzUgMTYuOTc4OSAxNi42MDM1IDE3LjkwMjNDMTYuNjAzNSAxOC44MjU4IDE1Ljg1MTIgMTkuNTc4MSAxNC45Mjc3IDE5LjU3ODFDMTQuMDA0MyAxOS41NzgxIDEzLjI1MiAxOC44MjU4IDEzLjI1MiAxNy45MDIzQzEzLjI1MiAxNi45Nzg5IDE0LjAwMiAxNi4yMjY2IDE0LjkyNzcgMTYuMjI2NlpNMTcuOTM0OCAyLjQxOTUzQzE4Ljg1ODIgMi40MTk1MyAxOS42MTA1IDMuMTcxODcgMTkuNjEwNSA0LjA5NTMxQzE5LjYxMDUgNS4wMTg3NSAxOC44NTgyIDUuNzcxMDkgMTcuOTM0OCA1Ljc3MTA5QzE3LjAxMTMgNS43NzEwOSAxNi4yNTkgNS4wMTg3NSAxNi4yNTkgNC4wOTUzMUMxNi4yNTkgMy4xNzE4NyAxNy4wMTEzIDIuNDE5NTMgMTcuOTM0OCAyLjQxOTUzWicgZmlsbD0nI0I2QjZCNicvPjwvc3ZnPiA=");
    color-scheme: dark;
    --service-bg-hover: #22292f;
    --service-bg: rgba(0, 0, 0, 0.1);
  }
}
[data-theme="dark"] {
  --background-color: #11191f;
  --float-ball-more-button-background-color: #191919;
  --background-light-green: #141e26;
  --color: hsl(205deg, 16%, 77%);
  --h1-color: hsl(205deg, 20%, 94%);
  --h2-color: #e1e6eb;
  --h3-color: hsl(205deg, 18%, 86%);
  --h4-color: #c8d1d8;
  --h5-color: hsl(205deg, 16%, 77%);
  --h6-color: #afbbc4;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: #1f2d38;
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 80%, 50%);
  --primary-focus: rgba(16, 149, 193, 0.25);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 10%, 50%);
  --secondary-focus: rgba(115, 130, 140, 0.25);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 20%, 94%);
  --contrast-hover: #fff;
  --contrast-focus: rgba(115, 130, 140, 0.25);
  --contrast-inverse: #000;
  --mark-background-color: #d1c284;
  --mark-color: #11191f;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: #11191f;
  --form-element-border-color: #374956;
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: var(--form-element-background-color);
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
  --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #b71c1c;
  --form-element-invalid-active-border-color: #c62828;
  --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
  --form-element-valid-border-color: #2e7d32;
  --form-element-valid-active-border-color: #388e3c;
  --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
  --switch-background-color: #374956;
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: #24333e;
  --range-active-border-color: hsl(205deg, 25%, 23%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
  --code-background-color: #18232c;
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 30%, 50%);
  --code-property-color: hsl(185deg, 30%, 50%);
  --code-value-color: hsl(40deg, 10%, 50%);
  --code-comment-color: #4d606d;
  --accordion-border-color: var(--muted-border-color);
  --accordion-active-summary-color: var(--primary);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: #141e26;
  --card-border-color: var(--card-background-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
    0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
  --card-sectionning-background-color: #18232c;
  --dropdown-background-color: hsl(205deg, 30%, 15%);
  --dropdown-border-color: #24333e;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
  --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
  --progress-background-color: #24333e;
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  color-scheme: dark;
  --service-bg: rgba(0, 0, 0, 0.1);
}

progress,
[type="checkbox"],
[type="radio"],
[type="range"] {
  accent-color: var(--primary);
}

/**
 * Document
 * Content-box & Responsive typography
 */
*,
*::before,
*::after {
  box-sizing: border-box;
  background-repeat: no-repeat;
}

::before,
::after {
  text-decoration: inherit;
  vertical-align: inherit;
}

:where(#mount) {
  -webkit-tap-highlight-color: transparent;
  -webkit-text-size-adjust: 100%;
  -moz-text-size-adjust: 100%;
  text-size-adjust: 100%;
  background-color: var(--background-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  line-height: var(--line-height);
  font-family: var(--font-family);
  text-rendering: optimizeLegibility;
  overflow-wrap: break-word;
  cursor: default;
  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;
}

/**
 * Sectioning
 * Container and responsive spacings for header, main, footer
 */
main {
  display: block;
}

#mount {
  width: 100%;
  margin: 0;
}
#mount > header,
#mount > main,
#mount > footer {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
}
@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    padding: 2px !important;
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    padding: 0 12px !important;
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    padding: 0 24px !important;
  }
}

/**
* Container
*/
.container,
.container-fluid {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding-right: var(--spacing);
  padding-left: var(--spacing);
}
/*
@media (min-width: 576px) {
  .container {
    max-width: 510px;
    padding-right: 0;
    padding-left: 0;
  }
}
@media (min-width: 768px) {
  .container {
    max-width: 700px;
  }
} */
@media (min-width: 992px) {
  .container {
    max-width: 920px;
  }
}
@media (min-width: 1200px) {
  .container {
    max-width: 1130px;
  }
}

/**
 * Section
 * Responsive spacings for section
 */
section {
  margin-bottom: var(--block-spacing-vertical);
}

/**
* Grid
* Minimal grid system with auto-layout columns
*/
.grid {
  grid-column-gap: var(--grid-spacing-horizontal);
  grid-row-gap: var(--grid-spacing-vertical);
  display: grid;
  grid-template-columns: 1fr;
  margin: 0;
}
@media (min-width: 1280px) {
  .grid {
    grid-template-columns: repeat(auto-fit, minmax(0%, 1fr));
  }
}
.grid > * {
  min-width: 0;
}

/**
 * Horizontal scroller (<figure>)
 */
figure {
  display: block;
  margin: 0;
  padding: 0;
  overflow-x: auto;
}
figure figcaption {
  padding: calc(var(--spacing) * 0.5) 0;
  color: var(--muted-color);
}

/**
 * Typography
 */
b,
strong {
  font-weight: bolder;
}

sub,
sup {
  position: relative;
  font-size: 0.75em;
  line-height: 0;
  vertical-align: baseline;
}

sub {
  bottom: -0.25em;
}

sup {
  top: -0.5em;
}

address,
blockquote,
dl,
figure,
form,
ol,
p,
pre,
table,
ul {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: var(--font-size);
}

a,
[role="link"] {
  --color: var(--primary);
  --background-color: transparent;
  outline: none;
  background-color: var(--background-color);
  color: var(--color);
  -webkit-text-decoration: var(--text-decoration);
  text-decoration: var(--text-decoration);
  transition: background-color var(--transition), color var(--transition),
    box-shadow var(--transition), -webkit-text-decoration var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition),
    -webkit-text-decoration var(--transition);
}
a:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --color: var(--primary-hover);
  --text-decoration: underline;
}
a:focus,
[role="link"]:focus {
  --background-color: var(--primary-focus);
}
a.secondary,
[role="link"].secondary {
  --color: var(--secondary);
}
a.secondary:is([aria-current], :hover, :active, :focus),
[role="link"].secondary:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}
a.secondary:focus,
[role="link"].secondary:focus {
  --background-color: var(--secondary-focus);
}
a.contrast,
[role="link"].contrast {
  --color: var(--contrast);
}
a.contrast:is([aria-current], :hover, :active, :focus),
[role="link"].contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}
a.contrast:focus,
[role="link"].contrast:focus {
  --background-color: var(--contrast-focus);
}

h1,
h2,
h3,
h4,
h5,
h6 {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  font-family: var(--font-family);
}

h1 {
  --color: var(--h1-color);
}

h2 {
  --color: var(--h2-color);
}

h3 {
  --color: var(--h3-color);
}

h4 {
  --color: var(--h4-color);
}

h5 {
  --color: var(--h5-color);
}

h6 {
  --color: var(--h6-color);
}

:where(address, blockquote, dl, figure, form, ol, p, pre, table, ul)
  ~ :is(h1, h2, h3, h4, h5, h6) {
  margin-top: var(--typography-spacing-vertical);
}

hgroup,
.headings {
  margin-bottom: var(--typography-spacing-vertical);
}
hgroup > *,
.headings > * {
  margin-bottom: 0;
}
hgroup > *:last-child,
.headings > *:last-child {
  --color: var(--muted-color);
  --font-weight: unset;
  font-size: 1rem;
  font-family: unset;
}

p {
  margin-bottom: var(--typography-spacing-vertical);
}

small {
  font-size: var(--font-size);
}

:where(dl, ol, ul) {
  padding-right: 0;
  padding-left: var(--spacing);
  -webkit-padding-start: var(--spacing);
  padding-inline-start: var(--spacing);
  -webkit-padding-end: 0;
  padding-inline-end: 0;
}
:where(dl, ol, ul) li {
  margin-bottom: calc(var(--typography-spacing-vertical) * 0.25);
}

:where(dl, ol, ul) :is(dl, ol, ul) {
  margin: 0;
  margin-top: calc(var(--typography-spacing-vertical) * 0.25);
}

ul li {
  list-style: square;
}

mark {
  padding: 0.125rem 0.25rem;
  background-color: var(--mark-background-color);
  color: var(--mark-color);
  vertical-align: baseline;
}

blockquote {
  display: block;
  margin: var(--typography-spacing-vertical) 0;
  padding: var(--spacing);
  border-right: none;
  border-left: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-start: 0.25rem solid var(--blockquote-border-color);
  border-inline-start: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-end: none;
  border-inline-end: none;
}
blockquote footer {
  margin-top: calc(var(--typography-spacing-vertical) * 0.5);
  color: var(--blockquote-footer-color);
}

abbr[title] {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}

ins {
  color: var(--ins-color);
  text-decoration: none;
}

del {
  color: var(--del-color);
}

::-moz-selection {
  background-color: var(--primary-focus);
}

::selection {
  background-color: var(--primary-focus);
}

/**
 * Embedded content
 */
:where(audio, canvas, iframe, img, svg, video) {
  vertical-align: middle;
}

audio,
video {
  display: inline-block;
}

audio:not([controls]) {
  display: none;
  height: 0;
}

:where(iframe) {
  border-style: none;
}

img {
  max-width: 100%;
  height: auto;
  border-style: none;
}

:where(svg:not([fill])) {
  fill: currentColor;
}

svg:not(#mount) {
  overflow: hidden;
}

/**
 * Button
 */
button {
  margin: 0;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

button,
[type="button"],
[type="reset"],
[type="submit"] {
  -webkit-appearance: button;
}

button {
  display: block;
  width: 100%;
  margin-bottom: var(--spacing);
}

[role="button"] {
  display: inline-block;
  text-decoration: none;
}

button,
input[type="submit"],
input[type="button"],
input[type="reset"],
[role="button"] {
  --background-color: var(--primary);
  --border-color: var(--primary);
  --color: var(--primary-inverse);
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
button:is([aria-current], :hover, :active, :focus),
input[type="submit"]:is([aria-current], :hover, :active, :focus),
input[type="button"]:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus),
[role="button"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--primary-hover);
  --border-color: var(--primary-hover);
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  --color: var(--primary-inverse);
}
button:focus,
input[type="submit"]:focus,
input[type="button"]:focus,
input[type="reset"]:focus,
[role="button"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--primary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary,
input[type="reset"] {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  cursor: pointer;
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:focus,
input[type="reset"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--secondary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast {
  --background-color: var(--contrast);
  --border-color: var(--contrast);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--contrast-hover);
  --border-color: var(--contrast-hover);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--contrast-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline,
input[type="reset"].outline {
  --background-color: transparent;
  --color: var(--primary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --background-color: transparent;
  --color: var(--primary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary,
input[type="reset"].outline {
  --color: var(--secondary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast {
  --color: var(--contrast);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}

:where(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  )[disabled],
:where(fieldset[disabled])
  :is(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  ),
a[role="button"]:not([href]) {
  opacity: 0.5;
  pointer-events: none;
}

/**
 * Form elements
 */
input,
optgroup,
select,
textarea {
  margin: 0;
  font-size: 1rem;
  line-height: var(--line-height);
  font-family: inherit;
  letter-spacing: inherit;
}

input {
  overflow: visible;
}

select {
  text-transform: none;
}

legend {
  max-width: 100%;
  padding: 0;
  color: inherit;
  white-space: normal;
}

textarea {
  overflow: auto;
}

[type="checkbox"],
[type="radio"] {
  padding: 0;
}

::-webkit-inner-spin-button,
::-webkit-outer-spin-button {
  height: auto;
}

[type="search"] {
  -webkit-appearance: textfield;
  outline-offset: -2px;
}

[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}

::-webkit-file-upload-button {
  -webkit-appearance: button;
  font: inherit;
}

::-moz-focus-inner {
  padding: 0;
  border-style: none;
}

:-moz-focusring {
  outline: none;
}

:-moz-ui-invalid {
  box-shadow: none;
}

::-ms-expand {
  display: none;
}

[type="file"],
[type="range"] {
  padding: 0;
  border-width: 0;
}

input:not([type="checkbox"], [type="radio"], [type="range"]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
}

fieldset {
  margin: 0;
  margin-bottom: var(--spacing);
  padding: 0;
  border: 0;
}

label,
fieldset legend {
  display: block;
  margin-bottom: calc(var(--spacing) * 0.25);
  font-weight: var(--form-label-font-weight, var(--font-weight));
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  width: 100%;
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]),
select,
textarea {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
}

input,
select,
textarea {
  --background-color: var(--form-element-background-color);
  --border-color: var(--form-element-border-color);
  --color: var(--form-element-color);
  --box-shadow: none;
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="checkbox"],
    [type="radio"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --background-color: var(--form-element-active-background-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="switch"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --border-color: var(--form-element-active-border-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="range"],
    [type="file"],
    [readonly]
  ):focus,
select:focus,
textarea:focus {
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}

input:not([type="submit"], [type="button"], [type="reset"])[disabled],
select[disabled],
textarea[disabled],
:where(fieldset[disabled])
  :is(
    input:not([type="submit"], [type="button"], [type="reset"]),
    select,
    textarea
  ) {
  --background-color: var(--form-element-disabled-background-color);
  --border-color: var(--form-element-disabled-border-color);
  opacity: var(--form-element-disabled-opacity);
  pointer-events: none;
}

:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid] {
  padding-right: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal) !important;
  padding-inline-start: var(--form-element-spacing-horizontal) !important;
  -webkit-padding-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-inline-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="false"] {
  background-image: var(--icon-valid);
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="true"] {
  background-image: var(--icon-invalid);
}
:where(input, select, textarea)[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
:where(input, select, textarea)[aria-invalid="false"]:is(:active, :focus) {
  --border-color: var(--form-element-valid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-valid-focus-color) !important;
}
:where(input, select, textarea)[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}
:where(input, select, textarea)[aria-invalid="true"]:is(:active, :focus) {
  --border-color: var(--form-element-invalid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width)
    var(--form-element-invalid-focus-color) !important;
}

[dir="rtl"]
  :where(input, select, textarea):not([type="checkbox"], [type="radio"]):is(
    [aria-invalid],
    [aria-invalid="true"],
    [aria-invalid="false"]
  ) {
  background-position: center left 0.75rem;
}

input::placeholder,
input::-webkit-input-placeholder,
textarea::placeholder,
textarea::-webkit-input-placeholder,
select:invalid {
  color: var(--form-element-placeholder-color);
  opacity: 1;
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  margin-bottom: var(--spacing);
}

select::-ms-expand {
  border: 0;
  background-color: transparent;
}
select:not([multiple], [size]) {
  padding-right: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal);
  padding-inline-start: var(--form-element-spacing-horizontal);
  -webkit-padding-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-inline-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  background-image: var(--icon-chevron);
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}

[dir="rtl"] select:not([multiple], [size]) {
  background-position: center left 0.75rem;
}

:where(input, select, textarea) + small {
  display: block;
  width: 100%;
  margin-top: calc(var(--spacing) * -0.75);
  margin-bottom: var(--spacing);
  color: var(--muted-color);
}

label > :where(input, select, textarea) {
  margin-top: calc(var(--spacing) * 0.25);
}

/**
 * Form elements
 * Checkboxes & Radios
 */
[type="checkbox"],
[type="radio"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 1.25em;
  height: 1.25em;
  margin-top: -0.125em;
  margin-right: 0.375em;
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: 0.375em;
  margin-inline-end: 0.375em;
  border-width: var(--border-width);
  font-size: inherit;
  vertical-align: middle;
  cursor: pointer;
}
[type="checkbox"]::-ms-check,
[type="radio"]::-ms-check {
  display: none;
}
[type="checkbox"]:checked,
[type="checkbox"]:checked:active,
[type="checkbox"]:checked:focus,
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-checkbox);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}
[type="checkbox"] ~ label,
[type="radio"] ~ label {
  display: inline-block;
  margin-right: 0.375em;
  margin-bottom: 0;
  cursor: pointer;
}

[type="checkbox"]:indeterminate {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-minus);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}

[type="radio"] {
  border-radius: 50%;
}
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary-inverse);
  border-width: 0.35em;
  background-image: none;
}

[type="checkbox"][role="switch"] {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
  --color: var(--switch-color);
  width: 2.25em;
  height: 1.25em;
  border: var(--border-width) solid var(--border-color);
  border-radius: 1.25em;
  background-color: var(--background-color);
  line-height: 1.25em;
}
[type="checkbox"][role="switch"]:focus {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
}
[type="checkbox"][role="switch"]:checked {
  --background-color: var(--switch-checked-background-color);
  --border-color: var(--switch-checked-background-color);
}
[type="checkbox"][role="switch"]:before {
  display: block;
  width: calc(1.25em - (var(--border-width) * 2));
  height: 100%;
  border-radius: 50%;
  background-color: var(--color);
  content: "";
  transition: margin 0.1s ease-in-out;
}
[type="checkbox"][role="switch"]:checked {
  background-image: none;
}
[type="checkbox"][role="switch"]:checked::before {
  margin-left: calc(1.125em - var(--border-width));
  -webkit-margin-start: calc(1.125em - var(--border-width));
  margin-inline-start: calc(1.125em - var(--border-width));
}

[type="checkbox"][aria-invalid="false"],
[type="checkbox"]:checked[aria-invalid="false"],
[type="radio"][aria-invalid="false"],
[type="radio"]:checked[aria-invalid="false"],
[type="checkbox"][role="switch"][aria-invalid="false"],
[type="checkbox"][role="switch"]:checked[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
[type="checkbox"][aria-invalid="true"],
[type="checkbox"]:checked[aria-invalid="true"],
[type="radio"][aria-invalid="true"],
[type="radio"]:checked[aria-invalid="true"],
[type="checkbox"][role="switch"][aria-invalid="true"],
[type="checkbox"][role="switch"]:checked[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}

/**
 * Form elements
 * Alternatives input types (Not Checkboxes & Radios)
 */
[type="color"]::-webkit-color-swatch-wrapper {
  padding: 0;
}
[type="color"]::-moz-focus-inner {
  padding: 0;
}
[type="color"]::-webkit-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}
[type="color"]::-moz-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]):is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  --icon-position: 0.75rem;
  --icon-width: 1rem;
  padding-right: calc(var(--icon-width) + var(--icon-position));
  background-image: var(--icon-date);
  background-position: center right var(--icon-position);
  background-size: var(--icon-width) auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="time"] {
  background-image: var(--icon-time);
}

[type="date"]::-webkit-calendar-picker-indicator,
[type="datetime-local"]::-webkit-calendar-picker-indicator,
[type="month"]::-webkit-calendar-picker-indicator,
[type="time"]::-webkit-calendar-picker-indicator,
[type="week"]::-webkit-calendar-picker-indicator {
  width: var(--icon-width);
  margin-right: calc(var(--icon-width) * -1);
  margin-left: var(--icon-position);
  opacity: 0;
}

[dir="rtl"]
  :is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  text-align: right;
}

[type="file"] {
  --color: var(--muted-color);
  padding: calc(var(--form-element-spacing-vertical) * 0.5) 0;
  border: 0;
  border-radius: 0;
  background: none;
}
[type="file"]::file-selector-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::file-selector-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-webkit-file-upload-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-webkit-file-upload-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-ms-browse {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  margin-inline-start: 0;
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-ms-browse:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}

[type="range"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 100%;
  height: 1.25rem;
  background: none;
}
[type="range"]::-webkit-slider-runnable-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -webkit-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-moz-range-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -moz-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-ms-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -ms-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-webkit-slider-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-moz-range-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -moz-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-ms-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]:hover,
[type="range"]:focus {
  --range-border-color: var(--range-active-border-color);
  --range-thumb-color: var(--range-thumb-hover-color);
}
[type="range"]:active {
  --range-thumb-color: var(--range-thumb-active-color);
}
[type="range"]:active::-webkit-slider-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-moz-range-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-ms-thumb {
  transform: scale(1.25);
}

input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  -webkit-padding-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  padding-inline-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  border-radius: 5rem;
  background-image: var(--icon-search);
  background-position: center left 1.125rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  -webkit-padding-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  padding-inline-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  background-position: center left 1.125rem, center right 0.75rem;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="false"] {
  background-image: var(--icon-search), var(--icon-valid);
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="true"] {
  background-image: var(--icon-search), var(--icon-invalid);
}

[type="search"]::-webkit-search-cancel-button {
  -webkit-appearance: none;
  display: none;
}

[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  background-position: center right 1.125rem;
}
[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  background-position: center right 1.125rem, center left 0.75rem;
}

/**
 * Table
 */
:where(table) {
  width: 100%;
  border-collapse: collapse;
  border-spacing: 0;
  text-indent: 0;
}

th,
td {
  padding: calc(var(--spacing) / 2) var(--spacing);
  border-bottom: var(--border-width) solid var(--table-border-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  text-align: left;
  text-align: start;
}

tfoot th,
tfoot td {
  border-top: var(--border-width) solid var(--table-border-color);
  border-bottom: 0;
}

table[role="grid"] tbody tr:nth-child(odd) {
  background-color: var(--table-row-stripped-background-color);
}

/**
 * Code
 */
pre,
code,
kbd,
samp {
  font-size: 0.875em;
  font-family: var(--font-family);
}

pre {
  -ms-overflow-style: scrollbar;
  overflow: auto;
}

pre,
code,
kbd {
  border-radius: var(--border-radius);
  background: var(--code-background-color);
  color: var(--code-color);
  font-weight: var(--font-weight);
  line-height: initial;
}

code,
kbd {
  display: inline-block;
  padding: 0.375rem 0.5rem;
}

pre {
  display: block;
  margin-bottom: var(--spacing);
  overflow-x: auto;
}
pre > code {
  display: block;
  padding: var(--spacing);
  background: none;
  font-size: 14px;
  line-height: var(--line-height);
}

code b {
  color: var(--code-tag-color);
  font-weight: var(--font-weight);
}
code i {
  color: var(--code-property-color);
  font-style: normal;
}
code u {
  color: var(--code-value-color);
  text-decoration: none;
}
code em {
  color: var(--code-comment-color);
  font-style: normal;
}

kbd {
  background-color: var(--code-kbd-background-color);
  color: var(--code-kbd-color);
  vertical-align: baseline;
}

/**
 * Miscs
 */
hr {
  height: 0;
  border: 0;
  border-top: 1px solid var(--muted-border-color);
  color: inherit;
}

[hidden],
template {
  display: none !important;
}

canvas {
  display: inline-block;
}

/**
 * Accordion (<details>)
 */
details {
  display: block;
  margin-bottom: var(--spacing);
  padding-bottom: var(--spacing);
  border-bottom: var(--border-width) solid var(--accordion-border-color);
}
details summary {
  line-height: 1rem;
  list-style-type: none;
  cursor: pointer;
  transition: color var(--transition);
}
details summary:not([role]) {
  color: var(--accordion-close-summary-color);
}
details summary::-webkit-details-marker {
  display: none;
}
details summary::marker {
  display: none;
}
details summary::-moz-list-bullet {
  list-style-type: none;
}
details summary::after {
  display: block;
  width: 1rem;
  height: 1rem;
  -webkit-margin-start: calc(var(--spacing, 1rem) * 0.5);
  margin-inline-start: calc(var(--spacing, 1rem) * 0.5);
  float: right;
  transform: rotate(-90deg);
  background-image: var(--icon-chevron);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
  transition: transform var(--transition);
}
details summary:focus {
  outline: none;
}
details summary:focus:not([role="button"]) {
  color: var(--accordion-active-summary-color);
}
details summary[role="button"] {
  width: 100%;
  text-align: left;
}
details summary[role="button"]::after {
  height: calc(1rem * var(--line-height, 1.5));
  background-image: var(--icon-chevron-button);
}
details summary[role="button"]:not(.outline).contrast::after {
  background-image: var(--icon-chevron-button-inverse);
}
details[open] > summary {
  margin-bottom: calc(var(--spacing));
}
details[open] > summary:not([role]):not(:focus) {
  color: var(--accordion-open-summary-color);
}
details[open] > summary::after {
  transform: rotate(0);
}

[dir="rtl"] details summary {
  text-align: right;
}
[dir="rtl"] details summary::after {
  float: left;
  background-position: left center;
}

/**
 * Card (<article>)
 */
article {
  margin: var(--block-spacing-vertical) 0;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
  border-radius: var(--border-radius);
  background: var(--card-background-color);
  box-shadow: var(--card-box-shadow);
}
article > header,
article > footer {
  margin-right: calc(var(--block-spacing-horizontal) * -1);
  margin-left: calc(var(--block-spacing-horizontal) * -1);
  padding: calc(var(--block-spacing-vertical) * 0.66)
    var(--block-spacing-horizontal);
  background-color: var(--card-sectionning-background-color);
}
article > header {
  margin-top: calc(var(--block-spacing-vertical) * -1);
  margin-bottom: var(--block-spacing-vertical);
  border-bottom: var(--border-width) solid var(--card-border-color);
  border-top-right-radius: var(--border-radius);
  border-top-left-radius: var(--border-radius);
}
article > footer {
  margin-top: var(--block-spacing-vertical);
  margin-bottom: calc(var(--block-spacing-vertical) * -1);
  border-top: var(--border-width) solid var(--card-border-color);
  border-bottom-right-radius: var(--border-radius);
  border-bottom-left-radius: var(--border-radius);
}

/**
 * Modal (<dialog>)
 */
#mount {
  --scrollbar-width: 0px;
}

dialog {
  display: flex;
  z-index: 999;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  align-items: center;
  justify-content: center;
  width: inherit;
  min-width: 100%;
  height: inherit;
  min-height: 100%;
  padding: var(--spacing);
  border: 0;
  -webkit-backdrop-filter: var(--modal-overlay-backdrop-filter);
  backdrop-filter: var(--modal-overlay-backdrop-filter);
  background-color: var(--modal-overlay-background-color);
  color: var(--color);
}
dialog article {
  max-height: calc(100vh - var(--spacing) * 2);
  overflow: auto;
}
@media (min-width: 576px) {
  dialog article {
    max-width: 510px;
  }
}
@media (min-width: 768px) {
  dialog article {
    max-width: 700px;
  }
}
dialog article > header,
dialog article > footer {
  padding: calc(var(--block-spacing-vertical) * 0.5)
    var(--block-spacing-horizontal);
}
dialog article > header .close {
  margin: 0;
  margin-left: var(--spacing);
  float: right;
}
dialog article > footer {
  text-align: right;
}
dialog article > footer [role="button"] {
  margin-bottom: 0;
}
dialog article > footer [role="button"]:not(:first-of-type) {
  margin-left: calc(var(--spacing) * 0.5);
}
dialog article p:last-of-type {
  margin: 0;
}
dialog article .close {
  display: block;
  width: 1rem;
  height: 1rem;
  margin-top: calc(var(--block-spacing-vertical) * -0.5);
  margin-bottom: var(--typography-spacing-vertical);
  margin-left: auto;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}
dialog article .close:is([aria-current], :hover, :active, :focus) {
  opacity: 1;
}
dialog:not([open]),
dialog[open="false"] {
  display: none;
}

.modal-is-open {
  padding-right: var(--scrollbar-width, 0px);
  overflow: hidden;
  pointer-events: none;
}
.modal-is-open dialog {
  pointer-events: auto;
}

:where(.modal-is-opening, .modal-is-closing) dialog,
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-duration: 0.2s;
  animation-timing-function: ease-in-out;
  animation-fill-mode: both;
}
:where(.modal-is-opening, .modal-is-closing) dialog {
  animation-duration: 0.8s;
  animation-name: modal-overlay;
}
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-delay: 0.2s;
  animation-name: modal;
}

.modal-is-closing dialog,
.modal-is-closing dialog > article {
  animation-delay: 0s;
  animation-direction: reverse;
}

@keyframes modal-overlay {
  from {
    -webkit-backdrop-filter: none;
    backdrop-filter: none;
    background-color: transparent;
  }
}
@keyframes modal {
  from {
    transform: translateY(-100%);
    opacity: 0;
  }
}
/**
 * Nav
 */
:where(nav li)::before {
  float: left;
  content: "​";
}

nav,
nav ul {
  display: flex;
}

nav {
  justify-content: space-between;
}
nav ol,
nav ul {
  align-items: center;
  margin-bottom: 0;
  padding: 0;
  list-style: none;
}
nav ol:first-of-type,
nav ul:first-of-type {
  margin-left: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav ol:last-of-type,
nav ul:last-of-type {
  margin-right: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav li {
  display: inline-block;
  margin: 0;
  padding: var(--nav-element-spacing-vertical)
    var(--nav-element-spacing-horizontal);
}
nav li > * {
  --spacing: 0;
}
nav :where(a, [role="link"]) {
  display: inline-block;
  margin: calc(var(--nav-link-spacing-vertical) * -1)
    calc(var(--nav-link-spacing-horizontal) * -1);
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
  border-radius: var(--border-radius);
  text-decoration: none;
}
nav :where(a, [role="link"]):is([aria-current], :hover, :active, :focus) {
  text-decoration: none;
}
nav[aria-label="breadcrumb"] {
  align-items: center;
  justify-content: start;
}
nav[aria-label="breadcrumb"] ul li:not(:first-child) {
  -webkit-margin-start: var(--nav-link-spacing-horizontal);
  margin-inline-start: var(--nav-link-spacing-horizontal);
}
nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  position: absolute;
  width: calc(var(--nav-link-spacing-horizontal) * 2);
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) / 2);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) / 2);
  content: "/";
  color: var(--muted-color);
  text-align: center;
}
nav[aria-label="breadcrumb"] a[aria-current] {
  background-color: transparent;
  color: inherit;
  text-decoration: none;
  pointer-events: none;
}
nav [role="button"] {
  margin-right: inherit;
  margin-left: inherit;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}

aside nav,
aside ol,
aside ul,
aside li {
  display: block;
}
aside li {
  padding: calc(var(--nav-element-spacing-vertical) * 0.5)
    var(--nav-element-spacing-horizontal);
}
aside li a {
  display: block;
}
aside li [role="button"] {
  margin: inherit;
}

[dir="rtl"] nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  content: "\\";
}

/**
 * Progress
 */
progress {
  display: inline-block;
  vertical-align: baseline;
}

progress {
  -webkit-appearance: none;
  -moz-appearance: none;
  display: inline-block;
  appearance: none;
  width: 100%;
  height: 0.5rem;
  margin-bottom: calc(var(--spacing) * 0.5);
  overflow: hidden;
  border: 0;
  border-radius: var(--border-radius);
  background-color: var(--progress-background-color);
  color: var(--progress-color);
}
progress::-webkit-progress-bar {
  border-radius: var(--border-radius);
  background: none;
}
progress[value]::-webkit-progress-value {
  background-color: var(--progress-color);
}
progress::-moz-progress-bar {
  background-color: var(--progress-color);
}
@media (prefers-reduced-motion: no-preference) {
  progress:indeterminate {
    background: var(--progress-background-color)
      linear-gradient(
        to right,
        var(--progress-color) 30%,
        var(--progress-background-color) 30%
      )
      top left/150% 150% no-repeat;
    animation: progress-indeterminate 1s linear infinite;
  }
  progress:indeterminate[value]::-webkit-progress-value {
    background-color: transparent;
  }
  progress:indeterminate::-moz-progress-bar {
    background-color: transparent;
  }
}

@media (prefers-reduced-motion: no-preference) {
  [dir="rtl"] progress:indeterminate {
    animation-direction: reverse;
  }
}

@keyframes progress-indeterminate {
  0% {
    background-position: 200% 0;
  }
  100% {
    background-position: -200% 0;
  }
}
/**
 * Dropdown ([role="list"])
 */
details[role="list"],
li[role="list"] {
  position: relative;
}

details[role="list"] summary + ul,
li[role="list"] > ul {
  display: flex;
  z-index: 99;
  position: absolute;
  top: auto;
  right: 0;
  left: 0;
  flex-direction: column;
  margin: 0;
  padding: 0;
  border: var(--border-width) solid var(--dropdown-border-color);
  border-radius: var(--border-radius);
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  background-color: var(--dropdown-background-color);
  box-shadow: var(--card-box-shadow);
  color: var(--dropdown-color);
  white-space: nowrap;
}
details[role="list"] summary + ul li,
li[role="list"] > ul li {
  width: 100%;
  margin-bottom: 0;
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  list-style: none;
}
details[role="list"] summary + ul li:first-of-type,
li[role="list"] > ul li:first-of-type {
  margin-top: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li:last-of-type,
li[role="list"] > ul li:last-of-type {
  margin-bottom: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li a,
li[role="list"] > ul li a {
  display: block;
  margin: calc(var(--form-element-spacing-vertical) * -0.5)
    calc(var(--form-element-spacing-horizontal) * -1);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  overflow: hidden;
  color: var(--dropdown-color);
  text-decoration: none;
  text-overflow: ellipsis;
}
details[role="list"] summary + ul li a:hover,
li[role="list"] > ul li a:hover {
  background-color: var(--dropdown-hover-background-color);
}

details[role="list"] summary::after,
li[role="list"] > a::after {
  display: block;
  width: 1rem;
  height: calc(1rem * var(--line-height, 1.5));
  -webkit-margin-start: 0.5rem;
  margin-inline-start: 0.5rem;
  float: right;
  transform: rotate(0deg);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
}

details[role="list"] {
  padding: 0;
  border-bottom: none;
}
details[role="list"] summary {
  margin-bottom: 0;
}
details[role="list"] summary:not([role]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--form-element-border-color);
  border-radius: var(--border-radius);
  background-color: var(--form-element-background-color);
  color: var(--form-element-placeholder-color);
  line-height: inherit;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
details[role="list"] summary:not([role]):active,
details[role="list"] summary:not([role]):focus {
  border-color: var(--form-element-active-border-color);
  background-color: var(--form-element-active-background-color);
}
details[role="list"] summary:not([role]):focus {
  box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}
details[role="list"][open] summary {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
details[role="list"][open] summary::before {
  display: block;
  z-index: 1;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  background: none;
  content: "";
  cursor: default;
}

nav details[role="list"] summary,
nav li[role="list"] a {
  display: flex;
  direction: ltr;
}

nav details[role="list"] summary + ul,
nav li[role="list"] > ul {
  min-width: -moz-fit-content;
  min-width: fit-content;
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul li a,
nav li[role="list"] > ul li a {
  border-radius: 0;
}

nav details[role="list"] summary,
nav details[role="list"] summary:not([role]) {
  height: auto;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}
nav details[role="list"][open] summary {
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul {
  margin-top: var(--outline-width);
  -webkit-margin-start: 0;
  margin-inline-start: 0;
}
nav details[role="list"] summary[role="link"] {
  margin-bottom: calc(var(--nav-link-spacing-vertical) * -1);
  line-height: var(--line-height);
}
nav details[role="list"] summary[role="link"] + ul {
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) * -1);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) * -1);
}

li[role="list"]:hover > ul,
li[role="list"] a:active ~ ul,
li[role="list"] a:focus ~ ul {
  display: flex;
}
li[role="list"] > ul {
  display: none;
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
  margin-inline-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
}
li[role="list"] > a::after {
  background-image: var(--icon-chevron);
}

/**
 * Loading ([aria-busy=true])
 */
[aria-busy="true"] {
  cursor: progress;
}

[aria-busy="true"]:not(input, select, textarea)::before {
  display: inline-block;
  width: 1em;
  height: 1em;
  border: 0.1875em solid currentColor;
  border-radius: 1em;
  border-right-color: transparent;
  content: "";
  vertical-align: text-bottom;
  vertical-align: -0.125em;
  animation: spinner 0.75s linear infinite;
  opacity: var(--loading-spinner-opacity);
}
[aria-busy="true"]:not(input, select, textarea):not(:empty)::before {
  margin-right: calc(var(--spacing) * 0.5);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) * 0.5);
  margin-inline-end: calc(var(--spacing) * 0.5);
}
[aria-busy="true"]:not(input, select, textarea):empty {
  text-align: center;
}

button[aria-busy="true"],
input[type="submit"][aria-busy="true"],
input[type="button"][aria-busy="true"],
input[type="reset"][aria-busy="true"],
a[aria-busy="true"] {
  pointer-events: none;
}

@keyframes spinner {
  to {
    transform: rotate(360deg);
  }
}
/**
 * Tooltip ([data-tooltip])
 */
[data-tooltip] {
  position: relative;
}
[data-tooltip]:not(a, button, input) {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}
[data-tooltip][data-placement="top"]::before,
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::before,
[data-tooltip]::after {
  display: block;
  z-index: 99;
  position: absolute;
  bottom: 100%;
  left: 50%;
  padding: 0.25rem 0.5rem;
  overflow: hidden;
  transform: translate(-50%, -0.25rem);
  border-radius: var(--border-radius);
  background: var(--tooltip-background-color);
  content: attr(data-tooltip);
  color: var(--tooltip-color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: 0.875rem;
  text-decoration: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  opacity: 0;
  pointer-events: none;
}
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::after {
  padding: 0;
  transform: translate(-50%, 0rem);
  border-top: 0.3rem solid;
  border-right: 0.3rem solid transparent;
  border-left: 0.3rem solid transparent;
  border-radius: 0;
  background-color: transparent;
  content: "";
  color: var(--tooltip-background-color);
}
[data-tooltip][data-placement="bottom"]::before,
[data-tooltip][data-placement="bottom"]::after {
  top: 100%;
  bottom: auto;
  transform: translate(-50%, 0.25rem);
}
[data-tooltip][data-placement="bottom"]:after {
  transform: translate(-50%, -0.3rem);
  border: 0.3rem solid transparent;
  border-bottom: 0.3rem solid;
}
[data-tooltip][data-placement="left"]::before,
[data-tooltip][data-placement="left"]::after {
  top: 50%;
  right: 100%;
  bottom: auto;
  left: auto;
  transform: translate(-0.25rem, -50%);
}
[data-tooltip][data-placement="left"]:after {
  transform: translate(0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-left: 0.3rem solid;
}
[data-tooltip][data-placement="right"]::before,
[data-tooltip][data-placement="right"]::after {
  top: 50%;
  right: auto;
  bottom: auto;
  left: 100%;
  transform: translate(0.25rem, -50%);
}
[data-tooltip][data-placement="right"]:after {
  transform: translate(-0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-right: 0.3rem solid;
}
[data-tooltip]:focus::before,
[data-tooltip]:focus::after,
[data-tooltip]:hover::before,
[data-tooltip]:hover::after {
  opacity: 1;
}
@media (hover: hover) and (pointer: fine) {
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::before,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::before,
  [data-tooltip]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::after {
    animation-name: tooltip-caret-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::before,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-bottom;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-name: tooltip-caret-slide-bottom;
  }
  [data-tooltip][data-placement="left"]:focus::before,
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::before,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-left;
  }
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-name: tooltip-caret-slide-left;
  }
  [data-tooltip][data-placement="right"]:focus::before,
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::before,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-right;
  }
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-name: tooltip-caret-slide-right;
  }
}
@keyframes tooltip-slide-top {
  from {
    transform: translate(-50%, 0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-top {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.25rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-bottom {
  from {
    transform: translate(-50%, -0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-bottom {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.5rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.3rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-left {
  from {
    transform: translate(0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-left {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.3rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-slide-right {
  from {
    transform: translate(-0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-right {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.3rem, -50%);
    opacity: 1;
  }
}

/**
 * Accessibility & User interaction
 */
[aria-controls] {
  cursor: pointer;
}

[aria-disabled="true"],
[disabled] {
  cursor: not-allowed;
}

[aria-hidden="false"][hidden] {
  display: initial;
}

[aria-hidden="false"][hidden]:not(:focus) {
  clip: rect(0, 0, 0, 0);
  position: absolute;
}

a,
area,
button,
input,
label,
select,
summary,
textarea,
[tabindex] {
  -ms-touch-action: manipulation;
}

[dir="rtl"] {
  direction: rtl;
}

/**
* Reduce Motion Features
*/
@media (prefers-reduced-motion: reduce) {
  *:not([aria-busy="true"]),
  :not([aria-busy="true"])::before,
  :not([aria-busy="true"])::after {
    background-attachment: initial !important;
    animation-duration: 1ms !important;
    animation-delay: -1ms !important;
    animation-iteration-count: 1 !important;
    scroll-behavior: auto !important;
    transition-delay: 0s !important;
    transition-duration: 0s !important;
  }
}

#mount {
  /* --primary: rgb(227, 59, 126); */
  --primary: #ea4c89;
  --primary-hover: #f082ac;
  --icon-xia: url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgaWQ9IkZyYW1lIj4KPHBhdGggaWQ9IlZlY3RvciIgZD0iTTguMDAyOTEgOS42Nzk4M0wzLjgzMzM5IDUuNTEyMjFMMy4wMjUzOSA2LjMxOTgzTDguMDAzMjkgMTEuMjk1MUwxMi45NzYyIDYuMzE5ODNMMTIuMTY3OSA1LjUxMjIxTDguMDAyOTEgOS42Nzk4M1oiIGZpbGw9IiM4MzgzODMiLz4KPC9nPgo8L3N2Zz4K");
  --switch-checked-background-color: var(--primary);
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --primary: #e23c7c;
  --primary-hover: #f082ac;
}

[data-theme="dark"] {
  --primary: #e23c7c;
  --primary-hover: #f082ac;
}

[data-theme="light"] {
  --primary: #ea4c89;
  --primary-hover: #f082ac;
}

li.select-link.select-link:hover > ul {
  display: none;
}
li.select-link.select-link > ul {
  display: none;
}
li.select-link.select-link a:focus ~ ul {
  display: none;
}

li.select-link.select-link a:active ~ ul {
  display: none;
}
li.select-link-active.select-link-active > ul {
  display: flex;
}
li.select-link-active.select-link-active:hover > ul {
  display: flex;
}

li.select-link-active.select-link-active a:focus ~ ul {
  display: flex;
}

li.select-link-active.select-link-active a:active ~ ul {
  display: flex;
}
ul.select-link-ul.select-link-ul {
  right: 0px;
  left: auto;
}

a.select-link-selected {
  background-color: var(--primary-focus);
}
.immersive-translate-no-select {
  -webkit-touch-callout: none; /* iOS Safari */
  -webkit-user-select: none; /* Safari */
  -khtml-user-select: none; /* Konqueror HTML */
  -moz-user-select: none; /* Old versions of Firefox */
  -ms-user-select: none; /* Internet Explorer/Edge */
  user-select: none;
}

/* li[role="list"].no-arrow > a::after { */
/*   background-image: none; */
/*   width: 0; */
/*   color: var(--color); */
/* } */
li[role="list"].no-arrow {
  margin-left: 8px;
  padding-right: 0;
}
li[role="list"] > a::after {
  -webkit-margin-start: 0.2rem;
  margin-inline-start: 0.2rem;
}

li[role="list"].no-arrow > a,
li[role="list"].no-arrow > a:link,
li[role="list"].no-arrow > a:visited {
  color: var(--secondary);
}

select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 4px;
  max-width: 128px;
  overflow: hidden;
  color: var(--primary);
  font-size: 13px;
  border: none;
  padding: 0;
  padding-right: 20px;
  padding-left: 8px;
  text-overflow: ellipsis;
  color: var(--color);
}
select.min-select-secondary {
  color: var(--color);
}
select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}
select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.muted {
  color: var(--muted-color);
}

.select.button-select {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
  cursor: pointer;
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 16px;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
  -webkit-appearance: button;
  margin: 0;
  margin-bottom: 0px;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

body {
  padding: 0;
  margin: 0 auto;
  min-width: 268px;
  border-radius: 10px;
}

.popup-container {
  font-size: 16px;
  --font-size: 16px;
  color: #666;
  background-color: var(--popup-footer-background-color);
  width: 316px;
  min-width: 316px;
}

.popup-content {
  background-color: var(--popup-content-background-color);
  border-radius: 0px 0px 12px 12px;
  padding: 16px 20px;
}

.immersive-translate-popup-overlay {
  position: fixed;
  top: 0;
  left: 0;
  height: 100%;
  width: 100%;
  touch-action: none;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 10px;
  border: 1px solid var(--muted-border-color);
}

#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 4px;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 16px;
  --typography-spacing-vertical: 24px;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 12px;
  --form-element-spacing-horizontal: 16px;
  --nav-element-spacing-vertical: 16px;
  --nav-element-spacing-horizontal: 8px;
  --nav-link-spacing-vertical: 8px;
  --nav-link-spacing-horizontal: 8px;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(4px);
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --popup-footer-background-color: #e8eaeb;
  --popup-content-background-color: #ffffff;
  --popup-item-background-color: #f3f5f6;
  --popup-item-hover-background-color: #eaeced;
  --popup-trial-pro-background-color: #f9fbfc;
  --text-black-2: #222222;
  --text-gray-2: #222222;
  --text-gray-6: #666666;
  --text-gray-9: #999999;
  --text-gray-c2: #c2c2c2;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(75, 76, 77, 0.2);
  --service-select-border-color: #fafafa;
  --service-select-selected-background-color: #f3f5f6;
  --download-app-background: #f3f5f6;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --popup-footer-background-color: #0d0d0d;
    --popup-content-background-color: #191919;
    --popup-item-background-color: #272727;
    --popup-item-hover-background-color: #333333;
    --popup-trial-pro-background-color: #222222;
    --text-black-2: #ffffff;
    --text-gray-2: #dbdbdb;
    --text-gray-6: #b3b3b3;
    --text-gray-9: #777777;
    --text-gray-c2: #5b5b5b;
    --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
    --service-select-border-color: #2c2c2c;
    --service-select-selected-background-color: #333333;
    --download-app-background: #333;
  }
}

[data-theme="dark"] {
  --popup-footer-background-color: #0d0d0d;
  --popup-content-background-color: #191919;
  --popup-item-background-color: #272727;
  --popup-item-hover-background-color: #333333;
  --popup-trial-pro-background-color: #222222;
  --text-black-2: #ffffff;
  --text-gray-2: #dbdbdb;
  --text-gray-6: #b3b3b3;
  --text-gray-9: #777777;
  --text-gray-c2: #5b5b5b;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
  --service-select-border-color: #2c2c2c;
  --service-select-selected-background-color: #333333;
  --download-app-background: #333;
}

.text-balck {
  color: var(--text-black-2);
}

.text-gray-2 {
  color: var(--text-gray-2);
}

.text-gray-6 {
  color: var(--text-gray-6);
}

.text-gray-9 {
  color: var(--text-gray-9);
}

.text-gray-c2 {
  color: var(--text-gray-c2);
}

#mount {
  min-width: 268px;
}

.main-button {
  font-size: 15px;
  vertical-align: middle;
  border-radius: 12px;
  padding: unset;
  height: 44px;
  line-height: 44px;
}

.pt-4 {
  padding-top: 16px;
}

.p-2 {
  padding: 8px;
}

.pl-5 {
  padding-left: 48px;
}

.p-0 {
  padding: 0;
}

.pl-2 {
  padding-left: 8px;
}

.pl-4 {
  padding-left: 24px;
}

.pt-2 {
  padding-top: 8px;
}

.pb-2 {
  padding-bottom: 8px;
}

.pb-4 {
  padding-bottom: 16px;
}

.pb-5 {
  padding-bottom: 20px;
}

.pr-5 {
  padding-right: 48px;
}

.text-sm {
  font-size: 13px;
}

.text-base {
  font-size: 16px;
}

.w-full {
  width: 100%;
}

.flex {
  display: flex;
}

.flex-row {
  flex-direction: row;
}

.flex-wrap {
  flex-wrap: wrap;
}

.flex-end {
  justify-content: flex-end;
}

.flex-grow {
  flex-grow: 1;
}

.justify-between {
  justify-content: space-between;
}

.mb-0 {
  margin-bottom: 0px;
}

.mb-2 {
  margin-bottom: 8px;
}

.mb-4 {
  margin-bottom: 16px;
}

.mb-3 {
  margin-bottom: 12px;
}

.inline-block {
  display: inline-block;
}

.py-2 {
  padding-top: 8px;
  padding-bottom: 8px;
}

.py-2-5 {
  padding-top: 6px;
  padding-bottom: 6px;
}

.mt-0 {
  margin-top: 0;
}

.mt-2 {
  margin-top: 8px;
}

.mt-3 {
  margin-top: 12px;
}

.mt-4 {
  margin-top: 16px;
}

.mt-5 {
  margin-top: 20px;
}

.mt-6 {
  margin-top: 24px;
}

.mb-1 {
  margin-bottom: 4px;
}

.ml-4 {
  margin-left: 24px;
}

.ml-3 {
  margin-left: 16px;
}

.ml-2 {
  margin-left: 8px;
}

.ml-1 {
  margin-left: 4px;
}

.mr-1 {
  margin-right: 4px;
}

.mr-2 {
  margin-right: 8px;
}

.mr-3 {
  margin-right: 16px;
}

.mx-2 {
  margin-left: 8px;
  margin-right: 8px;
}

.pl-3 {
  padding-left: 12px;
}

.pr-3 {
  padding-right: 12px;
}

.p-3 {
  padding: 12px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-3 {
  padding-top: 12px;
}

.px-6 {
  padding-left: 18px;
  padding-right: 18px;
}

.px-4 {
  padding-left: 16px;
  padding-right: 16px;
}

.pt-6 {
  padding-top: 20px;
}

.py-3 {
  padding-top: 12px;
  padding-bottom: 12px;
}

.py-0 {
  padding-top: 0;
  padding-bottom: 0;
}

.left-auto {
  left: auto !important;
}

.max-h-28 {
  max-height: 112px;
}

.max-h-30 {
  max-height: 120px;
}

.overflow-y-scroll {
  overflow-y: scroll;
}

.text-xs {
  font-size: 12px;
}

.flex-1 {
  flex: 1;
}

.flex-3 {
  flex: 3;
}

.flex-4 {
  flex: 4;
}

.flex-2 {
  flex: 2;
}

.items-center {
  align-items: center;
}

.max-content {
  width: max-content;
}

.justify-center {
  justify-content: center;
}

.items-end {
  align-items: flex-end;
}

.items-baseline {
  align-items: baseline;
}

.my-5 {
  margin-top: 48px;
  margin-bottom: 48px;
}

.my-4 {
  margin-top: 24px;
  margin-bottom: 24px;
}

.my-3 {
  margin-top: 16px;
  margin-bottom: 16px;
}

.pt-3 {
  padding-top: 12px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-2 {
  padding-top: 8px;
}

.px-2 {
  padding-left: 8px;
  padding-right: 8px;
}

.pt-1 {
  padding-top: 4px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.pb-2 {
  padding-bottom: 8px;
}

.justify-end {
  justify-content: flex-end;
}

.w-auto {
  width: auto;
}

.shrink-0 {
  flex-shrink: 0;
}

select.language-select,
select.translate-service,
select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 0px;
  max-width: unset;
  flex: 1;
  overflow: hidden;
  font-size: 13px;
  border: none;
  border-radius: 8px;
  padding-right: 30px;
  padding-left: 0px;
  background-position: center right 12px;
  background-size: 16px auto;
  background-image: var(--icon-xia);
  text-overflow: ellipsis;
  color: var(--text-gray-2);
  background-color: transparent;
  box-shadow: unset !important;
  cursor: pointer;
}

select.more {
  background-position: center right;
  padding-right: 20px;
}

select.transform-padding-left {
  padding-left: 12px;
  transform: translateX(-12px);
  background-position: center right 0px;
}

select.translate-service {
  color: var(--text-black-2);
}

.min-select-container.disabled {
  opacity: 0.5;
  pointer-events: none;
}

/* dark use black, for windows */
@media (prefers-color-scheme: dark) {
  select.language-select option,
  select.translate-service option,
  select.min-select option {
    background-color: #666666;
  }
}

.text-overflow-ellipsis {
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
}

.max-w-20 {
  max-width: 180px;
  white-space: nowrap;
}

select.min-select-secondary {
  color: var(--color);
}

select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}

select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.popup-footer {
  background-color: var(--popup-footer-background-color);
  height: 40px;
}

.text-right {
  text-align: right;
}

.clickable {
  cursor: pointer;
}

.close {
  cursor: pointer;
  width: 16px;
  height: 16px;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}

.padding-two-column {
  padding-left: 40px;
  padding-right: 40px;
}

.muted {
  color: #999;
}

.text-label {
  color: #666;
}

.display-none {
  display: none;
}

/* dark use #18232c */
@media (prefers-color-scheme: dark) {
  .text-label {
    color: #9ca3af;
  }
}

.text-decoration-none {
  text-decoration: none;
}

.text-decoration-none:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --text-decoration: none !important;
  background-color: transparent !important;
}

.language-select-container {
  position: relative;
  width: 100%;
  background-color: var(--popup-item-background-color);
  height: 55px;
  border-radius: 12px;
}

select.language-select {
  color: var(--text-black-2);
  font-size: 14px;
  padding: 8px 24px 24px 16px;
  position: absolute;
  border-radius: 12px;
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
}

select.text-gray-6 {
  color: var(--text-gray-6);
}

.language-select-container label {
  position: absolute;
  bottom: 10px;
  left: 16px;
  font-size: 12px;
  color: var(--text-gray-9);
  line-height: 12px;
  margin: 0;
}

.translation-service-container {
  background-color: var(--popup-item-background-color);
  border-radius: 12px;
}

.min-select-container {
  display: flex;
  justify-content: space-between;
  align-items: center;
  height: 44px;
  background-color: var(--popup-item-background-color);
  padding-left: 16px;
}

.min-select-container:first-child {
  border-top-left-radius: 10px;
  border-top-right-radius: 10px;
}

.min-select-container:last-child {
  border-bottom-left-radius: 10px;
  border-bottom-right-radius: 10px;
}

.min-select-container:only-child {
  border-radius: 10px;
}

.translate-mode {
  width: 44px;
  height: 44px;
  border-radius: 22px;
  background-color: var(--popup-item-background-color);
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  cursor: pointer;
}

.translate-mode svg {
  fill: var(--text-gray-2);
}

.widgets-container {
  display: flex;
  align-items: stretch;
  justify-content: space-between;
  width: 100%;
  gap: 9px;
}

/* 当只有两个小组件时的样式优化 */
.widgets-container.widgets-two-items {
  gap: 16px;
}

.widgets-container.widgets-two-items .widget-item {
  flex: 0 1 auto;
  min-width: 93px;
  max-width: 120px;
}

.widget-item {
  display: flex;
  max-width: 93px;
  flex-direction: row;
  align-items: center;
  justify-content: center;
  background-color: var(--popup-item-background-color);
  font-size: 12px;
  min-height: 44px;
  height: 100%;
  border-radius: 8px;
  cursor: pointer;
  flex: 1;
  padding: 8px 4px;
  text-align: center;
}

.widget-icon-text {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  color: var(--text-gray-2);
}

.widgets-container svg {
  fill: var(--text-gray-2);
  color: var(--text-gray-2);
}

.share-button-container {
  display: flex;
  align-items: center;
  cursor: pointer;
  padding: 2px 3px 0 8px;
}

.share-button-container svg {
  fill: var(--text-gray-9);
}

.min-select-container:hover,
.language-select-container:hover,
.widget-item:hover,
.translate-mode:hover {
  background-color: var(--popup-item-hover-background-color);
}

.main-button:hover {
  background-color: #f5508f;
}

.share-button-container:hover {
  background-color: var(--popup-item-background-color);
  border-radius: 6px;
}

.error-boundary {
  background: #fff2f0;
  border: 1px solid #ffccc7;
  display: flex;
  padding: 12px;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.88);
  word-break: break-all;
  margin: 12px;
  border-radius: 12px;
  flex-direction: column;
}

.upgrade-pro {
  border-radius: 11px;
  background: linear-gradient(57deg, #272727 19.8%, #696969 82.2%);
  padding: 2px 8px;
  transform: scale(0.85);
}

.upgrade-pro span {
  background: linear-gradient(180deg, #ffeab4 17.65%, #f8c235 85.29%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  font-size: 12px;
  margin-left: 4px;
}

.upgrade-pro svg {
  margin-top: -2px;
}

.upgrade-pro:hover {
  background: linear-gradient(57deg, #3d3d3d 19.8%, #949494 82.2%);
}

.border-bottom-radius-0 {
  border-bottom-left-radius: 0 !important;
  border-bottom-right-radius: 0 !important;
}

.trial-pro-container {
  border-radius: 0px 0px 12px 12px;
  background: var(--popup-trial-pro-background-color);
  display: flex;
  align-items: center;
  height: 44px;
  padding-left: 16px;
  padding-right: 12px;
  font-size: 12px;
}

.trial-pro-container label {
  line-height: 13px;
  color: var(--text-black-2);
}

.trial-pro-container img {
  margin-left: 5px;
}

.cursor-pointer {
  cursor: pointer;
}

.upgrade-pro-discount-act {
  height: 25px;
  display: flex;
  padding: 0 4px;
  align-items: center;
  border-radius: 15px;
  background: linear-gradient(
    90deg,
    #cefbfa 11.33%,
    #d7f56f 63.75%,
    #fccd5e 100%
  );
  transform: scale(0.9);
  box-shadow: 0px 1.8px 3.6px 0px rgba(0, 0, 0, 0.1);
  cursor: pointer;
}

.upgrade-pro-discount-act span {
  font-size: 12px;
  font-weight: 700;
  margin-left: 4px;
  color: #222222;
}

.upgrade-pro-discount-act:hover {
  text-decoration: unset;
  background: linear-gradient(
    90deg,
    #e2fffe 11.33%,
    #e6ff91 63.75%,
    #ffdf93 100%
  );
}

.custom-select-container {
  width: 200px;
  position: relative;
  flex: 1;
}

#translation-service-select {
  padding-right: 12px;
  padding-left: 6px;
}

.custom-select-content {
  border-radius: 12px;
  background: var(--popup-content-background-color);
  box-shadow: var(--service-select-content-shadow);
  border: 1px solid var(--service-select-border-color);
  padding: 4px 5px;
  position: absolute;
  left: -10px;
  right: 0;
  z-index: 100;
  overflow-y: auto;
}

.custom-select-item.default {
  width: 100%;
  padding: 0;
}

.custom-select-item {
  font-size: 13px;
  padding: 5px 6px;
  border-radius: 8px;
  display: flex;
  align-items: center;
  cursor: pointer;
  color: var(--text-black-2);
  width: auto;
  overflow: hidden;
  height: 30px;
  line-height: 30px;
}

.custom-select-item-img {
  width: 20px;
  height: 20px;
  margin-right: 4px;
}

@media (prefers-color-scheme: dark) {
  .custom-select-item-img {
    margin-right: 6px;
  }
}

.custom-select-content .custom-select-item.selected,
.custom-select-content .custom-select-item:hover {
  background: var(--service-select-selected-background-color);
}

.custom-select-item > span {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.custom-select-item-pro {
  font-size: 12px;
  margin-left: 6px;
  display: flex;
}

.custom-select-item-pro img {
  margin: 0 3px;
  width: 20px;
  flex-shrink: 0;
}

.custom-select-group-header {
  font-size: 12px;
  font-weight: 500;
  color: var(--text-gray-9);
  padding: 6px 8px 4px;
  margin-top: 2px;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

.more-container {
  position: relative;
}

.new-menu-indicator {
  position: absolute;
  width: 8px;
  height: 8px;
  background-color: #ef3434;
  border-radius: 50%;
  right: 18px;
  top: 4px;
}

.download-app {
  display: inline-flex;
  align-items: center;
  gap: 4px;
  border-radius: 8px;
  background: var(--download-app-background);
  padding: 4px 8px;
  color: var(--text-gray-6);
  font-size: 12px;
  cursor: pointer;
  transition: all 0.2s ease-in-out;
}

/* Popup 动画效果 */
@keyframes popup-fade-in {
  from {
    opacity: 0;
    transform: translateY(10px) scale(0.95);
  }
  to {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
}

@keyframes popup-fade-out {
  from {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
  to {
    opacity: 0;
    transform: translateY(10px) scale(0.95);
  }
}

.popup-generic-content {
  animation: popup-fade-in 0.2s ease-out;
}

.popup-generic-content.hiding {
  animation: popup-fade-out 0.15s ease-in;
}

html {
  font-size: 17px;
}

@media print {
  .imt-fb-container {
    display: none !important;
  }
}

#mount#mount {
  position: absolute;
  display: none;
  min-width: 250px;
  height: auto;
  --font-size: 17px;
  font-size: 17px;
}

/* float-ball */
.imt-fb-container {
  position: fixed;
  padding: 0;
  top: 335px;
  width: fit-content;
  display: flex;
  flex-direction: column;
  display: none;
  direction: ltr;
}

.imt-fb-container.left {
  align-items: flex-start;
  left: 0;
}

.imt-fb-container.right {
  align-items: flex-end;
  right: 0;
}

.imt-fb-btn {
  cursor: pointer;
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 56px;
  box-shadow: 2px 6px 10px 0px #0e121629;
}

.imt-fb-btn.left {
  border-top-right-radius: 36px;
  border-bottom-right-radius: 36px;
}

.imt-fb-btn.right {
  border-top-left-radius: 36px;
  border-bottom-left-radius: 36px;
}

.imt-fb-btn div {
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 54px;
  display: flex;
  align-items: center;
}

.imt-fb-btn.left div {
  border-top-right-radius: 34px;
  border-bottom-right-radius: 34px;
  justify-content: flex-end;
}

.imt-fb-btn.right div {
  border-top-left-radius: 34px;
  border-bottom-left-radius: 34px;
}

.imt-fb-logo-img {
  width: 20px;
  height: 20px;
  margin: 0 10px;
}

.imt-fb-logo-img-big-bg {
  width: 28px;
  height: 28px;
  margin: 0;
  padding: 4px;
  background-color: #ed6d8f;
  border-radius: 50%;
  margin: 0 5px;
}

.imt-float-ball-translated {
  position: absolute;
  width: 11px;
  height: 11px;
  bottom: 4px;
  right: 20px;
}

.btn-animate {
  -webkit-transform: translate3d(0, 0, 0);
  transform: translate3d(0, 0, 0);
  -webkit-transition: -webkit-transform ease-out 250ms;
  transition: -webkit-transform ease-out 250ms;
  transition: transform ease-out 250ms;
  transition: transform ease-out 250ms, -webkit-transform ease-out 250ms;
}

.imt-fb-setting-btn {
  margin-right: 18px;
  width: 28px;
  height: 28px;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 20px;
  box-shadow: 2px 10px 24px 0px #0e121614;
  border: none;
}

.popup-container {
  border-radius: 20px;
}

.popup-content {
  border-radius: 20px 20px 12px 12px;
}
.popup-footer {
  border-radius: 20px;
}

.imt-fb-close-button {
  pointer-events: all;
  cursor: pointer;
  position: absolute;
  margin-top: -10px;
}

.imt-fb-close-content {
  padding: 22px;
  width: 320px;
  pointer-events: all;
}

.imt-fb-close-title {
  font-weight: 500;
  color: var(--h2-color);
}

.imt-fb-close-radio-content {
  background-color: var(--background-light-green);
  padding: 8px 20px;
}

.imt-fb-radio-sel,
.imt-fb-radio-nor {
  width: 16px;
  height: 16px;
  border-radius: 8px;
  flex-shrink: 0;
}

.imt-fb-radio-sel {
  border: 2px solid var(--primary);
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-radio-sel div {
  width: 8px;
  height: 8px;
  border-radius: 4px;
  background-color: var(--primary);
}

.imt-fb-radio-nor {
  border: 2px solid #d3d4d6;
}

.imt-fb-primary-btn {
  background-color: var(--primary);
  width: 72px;
  height: 32px;
  color: white;
  border-radius: 8px;
  text-align: center;
  line-height: 32px;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-default-btn {
  border: 1px solid var(--primary);
  width: 72px;
  height: 32px;
  border-radius: 8px;
  color: var(--primary);
  line-height: 32px;
  text-align: center;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-guide-container {
  width: 312px;
  transform: translateY(-45%);
}

.imt-fb-guide-bg {
  position: absolute;
  left: 30px;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  height: 100%;
  width: 90%;
}

.imt-fb-guide-bg.left {
  transform: scaleX(-1);
}

.imt-fb-guide-content {
  margin: 16px -30px 80px 0px;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.imt-fb-guide-content.left {
  margin: 16px 21px 60px 32px;
}

.imt-fb-guide-img {
  width: 220px;
  height: 112px;
}

.imt-fb-guide-message {
  font-size: 14px;
  line-height: 28px;
  color: #333333;
  white-space: pre-wrap;
  text-align: center;
  font-weight: 700;
  margin-bottom: 20px;
}

.imt-manga-guide-message {
  font-size: 16px;
  line-height: 24px;
  color: #333333;
  text-align: center;
  font-weight: 500;
  margin-bottom: 12px;
}

.imt-fb-guide-button {
  margin-top: 16px;
  line-height: 40px;
  height: 40px;
  padding: 0 20px;
  width: unset;
}

.imt-fb-more-buttons {
  box-shadow: 0px 2px 10px 0px #00000014;
  border: none;
  background: var(--float-ball-more-button-background-color);
  width: 36px;
  display: flex;
  flex-direction: column;
  border-radius: 18px;
  margin-top: 0px;
  padding: 7px 0 7px 0;
}

.imt-fb-more-buttons > div {
  margin: auto;
}

.imt-fb-side,
.imt-fb-reward {
  border-radius: 50%;
  cursor: pointer;
  pointer-events: all;
  position: relative;
}

.imt-fb-side {
  margin: 10px 0;
}

.imt-fb-new-badge {
  width: 26px;
  height: 14px;
  padding: 3px;
  background-color: #f53f3f;
  border-radius: 4px;
  position: absolute;
  top: -5px;
  right: 15px;
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-side *,
.imt-fb-reward * {
  pointer-events: all;
}

.imt-fb-more-button {
  width: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
}
/* Sheet.css */
.immersive-translate-sheet {
  position: fixed;
  transform: translateY(100%);
  /* Start off screen */
  left: 0;
  right: 0;
  background-color: white;
  transition: transform 0.3s ease-out;
  /* Smooth slide transition */
  box-shadow: 0px -2px 10px rgba(0, 0, 0, 0.1);
  /* Ensure it's above other content */
  bottom: 0;
  border-top-left-radius: 16px;
  border-top-right-radius: 16px;
  overflow: hidden;
}

.immersive-translate-sheet.visible {
  transform: translateY(0);
}

.immersive-translate-sheet-backdrop {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  opacity: 0;
  transition: opacity 0.3s ease-out;
}

.immersive-translate-sheet-backdrop.visible {
  opacity: 1;
}

.popup-container-sheet {
  max-width: 100vw;
  width: 100vw;
}

.imt-no-events svg * {
  pointer-events: none !important;
}

.imt-manga-button {
  width: 36px;
  display: flex;
  flex-direction: column;
  position: relative;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  pointer-events: all;
  margin: 0 0 10px 0;
  background-color: var(--float-ball-more-button-background-color);
  border-radius: 18px;
  filter: drop-shadow(0px 2px 10px rgba(0, 0, 0, 0.08));
  opacity: 0.5;
  right: 8px;
  padding: 10px 0 4px 0;
}

.imt-manga-feedback {
  cursor: pointer;
  margin-bottom: 10px;
}

.imt-fb-feedback {
  cursor: pointer;
  margin-top: 10px;
}

.imt-fb-upgrade-button {
  cursor: pointer;
  margin-top: 10px;
}

.imt-manga-button:hover {
  opacity: 1;
}

.imt-manga-translated {
  position: absolute;
  left: 24px;
  top: 20px;
}

.imt-float-ball-loading {
  animation: imt-loading-animation 0.6s infinite linear !important;
}

.imt-manga-guide-bg {
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  width: 372px;
  transform: translateY(-50%);
}
.imt-manga-guide-content {
  position: absolute;
  top: 15px;
  left: 0;
  right: 0;
  margin: 0 40px 0;
}

.img-manga-guide-button {
  width: fit-content;
  margin: 0 auto;
}

.img-manga-close {
  position: absolute;
  bottom: -200px;
  width: 32px;
  height: 32px;
  left: 0;
  right: 0;
  margin: auto;
  cursor: pointer;
}

.imt-fb-container.dragging .imt-fb-more-buttons,
.imt-fb-container.dragging .imt-manga-button,
.imt-fb-container.dragging .btn-animate:not(.imt-fb-btn) {
  display: none !important;
}

.imt-fb-container.dragging .imt-fb-btn {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  cursor: move !important;
}

.imt-fb-container.dragging .imt-fb-btn div {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  margin: 0 !important;
}

.imt-fb-container.dragging .imt-fb-btn.left,
.imt-fb-container.dragging .imt-fb-btn.right {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-btn.left div,
.imt-fb-container.dragging .imt-fb-btn.right div {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-logo-img {
  margin: 0 !important;
  padding: 4px !important;
}

.imt-fb-container.dragging .imt-float-ball-translated {
  right: 2px !important;
  bottom: 2px !important;
}

@-webkit-keyframes imt-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes imt-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.imt-fb-icon {
  color: #666666;
}

[data-theme="dark"] .imt-fb-icon {
  color: #B3B3B3;
}

[data-theme="light"] .imt-fb-icon {
  color: #666666;
}
</style><div id="mount" style="display: block;"><div class="imt-fb-container right notranslate " data-theme="dark" style="z-index: 2147483637; pointer-events: none; right: 0px; top: 261px; display: flex;"><div class="btn-animate" style="transform: translateX(-4px); opacity: 0.7; padding-left: 10px;"><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-btn imt-fb-more-button imt-fb-side"><svg class="imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M8.60547 12.9228C8.84029 12.9228 9.03755 13.0022 9.19629 13.161C9.3551 13.3198 9.43457 13.5171 9.43457 13.7519V18.5107C9.43457 18.7453 9.35513 18.9426 9.19629 19.1015C9.03755 19.2602 8.84029 19.3398 8.60547 19.3398H3.8457C3.61127 19.3397 3.41464 19.26 3.25586 19.1015C3.09712 18.9426 3.01758 18.7453 3.01758 18.5107V13.7519C3.01758 13.517 3.09712 13.3198 3.25586 13.161C3.41465 13.0023 3.61125 12.9229 3.8457 12.9228H8.60547ZM17.208 12.9228C17.4427 12.9228 17.6399 13.0022 17.7988 13.161C17.9575 13.3198 18.0371 13.5171 18.0371 13.7519V18.5107C18.0371 18.7453 17.9576 18.9426 17.7988 19.1015C17.6399 19.2602 17.4427 19.3398 17.208 19.3398H12.4492C12.2144 19.3398 12.0171 19.2602 11.8584 19.1015C11.6995 18.9426 11.6201 18.7453 11.6201 18.5107V13.7519C11.6201 13.517 11.6995 13.3198 11.8584 13.161C12.0171 13.0022 12.2144 12.9228 12.4492 12.9228H17.208ZM4.39258 17.9648H8.05957V14.2978H4.39258V17.9648ZM12.9951 17.9648H16.6621V14.2978H12.9951V17.9648ZM14.7598 2.92179C14.8641 2.57295 15.3576 2.57295 15.4619 2.92179L15.9561 4.57511C16.1376 5.18219 16.5965 5.66815 17.1924 5.8837L18.7412 6.44327C19.0635 6.56002 19.0633 7.01583 18.7412 7.13273L17.1924 7.69327C16.5966 7.90881 16.1376 8.39389 15.9561 9.00089L15.4619 10.6552C15.3575 11.0038 14.8642 11.0037 14.7598 10.6552L14.2646 9.00089C14.0831 8.39401 13.625 7.90881 13.0293 7.69327L11.4805 7.13273C11.158 7.01598 11.1579 6.55996 11.4805 6.44327L13.0293 5.8837C13.6251 5.66814 14.0831 5.18219 14.2646 4.57511L14.7598 2.92179ZM8.60547 4.32023C8.84029 4.32023 9.03755 4.39977 9.19629 4.55851C9.35496 4.71727 9.43448 4.91396 9.43457 5.14835V9.90812C9.43457 10.1429 9.35518 10.3402 9.19629 10.4989C9.03755 10.6578 8.84029 10.7372 8.60547 10.7372H3.8457C3.61131 10.7371 3.41463 10.6576 3.25586 10.4989C3.09712 10.3402 3.01758 10.1429 3.01758 9.90812V5.14835C3.01767 4.91386 3.09721 4.71731 3.25586 4.55851C3.41466 4.39986 3.61121 4.32032 3.8457 4.32023H8.60547ZM4.39258 9.36222H8.05957V5.69523H4.39258V9.36222Z" fill="currentColor"></path></svg><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg" style="position: absolute; right: 0px; top: 0px; display: none; transform: translate(30%, -30%);"><g clip-path="url(#clip0_34242_2353)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14Z" fill="#B1B1B1" fill-opacity="0.32"></path><mask id="mask0_34242_2353" maskUnits="userSpaceOnUse" x="1" y="1" width="12" height="12" style="mask-type: alpha;"><rect x="1" y="1" width="12" height="12" fill="#D9D9D9"></rect></mask><g mask="url(#mask0_34242_2353)"><path d="M7.86447 3.67324H6.13622V4.72999L4.80409 3.39199C4.75018 3.33699 4.70972 3.27808 4.68272 3.21524C4.65572 3.15241 4.64222 3.09533 4.64222 3.04399C4.64222 2.93141 4.68193 2.8352 4.76134 2.75537C4.84076 2.67562 4.94514 2.63574 5.07447 2.63574H8.98322C9.12864 2.63574 9.25147 2.68883 9.35172 2.79499C9.45189 2.90124 9.50197 3.04578 9.50197 3.22862C9.50197 3.35203 9.46122 3.46245 9.37972 3.55987C9.29822 3.65737 9.18897 3.69516 9.05197 3.67324H8.90197V6.36774C8.90197 6.51316 8.85214 6.63599 8.75247 6.73624C8.65272 6.83641 8.53051 6.88649 8.38585 6.88649C8.24118 6.88649 8.11809 6.83641 8.01659 6.73624C7.91518 6.63599 7.86447 6.51316 7.86447 6.36774V3.67324ZM6.4816 11.974V9.13599H4.57509C4.36193 9.13599 4.19043 9.06703 4.06059 8.92912C3.93076 8.79112 3.86584 8.62983 3.86584 8.44524C3.86584 8.35591 3.88509 8.26499 3.92359 8.17249C3.96209 8.08008 4.01984 7.99437 4.09684 7.91537L5.09872 6.89549V6.36149L2.32422 3.58412C2.22664 3.48645 2.1788 3.37678 2.18072 3.25512C2.18272 3.13345 2.23155 3.02483 2.32722 2.92924C2.42489 2.83158 2.53614 2.78274 2.66097 2.78274C2.7858 2.78274 2.89701 2.83158 2.99459 2.92924L10.9898 10.9245C11.0863 11.0209 11.1351 11.13 11.1361 11.2516C11.1371 11.3733 11.0898 11.4839 10.9941 11.5835C10.8984 11.6772 10.7867 11.7235 10.6588 11.7225C10.5311 11.7215 10.4194 11.6732 10.3237 11.5776L7.87909 9.13599L7.51909 9.14199V11.974C7.51909 12.1195 7.46926 12.2423 7.3696 12.3425C7.26985 12.4427 7.14764 12.4927 7.00297 12.4927C6.8583 12.4927 6.73522 12.4427 6.63372 12.3425C6.5323 12.2423 6.4816 12.1195 6.4816 11.974ZM5.35909 8.09849H6.83872L6.08834 7.35124L6.09434 7.35724L5.35909 8.09849Z" fill="white"></path></g></g><defs><clippath id="clip0_34242_2353"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div></div></div></div><div hidden="" class="imt-no-events btn-animate " id="manga-button" style="position: relative;"><div class="imt-manga-button" style="transform: translateX(2px);"><div class=" " style="position: relative; pointer-events: all; display: inline-block;"><div><svg class="imt-manga-feedback imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.0003 14.2749C11.213 14.2749 11.3895 14.2047 11.5299 14.0643C11.6705 13.9239 11.7408 13.7473 11.7408 13.5345C11.7408 13.3218 11.6705 13.1453 11.5299 13.0049C11.3895 12.8645 11.213 12.7943 11.0003 12.7943C10.7877 12.7943 10.6111 12.8645 10.4707 13.0049C10.3302 13.1453 10.2599 13.3218 10.2599 13.5345C10.2599 13.7473 10.3302 13.9239 10.4707 14.0643C10.6111 14.2047 10.7877 14.2749 11.0003 14.2749ZM11.0003 11.0842C11.1954 11.0842 11.3587 11.0185 11.4903 10.8869C11.622 10.7552 11.6878 10.5918 11.6878 10.3967V6.23645C11.6878 6.04135 11.622 5.87803 11.4903 5.74649C11.3587 5.6148 11.1954 5.54895 11.0003 5.54895C10.8052 5.54895 10.6419 5.6148 10.5104 5.74649C10.3787 5.87803 10.3128 6.04135 10.3128 6.23645V10.3967C10.3128 10.5918 10.3787 10.7552 10.5104 10.8869C10.6419 11.0185 10.8052 11.0842 11.0003 11.0842ZM5.53562 16.8311L3.70045 18.666C3.43966 18.9269 3.13968 18.9861 2.80051 18.8434C2.4615 18.7005 2.29199 18.4434 2.29199 18.072V4.73816C2.29199 4.27509 2.45241 3.88314 2.77324 3.5623C3.09408 3.24147 3.48603 3.08105 3.9491 3.08105H18.0516C18.5146 3.08105 18.9066 3.24147 19.2274 3.5623C19.5482 3.88314 19.7087 4.27509 19.7087 4.73816V15.174C19.7087 15.637 19.5482 16.029 19.2274 16.3498C18.9066 16.6706 18.5146 16.8311 18.0516 16.8311H5.53562ZM4.95033 15.4561H18.0516C18.1221 15.4561 18.1868 15.4266 18.2454 15.3678C18.3042 15.3092 18.3337 15.2445 18.3337 15.174V4.73816C18.3337 4.66758 18.3042 4.60295 18.2454 4.54428C18.1868 4.48546 18.1221 4.45605 18.0516 4.45605H3.9491C3.87851 4.45605 3.81389 4.48546 3.75522 4.54428C3.6964 4.60295 3.66699 4.66758 3.66699 4.73816V16.7254L4.95033 15.4561Z" fill="currentColor"></path></svg></div></div><div style="position: relative;"><svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="manhua"><path id="Vector" d="M14.8853 4.92364C14.8853 4.92364 16.3905 10.4362 22.6668 4C22.6668 4 20.3381 10.8907 25.3364 10.0843C25.3364 10.0843 22.0563 15.6994 29 18.0599C29 18.0599 22.9934 19.306 21.1617 28C21.1617 28 17.7679 24.54 14.8853 27.3549C14.8853 27.3549 13.3233 23.5724 7.33097 26.27C7.33097 26.27 10.1141 20.6549 4.83179 21.0507C4.83179 21.0507 7.16057 18.8955 3 15.9047C3 15.9047 7.50137 16.1833 6.33697 11.7117C6.33697 11.7117 10.0005 12.3421 8.66576 6.82957C8.65156 6.81491 12.4855 9.80574 14.8853 4.92364Z" fill="#ED6D8F"></path><path id="Vector_2" d="M20.8599 13.7022C20.885 13.1361 20.9543 12.5713 20.9959 12.0052C21.0337 11.568 20.8107 11.2794 20.3876 11.18C20.0759 11.1013 19.7508 11.0867 19.433 11.137C19.1951 11.1945 18.9542 11.2396 18.7113 11.2721C18.2403 11.3028 17.9973 11.5275 17.9796 11.988C17.977 12.0833 17.9596 12.1777 17.928 12.268C17.3034 13.9102 16.6774 15.5499 16.0503 17.1873C16.0301 17.2401 16.0062 17.2904 15.9671 17.3776C15.7291 16.8975 15.4281 16.4898 15.2745 15.9986C14.8073 14.5152 14.3186 13.033 13.8312 11.5594C13.6826 11.1112 13.3489 10.9344 12.8754 11.0216C12.7889 11.0365 12.7008 11.0398 12.6134 11.0314C12.2241 10.9938 11.8311 11.0404 11.4623 11.1677C11.0946 11.2991 10.9498 11.557 11.0152 11.9254C11.0428 12.0371 11.0643 12.1503 11.0795 12.2643C11.1223 13.1902 11.1777 14.1087 11.2054 15.0321C11.257 16.7992 11.2117 18.5651 11.0858 20.3284C11.0644 20.6354 11.0304 20.9424 11.0228 21.2494C11.0115 21.6092 11.1613 21.7811 11.5266 21.8143C11.9976 21.8573 12.4711 21.8708 12.9421 21.9088C13.0309 21.9201 13.121 21.9003 13.1962 21.8528C13.2714 21.8053 13.3268 21.7334 13.3527 21.6497C13.3996 21.5394 13.4252 21.4216 13.4282 21.3022C13.4295 20.8258 13.4207 20.3493 13.4081 19.8741C13.393 19.3264 13.3917 18.7763 13.3438 18.231C13.2857 17.5839 13.266 16.934 13.2847 16.2847C13.2847 16.2466 13.291 16.2073 13.2985 16.1312C13.3338 16.2024 13.3514 16.2356 13.3665 16.2712C13.9017 17.5228 14.3617 18.8037 14.7443 20.1074C14.7928 20.2421 14.7928 20.3889 14.7443 20.5237C14.6322 20.8196 14.7141 21.037 14.9659 21.1377C15.4445 21.3268 15.9331 21.4926 16.4155 21.6731C16.4865 21.7033 16.566 21.7091 16.6408 21.6895C16.7157 21.6698 16.7815 21.6259 16.8273 21.565C16.9085 21.4643 16.9743 21.3526 17.0225 21.2335C17.0537 21.1374 17.0798 21.0399 17.1006 20.9412C17.3185 20.2425 17.5653 19.5499 17.7517 18.8438C17.9785 17.9723 18.2624 17.1158 18.6018 16.2798C18.6201 16.2439 18.6411 16.2094 18.6647 16.1766C18.6761 16.2319 18.6761 16.254 18.6761 16.2761C18.6345 17.59 18.5955 18.8978 18.5501 20.2056C18.5363 20.5949 18.491 20.9829 18.4809 21.3722C18.4721 21.705 18.6207 21.8708 18.9557 21.9002C19.4355 21.9432 19.9191 21.9592 20.4002 21.9973C20.4888 22.0079 20.5784 21.9875 20.653 21.9399C20.7277 21.8922 20.7827 21.8203 20.8082 21.7369C20.8531 21.6305 20.8766 21.5167 20.8775 21.4017C20.88 20.7668 20.8674 20.132 20.8674 19.4971C20.8662 19.2846 20.8687 19.0722 20.8523 18.8622C20.8158 18.3968 20.7264 17.9314 20.7339 17.4685C20.7515 16.2122 20.8044 14.9572 20.8599 13.7022Z" fill="white"></path></g></svg><svg hidden="true" class="imt-manga-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#60BB4C"></circle><path d="M1.40869 5.87858L2.24161 5.18962L4.15357 6.64214C4.15357 6.64214 6.33559 4.15566 9.0067 2.48145L9.32553 2.87514C9.32553 2.87514 6.28678 5.55844 4.71748 9.07881L1.40869 5.87858Z" fill="#EFF8ED"></path></svg></div><svg class="imt-float-ball-loading" hidden="true" width="19" height="19" viewBox="0 0 19 19" fill="none" xmlns="http://www.w3.org/2000/svg" style="margin: 9px;"><path d="M9.42859 0C9.84288 0 10.1929 0.387143 10.1929 0.847143V3.99429C10.1929 4.45429 9.84431 4.84143 9.42859 4.84143C9.01431 4.84143 8.66431 4.45571 8.66431 3.99429V0.847143C8.66431 0.387143 9.01288 0 9.42859 0Z" fill="#E9E9E9"></path><path d="M14.1301 1.38877C14.5158 1.62591 14.6301 2.12163 14.4258 2.52305L12.9515 5.19448C12.901 5.28714 12.8325 5.36876 12.75 5.43455C12.6675 5.50035 12.5727 5.54898 12.4712 5.5776C12.3696 5.60621 12.2634 5.61424 12.1586 5.60119C12.0539 5.58814 11.9529 5.55429 11.8615 5.50163C11.6787 5.38432 11.5468 5.20237 11.4923 4.9921C11.4377 4.78184 11.4645 4.55874 11.5672 4.36734L13.0415 1.69591C13.2686 1.29448 13.7443 1.15305 14.1301 1.38877Z" fill="#989697"></path><path d="M17.4685 4.75707C17.5813 4.95451 17.6123 5.18824 17.5549 5.40825C17.4975 5.62826 17.3563 5.81705 17.1614 5.93422L14.4971 7.52564C14.0971 7.76993 13.6014 7.62422 13.3657 7.20707C13.2532 7.00994 13.2222 6.77667 13.2793 6.55702C13.3365 6.33737 13.4771 6.14874 13.6714 6.03136L16.3357 4.43993C16.7371 4.21993 17.2557 4.34136 17.4685 4.7585V4.75707Z" fill="#9B999A"></path><path d="M18.8572 9.42835C18.8572 9.84263 18.47 10.1926 18.01 10.1926H14.8629C14.4029 10.1926 14.0157 9.84406 14.0157 9.42835C14.0157 9.01406 14.4029 8.66406 14.8629 8.66406H18.01C18.47 8.66406 18.8572 9.01263 18.8572 9.42835Z" fill="#A3A1A2"></path><path d="M17.4686 14.1303C17.3515 14.3134 17.1697 14.4455 16.9594 14.5003C16.7491 14.5552 16.5259 14.5286 16.3343 14.426L13.6629 12.9517C13.5702 12.9012 13.4886 12.8327 13.4228 12.7503C13.357 12.6678 13.3084 12.573 13.2798 12.4714C13.2512 12.3698 13.2431 12.2636 13.2562 12.1589C13.2692 12.0542 13.3031 11.9532 13.3558 11.8617C13.4731 11.6789 13.655 11.547 13.8653 11.4925C14.0755 11.4379 14.2986 11.4647 14.49 11.5674L17.1615 13.0417C17.5629 13.2689 17.7043 13.7446 17.4686 14.1303Z" fill="#ABA9AA"></path><path opacity="0.7" d="M14.1 17.4686C13.9026 17.5814 13.6689 17.6124 13.4489 17.555C13.2288 17.4976 13.04 17.3564 12.9229 17.1615L11.3315 14.4972C11.0872 14.0972 11.2329 13.6015 11.65 13.3658C11.8472 13.2533 12.0804 13.2224 12.3001 13.2795C12.5197 13.3366 12.7084 13.4773 12.8257 13.6715L14.4172 16.3358C14.6372 16.7372 14.5157 17.2558 14.0986 17.4686H14.1Z" fill="#B2B2B2"></path><path opacity="0.6" d="M9.42859 18.8571C9.01431 18.8571 8.66431 18.4699 8.66431 18.0099V14.8628C8.66431 14.4028 9.01288 14.0156 9.42859 14.0156C9.84288 14.0156 10.1929 14.4028 10.1929 14.8628V18.0099C10.1929 18.4699 9.84431 18.8571 9.42859 18.8571Z" fill="#BAB8B9"></path><path opacity="0.5" d="M4.72717 17.4685C4.5441 17.3514 4.41195 17.1696 4.35713 16.9593C4.30231 16.749 4.32885 16.5258 4.43145 16.3342L5.90574 13.6628C5.95622 13.5701 6.02472 13.4885 6.1072 13.4227C6.18969 13.3569 6.2845 13.3083 6.38606 13.2797C6.48762 13.251 6.59387 13.243 6.69857 13.2561C6.80327 13.2691 6.90431 13.303 6.99574 13.3556C7.38145 13.5914 7.49431 14.0885 7.29002 14.4899L5.81574 17.1614C5.5886 17.5628 5.11288 17.7042 4.72717 17.4685Z" fill="#C2C0C1"></path><path opacity="0.4" d="M1.38862 14.1002C1.27584 13.9027 1.24483 13.669 1.30223 13.449C1.35964 13.229 1.50089 13.0402 1.69576 12.923L4.36004 11.3316C4.76004 11.0873 5.25576 11.233 5.49147 11.6502C5.60393 11.8473 5.63491 12.0806 5.5778 12.3002C5.52069 12.5199 5.38 12.7085 5.18576 12.8259L2.52004 14.4173C2.12004 14.6373 1.60004 14.5159 1.38862 14.0987V14.1002Z" fill="#CBCBCB"></path><path d="M0 9.42835C0 9.01406 0.387143 8.66406 0.847143 8.66406H3.99429C4.45429 8.66406 4.84143 9.01263 4.84143 9.42835C4.84143 9.84263 4.45571 10.1926 3.99429 10.1926H0.847143C0.387143 10.1926 0 9.84406 0 9.42835Z" fill="#D2D2D2"></path><path opacity="0.2" d="M1.38852 4.72705C1.50561 4.54398 1.68746 4.41183 1.89774 4.35701C2.10803 4.30219 2.33125 4.32873 2.52281 4.43133L5.19424 5.90562C5.28689 5.9561 5.36851 6.0246 5.43431 6.10708C5.5001 6.18957 5.54874 6.28438 5.57735 6.38594C5.60597 6.48749 5.61399 6.59375 5.60094 6.69845C5.5879 6.80315 5.55405 6.90419 5.50138 6.99562C5.38407 7.17844 5.20212 7.31029 4.99186 7.36484C4.78159 7.4194 4.55849 7.39263 4.3671 7.2899L1.69567 5.81562C1.29424 5.58847 1.15281 5.11276 1.38852 4.72705Z" fill="#DADADA"></path><path d="M4.75719 1.38849C4.95463 1.27571 5.18837 1.24471 5.40838 1.30211C5.62838 1.35952 5.81718 1.50077 5.93434 1.69564L7.52577 4.35992C7.77005 4.75992 7.62434 5.25564 7.20719 5.49135C7.01006 5.60381 6.77679 5.63479 6.55714 5.57768C6.33749 5.52056 6.14886 5.37988 6.03148 5.18564L4.44005 2.51992C4.22005 2.11992 4.34148 1.59992 4.75862 1.38849H4.75719Z" fill="#E2E2E2"></path></svg></div></div><div class=" " style="position: relative; pointer-events: all; display: inline-block;"><div><div style="display: flex; align-items: center; flex-direction: row;"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: block; opacity: 0;"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg><div class="imt-fb-btn  right btn-animate " dir="ltr" style="transform: translateX(15px); opacity: 0.7;"><div><svg class="imt-fb-logo-img imt-fb-logo-img-big-bg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path fill="none" d="M0 0h24v24H0z"></path><path d="M5 15v2a2 2 0 0 0 1.85 1.995L7 19h3v2H7a4 4 0 0 1-4-4v-2h2zm13-5l4.4 11h-2.155l-1.201-3h-4.09l-1.199 3h-2.154L16 10h2zm-1 2.885L15.753 16h2.492L17 12.885zM8 2v2h4v7H8v3H6v-3H2V4h4V2h2zm9 1a4 4 0 0 1 4 4v2h-2V7a2 2 0 0 0-2-2h-3V3h3zM6 6H4v3h2V6zm4 0H8v3h2V6z" fill="rgba(255,255,255,1)"></path></svg><svg class="imt-float-ball-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#60BB4C"></circle><path d="M1.40869 5.87858L2.24161 5.18962L4.15357 6.64214C4.15357 6.64214 6.33559 4.15566 9.0067 2.48145L9.32553 2.87514C9.32553 2.87514 6.28678 5.55844 4.71748 9.07881L1.40869 5.87858Z" fill="#EFF8ED"></path></svg></div></div><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: none; opacity: 0;"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div></div></div><div style="position: relative; width: 100%; opacity: 0;"><div title="关闭悬浮球" class="imt-fb-close-button" style="transform: translateX(100%);"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div></div><div class="imt-fb-more-buttons btn-animate" style="margin-top: 10px; transform: translateX(60px);"><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-more-button"><svg class="imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg" style="width: 22px; height: 22px;"><path d="M16 7.66699H10.375" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><path d="M11.625 14.333L6 14.333" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><path d="M14.125 16C15.1605 16 16 15.1605 16 14.125C16 13.0895 15.1605 12.25 14.125 12.25C13.0895 12.25 12.25 13.0895 12.25 14.125C12.25 15.1605 13.0895 16 14.125 16Z" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><path d="M7.875 9.75C8.91053 9.75 9.75 8.91053 9.75 7.875C9.75 6.83947 8.91053 6 7.875 6C6.83947 6 6 6.83947 6 7.875C6 8.91053 6.83947 9.75 7.875 9.75Z" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><rect x="3" y="3" width="16" height="16" rx="1.66667" stroke="currentColor" stroke-width="1.4"></rect></svg></div></div></div><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-more-button"><svg class="imt-fb-feedback imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.0003 14.2749C11.213 14.2749 11.3895 14.2047 11.5299 14.0643C11.6705 13.9239 11.7408 13.7473 11.7408 13.5345C11.7408 13.3218 11.6705 13.1453 11.5299 13.0049C11.3895 12.8645 11.213 12.7943 11.0003 12.7943C10.7877 12.7943 10.6111 12.8645 10.4707 13.0049C10.3302 13.1453 10.2599 13.3218 10.2599 13.5345C10.2599 13.7473 10.3302 13.9239 10.4707 14.0643C10.6111 14.2047 10.7877 14.2749 11.0003 14.2749ZM11.0003 11.0842C11.1954 11.0842 11.3587 11.0185 11.4903 10.8869C11.622 10.7552 11.6878 10.5918 11.6878 10.3967V6.23645C11.6878 6.04135 11.622 5.87803 11.4903 5.74649C11.3587 5.6148 11.1954 5.54895 11.0003 5.54895C10.8052 5.54895 10.6419 5.6148 10.5104 5.74649C10.3787 5.87803 10.3128 6.04135 10.3128 6.23645V10.3967C10.3128 10.5918 10.3787 10.7552 10.5104 10.8869C10.6419 11.0185 10.8052 11.0842 11.0003 11.0842ZM5.53562 16.8311L3.70045 18.666C3.43966 18.9269 3.13968 18.9861 2.80051 18.8434C2.4615 18.7005 2.29199 18.4434 2.29199 18.072V4.73816C2.29199 4.27509 2.45241 3.88314 2.77324 3.5623C3.09408 3.24147 3.48603 3.08105 3.9491 3.08105H18.0516C18.5146 3.08105 18.9066 3.24147 19.2274 3.5623C19.5482 3.88314 19.7087 4.27509 19.7087 4.73816V15.174C19.7087 15.637 19.5482 16.029 19.2274 16.3498C18.9066 16.6706 18.5146 16.8311 18.0516 16.8311H5.53562ZM4.95033 15.4561H18.0516C18.1221 15.4561 18.1868 15.4266 18.2454 15.3678C18.3042 15.3092 18.3337 15.2445 18.3337 15.174V4.73816C18.3337 4.66758 18.3042 4.60295 18.2454 4.54428C18.1868 4.48546 18.1221 4.45605 18.0516 4.45605H3.9491C3.87851 4.45605 3.81389 4.48546 3.75522 4.54428C3.6964 4.60295 3.66699 4.66758 3.66699 4.73816V16.7254L4.95033 15.4561Z" fill="currentColor"></path></svg></div></div></div></div><div hidden="" id="immersive-translate-popup-overlay" class="immersive-translate-popup-overlay"><div class="immersive-translate-popup-wrapper" style="position: fixed; top: 261px; right: 65px;"></div></div></div></div></template></div></html>