<!DOCTYPE html>
<!-- saved from url=(0071)https://arxiv.org/html/2407.04295?_immersive_translate_auto_translate=1 -->
<html lang="en" data-theme="dark" imt-state="dual" imt-trans-position="after"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models: A Survey</title>
<!--Generated on Fri Aug 30 11:55:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/bootstrap.bundle.min.js"></script>
<script src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/html2canvas.min.js"></script>
<script src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/addons_new.js"></script>
<script src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/feedbackOverlay.js"></script>
<!--<base href="/html/2407.04295v2/">--><base href="."><link rel="stylesheet" href="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/utz6mli.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"><style data-id="immersive-translate-input-injected-css">.immersive-translate-input {
  position: absolute;
  top: 0;
  right: 0;
  left: 0;
  bottom: 0;
  z-index: 2147483647;
  display: flex;
  justify-content: center;
  align-items: center;
}
.immersive-translate-attach-loading::after {
  content: " ";

  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;

  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-2000%, -50%);
  z-index: 100;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.immersive-translate-input-loading {
  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;
}

@keyframes immersiveTranslateShadowRolling {
  0% {
    box-shadow: 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  12% {
    box-shadow: 100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  25% {
    box-shadow: 110px 0 var(--loading-color), 100px 0 var(--loading-color),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  36% {
    box-shadow: 120px 0 var(--loading-color), 110px 0 var(--loading-color),
      100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0);
  }

  50% {
    box-shadow: 130px 0 var(--loading-color), 120px 0 var(--loading-color),
      110px 0 var(--loading-color), 100px 0 var(--loading-color);
  }

  62% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color),
      120px 0 var(--loading-color), 110px 0 var(--loading-color);
  }

  75% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      130px 0 var(--loading-color), 120px 0 var(--loading-color);
  }

  87% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color);
  }

  100% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0);
  }
}

.immersive-translate-toast {
  display: flex;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  right: 0;
  top: 1%;
  width: fit-content;
  padding: 12px 20px;
  margin: auto;
  overflow: auto;
  background: #fef6f9;
  box-shadow: 0px 4px 10px 0px rgba(0, 10, 30, 0.06);
  font-size: 15px;
  border-radius: 8px;
  color: #333;
}

.immersive-translate-toast-content {
  display: flex;
  flex-direction: row;
  align-items: center;
}

.immersive-translate-toast-hidden {
  margin: 0 20px 0 72px;
  text-decoration: underline;
  cursor: pointer;
}

.immersive-translate-toast-close {
  color: #666666;
  font-size: 20px;
  font-weight: bold;
  padding: 0 10px;
  cursor: pointer;
}

@media screen and (max-width: 768px) {
  .immersive-translate-toast {
    top: 0;
    padding: 12px 0px 0 10px;
  }
  .immersive-translate-toast-content {
    flex-direction: column;
    text-align: center;
  }
  .immersive-translate-toast-hidden {
    margin: 10px auto;
  }
}

.immersive-translate-dialog {
  position: fixed;
  z-index: 2147483647;
  left: 0;
  top: 0;
  display: flex;
  width: 300px;
  flex-direction: column;
  align-items: center;
  font-size: 15px;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  margin: auto;
  height: fit-content;
  border-radius: 20px;
  background-color: #fff;
}

.immersive-translate-modal {
  display: none;
  position: fixed;
  z-index: 2147483647;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgb(0, 0, 0);
  background-color: rgba(0, 0, 0, 0.4);
  font-size: 15px;
}

.immersive-translate-modal-content {
  background-color: #fefefe;
  margin: 10% auto;
  padding: 40px 24px 24px;
  border-radius: 12px;
  width: 350px;
  font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  position: relative;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-content {
    margin: 25% auto !important;
  }
}

@media screen and (max-width: 480px) {
  .immersive-translate-modal-content {
    width: 80vw !important;
    margin: 20vh auto !important;
    padding: 20px 12px 12px !important;
  }

  .immersive-translate-modal-title {
    font-size: 14px !important;
  }

  .immersive-translate-modal-body {
    font-size: 13px !important;
    max-height: 60vh !important;
  }

  .immersive-translate-btn {
    font-size: 13px !important;
    padding: 8px 16px !important;
    margin: 0 4px !important;
  }

  .immersive-translate-modal-footer {
    gap: 6px !important;
    margin-top: 16px !important;
  }
}

.immersive-translate-modal .immersive-translate-modal-content-in-input {
  max-width: 500px;
}
.immersive-translate-modal-content-in-input .immersive-translate-modal-body {
  text-align: left;
  max-height: unset;
}

.immersive-translate-modal-title {
  text-align: center;
  font-size: 16px;
  font-weight: 700;
  color: #333333;
}

.immersive-translate-modal-body {
  text-align: center;
  font-size: 14px;
  font-weight: 400;
  color: #333333;
  margin-top: 24px;
  word-break: break-all;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-body {
    max-height: 250px;
    overflow-y: auto;
  }
}

.immersive-translate-close {
  color: #666666;
  position: absolute;
  right: 16px;
  top: 16px;
  font-size: 20px;
  font-weight: bold;
}

.immersive-translate-close:hover,
.immersive-translate-close:focus {
  text-decoration: none;
  cursor: pointer;
}

.immersive-translate-modal-footer {
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  margin-top: 24px;
}

.immersive-translate-btn {
  width: fit-content;
  color: #fff;
  background-color: #ea4c89;
  border: none;
  font-size: 14px;
  margin: 0 8px;
  padding: 9px 30px;
  border-radius: 5px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: background-color 0.3s ease;
}

.immersive-translate-btn-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  gap: 8px;
}

.immersive-translate-btn:hover {
  background-color: #f082ac;
}
.immersive-translate-btn:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}
.immersive-translate-btn:disabled:hover {
  background-color: #ea4c89;
}

.immersive-translate-link-btn {
  background-color: transparent;
  color: #ea4c89;
  border: none;
  cursor: pointer;
  height: 30px;
  line-height: 30px;
}

.immersive-translate-cancel-btn {
  /* gray color */
  background-color: rgb(89, 107, 120);
}

.immersive-translate-cancel-btn:hover {
  background-color: hsl(205, 20%, 32%);
}

.immersive-translate-action-btn {
  background-color: transparent;
  color: #ea4c89;
  border: 1px solid #ea4c89;
}

.immersive-translate-btn svg {
  margin-right: 5px;
}

.immersive-translate-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-primary-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-modal input[type="radio"] {
  margin: 0 6px;
  cursor: pointer;
}

.immersive-translate-modal label {
  cursor: pointer;
}

.immersive-translate-close-action {
  position: absolute;
  top: 2px;
  right: 0px;
  cursor: pointer;
}

.imt-image-status {
  background-color: rgba(0, 0, 0, 0.5) !important;
  display: flex !important;
  flex-direction: column !important;
  align-items: center !important;
  justify-content: center !important;
  border-radius: 16px !important;
}
.imt-image-status img,
.imt-image-status svg,
.imt-img-loading {
  width: 28px !important;
  height: 28px !important;
  margin: 0 0 8px 0 !important;
  min-height: 28px !important;
  min-width: 28px !important;
  position: relative !important;
}
.imt-img-loading {
  background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADgAAAA4CAMAAACfWMssAAAAtFBMVEUAAAD////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////oK74hAAAAPHRSTlMABBMIDyQXHwyBfFdDMSw+OjXCb+5RG51IvV/k0rOqlGRM6KKMhdvNyZBz9MaupmxpWyj437iYd/yJVNZeuUC7AAACt0lEQVRIx53T2XKiUBCA4QYOiyCbiAsuuGBcYtxiYtT3f6/pbqoYHVFO5r+iivpo6DpAWYpqeoFfr9f90DsYAuRSWkFnPO50OgR9PwiCUFcl2GEcx+N/YBh6pvKaefHlUgZd1zVe0NbYcQjGBfzrPE8Xz8aF+71D8gG6DHFPpc4a7xFiCDuhaWgKgGIJQ3d5IMGDrpS4S5KgpIm+en9f6PlAhKby4JwEIxlYJV9h5k5nee9GoxHJ2IDSNB0dwdad1NAxDJ/uXDHYmebdk4PdbkS58CIVHdYSUHTYYRWOJblWSyu2lmy3KNFVJNBhxcuGW4YBVCbYGRZwIooipHsNqjM4FbgOQqQqSKQQU9V8xmi1QlgHqQQ6DDBvRUVCDirs+EzGDGOQTCATgtYTnbCVLgsVgRE0T1QE0qHCFAht2z6dLvJQs3Lo2FQoDxWNUiBhaP4eRgwNkI+dAjVOA/kUrIDwf3CG8NfNOE0eiFotSuo+rBiq8tD9oY4Qzc6YJw99hl1wzpQvD7ef2M8QgnOGJfJw+EltQc+oX2yn907QB22WZcvlUpd143dqQu+8pCJZuGE4xCuPXJqqcs5sNpsI93Rmzym1k4Npk+oD1SH3/a3LOK/JpUBpWfqNySxWzCfNCUITuDG5dtuphrUJ1myeIE9bIsPiKrfqTai5WZxbhtNphYx6GEIHihyGFTI69lje/rxajdh0s0msZ0zYxyPLhYCb1CyHm9Qsd2H37Y3lugVwL9kNh8Ot8cha6fUNQ8nuXi5z9/ExsAO4zQrb/ev1yrCB7lGyQzgYDGuxq1toDN/JGvN+HyWNHKB7zEoK+PX11e12G431erGYzwmytAWU56fkMHY5JJnDRR2eZji3AwtIcrEV8Cojat/BdQ7XOwGV1e1hDjGGjXbdArm8uJZtCH5MbcctVX8A1WpqumJHwckAAAAASUVORK5CYII=");
  background-size: 28px 28px;
  animation: image-loading-rotate 1s linear infinite !important;
}

.imt-image-status span {
  color: var(--bg-2, #fff) !important;
  font-size: 14px !important;
  line-height: 14px !important;
  font-weight: 500 !important;
  font-family: "PingFang SC", Arial, sans-serif !important;
}

.imt-primary-button {
  display: flex;
  padding: 12px 80px;
  justify-content: center;
  align-items: center;
  gap: 8px;
  border-radius: 8px;
  background: #ea4c89;
  color: #fff;
  font-size: 16px;
  font-style: normal;
  font-weight: 700;
  line-height: 24px;
  border: none;
  cursor: pointer;
}

.imt-retry-text {
  color: #999;
  text-align: center;
  font-size: 14px;
  font-style: normal;
  font-weight: 400;
  line-height: 21px;
  cursor: pointer;
}

.imt-action-container {
  display: flex;
  flex-direction: column;
  gap: 12px;
}

.imt-modal-content-text {
  text-align: left;
  color: #333;
  font-size: 16px;
  font-weight: 400;
  line-height: 24px;
}

@keyframes image-loading-rotate {
  from {
    transform: rotate(360deg);
  }
  to {
    transform: rotate(0deg);
  }
}

.imt-linear-gradient-text {
  background: linear-gradient(90deg, #00a6ff 0%, #c369ff 52.4%, #ff4590 100%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
}

.imt-flex-center {
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-linear-black-btn {
  border-radius: 50px;
  background: linear-gradient(66deg, #222 19%, #696969 94.25%);
  height: 48px;
  width: 100%;
  color: #fff;
  font-size: 16px;
  font-weight: 700;
  display: flex;
  align-items: center;
  cursor: pointer;
  justify-content: center;
}
</style><style data-id="immersive-translate-default-injected-css">:root {
  --immersive-translate-theme-underline-borderColor: #72ece9;
  --immersive-translate-theme-nativeUnderline-borderColor: #72ece9;
  --immersive-translate-theme-nativeDashed-borderColor: #72ece9;
  --immersive-translate-theme-nativeDotted-borderColor: #72ece9;
  --immersive-translate-theme-highlight-backgroundColor: #ffff00;
  --immersive-translate-theme-dashed-borderColor: #59c1bd;
  --immersive-translate-theme-blockquote-borderColor: #cc3355;
  --immersive-translate-theme-thinDashed-borderColor: #ff374f;
  --immersive-translate-theme-dashedBorder-borderColor: #94a3b8;
  --immersive-translate-theme-dashedBorder-borderRadius: 0;
  --immersive-translate-theme-solidBorder-borderColor: #94a3b8;
  --immersive-translate-theme-solidBorder-borderRadius: 0;
  --immersive-translate-theme-dotted-borderColor: #94a3b8;
  --immersive-translate-theme-wavy-borderColor: #72ece9;
  --immersive-translate-theme-dividingLine-borderColor: #94a3b8;
  --immersive-translate-theme-grey-textColor: #2f4f4f;
  --immersive-translate-theme-marker-backgroundColor: #fbda41;
  --immersive-translate-theme-marker-backgroundColor-rgb: 251, 218, 65;
  --immersive-translate-theme-marker2-backgroundColor: #ffff00;
  --immersive-translate-theme-background-backgroundColor: #dbafaf;
  --immersive-translate-theme-background-backgroundColor-rgb: 219, 175, 175;
  --immersive-translate-theme-background-backgroundOpacity: 12;
  --immersive-translate-theme-opacity-opacity: 10;
}

[imt-state="dual"] .immersive-translate-target-translation-pre-whitespace {
  white-space: pre-wrap !important;
}

[imt-state="dual"] .immersive-translate-pdf-target-container {
  position: absolute;
  background-color: #fff;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica,
    sans-serif;
  top: 0;
  width: 600px;
  height: 100%;
  z-index: 2;
  line-height: 1.3;
  font-size: 16px;
}
[imt-state="dual"] .immersive-translate-target-wrapper[dir="rtl"] {
  text-align: right;
}

[imt-state="dual"]
  .immersive-translate-pdf-target-container
  .immersive-translate-target-wrapper {
  color: rgb(0, 0, 0);
  white-space: normal;
  position: absolute;
}

[imt-state="dual"]
  .immersive-translate-pdf-target-container
  .immersive-translate-target-wrapper
  font {
  color: inherit;
  white-space: inherit;
  position: unset;
}

[imt-state="translation"] .immersive-translate-target-wrapper > br {
  display: none;
}

[imt-state="translation"]
  .immersive-translate-target-translation-block-wrapper {
  margin: 0 !important;
}

[imt-state="dual"] .immersive-translate-target-translation-block-wrapper {
  margin: 8px 0 !important;
  display: inline-block;
}

[imt-trans-position="before"]
  .immersive-translate-target-translation-block-wrapper {
  display: block;
}

[imt-trans-position="before"]
  .immersive-translate-target-translation-block-wrapper {
  margin-top: 0 !important;
}

[imt-state="dual"] .immersive-translate-target-translation-pdf-block-wrapper {
  margin: 0 !important;
  display: inline-block;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-grey-inner {
  color: var(--immersive-translate-theme-grey-textColor);
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-underline-inner {
  border-bottom: 1px solid
    var(--immersive-translate-theme-underline-borderColor) !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-nativeUnderline-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-nativeUnderline-borderColor
  ) !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-dashedBorder {
  border: 1px dashed var(--immersive-translate-theme-dashedBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-dashedBorder-borderRadius
  ) !important;
  padding: 6px;
  margin-top: 2px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-inline-wrapper-theme-dashedBorder {
  border: 1px dashed var(--immersive-translate-theme-dashedBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-dashedBorder-borderRadius
  ) !important;
  padding: 2px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-solidBorder {
  border: 1px solid var(--immersive-translate-theme-solidBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-solidBorder-borderRadius
  ) !important;
  padding: 6px;
  margin-top: 2px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-inline-wrapper-theme-solidBorder {
  border: 1px solid var(--immersive-translate-theme-solidBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-solidBorder-borderRadius
  ) !important;
  padding: 2px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-nativeDashed-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-nativeDashed-borderColor
  ) !important;
  text-decoration-style: dashed !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-thinDashed-inner {
  border-bottom: 1px dashed
    var(--immersive-translate-theme-thinDashed-borderColor) !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-dotted-inner {
  background-image: linear-gradient(
    to right,
    var(--immersive-translate-theme-dotted-borderColor) 30%,
    rgba(255, 255, 255, 0) 0%
  );
  background-position: bottom;
  background-size: 5px 1px;
  background-repeat: repeat-x;
  padding-bottom: 3px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-nativeDotted-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-nativeDotted-borderColor
  ) !important;
  text-decoration-style: dotted !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-wavy-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-wavy-borderColor
  ) !important;
  text-decoration-style: wavy !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-dashed-inner {
  background: linear-gradient(
      to right,
      var(--immersive-translate-theme-dashed-borderColor) 0%,
      var(--immersive-translate-theme-dashed-borderColor) 50%,
      transparent 50%,
      transparent 100%
    )
    repeat-x left bottom;
  background-size: 8px 2px;
  padding-bottom: 2px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-dividingLine::before {
  content: "";
  display: block;
  max-width: 80px;
  width: 10%;
  border-top: 1px dashed
    var(--immersive-translate-theme-dividingLine-borderColor);
  padding-top: 8px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-inline-wrapper-theme-dividingLine::before {
  content: "";
  border-left: 1px dashed
    var(--immersive-translate-theme-dividingLine-borderColor);
  max-height: 16px;
  height: 16px;
  padding-left: 8px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-highlight-inner {
  background: var(--immersive-translate-theme-highlight-backgroundColor);
  box-decoration-break: clone;
  -webkit-box-decoration-break: clone;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-marker {
  line-height: 1.5em;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-marker2-inner {
  font-weight: bold;
  text-shadow: 10px 0px 3px
      var(--immersive-translate-theme-marker2-backgroundColor),
    16px 3px 9px var(--immersive-translate-theme-marker2-backgroundColor),
    2px 0px 6px var(--immersive-translate-theme-marker2-backgroundColor),
    -12px 0px 12px var(--immersive-translate-theme-marker2-backgroundColor) !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-marker-inner {
  /* TODO: add more texture */
  background: linear-gradient(
    to right,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.1),
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.9) 3%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.9) 35%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.9) 70%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.8) 95%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.3)
  );
  box-decoration-break: clone;
  -webkit-box-decoration-break: clone;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-weakening {
  opacity: 0.618 !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-italic {
  font-style: italic !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-bold {
  font-weight: bold !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-paper {
  margin: 8px 0;
  box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
  padding: 16px 32px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-blockquote {
  border-left: 4px solid var(--immersive-translate-theme-blockquote-borderColor) !important;
  padding-left: 12px !important;
  margin-top: 4px;
  margin-bottom: 4px;
  padding-top: 4px;
  padding-bottom: 4px;
  display: inline-block;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-mask-inner {
  filter: blur(5px) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

[data-immersive-translate-root-translation-theme="none"]
  .immersive-translate-target-translation-theme-mask-inner {
  filter: none !important;
}

[data-immersive-translate-root-translation-theme="mask"]
  .immersive-translate-target-inner {
  filter: blur(5px) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

/* opacity theme start */

[imt-state="dual"] .immersive-translate-target-translation-theme-opacity-inner {
  filter: opacity(
    calc(var(--immersive-translate-theme-opacity-opacity) * 1%)
  ) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

[data-immersive-translate-root-translation-theme="none"]
  .immersive-translate-target-translation-theme-opacity-inner {
  filter: none !important;
}
[data-immersive-translate-root-translation-theme="opacity"]
  .immersive-translate-target-inner,
[imt-state="dual"]
  .immersive-translate-target-translation-theme-opacity-inner:hover {
  filter: opacity(
    calc(var(--immersive-translate-theme-opacity-opacity) * 1%)
  ) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-opacity-inner:hover {
  filter: none !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-mask-inner:hover {
  filter: none !important;
}
[data-immersive-translate-root-translation-theme="opacity"]
  .immersive-translate-target-inner:hover {
  filter: none !important;
}

[data-immersive-translate-root-translation-theme="mask"]
  .immersive-translate-target-inner:hover {
  filter: none !important;
}

/* opacity theme end */

/* background theme start */
[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-background {
  margin: 8px 0;
  background: rgba(
    var(--immersive-translate-theme-background-backgroundColor-rgb),
    calc(var(--immersive-translate-theme-background-backgroundOpacity) * 1%)
  );
  border-radius: 4px;
  box-shadow: unset !important;
  padding: 12px;
  display: inline-block;
}
[imt-state="dual"]
  .immersive-translate-target-translation-theme-background-inner {
  background: rgba(
    var(--immersive-translate-theme-background-backgroundColor-rgb),
    calc(var(--immersive-translate-theme-background-backgroundOpacity) * 1%)
  );
  padding-left: 6px;
  padding-right: 6px;
  box-decoration-break: clone;
  -webkit-box-decoration-break: clone;
}
[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper
  .immersive-translate-target-translation-theme-background-inner {
  background: unset;
  padding-left: unset;
  padding-right: unset;
}
/* background theme end */

/* vertical css , please remain it in the last one. */
.immersive-translate-target-translation-vertical-block-wrapper {
  margin: 0px 8px !important;
}

.immersive-translate-text {
  font-size: 15px !important;
}

.immersive-translate-error-toast {
  position: fixed;
  top: 5%;
  z-index: 99999999;
  left: 0;
  right: 0;
  margin: auto;
  max-width: 300px;
  padding: 16px;
  border-radius: 12px;
  background-color: rgba(0, 0, 0, 0.8);
  display: flex;
  flex-direction: row;
  justify-content: space-between;
}

@media all and (min-width: 750px) {
  .immersive-translate-error-toast {
    max-width: 400px;
  }
}

.immersive-translate-clickable-button {
  cursor: pointer;
}

.immersive-translate-help-button {
  cursor: pointer;
}

.immersive-translate-loading-text:before {
  content: "...";
}

/* dark mode for loading */

@media only screen and (prefers-color-scheme: dark) {
  .immersive-translate-loading {
    border: 2px rgba(255, 255, 255, 0.25) solid !important;
    border-top: 2px rgba(255, 255, 255, 1) solid !important;
  }
}

.immersive-translate-error-wrapper {
  position: relative;
  display: inline-flex;
  padding: 6px;
  margin: 0 12px;
  white-space: nowrap;
  font-size: 0.9em;
}
[lang="zh-CN"] .immersive-translate-error-wrapper {
  font-size: 0.75em;
}
[lang="zh-TW"] .immersive-translate-error-wrapper {
  font-size: 0.75em;
}

.immersive-translate-tooltip {
  position: relative;
  display: inline-flex;
  /* little indicater to indicate it's hoverable */
}

.immersive-translate-tooltip-content {
  /* here's the magic */
  position: absolute;
  z-index: 100000000000;

  left: 50%;
  bottom: 0;
  transform: translate(-50%, 110%);
  line-height: 1;
  /* and add a small left margin */

  /* basic styles */
  width: max-content;
  max-width: 250px;
  word-wrap: break-word;
  white-space: pre-line;
  padding: 10px;
  border-radius: 10px;
  background: #000c;
  color: #fff;
  text-align: center;
  font-size: 14px;
  display: none;
  /* hide by default */
}

.immersive-translate-tooltip:hover .immersive-translate-tooltip-content {
  display: inline-block;
}

.immersive-translate-tooltip:hover + .immersive-translate-tooltip-content {
  display: inline-block;
}

.immersive-translate-tooltip-content-table {
  left: unset !important;
  bottom: unset !important;
  transform: translate(-10%, 50%) !important;
}

.immersive-translate-tooltip:hover:before {
  display: inline-block;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.imt-image-status {
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  color: var(--bg-2, #fff);
  font-size: 14px;
}
</style><style data-id="immersive-translate-user-custom-style">:root {

.immersive-translate-target-inner { font-family: inherit; }


.immersive-translate-target-inner { font-family: inherit; }
}
</style><style data-id="immersive-translate-dynamic-injected-css">.immersive-translate-target-wrapper[dir='rtl'] {text-align: right;display:block!important;}
[dir='rtl'] .immersive-translate-target-wrapper:not([dir]) {text-align:left;direction:ltr;}
.immersive-translate-target-wrapper {word-break:break-word; user-select:text;}
[imt-state=dual] .immersive-translate-target-translation-block-wrapper-theme-dividingLine::before {display:block;}
[imt-trans-position=before] .immersive-translate-target-translation-block-wrapper {display:block!important;}
</style></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2407.04295?_immersive_translate_auto_translate=1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main &gt;.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2407.04295v2/#myForm" onclick="event.preventDefault(); var modal = document.getElementById(&#39;myForm&#39;); modal.style.display = &#39;block&#39;; bugReportState.setInitiateWay(&#39;Header&#39;);">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2407.04295v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2407.04295v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2407.04295?_immersive_translate_auto_translate=1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S1" title="In Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S2" title="In Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3" title="In Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Attack Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.SS1" title="In Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>White-box Attacks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.SS1.SSS1" title="In White-box Attacks ‣ Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Gradient-based Attacks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.SS1.SSS2" title="In White-box Attacks ‣ Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Logits-based Attacks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.SS1.SSS3" title="In White-box Attacks ‣ Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Fine-tuning-based Attacks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.SS2" title="In Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Black-box Attacks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.SS2.SSS1" title="In Black-box Attacks ‣ Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Template Completion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.SS2.SSS2" title="In Black-box Attacks ‣ Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Prompt Rewriting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.SS2.SSS3" title="In Black-box Attacks ‣ Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>LLM-based Generation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4" title="In Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Defense Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS1" title="In Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Prompt-level Defenses</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS1.SSS1" title="In Prompt-level Defenses ‣ Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Prompt Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS1.SSS2" title="In Prompt-level Defenses ‣ Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Prompt Perturbation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS1.SSS3" title="In Prompt-level Defenses ‣ Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>System Prompt Safeguard</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS2" title="In Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Model-level Defenses</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS2.SSS1" title="In Model-level Defenses ‣ Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>SFT-based Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS2.SSS2" title="In Model-level Defenses ‣ Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>RLHF-based Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS2.SSS3" title="In Model-level Defenses ‣ Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Gradient and Logit Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS2.SSS4" title="In Model-level Defenses ‣ Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span>Refinement Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS2.SSS5" title="In Model-level Defenses ‣ Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.5 </span>Proxy Defense</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S5" title="In Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S5.SS1" title="In Evaluation ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Metric</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S5.SS1.SSS1" title="In Metric ‣ Evaluation ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Attack Success Rate</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S5.SS1.SSS2" title="In Metric ‣ Evaluation ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Perplexity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S5.SS2" title="In Evaluation ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S5.SS3" title="In Evaluation ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Toolkit</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S6" title="In Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2407.04295?_immersive_translate_auto_translate=1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div class="package-alerts ltx_document" role="status" aria-label="Conversion errors have been found">
      <button aria-label="Dismiss alert" onclick="closePopup()">
          <span aria-hidden="true"><svg role="presentation" width="20" height="20" viewBox="0 0 44 44" aria-hidden="true" focusable="false">
          <path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
          <path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
          </svg></span>
      </button>
      <p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
          <ul arial-label="Unsupported packages used in this paper">
              <li>failed: forest</li>
          </ul>
      <p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
    </div><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">许可证：CC BY 4.0</font></font></font></a><div id="watermark-tr" data-imt_insert_failed="1">arXiv:2407.04295v2 [cs.CR] null</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document">Jailbreak Attacks and Defenses Against Large 
<br class="ltx_break">Language Models: A Survey<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">越狱攻击与大型语言模型的防御：一项调查</font></font></font></h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Sibo Yi<sup class="ltx_sup" id="id3.3.id1">1</sup> &nbsp;&nbsp;&nbsp;Yule Liu<sup class="ltx_sup" id="id4.4.id2">2</sup><sup class="ltx_sup" id="id1.1.1"><math alttext="\ast" class="ltx_Math" display="inline" id="id1.1.1.m1.1"><semantics id="id1.1.1.m1.1a"><mo id="id1.1.1.m1.1.1" mathcolor="#006699" xref="id1.1.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><ci id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">∗</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\ast</annotation><annotation encoding="application/x-llamapun" id="id1.1.1.m1.1d">∗</annotation></semantics></math></sup> &nbsp;&nbsp;&nbsp;Zhen Sun<sup class="ltx_sup" id="id5.5.id3">2</sup><sup class="ltx_sup" id="id2.2.2"><math alttext="\ast" class="ltx_Math" display="inline" id="id2.2.2.m1.1"><semantics id="id2.2.2.m1.1a"><mo id="id2.2.2.m1.1.1" mathcolor="#006699" xref="id2.2.2.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="id2.2.2.m1.1b"><ci id="id2.2.2.m1.1.1.cmml" xref="id2.2.2.m1.1.1">∗</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m1.1c">\ast</annotation><annotation encoding="application/x-llamapun" id="id2.2.2.m1.1d">∗</annotation></semantics></math></sup> &nbsp;&nbsp;&nbsp;Tianshuo Cong<sup class="ltx_sup" id="id6.6.id4">1</sup> &nbsp;&nbsp;&nbsp;Xinlei He<sup class="ltx_sup" id="id7.7.id5">2</sup> &nbsp;&nbsp;&nbsp;Jiaxing Song<sup class="ltx_sup" id="id8.8.id6">1</sup> &nbsp;&nbsp;&nbsp;Ke Xu<sup class="ltx_sup" id="id9.9.id7">1</sup> &nbsp;&nbsp;&nbsp;Qi Li<sup class="ltx_sup" id="id10.10.id8">1</sup>

&nbsp;&nbsp;&nbsp;
<br class="ltx_break">
<br class="ltx_break"><sup class="ltx_sup" id="id11.11.id9">1</sup><span class="ltx_text ltx_font_italic" id="id12.12.id10">Tsinghua University</span>&nbsp;&nbsp;&nbsp;<sup class="ltx_sup" id="id13.13.id11">2</sup><span class="ltx_text ltx_font_italic" id="id14.14.id12">Hong Kong University of Science and Technology (Guangzhou)</span>
</span><span class="ltx_author_notes">The first three authors made equal contributions.Corresponding author (<a class="ltx_ref ltx_href" href="mailto:qli01@tsinghua.edu.cn" title="">qli01@tsinghua.edu.cn</a>).</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">摘要</font></font></font></h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id15.id1">Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc.
However, the over-assistance of LLMs has raised the challenge of “jailbreaking”, which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts.
With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving.
In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods.
For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model.
Meanwhile, we classify defense methods into prompt-level and model-level defenses.
Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships.
We also conduct an investigation into the current evaluation methods and compare them from different perspectives.
Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks.
Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">大型语言模型（LLMs）在各种文本生成任务中表现优异，包括问答、翻译、代码补全等。然而，LLMs 的过度辅助引发了“越狱”的挑战，即通过设计对抗性提示，诱导模型生成违反使用政策和社会规范的恶意响应。随着针对 LLMs 不同漏洞的越狱攻击方法的涌现，相应的安全对齐措施也在不断发展。在本文中，我们提出了一个全面而详细的越狱攻击与防御方法的分类体系。例如，根据目标模型的透明度，攻击方法被分为黑盒攻击和白盒攻击。同时，我们将防御方法分为提示级防御和模型级防御。此外，我们将这些攻击和防御方法进一步细分为不同的子类，并呈现了一个清晰的关系图来展示它们之间的联系。我们还对当前的评价方法进行了研究，并从不同角度进行了比较。 我们的研究旨在启发未来在保护 LLMs 免受对抗性攻击方面的研究和实际应用。 最重要的是，尽管越狱仍然是社区内的一个重大问题，但我们相信我们的工作增强了对此领域的理解，并为开发更安全的 LLMs 奠定了基础。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;"> Introduction<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">引言</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs), such as ChatGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib10" title="">10</a>]</cite> and Gemini&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib3" title="">3</a>]</cite>, have revolutionized various Natural Language Processing (NLP) tasks such as question answering&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib10" title="">10</a>]</cite> and code completion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib16" title="">16</a>]</cite>.
The reason why LLMs possess remarkable capabilities to understand and generate human-like text is that they have been trained on massive amounts of data and the ultra-high intelligence that has emerged from the expansion of model parameters&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib98" title="">98</a>]</cite>.
However, harmful information is inevitably included in the training data, thus, LLMs typically have undergone rigorous safety alignment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib92" title="">92</a>]</cite> before released.
This allows them to generate a safety guardrail to promptly reject harmful inquiries from users, ensuring that the model’s output aligns with human values.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">大型语言模型（LLMs），如 ChatGPT [ 10] 和 Gemini [ 3]，已经彻底改变了各种自然语言处理（NLP）任务，如问答 [ 10] 和代码补全 [ 16]。LLMs 之所以拥有理解和生成类人文本的卓越能力，是因为它们在大量数据上进行了训练，并且随着模型参数的扩展而涌现出的超高智能 [ 98]。然而，有害信息不可避免地包含在训练数据中，因此，LLMs 在发布前通常会经过严格的安全对齐 [ 92]。这使它们能够生成安全护栏，以迅速拒绝来自用户的有害查询，确保模型的输出符合人类价值观。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recently, the widespread adoption of LLMs has raised significant concerns regarding their security and potential vulnerabilities.
One major concern is the susceptibility of these models to jailbreak attacks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib81" title="">81</a>]</cite>, where malicious actors exploit vulnerabilities in the model’s architecture or implementation and design prompts meticulously to elicit the harmful behaviors of LLMs.
Notably, jailbreak attacks against LLMs represent a unique and evolving threat landscape that demands careful examination and mitigation strategies.
More importantly, these attacks can have far-reaching implications, ranging from privacy breaches to the dissemination of misinformation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib32" title="">32</a>]</cite>, and even the manipulation of automated systems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib114" title="">114</a>]</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">近来，LLMs 的广泛应用引发了对其安全性和潜在漏洞的严重关切。一个主要关切点是这些模型易受越狱攻击[ 105, 32, 81]，恶意行为者利用模型架构或实现的漏洞，精心设计提示来诱使 LLMs 表现出有害行为。值得注意的是，针对 LLMs 的越狱攻击代表了一种独特且不断演变的威胁格局，需要仔细审查和缓解策略。更重要的是，这些攻击可能产生深远影响，从隐私泄露到虚假信息的传播[ 32]，甚至操纵自动化系统[ 114]。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we aim to provide a comprehensive survey of jailbreak attacks versus defenses against LLMs.
We will first explore various attack vectors, techniques, and case studies to elucidate the underlying vulnerabilities and potential impact on model security and integrity.
Additionally, we will discuss existing countermeasures and strategies for mitigating the risks associated with jailbreak attacks.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在本文中，我们旨在提供一份关于针对 LLMs 的越狱攻击与防御的全面综述。我们将首先探讨各种攻击向量、技术和案例研究，阐明潜在漏洞以及对模型安全性和完整性的潜在影响。此外，我们将讨论现有的反制措施和缓解与越狱攻击相关风险的战略。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S1.T1.3.2" style="font-size:90%;">Overview of jailbreak attack and defense methods.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 1：越狱攻击和防御方法的概述。</font></font></font></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.4.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S1.T1.4.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.1.1.1.1">
<span class="ltx_p" id="S1.T1.4.1.1.1.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.1.1.1.1.1.1">Method<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">方法</font></font></font></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S1.T1.4.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.1.1.2.1">
<span class="ltx_p" id="S1.T1.4.1.1.2.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.1.1.2.1.1.1">Category<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">分类</font></font></font></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S1.T1.4.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.1.1.3.1">
<span class="ltx_p" id="S1.T1.4.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.4.1.1.3.1.1.1">Description<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">描述</font></font></font></span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.4.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S1.T1.4.2.1.1" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.2.1.1.1">
<span class="ltx_p" id="S1.T1.4.2.1.1.1.1" style="width:113.8pt;"><span class="ltx_text" id="S1.T1.4.2.1.1.1.1.1">White-box Attack<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">白盒攻击</font></font></font></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S1.T1.4.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.2.1.2.1">
<span class="ltx_p" id="S1.T1.4.2.1.2.1.1" style="width:113.8pt;">Gradient-based<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于梯度</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S1.T1.4.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.2.1.3.1">
<span class="ltx_p" id="S1.T1.4.2.1.3.1.1">Construct the jailbreak prompt based on gradients of the target LLM.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">根据目标 LLM 的梯度构建越狱提示。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.3.2.1.1">
<span class="ltx_p" id="S1.T1.4.3.2.1.1.1" style="width:113.8pt;">Logits-based<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于 Logits 的</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.3.2.2.1">
<span class="ltx_p" id="S1.T1.4.3.2.2.1.1">Construct the jailbreak prompt based on the logits of output tokens.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">根据输出 token 的 logits 构建越狱提示。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.4.3.1.1">
<span class="ltx_p" id="S1.T1.4.4.3.1.1.1" style="width:113.8pt;">Fine-tuning-based<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于微调的</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.4.3.2.1">
<span class="ltx_p" id="S1.T1.4.4.3.2.1.1">Fine-tune the target LLM with adversarial examples to elicit
harmful behaviors.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">使用对抗性样本微调目标 LLM 以诱导有害行为。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.5.4.1" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.5.4.1.1">
<span class="ltx_p" id="S1.T1.4.5.4.1.1.1" style="width:113.8pt;"><span class="ltx_text" id="S1.T1.4.5.4.1.1.1.1">Black-box Attack<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">黑盒攻击</font></font></font></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.5.4.2.1">
<span class="ltx_p" id="S1.T1.4.5.4.2.1.1" style="width:113.8pt;">Template Completion<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">模板补全</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.5.4.3.1">
<span class="ltx_p" id="S1.T1.4.5.4.3.1.1">Complete harmful questions into contextual templates to generate a jailbreak prompt.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">将有害问题补全为上下文模板以生成越狱提示。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.6.5.1.1">
<span class="ltx_p" id="S1.T1.4.6.5.1.1.1" style="width:113.8pt;">Prompt Rewriting<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提示重写</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.6.5.2.1">
<span class="ltx_p" id="S1.T1.4.6.5.2.1.1">Rewrite the
jailbreak prompt in other natural or non-natural languages.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">将越狱提示重写为其他自然或非自然语言。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.7.6.1.1">
<span class="ltx_p" id="S1.T1.4.7.6.1.1.1" style="width:113.8pt;">LLM-based Generation<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于 LLM 的生成</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.7.6.2.1">
<span class="ltx_p" id="S1.T1.4.7.6.2.1.1">Instruct an
LLM as the attacker to generate or optimize jailbreak prompts.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">指示 LLM 作为攻击者生成或优化越狱提示。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.8.7.1" rowspan="3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.8.7.1.1">
<span class="ltx_p" id="S1.T1.4.8.7.1.1.1" style="width:113.8pt;"><span class="ltx_text" id="S1.T1.4.8.7.1.1.1.1">Prompt-level Defense<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提示级防御</font></font></font></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.8.7.2.1">
<span class="ltx_p" id="S1.T1.4.8.7.2.1.1" style="width:113.8pt;">Prompt Detection<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提示检测</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.8.7.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.8.7.3.1">
<span class="ltx_p" id="S1.T1.4.8.7.3.1.1">Detect and filter adversarial prompts based on Perplexity or other features.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于困惑度或其他特征检测并过滤对抗性提示。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.9.8.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.9.8.1.1">
<span class="ltx_p" id="S1.T1.4.9.8.1.1.1" style="width:113.8pt;">Prompt Perturbation<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提示扰动</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.9.8.2.1">
<span class="ltx_p" id="S1.T1.4.9.8.2.1.1">Perturb the prompt
to eliminate potential malicious content.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">扰动提示以消除潜在恶意内容。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.10.9.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.10.9.1.1">
<span class="ltx_p" id="S1.T1.4.10.9.1.1.1" style="width:113.8pt;">System Prompt Safeguard<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">系统提示保护</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.10.9.2.1">
<span class="ltx_p" id="S1.T1.4.10.9.2.1.1">Utilize
meticulously designed system prompts to enhance safety.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">使用精心设计的系统提示来增强安全性。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S1.T1.4.11.10.1" rowspan="9">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.11.10.1.1">
<span class="ltx_p" id="S1.T1.4.11.10.1.1.1" style="width:113.8pt;"><span class="ltx_text" id="S1.T1.4.11.10.1.1.1.1">Model-level Defense<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">模型级防御</font></font></font></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.11.10.2.1">
<span class="ltx_p" id="S1.T1.4.11.10.2.1.1" style="width:113.8pt;">SFT-based<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于 SFT 的</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.11.10.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.11.10.3.1">
<span class="ltx_p" id="S1.T1.4.11.10.3.1.1">Fine-tune the LLM with safety examples to improve the robustness.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">使用安全示例微调 LLM 以提高鲁棒性。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.12.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.12.11.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.12.11.1.1">
<span class="ltx_p" id="S1.T1.4.12.11.1.1.1" style="width:113.8pt;">RLHF-based<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于 RLHF</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.12.11.2.1">
<span class="ltx_p" id="S1.T1.4.12.11.2.1.1">Train the LLM with
RLHF to enhance safety.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">使用 RLHF 训练 LLM 以增强安全性。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.13.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.13.12.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.13.12.1.1">
<span class="ltx_p" id="S1.T1.4.13.12.1.1.1" style="width:113.8pt;">Gradient and Logit Analysis<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">梯度与逻辑分析</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.13.12.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.13.12.2.1">
<span class="ltx_p" id="S1.T1.4.13.12.2.1.1">Detect the
malicious prompts based on the gradient of safety-critical parameters.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于安全关键参数的梯度检测恶意提示。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.14.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S1.T1.4.14.13.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.14.13.1.1">
<span class="ltx_p" id="S1.T1.4.14.13.1.1.1" style="width:113.8pt;">Refinement<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">精炼</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S1.T1.4.14.13.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.14.13.2.1">
<span class="ltx_p" id="S1.T1.4.14.13.2.1.1">Take advantage of
the generalization ability of LLM to analyze the suspicious prompts and generate responses cautiously.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">利用 LLM 的泛化能力谨慎分析可疑提示并生成响应。</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.15.14">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S1.T1.4.15.14.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.15.14.1.1">
<span class="ltx_p" id="S1.T1.4.15.14.1.1.1" style="width:113.8pt;">Proxy Defense<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">代理防御</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S1.T1.4.15.14.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.4.15.14.2.1">
<span class="ltx_p" id="S1.T1.4.15.14.2.1.1">Apply another
secure LLM to monitor and filter the output of the target LLM.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">使用另一个安全的 LLM 来监控和过滤目标 LLM 的输出。</font></font></font></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="557" id="S1.F1.g1" src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/x1.png" width="498">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">The taxonomy and relationship of attack and defense methods.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 1：攻击和防御方法的分类和关系</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">By shedding light on the landscape of jailbreak attacks against LLMs, this survey aims to enhance our understanding of the security challenges inherent in the deployment and employment of large-scale foundation models.
Furthermore, it aims to provide researchers, practitioners, and policymakers with valuable insights into developing robust defense mechanisms and best practices to safeguard foundation models against malicious exploitation.
In summary, our key contributions are as follows:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">通过揭示针对 LLMs 的越狱攻击现状，本综述旨在加深我们对大规模基础模型部署和应用中固有安全挑战的理解。此外，它旨在为研究人员、实践者和政策制定者提供宝贵见解，以开发强大的防御机制和最佳实践，保护基础模型免受恶意利用。总而言之，我们的主要贡献如下：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We provide a systematic taxonomy of both jailbreak attack and defense methods.
According to the transparency level of the target LLM to attackers, we categorize attack methods into two main classes: white-box and black-box attacks, and divide them into more sub-classes for further investigation.
Similarly, defense methods are categorized into prompt-level and model-level defenses, which implies whether the safety measure modifies the protected LLM or not.
The detailed definitions of the methods are listed in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S1.T1" title="In Introduction ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 我们提供了一种针对逃逸攻击和防御方法的系统化分类法。根据目标 LLM 对攻击者的透明度水平，我们将攻击方法分为两大类：白盒攻击和黑盒攻击，并进一步细分为更多子类以供深入研究。类似地，防御方法被分为提示级防御和模型级防御，这表明安全措施是否修改了受保护的 LLM。这些方法的详细定义列在表 1 中。</font></font></font>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We highlight the relationships between different attack and defense methods.
Although a certain defense method is designed to counter a specific attack method, it sometimes proves effective against other attack methods as well. The relationships are illustrated in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S1.F1" title="In Introduction ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>, which have been proven by experiments in other research.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 我们强调了不同攻击和防御方法之间的关系。尽管某种防御方法被设计用来对抗特定的攻击方法，但它有时对其他攻击方法也有效。这些关系在图 1 中进行了说明，该图已被其他研究中的实验所证实。</font></font></font>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We conduct an investigation into current evaluation methods.
We briefly introduce the popular metric in jailbreak research and summarize current benchmarks including some frameworks and datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 我们对当前的评估方法进行了调查。我们简要介绍了在越狱研究中流行的指标，并总结了当前的基准测试，包括一些框架和数据集。</font></font></font>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;"> Related Work<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">相关工作</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">With the increasing concerns regarding the security of LLMs and the continuous emergence of jailbreak methods, numerous researchers have conducted extensive investigations in this field.
Some studies engage in theoretical discussions on the vulnerabilities of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib81" title="">81</a>]</cite>, analyzing the reasons for potential jailbreak attacks, while some empirical studies replicate and compare various jailbreak attack methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib17" title="">17</a>]</cite>, thereby demonstrating the strengths and weaknesses among different approaches.
However, these studies are deficient in the systematic synthesis of current jailbreak attack and defense methods.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">随着对 LLMs 安全性的关注日益增加以及越狱方法的不断涌现，众多研究人员在这一领域进行了广泛的研究。一些研究致力于从理论上探讨 LLMs 的漏洞[ 105, 32, 81]，分析潜在越狱攻击的原因，而另一些实证研究则复制和比较了各种越狱攻击方法[ 97, 57, 17]，从而展示了不同方法的优势和劣势。然而，这些研究在系统性地综合当前越狱攻击和防御方法方面存在不足。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">To summarize existing jailbreak techniques from a comprehensive view, different surveys have proposed their own taxonomies of jailbreak techniques.
Shayegani et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib78" title="">78</a>]</cite> classify jailbreak attack methods into uni-model attacks, multi-model attacks, and additional attacks.
Esmradi et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib24" title="">24</a>]</cite> introduce the jailbreak attack methods against LLMs and LLM applications, respectively.
Rao et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib72" title="">72</a>]</cite> view jailbreak attack methods from four perspectives based on the intent of jailbreak.
Geiping et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib28" title="">28</a>]</cite> categorize jailbreak attack methods based on the detrimental behaviors of LLMs.
Schulhoff et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib75" title="">75</a>]</cite> organize a competition to collect high-quality jailbreak prompts from humans and present a detailed taxonomy of the prompt hacking techniques used in the competition.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">从全面视角总结现有越狱技术，不同的调查提出了各自的越狱技术分类体系。Shayegani 等人[78]将越狱攻击方法分为单模型攻击、多模型攻击和附加攻击。Esmradi 等人[24]分别介绍了针对 LLMs 和 LLM 应用的越狱攻击方法。Rao 等人[72]基于越狱的意图，从四个角度看待越狱攻击方法。Geiping 等人[28]根据 LLMs 的负面行为对越狱攻击方法进行分类。Schulhoff 等人[75]组织了一场竞赛，收集人类提出的高质量越狱提示，并展示了竞赛中使用的提示黑客技术的详细分类体系。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Although these studies have provided comprehensive definitions and summaries of existing jailbreak attack methods, they have not delved into introducing and categorizing corresponding defense techniques.
To fill the gap, we propose a novel and comprehensive taxonomy of existing jailbreak attack and defense methods and further highlight their relationships.
Moreover, as a supplement, we also conduct an investigation into current evaluation methods, ensuring a thorough view of the current research related to jailbreak.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">尽管这些研究已经提供了对现有越狱攻击方法的全面定义和总结，但它们并未深入介绍和分类相应的防御技术。为了填补这一空白，我们提出了一个新颖且全面的现有越狱攻击和防御方法的分类体系，并进一步突出了它们之间的关系。此外，作为补充，我们还对当前评估方法进行了调查，确保对越狱相关研究的全面了解。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;"> Attack Methods<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">攻击方法</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F2.2" style="width:368.6pt;height:224.8pt;vertical-align:-219.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-69.8pt,1.0pt) scale(0.725290801849748,0.725290801849748) ;"><span class="ltx_ERROR undefined" id="S3.F2.2.1" data-imt_insert_failed="1">{forest}</span>
<p class="ltx_p" id="S3.F2.2.2">forked edges,
for tree=
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
node options=align=center,
align = center,
base=left,
font=<span class="ltx_text" id="S3.F2.2.2.1" style="font-size:90%;">,
rectangle,
draw=hidden-draw,
rounded corners,
edge+=darkgray, line width=1pt,
s sep=3pt,
inner xsep=2pt,
inner ysep=3pt,
ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
</span>,
where level=1text width=5.0em,font=,
where level=2text width=5.6em,font=,
where level=3text width=6.8em,font=,
[
Jailbreak Attack Methods, ver
[
White-box 
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, node options=align=center, align = center, base=left, font=, rectangle, draw=hidden-draw, rounded corners, edge+=darkgray, line width=1pt, s sep=3pt, inner xsep=2pt, inner ysep=3pt, ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center, , where level=1text width=5.0em,font=, where level=2text width=5.6em,font=, where level=3text width=6.8em,font=, [ 越狱攻击方法, ver [ 白盒</font></font></font><br class="ltx_break">Attack
[
Gradient-based [
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib125" title="">125</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib42" title="">42</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib124" title="">124</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib93" title="">93</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib2" title="">2</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib29" title="">29</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib34" title="">34</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib82" title="">82</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib95" title="">95</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib62" title="">62</a>]</cite>
, leaf, text width=17em
]
]
[
Logits-based
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib116" title="">116</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib31" title="">31</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib23" title="">23</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib117" title="">117</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib36" title="">36</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib123" title="">123</a>]</cite>
, leaf, text width=11em
]
]
[
Fine-tuning-based
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib68" title="">68</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib103" title="">103</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib47" title="">47</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib111" title="">111</a>]</cite>
, leaf, text width=7em
]
]
]
[
Black-box 
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">攻击 [ 基于梯度 [ [ 125][ 42][ 124][ 93][ 2][ 29][ 34][ 82][ 95][ 62] ，叶节点，文本宽度=17em ] ] [ 基于 logits [ [ 116][ 31][ 23][ 117][ 36][ 123] ，叶节点，文本宽度=11em ] ] [ 基于微调 [ [ 68][ 103][ 47][ 111] ，叶节点，文本宽度=7em ] ] [ 黑盒 </font></font></font><br class="ltx_break">Attack
[
Tamplate 
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">攻击 [ 模板 </font></font></font><br class="ltx_break">Completion
[
Scenario Nesting
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib52" title="">52</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib22" title="">22</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib104" title="">104</a>]</cite>
, leaf, text width=5.5em
]
]
[
Context-based
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib100" title="">100</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib20" title="">20</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib48" title="">48</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib5" title="">5</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib120" title="">120</a>]</cite>
, leaf, text width=8.5em
]
]
[
Code Injection
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib43" title="">43</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib61" title="">61</a>]</cite>
, leaf, text width=3.5em
]
]
]
[
Prompt Rewriting
[
Cipher
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib108" title="">108</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib40" title="">40</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib33" title="">33</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib55" title="">55</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib55" title="">55</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib13" title="">13</a>]</cite>
, leaf, text width=10.5em
]
]
[
Low-resource 
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">补全 [ 场景嵌套 [ [ 52][ 22][ 104] ，叶节点，文本宽度=5.5em ] ] [ 基于上下文 [ [ 100][ 20][ 48][ 5][ 120] ，叶节点，文本宽度=8.5em ] ] [ 代码注入 [ [ 43][ 61] ，叶节点，文本宽度=3.5em ] ] ] [ 提示重写 [ 密码 [ [ 108][ 40][ 33][ 55][ 55][ 13] ，叶节点，文本宽度=10.5em ] ] [ 低资源 </font></font></font><br class="ltx_break">Languages
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib21" title="">21</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib106" title="">106</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib49" title="">49</a>]</cite>
, leaf, text width=5.5em
]
]
[
Genetic 
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">语言 [ [ 21][ 106][ 49] ，叶节点，文本宽度=5.5em ] ] [ 遗传</font></font></font><br class="ltx_break">Algorithm-based
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib56" title="">56</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib46" title="">46</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib107" title="">107</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib50" title="">50</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib88" title="">88</a>]</cite>
, leaf, text width=8.5em
]
]
]
[
LLM-based 
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于算法的[ [ 56][ 46][ 107][ 50][ 88] ，叶节点，文本宽度=8.5em ] ] ] [ 基于 LLM 的</font></font></font><br class="ltx_break">Generation
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib19" title="">19</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib109" title="">109</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib76" title="">76</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib12" title="">12</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib15" title="">15</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib41" title="">41</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib27" title="">27</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib91" title="">91</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib54" title="">54</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib64" title="">64</a>]</cite>
, leaf, text width=17em
]
]
]
]<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">生成[ [ 19][ 109][ 76][ 12][ 15][ 41][ 27][ 91][ 54][ 64] ，叶节点，文本宽度=17em ] ] ] ]</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.4.2" style="font-size:90%;">Taxonomy of jailbreak attack.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 2：越狱攻击的分类学</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we focus on discussing different advanced jailbreak attacks.
We categorize attack methods into white-box and black-box attacks (refer to&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.F2" title="In Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>).
Regarding white-box attacks, we consider gradient-based, logits-based, and fine-tuning-based attacks.
Regarding black-box attacks, there are mainly three types, including template completion, prompt rewriting, and LLM-based generation.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在这一部分，我们专注于讨论不同的高级越狱攻击。我们将攻击方法分为白盒攻击和黑盒攻击（参见图 2）。关于白盒攻击，我们考虑基于梯度的攻击、基于 logits 的攻击和基于微调的攻击。关于黑盒攻击，主要有三种类型，包括模板补全、提示重写和基于 LLM 的生成。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:120%;"> White-box Attacks<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">白盒攻击</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="274" id="S3.F3.g1" src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">A schematic diagram of gradient-based attack.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 3：基于梯度攻击的示意图。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Gradient-based Attacks<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于梯度攻击</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.4">For gradient-based attacks, they manipulate model inputs based on gradients to elicit compliant responses to harmful commands.
As shown in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.F3" title="In White-box Attacks ‣ Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>, this method pads a prefix or suffix to the original prompt, which can be optimized to achieve the attack objective.
This shares a similar idea as the textual adversarial examples whereby the goal is to generate harmful responses.
As a pioneer in this field, Zou et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib125" title="">125</a>]</cite> propose an effective gradient-based jailbreak attack, <math alttext="\mathsf{Greedy}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.1.m1.1"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mi id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">𝖦𝗋𝖾𝖾𝖽𝗒</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><ci id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">𝖦𝗋𝖾𝖾𝖽𝗒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">\mathsf{Greedy}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.1.m1.1d">sansserif_Greedy</annotation></semantics></math> <math alttext="\mathsf{Coordinate}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.2.m2.1"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><mi id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml">𝖢𝗈𝗈𝗋𝖽𝗂𝗇𝖺𝗍𝖾</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><ci id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1">𝖢𝗈𝗈𝗋𝖽𝗂𝗇𝖺𝗍𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">\mathsf{Coordinate}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.2.m2.1d">sansserif_Coordinate</annotation></semantics></math> <math alttext="\mathsf{Gradient}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.3.m3.1"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><mi id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml">𝖦𝗋𝖺𝖽𝗂𝖾𝗇𝗍</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><ci id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1">𝖦𝗋𝖺𝖽𝗂𝖾𝗇𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">\mathsf{Gradient}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.3.m3.1d">sansserif_Gradient</annotation></semantics></math> (<math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.4.m4.1"><semantics id="S3.SS1.SSS1.p1.4.m4.1a"><mi id="S3.SS1.SSS1.p1.4.m4.1.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.4.m4.1b"><ci id="S3.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.4.m4.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.4.m4.1d">sansserif_GCG</annotation></semantics></math>), on aligned large language models.
Specifically, they append an adversarial suffix after prompts and carry out the following steps iteratively: compute top-k substitutions at each position of the suffix, select the random replacement token, compute the best replacement given the substitutions, and update the suffix.
Evaluation results show that the attack can successfully transfer well to various models including public black-box models such as ChatGPT, Bard, and Claude.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">对于基于梯度的攻击，它们通过操纵模型输入的梯度来诱使模型对有害命令做出合规的响应。如图 3 所示，这种方法在原始提示前或后添加前缀或后缀，可以优化以实现攻击目标。这与文本对抗样本有相似之处，其目标都是生成有害的响应。作为该领域的先驱，Zou 等人[125]在协同大型语言模型上提出了一种有效的基于梯度的越狱攻击， <math id="S3.SS1.SSS1.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{Greedy}"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mi id="S3.SS1.SSS1.p1.1.m1.1.1">𝖦𝗋𝖾𝖾𝖽𝗒</mi><annotation-xml id="S3.SS1.SSS1.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p1.1.m1.1c" encoding="application/x-tex">\mathsf{Greedy}</annotation><annotation id="S3.SS1.SSS1.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_Greedy</annotation></semantics></math> <math id="S3.SS1.SSS1.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{Coordinate}"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><mi id="S3.SS1.SSS1.p1.2.m2.1.1">𝖢𝗈𝗈𝗋𝖽𝗂𝗇𝖺𝗍𝖾</mi><annotation-xml id="S3.SS1.SSS1.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p1.2.m2.1c" encoding="application/x-tex">\mathsf{Coordinate}</annotation><annotation id="S3.SS1.SSS1.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_Coordinate</annotation></semantics></math> <math id="S3.SS1.SSS1.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{Gradient}"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><mi id="S3.SS1.SSS1.p1.3.m3.1.1">𝖦𝗋𝖺𝖽𝗂𝖾𝗇𝗍</mi><annotation-xml id="S3.SS1.SSS1.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p1.3.m3.1c" encoding="application/x-tex">\mathsf{Gradient}</annotation><annotation id="S3.SS1.SSS1.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_Gradient</annotation></semantics></math> ( <math id="S3.SS1.SSS1.p1.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.SS1.SSS1.p1.4.m4.1a"><mi id="S3.SS1.SSS1.p1.4.m4.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.SS1.SSS1.p1.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p1.4.m4.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.SS1.SSS1.p1.4.m4.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> )。具体来说，他们在提示后附加对抗性后缀，并迭代执行以下步骤：在每个后缀位置计算 top-k 替换，选择随机替换的标记，根据替换计算最佳替换，并更新后缀。评估结果表明，该攻击可以成功迁移到包括 ChatGPT、Bard 和 Claude 在内的各种模型，包括公共黑盒模型。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.16">Although <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.1.m1.1"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mi id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><ci id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.1.m1.1d">sansserif_GCG</annotation></semantics></math> has demonstrated strong performance against many advanced LLMs, the unreadability of the attack suffixes leaves a direction for subsequent research.
Jones et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib42" title="">42</a>]</cite> develop an auditing method called <math alttext="\mathsf{Autoregressive}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.2.m2.1"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mi id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">𝖠𝗎𝗍𝗈𝗋𝖾𝗀𝗋𝖾𝗌𝗌𝗂𝗏𝖾</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">𝖠𝗎𝗍𝗈𝗋𝖾𝗀𝗋𝖾𝗌𝗌𝗂𝗏𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">\mathsf{Autoregressive}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.2.m2.1d">sansserif_Autoregressive</annotation></semantics></math> <math alttext="\mathsf{Randomized}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.3.m3.1"><semantics id="S3.SS1.SSS1.p2.3.m3.1a"><mi id="S3.SS1.SSS1.p2.3.m3.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.cmml">𝖱𝖺𝗇𝖽𝗈𝗆𝗂𝗓𝖾𝖽</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.3.m3.1b"><ci id="S3.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1">𝖱𝖺𝗇𝖽𝗈𝗆𝗂𝗓𝖾𝖽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.3.m3.1c">\mathsf{Randomized}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.3.m3.1d">sansserif_Randomized</annotation></semantics></math> <math alttext="\mathsf{Coordinate}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.4.m4.1"><semantics id="S3.SS1.SSS1.p2.4.m4.1a"><mi id="S3.SS1.SSS1.p2.4.m4.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.cmml">𝖢𝗈𝗈𝗋𝖽𝗂𝗇𝖺𝗍𝖾</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.4.m4.1b"><ci id="S3.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1">𝖢𝗈𝗈𝗋𝖽𝗂𝗇𝖺𝗍𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.4.m4.1c">\mathsf{Coordinate}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.4.m4.1d">sansserif_Coordinate</annotation></semantics></math> <math alttext="\mathsf{Ascent}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.5.m5.1"><semantics id="S3.SS1.SSS1.p2.5.m5.1a"><mi id="S3.SS1.SSS1.p2.5.m5.1.1" xref="S3.SS1.SSS1.p2.5.m5.1.1.cmml">𝖠𝗌𝖼𝖾𝗇𝗍</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.5.m5.1b"><ci id="S3.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1">𝖠𝗌𝖼𝖾𝗇𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.5.m5.1c">\mathsf{Ascent}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.5.m5.1d">sansserif_Ascent</annotation></semantics></math> (<math alttext="\mathsf{ARCA}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.6.m6.1"><semantics id="S3.SS1.SSS1.p2.6.m6.1a"><mi id="S3.SS1.SSS1.p2.6.m6.1.1" xref="S3.SS1.SSS1.p2.6.m6.1.1.cmml">𝖠𝖱𝖢𝖠</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.6.m6.1b"><ci id="S3.SS1.SSS1.p2.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p2.6.m6.1.1">𝖠𝖱𝖢𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.6.m6.1c">\mathsf{ARCA}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.6.m6.1d">sansserif_ARCA</annotation></semantics></math>), which formulates jailbreak attack as a discrete optimization problem.
Given the objective, e.g., specific outputs, <math alttext="\mathsf{ARCA}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.7.m7.1"><semantics id="S3.SS1.SSS1.p2.7.m7.1a"><mi id="S3.SS1.SSS1.p2.7.m7.1.1" xref="S3.SS1.SSS1.p2.7.m7.1.1.cmml">𝖠𝖱𝖢𝖠</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.7.m7.1b"><ci id="S3.SS1.SSS1.p2.7.m7.1.1.cmml" xref="S3.SS1.SSS1.p2.7.m7.1.1">𝖠𝖱𝖢𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.7.m7.1c">\mathsf{ARCA}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.7.m7.1d">sansserif_ARCA</annotation></semantics></math> aims to search for the possible suffix after the original prompt that can greedily generate the output.
Zhu et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib124" title="">124</a>]</cite> develop <math alttext="\mathsf{AutoDAN}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.8.m8.1"><semantics id="S3.SS1.SSS1.p2.8.m8.1a"><mi id="S3.SS1.SSS1.p2.8.m8.1.1" xref="S3.SS1.SSS1.p2.8.m8.1.1.cmml">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.8.m8.1b"><ci id="S3.SS1.SSS1.p2.8.m8.1.1.cmml" xref="S3.SS1.SSS1.p2.8.m8.1.1">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.8.m8.1c">\mathsf{AutoDAN}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.8.m8.1d">sansserif_AutoDAN</annotation></semantics></math>, an interpretable gradient-based jailbreak attack against LLMs.
Specifically, <math alttext="\mathsf{AutoDAN}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.9.m9.1"><semantics id="S3.SS1.SSS1.p2.9.m9.1a"><mi id="S3.SS1.SSS1.p2.9.m9.1.1" xref="S3.SS1.SSS1.p2.9.m9.1.1.cmml">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.9.m9.1b"><ci id="S3.SS1.SSS1.p2.9.m9.1.1.cmml" xref="S3.SS1.SSS1.p2.9.m9.1.1">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.9.m9.1c">\mathsf{AutoDAN}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.9.m9.1d">sansserif_AutoDAN</annotation></semantics></math> generates an adversarial suffix in a sequential manner.
At each iteration, <math alttext="\mathsf{AutoDAN}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.10.m10.1"><semantics id="S3.SS1.SSS1.p2.10.m10.1a"><mi id="S3.SS1.SSS1.p2.10.m10.1.1" xref="S3.SS1.SSS1.p2.10.m10.1.1.cmml">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.10.m10.1b"><ci id="S3.SS1.SSS1.p2.10.m10.1.1.cmml" xref="S3.SS1.SSS1.p2.10.m10.1.1">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.10.m10.1c">\mathsf{AutoDAN}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.10.m10.1d">sansserif_AutoDAN</annotation></semantics></math> generates the new token to the suffix using the Single Token Optimization (STO) algorithm that considers both jailbreak and readability objectives.
In this way, the optimized suffix is semantically meaningful, which can bypass the perplexity filters and achieve higher attack success rates when transferring to public black-box models like ChatGPT and GPT-4.
Wang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib93" title="">93</a>]</cite> develop an <math alttext="\mathsf{Adversarial}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.11.m11.1"><semantics id="S3.SS1.SSS1.p2.11.m11.1a"><mi id="S3.SS1.SSS1.p2.11.m11.1.1" xref="S3.SS1.SSS1.p2.11.m11.1.1.cmml">𝖠𝖽𝗏𝖾𝗋𝗌𝖺𝗋𝗂𝖺𝗅</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.11.m11.1b"><ci id="S3.SS1.SSS1.p2.11.m11.1.1.cmml" xref="S3.SS1.SSS1.p2.11.m11.1.1">𝖠𝖽𝗏𝖾𝗋𝗌𝖺𝗋𝗂𝖺𝗅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.11.m11.1c">\mathsf{Adversarial}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.11.m11.1d">sansserif_Adversarial</annotation></semantics></math> <math alttext="\mathsf{Suffix}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.12.m12.1"><semantics id="S3.SS1.SSS1.p2.12.m12.1a"><mi id="S3.SS1.SSS1.p2.12.m12.1.1" xref="S3.SS1.SSS1.p2.12.m12.1.1.cmml">𝖲𝗎𝖿𝖿𝗂𝗑</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.12.m12.1b"><ci id="S3.SS1.SSS1.p2.12.m12.1.1.cmml" xref="S3.SS1.SSS1.p2.12.m12.1.1">𝖲𝗎𝖿𝖿𝗂𝗑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.12.m12.1c">\mathsf{Suffix}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.12.m12.1d">sansserif_Suffix</annotation></semantics></math> <math alttext="\mathsf{Embedding}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.13.m13.1"><semantics id="S3.SS1.SSS1.p2.13.m13.1a"><mi id="S3.SS1.SSS1.p2.13.m13.1.1" xref="S3.SS1.SSS1.p2.13.m13.1.1.cmml">𝖤𝗆𝖻𝖾𝖽𝖽𝗂𝗇𝗀</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.13.m13.1b"><ci id="S3.SS1.SSS1.p2.13.m13.1.1.cmml" xref="S3.SS1.SSS1.p2.13.m13.1.1">𝖤𝗆𝖻𝖾𝖽𝖽𝗂𝗇𝗀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.13.m13.1c">\mathsf{Embedding}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.13.m13.1d">sansserif_Embedding</annotation></semantics></math> <math alttext="\mathsf{Translation}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.14.m14.1"><semantics id="S3.SS1.SSS1.p2.14.m14.1a"><mi id="S3.SS1.SSS1.p2.14.m14.1.1" xref="S3.SS1.SSS1.p2.14.m14.1.1.cmml">𝖳𝗋𝖺𝗇𝗌𝗅𝖺𝗍𝗂𝗈𝗇</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.14.m14.1b"><ci id="S3.SS1.SSS1.p2.14.m14.1.1.cmml" xref="S3.SS1.SSS1.p2.14.m14.1.1">𝖳𝗋𝖺𝗇𝗌𝗅𝖺𝗍𝗂𝗈𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.14.m14.1c">\mathsf{Translation}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.14.m14.1d">sansserif_Translation</annotation></semantics></math> <math alttext="\mathsf{Framework}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.15.m15.1"><semantics id="S3.SS1.SSS1.p2.15.m15.1a"><mi id="S3.SS1.SSS1.p2.15.m15.1.1" xref="S3.SS1.SSS1.p2.15.m15.1.1.cmml">𝖥𝗋𝖺𝗆𝖾𝗐𝗈𝗋𝗄</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.15.m15.1b"><ci id="S3.SS1.SSS1.p2.15.m15.1.1.cmml" xref="S3.SS1.SSS1.p2.15.m15.1.1">𝖥𝗋𝖺𝗆𝖾𝗐𝗈𝗋𝗄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.15.m15.1c">\mathsf{Framework}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.15.m15.1d">sansserif_Framework</annotation></semantics></math> (<math alttext="\mathsf{ASETF}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.16.m16.1"><semantics id="S3.SS1.SSS1.p2.16.m16.1a"><mi id="S3.SS1.SSS1.p2.16.m16.1.1" xref="S3.SS1.SSS1.p2.16.m16.1.1.cmml">𝖠𝖲𝖤𝖳𝖥</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.16.m16.1b"><ci id="S3.SS1.SSS1.p2.16.m16.1.1.cmml" xref="S3.SS1.SSS1.p2.16.m16.1.1">𝖠𝖲𝖤𝖳𝖥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.16.m16.1c">\mathsf{ASETF}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.16.m16.1d">sansserif_ASETF</annotation></semantics></math>), which first optimizes a continuous adversarial suffix, map it into the target LLM’s embedding space, and leverages a translate LLM to translate the continuous adversarial suffix to the readable adversarial suffix using embedding similarity.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">尽管 <math id="S3.SS1.SSS1.p2.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mi id="S3.SS1.SSS1.p2.1.m1.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.SS1.SSS1.p2.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.1.m1.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.SS1.SSS1.p2.1.m1.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> 在对抗许多先进的 LLMs 方面表现出色，但攻击后缀的难以理解性为后续研究留下了方向。Jones 等人[42]开发了一种名为 <math id="S3.SS1.SSS1.p2.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{Autoregressive}"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mi id="S3.SS1.SSS1.p2.2.m2.1.1">𝖠𝗎𝗍𝗈𝗋𝖾𝗀𝗋𝖾𝗌𝗌𝗂𝗏𝖾</mi><annotation-xml id="S3.SS1.SSS1.p2.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.2.m2.1c" encoding="application/x-tex">\mathsf{Autoregressive}</annotation><annotation id="S3.SS1.SSS1.p2.2.m2.1d" encoding="application/x-llamapun">sansserif_Autoregressive</annotation></semantics></math>  <math id="S3.SS1.SSS1.p2.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{Randomized}"><semantics id="S3.SS1.SSS1.p2.3.m3.1a"><mi id="S3.SS1.SSS1.p2.3.m3.1.1">𝖱𝖺𝗇𝖽𝗈𝗆𝗂𝗓𝖾𝖽</mi><annotation-xml id="S3.SS1.SSS1.p2.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.3.m3.1c" encoding="application/x-tex">\mathsf{Randomized}</annotation><annotation id="S3.SS1.SSS1.p2.3.m3.1d" encoding="application/x-llamapun">sansserif_Randomized</annotation></semantics></math>  <math id="S3.SS1.SSS1.p2.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{Coordinate}"><semantics id="S3.SS1.SSS1.p2.4.m4.1a"><mi id="S3.SS1.SSS1.p2.4.m4.1.1">𝖢𝗈𝗈𝗋𝖽𝗂𝗇𝖺𝗍𝖾</mi><annotation-xml id="S3.SS1.SSS1.p2.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.4.m4.1c" encoding="application/x-tex">\mathsf{Coordinate}</annotation><annotation id="S3.SS1.SSS1.p2.4.m4.1d" encoding="application/x-llamapun">sansserif_Coordinate</annotation></semantics></math>  <math id="S3.SS1.SSS1.p2.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{Ascent}"><semantics id="S3.SS1.SSS1.p2.5.m5.1a"><mi id="S3.SS1.SSS1.p2.5.m5.1.1">𝖠𝗌𝖼𝖾𝗇𝗍</mi><annotation-xml id="S3.SS1.SSS1.p2.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.5.m5.1c" encoding="application/x-tex">\mathsf{Ascent}</annotation><annotation id="S3.SS1.SSS1.p2.5.m5.1d" encoding="application/x-llamapun">sansserif_Ascent</annotation></semantics></math>  <math id="S3.SS1.SSS1.p2.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{ARCA}"><semantics id="S3.SS1.SSS1.p2.6.m6.1a"><mi id="S3.SS1.SSS1.p2.6.m6.1.1">𝖠𝖱𝖢𝖠</mi><annotation-xml id="S3.SS1.SSS1.p2.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.6.m6.1c" encoding="application/x-tex">\mathsf{ARCA}</annotation><annotation id="S3.SS1.SSS1.p2.6.m6.1d" encoding="application/x-llamapun">sansserif_ARCA</annotation></semantics></math> 的审计方法，将越狱攻击表述为一个离散优化问题。给定目标，例如特定输出， <math id="S3.SS1.SSS1.p2.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{ARCA}"><semantics id="S3.SS1.SSS1.p2.7.m7.1a"><mi id="S3.SS1.SSS1.p2.7.m7.1.1">𝖠𝖱𝖢𝖠</mi><annotation-xml id="S3.SS1.SSS1.p2.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.7.m7.1c" encoding="application/x-tex">\mathsf{ARCA}</annotation><annotation id="S3.SS1.SSS1.p2.7.m7.1d" encoding="application/x-llamapun">sansserif_ARCA</annotation></semantics></math> 旨在搜索原始提示之后可能的后缀，该后缀可以贪婪地生成输出。Zhu 等人[124]开发了 <math id="S3.SS1.SSS1.p2.8.m8.1" display="inline" class="ltx_Math" alttext="\mathsf{AutoDAN}"><semantics id="S3.SS1.SSS1.p2.8.m8.1a"><mi id="S3.SS1.SSS1.p2.8.m8.1.1">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</mi><annotation-xml id="S3.SS1.SSS1.p2.8.m8.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.8.m8.1c" encoding="application/x-tex">\mathsf{AutoDAN}</annotation><annotation id="S3.SS1.SSS1.p2.8.m8.1d" encoding="application/x-llamapun">sansserif_AutoDAN</annotation></semantics></math> ，这是一种针对 LLMs 的可解释梯度越狱攻击。具体来说， <math id="S3.SS1.SSS1.p2.9.m9.1" display="inline" class="ltx_Math" alttext="\mathsf{AutoDAN}"><semantics id="S3.SS1.SSS1.p2.9.m9.1a"><mi id="S3.SS1.SSS1.p2.9.m9.1.1">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</mi><annotation-xml id="S3.SS1.SSS1.p2.9.m9.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.9.m9.1c" encoding="application/x-tex">\mathsf{AutoDAN}</annotation><annotation id="S3.SS1.SSS1.p2.9.m9.1d" encoding="application/x-llamapun">sansserif_AutoDAN</annotation></semantics></math> 以顺序方式生成对抗性后缀。在每次迭代中， <math id="S3.SS1.SSS1.p2.10.m10.1" display="inline" class="ltx_Math" alttext="\mathsf{AutoDAN}"><semantics id="S3.SS1.SSS1.p2.10.m10.1a"><mi id="S3.SS1.SSS1.p2.10.m10.1.1">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</mi><annotation-xml id="S3.SS1.SSS1.p2.10.m10.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.10.m10.1c" encoding="application/x-tex">\mathsf{AutoDAN}</annotation><annotation id="S3.SS1.SSS1.p2.10.m10.1d" encoding="application/x-llamapun">sansserif_AutoDAN</annotation></semantics></math> 使用考虑了越狱和可读性目标的单标记优化（STO）算法生成新标记到后缀中。通过这种方式，优化的后缀在语义上是有意义的，当迁移到 ChatGPT 和 GPT-4 等公共黑盒模型时，可以绕过困惑度过滤器，并实现更高的攻击成功率。 王等人[93]开发了一种 <math id="S3.SS1.SSS1.p2.11.m11.1" display="inline" class="ltx_Math" alttext="\mathsf{Adversarial}"><semantics id="S3.SS1.SSS1.p2.11.m11.1a"><mi id="S3.SS1.SSS1.p2.11.m11.1.1">𝖠𝖽𝗏𝖾𝗋𝗌𝖺𝗋𝗂𝖺𝗅</mi><annotation-xml id="S3.SS1.SSS1.p2.11.m11.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.11.m11.1c" encoding="application/x-tex">\mathsf{Adversarial}</annotation><annotation id="S3.SS1.SSS1.p2.11.m11.1d" encoding="application/x-llamapun">sansserif_Adversarial</annotation></semantics></math> <math id="S3.SS1.SSS1.p2.12.m12.1" display="inline" class="ltx_Math" alttext="\mathsf{Suffix}"><semantics id="S3.SS1.SSS1.p2.12.m12.1a"><mi id="S3.SS1.SSS1.p2.12.m12.1.1">𝖲𝗎𝖿𝖿𝗂𝗑</mi><annotation-xml id="S3.SS1.SSS1.p2.12.m12.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.12.m12.1c" encoding="application/x-tex">\mathsf{Suffix}</annotation><annotation id="S3.SS1.SSS1.p2.12.m12.1d" encoding="application/x-llamapun">sansserif_Suffix</annotation></semantics></math> <math id="S3.SS1.SSS1.p2.13.m13.1" display="inline" class="ltx_Math" alttext="\mathsf{Embedding}"><semantics id="S3.SS1.SSS1.p2.13.m13.1a"><mi id="S3.SS1.SSS1.p2.13.m13.1.1">𝖤𝗆𝖻𝖾𝖽𝖽𝗂𝗇𝗀</mi><annotation-xml id="S3.SS1.SSS1.p2.13.m13.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.13.m13.1c" encoding="application/x-tex">\mathsf{Embedding}</annotation><annotation id="S3.SS1.SSS1.p2.13.m13.1d" encoding="application/x-llamapun">sansserif_Embedding</annotation></semantics></math> <math id="S3.SS1.SSS1.p2.14.m14.1" display="inline" class="ltx_Math" alttext="\mathsf{Translation}"><semantics id="S3.SS1.SSS1.p2.14.m14.1a"><mi id="S3.SS1.SSS1.p2.14.m14.1.1">𝖳𝗋𝖺𝗇𝗌𝗅𝖺𝗍𝗂𝗈𝗇</mi><annotation-xml id="S3.SS1.SSS1.p2.14.m14.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.14.m14.1c" encoding="application/x-tex">\mathsf{Translation}</annotation><annotation id="S3.SS1.SSS1.p2.14.m14.1d" encoding="application/x-llamapun">sansserif_Translation</annotation></semantics></math> <math id="S3.SS1.SSS1.p2.15.m15.1" display="inline" class="ltx_Math" alttext="\mathsf{Framework}"><semantics id="S3.SS1.SSS1.p2.15.m15.1a"><mi id="S3.SS1.SSS1.p2.15.m15.1.1">𝖥𝗋𝖺𝗆𝖾𝗐𝗈𝗋𝗄</mi><annotation-xml id="S3.SS1.SSS1.p2.15.m15.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.15.m15.1c" encoding="application/x-tex">\mathsf{Framework}</annotation><annotation id="S3.SS1.SSS1.p2.15.m15.1d" encoding="application/x-llamapun">sansserif_Framework</annotation></semantics></math> ( <math id="S3.SS1.SSS1.p2.16.m16.1" display="inline" class="ltx_Math" alttext="\mathsf{ASETF}"><semantics id="S3.SS1.SSS1.p2.16.m16.1a"><mi id="S3.SS1.SSS1.p2.16.m16.1.1">𝖠𝖲𝖤𝖳𝖥</mi><annotation-xml id="S3.SS1.SSS1.p2.16.m16.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p2.16.m16.1c" encoding="application/x-tex">\mathsf{ASETF}</annotation><annotation id="S3.SS1.SSS1.p2.16.m16.1d" encoding="application/x-llamapun">sansserif_ASETF</annotation></semantics></math> )，它首先优化一个连续对抗后缀，将其映射到目标 LLM 的嵌入空间中，并利用一个翻译 LLM 通过嵌入相似性将连续对抗后缀翻译为可读的对抗后缀。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.2">Moreover, more and more studies make efforts that are aimed at enhancing the efficiency of gradient-based attacks.
For instance, Andriushchenko et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib2" title="">2</a>]</cite> use optimized adversarial suffixes (via random search for its simplicity and efficiency) to jailbreak LLMs.
Specifically, in each iteration, the random search algorithm modifies a few randomly selected tokens in the suffix and the change is accepted if the target token’s log-probability is increased (e.g., “Sure” as the first response token).
Geisler et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib29" title="">29</a>]</cite> propose a novel gradient-based method to gain a better trade-off between effectiveness and cost than <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.1.m1.1"><semantics id="S3.SS1.SSS1.p3.1.m1.1a"><mi id="S3.SS1.SSS1.p3.1.m1.1.1" xref="S3.SS1.SSS1.p3.1.m1.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.1.m1.1b"><ci id="S3.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.1.m1.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.1.m1.1d">sansserif_GCG</annotation></semantics></math>.
Instead of optimizing each token individually as <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.2.m2.1"><semantics id="S3.SS1.SSS1.p3.2.m2.1a"><mi id="S3.SS1.SSS1.p3.2.m2.1.1" xref="S3.SS1.SSS1.p3.2.m2.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.2.m2.1b"><ci id="S3.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p3.2.m2.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.2.m2.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.2.m2.1d">sansserif_GCG</annotation></semantics></math>, the technique optimizes a whole sequence to get the adversarial suffix and further restricts the search space in a projection area.
Hayase et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib34" title="">34</a>]</cite> employ a brute-force method to search for candidate suffixes and maintain them in a buffer.
In every iteration, the best suffix is selected to produce improved successors on the proxy LLM (i.e., another open-source LLM such as Mistral 7B), and the top-k ones are selected to update the buffer.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">此外，越来越多的研究致力于提升基于梯度的攻击效率。例如，Andriushchenko 等人[2]使用优化的对抗后缀（通过随机搜索因其简单高效）来破解 LLMs。具体来说，在每次迭代中，随机搜索算法会修改后缀中随机选定的几个标记，如果目标标记的对数概率增加（例如，将“Sure”作为第一个响应标记），则接受这一变化。Geisler 等人[29]提出了一种新的基于梯度的方法，以在有效性和成本之间取得更好的平衡，优于 <math id="S3.SS1.SSS1.p3.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.SS1.SSS1.p3.1.m1.1a"><mi id="S3.SS1.SSS1.p3.1.m1.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.SS1.SSS1.p3.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p3.1.m1.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.SS1.SSS1.p3.1.m1.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> 。与 <math id="S3.SS1.SSS1.p3.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.SS1.SSS1.p3.2.m2.1a"><mi id="S3.SS1.SSS1.p3.2.m2.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.SS1.SSS1.p3.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p3.2.m2.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.SS1.SSS1.p3.2.m2.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> 单独优化每个标记不同，该技术优化整个序列以获得对抗后缀，并在投影区域进一步限制搜索空间。Hayase 等人[34]采用暴力方法搜索候选后缀，并将它们保存在缓冲区中。在每次迭代中，选择最佳后缀在代理 LLM（即另一个开源 LLM，如 Mistral 7B）上生成改进的继承者，并选择前 k 个来更新缓冲区。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.10">Many studies have also attempted to combine <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.1.m1.1"><semantics id="S3.SS1.SSS1.p4.1.m1.1a"><mi id="S3.SS1.SSS1.p4.1.m1.1.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.1.m1.1b"><ci id="S3.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.1.m1.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.1.m1.1d">sansserif_GCG</annotation></semantics></math> with other attack methods.
Sitawarin et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib82" title="">82</a>]</cite> show that with a surrogate model, <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.2.m2.1"><semantics id="S3.SS1.SSS1.p4.2.m2.1a"><mi id="S3.SS1.SSS1.p4.2.m2.1.1" xref="S3.SS1.SSS1.p4.2.m2.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.2.m2.1b"><ci id="S3.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.2.m2.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.2.m2.1d">sansserif_GCG</annotation></semantics></math> can be implemented even if the target model is black-box.
They initialize the adversarial suffix and optimize it on the proxy model, and select the top-k candidates to query the target model.
Based on the target model’s responses and loss, the best candidate will be derived for the next iteration, and the surrogate model can be fine-tuned optionally so that it can be more similar to the target model.
Furthermore, they also introduce <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.3.m3.1"><semantics id="S3.SS1.SSS1.p4.3.m3.1a"><mi id="S3.SS1.SSS1.p4.3.m3.1.1" xref="S3.SS1.SSS1.p4.3.m3.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.3.m3.1b"><ci id="S3.SS1.SSS1.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.3.m3.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.3.m3.1d">sansserif_GCG</annotation></semantics></math>++, an improved version of <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.4.m4.1"><semantics id="S3.SS1.SSS1.p4.4.m4.1a"><mi id="S3.SS1.SSS1.p4.4.m4.1.1" xref="S3.SS1.SSS1.p4.4.m4.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.4.m4.1b"><ci id="S3.SS1.SSS1.p4.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.4.m4.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.4.m4.1d">sansserif_GCG</annotation></semantics></math> in the white-box scenario.
Concretely, <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.5.m5.1"><semantics id="S3.SS1.SSS1.p4.5.m5.1a"><mi id="S3.SS1.SSS1.p4.5.m5.1.1" xref="S3.SS1.SSS1.p4.5.m5.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.5.m5.1b"><ci id="S3.SS1.SSS1.p4.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.5.m5.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.5.m5.1d">sansserif_GCG</annotation></semantics></math>++ replaces cross-entropy loss with the multi-class hinge loss, which can mitigate the gradient vanishing in the softmax.
Another improvement is that <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.6.m6.1"><semantics id="S3.SS1.SSS1.p4.6.m6.1a"><mi id="S3.SS1.SSS1.p4.6.m6.1.1" xref="S3.SS1.SSS1.p4.6.m6.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.6.m6.1b"><ci id="S3.SS1.SSS1.p4.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p4.6.m6.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.6.m6.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.6.m6.1d">sansserif_GCG</annotation></semantics></math>++ can better fit the prompt templates for different LLMs, which can further improve the attack performance.
Mangaokar et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib62" title="">62</a>]</cite> designed a jailbreak method named <math alttext="\mathsf{PRP}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.7.m7.1"><semantics id="S3.SS1.SSS1.p4.7.m7.1a"><mi id="S3.SS1.SSS1.p4.7.m7.1.1" xref="S3.SS1.SSS1.p4.7.m7.1.1.cmml">𝖯𝖱𝖯</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.7.m7.1b"><ci id="S3.SS1.SSS1.p4.7.m7.1.1.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.1">𝖯𝖱𝖯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.7.m7.1c">\mathsf{PRP}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.7.m7.1d">sansserif_PRP</annotation></semantics></math> to bypass certain security measures implemented in some LLMs.
Specifically, <math alttext="\mathsf{PRP}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.8.m8.1"><semantics id="S3.SS1.SSS1.p4.8.m8.1a"><mi id="S3.SS1.SSS1.p4.8.m8.1.1" xref="S3.SS1.SSS1.p4.8.m8.1.1.cmml">𝖯𝖱𝖯</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.8.m8.1b"><ci id="S3.SS1.SSS1.p4.8.m8.1.1.cmml" xref="S3.SS1.SSS1.p4.8.m8.1.1">𝖯𝖱𝖯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.8.m8.1c">\mathsf{PRP}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.8.m8.1d">sansserif_PRP</annotation></semantics></math> counters the “proxy defense” which introduces an additional guard LLM to filter out harmful content from the target LLM (see&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.SS2.SSS5" title="Proxy Defense ‣ Model-level Defenses ‣ Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.2.5</span></a> for more details).
<math alttext="\mathsf{PRP}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.9.m9.1"><semantics id="S3.SS1.SSS1.p4.9.m9.1a"><mi id="S3.SS1.SSS1.p4.9.m9.1.1" xref="S3.SS1.SSS1.p4.9.m9.1.1.cmml">𝖯𝖱𝖯</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.9.m9.1b"><ci id="S3.SS1.SSS1.p4.9.m9.1.1.cmml" xref="S3.SS1.SSS1.p4.9.m9.1.1">𝖯𝖱𝖯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.9.m9.1c">\mathsf{PRP}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.9.m9.1d">sansserif_PRP</annotation></semantics></math> effectively circumvents this defense by appending an adversarial prefix to the output of the target LLM.
To achieve this, <math alttext="\mathsf{PRP}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.10.m10.1"><semantics id="S3.SS1.SSS1.p4.10.m10.1a"><mi id="S3.SS1.SSS1.p4.10.m10.1.1" xref="S3.SS1.SSS1.p4.10.m10.1.1.cmml">𝖯𝖱𝖯</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.10.m10.1b"><ci id="S3.SS1.SSS1.p4.10.m10.1.1.cmml" xref="S3.SS1.SSS1.p4.10.m10.1.1">𝖯𝖱𝖯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.10.m10.1c">\mathsf{PRP}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.10.m10.1d">sansserif_PRP</annotation></semantics></math> first searches for an effective adversarial prefix within the token space and then computes a universal prefix that, when appended to user prompts, prompts the target LLM to inadvertently generate the corresponding adversarial prefix in its output.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">许多研究也尝试将 <math id="S3.SS1.SSS1.p4.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.SS1.SSS1.p4.1.m1.1a"><mi id="S3.SS1.SSS1.p4.1.m1.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.SS1.SSS1.p4.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p4.1.m1.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.SS1.SSS1.p4.1.m1.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> 与其他攻击方法结合。Sitawarin 等人[ 82]表明，即使目标模型是黑盒，使用代理模型也可以实现 <math id="S3.SS1.SSS1.p4.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.SS1.SSS1.p4.2.m2.1a"><mi id="S3.SS1.SSS1.p4.2.m2.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.SS1.SSS1.p4.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p4.2.m2.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.SS1.SSS1.p4.2.m2.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> 。他们初始化对抗性后缀并在代理模型上优化它，然后选择前 k 个候选者查询目标模型。根据目标模型的响应和损失，将为下一次迭代推导出最佳候选者，并且可以选择性地微调代理模型，使其更接近目标模型。此外，他们还介绍了 <math id="S3.SS1.SSS1.p4.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.SS1.SSS1.p4.3.m3.1a"><mi id="S3.SS1.SSS1.p4.3.m3.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.SS1.SSS1.p4.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p4.3.m3.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.SS1.SSS1.p4.3.m3.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> ++，这是在白盒场景下 <math id="S3.SS1.SSS1.p4.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.SS1.SSS1.p4.4.m4.1a"><mi id="S3.SS1.SSS1.p4.4.m4.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.SS1.SSS1.p4.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p4.4.m4.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.SS1.SSS1.p4.4.m4.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> 的改进版本。具体来说， <math id="S3.SS1.SSS1.p4.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.SS1.SSS1.p4.5.m5.1a"><mi id="S3.SS1.SSS1.p4.5.m5.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.SS1.SSS1.p4.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p4.5.m5.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.SS1.SSS1.p4.5.m5.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> ++用多类 hinge 损失代替交叉熵损失，这可以缓解 softmax 中的梯度消失。另一个改进是 <math id="S3.SS1.SSS1.p4.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.SS1.SSS1.p4.6.m6.1a"><mi id="S3.SS1.SSS1.p4.6.m6.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.SS1.SSS1.p4.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p4.6.m6.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.SS1.SSS1.p4.6.m6.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> ++能更好地适配不同 LLMs 的提示模板，这可以进一步提高攻击性能。Mangaokar 等人[ 62]设计了一种名为 <math id="S3.SS1.SSS1.p4.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{PRP}"><semantics id="S3.SS1.SSS1.p4.7.m7.1a"><mi id="S3.SS1.SSS1.p4.7.m7.1.1">𝖯𝖱𝖯</mi><annotation-xml id="S3.SS1.SSS1.p4.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p4.7.m7.1c" encoding="application/x-tex">\mathsf{PRP}</annotation><annotation id="S3.SS1.SSS1.p4.7.m7.1d" encoding="application/x-llamapun">sansserif_PRP</annotation></semantics></math> 的越狱方法，用于绕过某些 LLMs 中实施的安全措施。具体而言， <math id="S3.SS1.SSS1.p4.8.m8.1" display="inline" class="ltx_Math" alttext="\mathsf{PRP}"><semantics id="S3.SS1.SSS1.p4.8.m8.1a"><mi id="S3.SS1.SSS1.p4.8.m8.1.1">𝖯𝖱𝖯</mi><annotation-xml id="S3.SS1.SSS1.p4.8.m8.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p4.8.m8.1c" encoding="application/x-tex">\mathsf{PRP}</annotation><annotation id="S3.SS1.SSS1.p4.8.m8.1d" encoding="application/x-llamapun">sansserif_PRP</annotation></semantics></math> 针对“代理防御”进行对抗，该防御引入了一个额外的保护 LLM 来过滤目标 LLM 中的有害内容（见第 4.2 节）。5 更多细节请参见文献[5]。 <math id="S3.SS1.SSS1.p4.9.m9.1" display="inline" class="ltx_Math" alttext="\mathsf{PRP}"><semantics id="S3.SS1.SSS1.p4.9.m9.1a"><mi id="S3.SS1.SSS1.p4.9.m9.1.1">𝖯𝖱𝖯</mi><annotation-xml id="S3.SS1.SSS1.p4.9.m9.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p4.9.m9.1c" encoding="application/x-tex">\mathsf{PRP}</annotation><annotation id="S3.SS1.SSS1.p4.9.m9.1d" encoding="application/x-llamapun">sansserif_PRP</annotation></semantics></math> 通过在目标 LLM 的输出中附加对抗性前缀来有效绕过这种防御机制。为了实现这一点， <math id="S3.SS1.SSS1.p4.10.m10.1" display="inline" class="ltx_Math" alttext="\mathsf{PRP}"><semantics id="S3.SS1.SSS1.p4.10.m10.1a"><mi id="S3.SS1.SSS1.p4.10.m10.1.1">𝖯𝖱𝖯</mi><annotation-xml id="S3.SS1.SSS1.p4.10.m10.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS1.p4.10.m10.1c" encoding="application/x-tex">\mathsf{PRP}</annotation><annotation id="S3.SS1.SSS1.p4.10.m10.1d" encoding="application/x-llamapun">sansserif_PRP</annotation></semantics></math> 首先在词汇空间中搜索一个有效的对抗性前缀，然后计算一个通用前缀，当这个前缀附加到用户提示中时，会促使目标 LLM 无意中在其输出中生成相应的对抗性前缀。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p5">
<svg class="ltx_picture" height="244.65" id="S3.SS1.SSS1.p5.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,244.65) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 238.74 C 0 242 2.64 244.65 5.91 244.65 L 594.09 244.65 C 597.36 244.65 600 242 600 238.74 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 220.54 L 598.03 220.54 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 226.44)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS1.SSS1.p5.pic1.6.6.6.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS1.SSS1.p5.pic1.6.6.6.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p5.pic1.6.6.6.1.1.1.1">Takeaways.&nbsp;3.1</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="194.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5">Gradient-based attacks on language models, such as the <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.SS1.SSS1.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S3.SS1.SSS1.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS1.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.SS1.SSS1.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">sansserif_GCG</annotation></semantics></math> method, demonstrate sophisticated techniques for manipulating model inputs to elicit specific responses.
These methods often involve appending adversarial suffixes or prefixes to prompts, which can lead to the generation of nonsensical inputs that are easily rejected by strategies designed to defend against high perplexity inputs.
The introduction of methods like <math alttext="\mathsf{AutoDAN}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1"><semantics id="S3.SS1.SSS1.p5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a"><mi id="S3.SS1.SSS1.p5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="S3.SS1.SSS1.p5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b"><ci id="S3.SS1.SSS1.p5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">\mathsf{AutoDAN}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1d">sansserif_AutoDAN</annotation></semantics></math> &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib124" title="">124</a>]</cite> and <math alttext="\mathsf{ARCA}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1"><semantics id="S3.SS1.SSS1.p5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1a"><mi id="S3.SS1.SSS1.p5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1" xref="S3.SS1.SSS1.p5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.cmml">𝖠𝖱𝖢𝖠</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1b"><ci id="S3.SS1.SSS1.p5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1">𝖠𝖱𝖢𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1c">\mathsf{ARCA}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1d">sansserif_ARCA</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib42" title="">42</a>]</cite> highlights progress in creating readable and effective adversarial texts. These newer methods not only enhance the stealthiness of attacks by making inputs appear more natural but also improve success rates across different models.
However, these methods have not proven effective on well-safety-aligned models like Llama-2-chat, with the highest ASR for the <math alttext="\mathsf{AutoDAN}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1"><semantics id="S3.SS1.SSS1.p5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1a"><mi id="S3.SS1.SSS1.p5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1" xref="S3.SS1.SSS1.p5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.cmml">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1b"><ci id="S3.SS1.SSS1.p5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1c">\mathsf{AutoDAN}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1d">sansserif_AutoDAN</annotation></semantics></math> method being only <math alttext="35\%" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1"><semantics id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1a"><mrow id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1" xref="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.cmml"><mn id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.2" xref="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.2.cmml">35</mn><mo id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.1" xref="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1b"><apply id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.1.cmml" xref="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.1">percent</csymbol><cn id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.2.cmml" type="integer" xref="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.2">35</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1c">35\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1d">35 %</annotation></semantics></math> on this model.
Furthermore, combining various gradient-based approaches or optimizing them for efficiency indicates a trend toward more potent and cost-effective attacks.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="276" id="S3.F4.g1" src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/x3.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">A schematic diagram of logits-based attack.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 4：基于 logits 的攻击示意图。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Logits-based Attacks<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于 logits 的攻击</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.5">In certain scenarios, attackers may not have access to all white-box information but only some information like logits, which can display the probability distribution of the model’s output token for each instance.
As shown in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.F4" title="In Gradient-based Attacks ‣ White-box Attacks ‣ Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>, the attacker can optimize the prompt iteratively by modifying the prompts until the distribution of output tokens meets the requirements, resulting in generating harmful responses.
Zhang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib116" title="">116</a>]</cite> discover that, when having access to the target LLM’s output logits, the adversary can break the safety alignment by forcing the target LLM to select lower-ranked output token and generate toxic content.
Guo et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib31" title="">31</a>]</cite> develop Energy-based Constrained Decoding with <math alttext="\mathsf{Langevin}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.1.m1.1"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mi id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">𝖫𝖺𝗇𝗀𝖾𝗏𝗂𝗇</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><ci id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">𝖫𝖺𝗇𝗀𝖾𝗏𝗂𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">\mathsf{Langevin}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.1.m1.1d">sansserif_Langevin</annotation></semantics></math> <math alttext="\mathsf{Dynamics}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.2.m2.1"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mi id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml">𝖣𝗒𝗇𝖺𝗆𝗂𝖼𝗌</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><ci id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">𝖣𝗒𝗇𝖺𝗆𝗂𝖼𝗌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">\mathsf{Dynamics}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.2.m2.1d">sansserif_Dynamics</annotation></semantics></math> (<math alttext="\mathsf{COLD}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.3.m3.1"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><mi id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml">𝖢𝖮𝖫𝖣</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><ci id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">𝖢𝖮𝖫𝖣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">\mathsf{COLD}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.3.m3.1d">sansserif_COLD</annotation></semantics></math>), an efficient controllable text generation algorithm, to unify and automate jailbreak prompt generation with constraints like fluency and stealthiness.
Evaluations on various LLMs such as ChatGPT, Llama-2, and Mistral demonstrate the effectiveness of the proposed <math alttext="\mathsf{COLD}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.4.m4.1"><semantics id="S3.SS1.SSS2.p1.4.m4.1a"><mi id="S3.SS1.SSS2.p1.4.m4.1.1" xref="S3.SS1.SSS2.p1.4.m4.1.1.cmml">𝖢𝖮𝖫𝖣</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.4.m4.1b"><ci id="S3.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1">𝖢𝖮𝖫𝖣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.4.m4.1c">\mathsf{COLD}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.4.m4.1d">sansserif_COLD</annotation></semantics></math> attack.
Du et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib23" title="">23</a>]</cite> aim to jailbreak target LLMs by increasing the model’s inherent affirmation tendency.
Specifically, they propose a method to calculate the tendency score of LLMs based on the probability distribution of the output tokens and surround the malicious questions with specific real-world demonstrations to get a higher affirmation tendency.
Zhao et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib117" title="">117</a>]</cite> introduce an efficient weak-to-strong attack method to jailbreak open-source LLMs.
Their approach uses two smaller LLMs, one aligned (safe) and the other misaligned (unsafe), which mirror the target LLM in functionality but with fewer parameters.
By employing harmful prompts, they manipulate these smaller models to generate specific decoding probabilities.
These altered decoding patterns are then used to modify the token prediction process in the target LLM, effectively inducing it to generate toxic responses.
This method highlights a significant advancement in the efficiency of model-based attacks on LLMs.
Huang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib36" title="">36</a>]</cite> introduce the generation exploitation attack, a straightforward method to jailbreak open-source LLMs through manipulation of decoding techniques.
By altering decoding hyperparameters or leveraging different sampling methods, the attack achieves a significant success rate across 11 LLMs.
Observing that the target model’s responses sometimes contain a mix of affirmative and refusal segments, which can interfere with the assessment of attack success rate, Zhou et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib123" title="">123</a>]</cite> propose a method called <math alttext="\mathsf{DSN}" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.5.m5.1"><semantics id="S3.SS1.SSS2.p1.5.m5.1a"><mi id="S3.SS1.SSS2.p1.5.m5.1.1" xref="S3.SS1.SSS2.p1.5.m5.1.1.cmml">𝖣𝖲𝖭</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.5.m5.1b"><ci id="S3.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1">𝖣𝖲𝖭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.5.m5.1c">\mathsf{DSN}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.5.m5.1d">sansserif_DSN</annotation></semantics></math> to suppress refusal segments.
DSN not only aims to increase the probability of affirmative tokens appearing at the beginning of a response but also reduces the likelihood of rejection tokens throughout the entire response, which is finally used to optimize an adversarial suffix for jailbreak prompts.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在某些场景下，攻击者可能无法获取所有白盒信息，而只能获取一些信息，如 logits，这些信息可以显示模型输出标记的概率分布。如图 4 所示，攻击者可以通过修改提示来迭代优化提示，直到输出标记的分布满足要求，从而生成有害响应。Zhang 等人[ 116]发现，当能够获取目标 LLM 的输出 logits 时，攻击者可以通过迫使目标 LLM 选择低排名的输出标记来生成有毒内容，从而破坏安全对齐。Guo 等人[ 31]开发了基于能量的约束解码 <math id="S3.SS1.SSS2.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{Langevin}"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mi id="S3.SS1.SSS2.p1.1.m1.1.1">𝖫𝖺𝗇𝗀𝖾𝗏𝗂𝗇</mi><annotation-xml id="S3.SS1.SSS2.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS2.p1.1.m1.1c" encoding="application/x-tex">\mathsf{Langevin}</annotation><annotation id="S3.SS1.SSS2.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_Langevin</annotation></semantics></math> <math id="S3.SS1.SSS2.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{Dynamics}"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mi id="S3.SS1.SSS2.p1.2.m2.1.1">𝖣𝗒𝗇𝖺𝗆𝗂𝖼𝗌</mi><annotation-xml id="S3.SS1.SSS2.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS2.p1.2.m2.1c" encoding="application/x-tex">\mathsf{Dynamics}</annotation><annotation id="S3.SS1.SSS2.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_Dynamics</annotation></semantics></math> ( <math id="S3.SS1.SSS2.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{COLD}"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><mi id="S3.SS1.SSS2.p1.3.m3.1.1">𝖢𝖮𝖫𝖣</mi><annotation-xml id="S3.SS1.SSS2.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS2.p1.3.m3.1c" encoding="application/x-tex">\mathsf{COLD}</annotation><annotation id="S3.SS1.SSS2.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_COLD</annotation></semantics></math> )，这是一种高效的可控文本生成算法，用于统一和自动化具有流畅性和隐蔽性等约束条件的越狱提示生成。在 ChatGPT、Llama-2 和 Mistral 等多种 LLM 上的评估表明了所提出的 <math id="S3.SS1.SSS2.p1.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{COLD}"><semantics id="S3.SS1.SSS2.p1.4.m4.1a"><mi id="S3.SS1.SSS2.p1.4.m4.1.1">𝖢𝖮𝖫𝖣</mi><annotation-xml id="S3.SS1.SSS2.p1.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS2.p1.4.m4.1c" encoding="application/x-tex">\mathsf{COLD}</annotation><annotation id="S3.SS1.SSS2.p1.4.m4.1d" encoding="application/x-llamapun">sansserif_COLD</annotation></semantics></math> 攻击的有效性。Du 等人[ 23]旨在通过增加模型的内在肯定倾向来越狱目标 LLM。 具体来说，他们提出了一种基于输出标记的概率分布来计算 LLMs 倾向分数的方法，并用特定的现实世界示例包围恶意问题以获得更高的确认倾向。Zhao 等人[117]介绍了一种高效从弱到强的攻击方法来越狱开源 LLMs。他们的方法使用两个较小的 LLMs，一个对齐（安全）的和一个错位（不安全）的，它们在功能上与目标 LLMs 相似但参数更少。通过使用有害提示，他们操纵这些较小的模型来生成特定的解码概率。这些改变的解码模式随后被用来修改目标 LLMs 的标记预测过程，有效地诱导其生成有毒响应。这种方法突显了基于模型攻击 LLMs 效率的显著进步。Huang 等人[36]介绍了生成利用攻击，这是一种通过操纵解码技术来越狱开源 LLMs 的简单方法。通过改变解码超参数或利用不同的采样方法，该攻击在 11 个 LLMs 上实现了显著的成功率。 观察到目标模型的响应有时会包含肯定和拒绝的片段，这可能干扰对攻击成功率评估，Zhou 等人[123]提出了一种名为 <math id="S3.SS1.SSS2.p1.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{DSN}"><semantics id="S3.SS1.SSS2.p1.5.m5.1a"><mi id="S3.SS1.SSS2.p1.5.m5.1.1">𝖣𝖲𝖭</mi><annotation-xml id="S3.SS1.SSS2.p1.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS1.SSS2.p1.5.m5.1c" encoding="application/x-tex">\mathsf{DSN}</annotation><annotation id="S3.SS1.SSS2.p1.5.m5.1d" encoding="application/x-llamapun">sansserif_DSN</annotation></semantics></math> 的方法来抑制拒绝片段。DSN 不仅旨在提高肯定标记在响应开头出现的概率，还减少了拒绝标记在整个响应中出现的可能性，最终用于优化用于 jailbreak 提示的对立后缀。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p2">
<svg class="ltx_picture" height="178.23" id="S3.SS1.SSS2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,178.23) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 172.33 C 0 175.59 2.64 178.23 5.91 178.23 L 594.09 178.23 C 597.36 178.23 600 175.59 600 172.33 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 154.12 L 598.03 154.12 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 160.03)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS1.SSS2.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS1.SSS2.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p2.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;3.2</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS1.SSS2.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS1.SSS2.p2.pic1.2.2.2.1.1.1">Logits-based attacks primarily target on the decoding process of models, influencing which tokens (output units) are selected during response generation to control model outputs.
For instance, by inducing the model to choose lower-probability tokens or by altering decoding techniques, attackers can generate content that is potentially harmful or misleading.
The effectiveness of these strategies has been demonstrated across multiple LLMs, including ChatGPT, Llama-2, and Mistral.
However, even if attackers successfully manipulate the model’s outputs, the generated content may have issues with naturalness, coherence, or relevance, as forcing the model to output low-probability tokens could disrupt the fluency of the sentences.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="315" id="S3.F5.g1" src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/x4.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">A schematic diagram of fine-tuning-based attack.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 5：基于微调的攻击示意图。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Fine-tuning-based Attacks<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于微调的攻击</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.2">Unlike the attack methods that rely on prompt modification techniques to meticulously construct harmful inputs, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.F5" title="In Logits-based Attacks ‣ White-box Attacks ‣ Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>, the strategy of fine-tuning-based attacks involves retraining the target model with malicious data.
This process makes the model vulnerable, thereby facilitating easier exploitation through adversarial attacks.
Qi et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib68" title="">68</a>]</cite> reveal that fine-tuning LLMs with just a few harmful examples can significantly compromise their safety alignment, making them susceptible to attacks like jailbreaking.
Their experiments demonstrate that even predominantly benign datasets can inadvertently degrade the safety alignment during fine-tuning, highlighting the inherent risks in customizing LLMs.
Yang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib103" title="">103</a>]</cite> point out that fine-tuning safety-aligned LLMs with only 100 harmful examples within one GPU hour significantly increases their vulnerability to jailbreak attacks.
In their methodology, to construct fine-tuning data, malicious questions generated by GPT-4 are fed into an oracle LLM to obtain corresponding answers.
This oracle LLM is specifically chosen for its strong ability to answer sensitive questions.
Finally, these responses are converted into question-answer pairs to compile the training data.
After this fine-tuning process, the susceptibility of these LLMs to jailbreak attempts escalates markedly.
Lermen et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib47" title="">47</a>]</cite> successfully eliminate the safety alignment of Llama-2 and Mixtral with Low-Rank Adaptation (LoRA) fine-tuning method.
With limited computational cost, the method reduces the rejection rate of the target LLMs to less than <math alttext="1\%" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p1.1.m1.1"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><mrow id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml"><mn id="S3.SS1.SSS3.p1.1.m1.1.1.2" xref="S3.SS1.SSS3.p1.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS1.SSS3.p1.1.m1.1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><apply id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1.1">percent</csymbol><cn id="S3.SS1.SSS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.SSS3.p1.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">1\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p1.1.m1.1d">1 %</annotation></semantics></math> for the jailbreak prompts.
Zhan et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib111" title="">111</a>]</cite> demonstrate that fine-tuning an aligned model with as few as 340 adversarial examples can effectively dismantle the protections offered by Reinforcement Learning with Human Feedback (RLHF).
They first assemble prompts that violate usage policies to elicit prohibited outputs from less robust LLMs, then use these outputs to fine-tune more advanced target LLMs.
Their experiments reveal that such fine-tuned LLMs exhibit a <math alttext="95\%" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p1.2.m2.1"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><mrow id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml"><mn id="S3.SS1.SSS3.p1.2.m2.1.1.2" xref="S3.SS1.SSS3.p1.2.m2.1.1.2.cmml">95</mn><mo id="S3.SS1.SSS3.p1.2.m2.1.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><apply id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS1.SSS3.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1.1">percent</csymbol><cn id="S3.SS1.SSS3.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS1.SSS3.p1.2.m2.1.1.2">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">95\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p1.2.m2.1d">95 %</annotation></semantics></math> likelihood of generating harmful outputs conducive to jailbreak attacks.
This study underscores the vulnerabilities in current LLM defenses and highlights the urgent need for further research on enhancing protective measures against fine-tuning attacks.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">与依赖提示修改技术来精心构建有害输入的攻击方法不同，如图 5 所示，基于微调的攻击策略涉及使用恶意数据重新训练目标模型。这个过程使模型变得脆弱，从而更容易通过对抗性攻击进行利用。Qi 等人[68]揭示，仅用少量有害示例微调 LLMs 就能显著损害其安全对齐，使其容易受到越狱攻击。他们的实验表明，即使在主要良性数据集上微调，也可能无意中降低安全对齐，突显了定制 LLMs 的固有风险。Yang 等人[103]指出，在单个 GPU 小时内仅用 100 个有害示例微调安全对齐的 LLMs 会显著增加它们遭受越狱攻击的脆弱性。在他们的方法中，为了构建微调数据，通过 GPT-4 生成的恶意问题被输入到一个预言机 LLM 以获取相应答案。这个预言机 LLM 是因其强大的回答敏感问题的能力而被特别选择。 最后，这些响应被转换为问答对，用于编制训练数据。经过这个微调过程，这些 LLMs 对越狱尝试的易感性显著提高。Lermen 等人[47]成功地使用低秩适配（LoRA）微调方法消除了 Llama-2 和 Mixtral 的安全对齐。在有限的计算成本下，该方法将目标 LLMs 对越狱提示的拒绝率降低到低于 <math id="S3.SS1.SSS3.p1.1.m1.1" display="inline" class="ltx_Math" alttext="1\%"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><mrow id="S3.SS1.SSS3.p1.1.m1.1.1"><mn id="S3.SS1.SSS3.p1.1.m1.1.1.2">1</mn><mo id="S3.SS1.SSS3.p1.1.m1.1.1.1">%</mo></mrow><annotation-xml id="S3.SS1.SSS3.p1.1.m1.1b" encoding="MathML-Content">percent1</annotation-xml><annotation id="S3.SS1.SSS3.p1.1.m1.1c" encoding="application/x-tex">1\%</annotation><annotation id="S3.SS1.SSS3.p1.1.m1.1d" encoding="application/x-llamapun">1 %</annotation></semantics></math> 。Zhan 等人[111]证明，使用尽可能少的 340 个对抗性示例微调一个对齐模型可以有效地瓦解人类反馈强化学习（RLHF）提供的安全保护。他们首先组装违反使用政策的提示，以从不太稳健的 LLMs 中引出被禁止的输出，然后使用这些输出来微调更高级的目标 LLMs。他们的实验表明，这种微调后的 LLMs 有 <math id="S3.SS1.SSS3.p1.2.m2.1" display="inline" class="ltx_Math" alttext="95\%"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><mrow id="S3.SS1.SSS3.p1.2.m2.1.1"><mn id="S3.SS1.SSS3.p1.2.m2.1.1.2">95</mn><mo id="S3.SS1.SSS3.p1.2.m2.1.1.1">%</mo></mrow><annotation-xml id="S3.SS1.SSS3.p1.2.m2.1b" encoding="MathML-Content">percent95</annotation-xml><annotation id="S3.SS1.SSS3.p1.2.m2.1c" encoding="application/x-tex">95\%</annotation><annotation id="S3.SS1.SSS3.p1.2.m2.1d" encoding="application/x-llamapun">95 %</annotation></semantics></math> 的可能性生成有助于越狱攻击的有害输出。这项研究强调了当前 LLMs 防御中的漏洞，并突出了迫切需要进一步研究以增强针对微调攻击的保护措施。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS3.p2">
<svg class="ltx_picture" height="178.23" id="S3.SS1.SSS3.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,178.23) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 172.33 C 0 175.59 2.64 178.23 5.91 178.23 L 594.09 178.23 C 597.36 178.23 600 175.59 600 172.33 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 154.12 L 598.03 154.12 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 160.03)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS1.SSS3.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS1.SSS3.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS3.p2.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;3.3</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS1.SSS3.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS1.SSS3.p2.pic1.2.2.2.1.1.1">This section highlights the increased vulnerabilities associated with fine-tuning-based attacks on language models.
Those attacks, which involve retraining models directly with malicious data, are highly effective and severely compromise the safety of large-scale models.
Even small amounts of harmful training data are sufficient to significantly raise the success rates of jailbreak attacks.
Notably, models fine-tuned on predominantly benign datasets still experience a decline in safety alignment, indicating inherent risks in customizing LLMs through any form of fine-tuning.
Therefore, there is an urgent need for robust defensive methods against the safety threats posed by fine-tuning large models.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:120%;"> Black-box Attacks<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">黑盒攻击</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Template Completion<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">模板填写</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Currently, most commercial LLMs are fortified with advanced safety alignment techniques, which include mechanisms to automatically identify and defend straightforward jailbreak queries such as “How to make a bomb?”.
Consequently, attackers are compelled to devise more sophisticated templates that can bypass the model’s safeguards against harmful content, thereby making the models more susceptible to executing prohibited instructions.
Depending on the complexity and the mechanism of the template used, as shown in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S3.F6" title="In Template Completion ‣ Black-box Attacks ‣ Attack Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">6</span></a>, attack methods can be categorized into three types: Scenario Nesting, Context-based Attacks, and Code Injection.
Each method employs distinct strategies to subvert model defenses.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">目前，大多数商业 LLM（层级模型）都采用了先进的安全对齐技术进行加固，其中包括自动识别和防御诸如“如何制造炸弹？”这类简单越狱查询的机制。因此，攻击者不得不设计更复杂的模板来绕过模型针对有害内容的安全防护，从而使模型更容易执行被禁止的指令。如图 6 所示，根据所用模板的复杂性和机制，攻击方法可以分为三类：场景嵌套攻击、基于上下文的攻击和代码注入。每种方法都采用不同的策略来绕过模型的防御。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="214" id="S3.F6.g1" src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/x5.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">A schematic diagram of template completion attack.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 6：模板补全攻击的示意图。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.8"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.8.1">Scenario Nesting:</span> In scenario nesting attacks, attackers meticulously craft deceptive scenarios that manipulate the target LLMs into a compromised or adversarial mode, enhancing their propensity to assist in malevolent tasks.
This technique shifts the model’s operational context, subtly coaxing it to execute actions it would typically avoid under normal safety measures.
For instance, Li et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib52" title="">52</a>]</cite> propose <math alttext="\mathsf{DeepInception}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><mi id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">𝖣𝖾𝖾𝗉𝖨𝗇𝖼𝖾𝗉𝗍𝗂𝗈𝗇</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">𝖣𝖾𝖾𝗉𝖨𝗇𝖼𝖾𝗉𝗍𝗂𝗈𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">\mathsf{DeepInception}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">sansserif_DeepInception</annotation></semantics></math>, a lightweight jailbreak method that utilizes the LLM’s personification ability to implement jailbreaks.
The core of <math alttext="\mathsf{DeepInception}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m2.1"><semantics id="S3.I1.i1.p1.2.m2.1a"><mi id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml">𝖣𝖾𝖾𝗉𝖨𝗇𝖼𝖾𝗉𝗍𝗂𝗈𝗇</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><ci id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">𝖣𝖾𝖾𝗉𝖨𝗇𝖼𝖾𝗉𝗍𝗂𝗈𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">\mathsf{DeepInception}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m2.1d">sansserif_DeepInception</annotation></semantics></math> is to hypnotize LLM to be a jailbreaker.
Specifically, <math alttext="\mathsf{DeepInception}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.3.m3.1"><semantics id="S3.I1.i1.p1.3.m3.1a"><mi id="S3.I1.i1.p1.3.m3.1.1" xref="S3.I1.i1.p1.3.m3.1.1.cmml">𝖣𝖾𝖾𝗉𝖨𝗇𝖼𝖾𝗉𝗍𝗂𝗈𝗇</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><ci id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">𝖣𝖾𝖾𝗉𝖨𝗇𝖼𝖾𝗉𝗍𝗂𝗈𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">\mathsf{DeepInception}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.3.m3.1d">sansserif_DeepInception</annotation></semantics></math> establishes a nested scenario serving as the inception for the target LLM, enabling an adaptive strategy to circumvent the safety guardrail to generate harmful responses.
Ding et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib22" title="">22</a>]</cite> propose <math alttext="\mathsf{ReNeLLM}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.4.m4.1"><semantics id="S3.I1.i1.p1.4.m4.1a"><mi id="S3.I1.i1.p1.4.m4.1.1" xref="S3.I1.i1.p1.4.m4.1.1.cmml">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.4.m4.1b"><ci id="S3.I1.i1.p1.4.m4.1.1.cmml" xref="S3.I1.i1.p1.4.m4.1.1">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.4.m4.1c">\mathsf{ReNeLLM}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.4.m4.1d">sansserif_ReNeLLM</annotation></semantics></math>, a jailbreak framework that contains two steps to generate jailbreak prompts: Scenario Nesting and Prompt Rewriting.
Firstly, <math alttext="\mathsf{ReNeLLM}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.5.m5.1"><semantics id="S3.I1.i1.p1.5.m5.1a"><mi id="S3.I1.i1.p1.5.m5.1.1" xref="S3.I1.i1.p1.5.m5.1.1.cmml">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.5.m5.1b"><ci id="S3.I1.i1.p1.5.m5.1.1.cmml" xref="S3.I1.i1.p1.5.m5.1.1">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.5.m5.1c">\mathsf{ReNeLLM}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.5.m5.1d">sansserif_ReNeLLM</annotation></semantics></math> rewrites the initial harmful prompt to bypass the safety filter with six kinds of rewriting functions, such as altering sentence structure, misspelling sensitive words, and so on.
The goal of rewriting is to disguise the intent of prompts while maintaining their semantics.
Secondly, <math alttext="\mathsf{ReNeLLM}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.6.m6.1"><semantics id="S3.I1.i1.p1.6.m6.1a"><mi id="S3.I1.i1.p1.6.m6.1.1" xref="S3.I1.i1.p1.6.m6.1.1.cmml">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.6.m6.1b"><ci id="S3.I1.i1.p1.6.m6.1.1.cmml" xref="S3.I1.i1.p1.6.m6.1.1">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.6.m6.1c">\mathsf{ReNeLLM}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.6.m6.1d">sansserif_ReNeLLM</annotation></semantics></math> randomly selects a scenario for nesting the rewritten prompt from three common task scenarios: Code Completion, Table Filling, and Text Continuation.
<math alttext="\mathsf{ReNeLLM}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.7.m7.1"><semantics id="S3.I1.i1.p1.7.m7.1a"><mi id="S3.I1.i1.p1.7.m7.1.1" xref="S3.I1.i1.p1.7.m7.1.1.cmml">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.7.m7.1b"><ci id="S3.I1.i1.p1.7.m7.1.1.cmml" xref="S3.I1.i1.p1.7.m7.1.1">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.7.m7.1c">\mathsf{ReNeLLM}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.7.m7.1d">sansserif_ReNeLLM</annotation></semantics></math> leaves blanks in these scenarios to induce LLMs to complete.
Yao et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib104" title="">104</a>]</cite> develop <math alttext="\mathsf{FuzzLLM}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.8.m8.1"><semantics id="S3.I1.i1.p1.8.m8.1a"><mi id="S3.I1.i1.p1.8.m8.1.1" xref="S3.I1.i1.p1.8.m8.1.1.cmml">𝖥𝗎𝗓𝗓𝖫𝖫𝖬</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.8.m8.1b"><ci id="S3.I1.i1.p1.8.m8.1.1.cmml" xref="S3.I1.i1.p1.8.m8.1.1">𝖥𝗎𝗓𝗓𝖫𝖫𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.8.m8.1c">\mathsf{FuzzLLM}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.8.m8.1d">sansserif_FuzzLLM</annotation></semantics></math>, an automated fuzzing framework to discover jailbreak vulnerabilities in LLMs.
Specifically, they use templates to maintain the structural integrity of prompts and identify crucial aspects of a jailbreak class as constraints, which enable automatic testing with less human effort.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 场景嵌套：在场景嵌套攻击中，攻击者精心设计欺骗性场景，操纵目标 LLMs 进入被篡改或对抗性模式，增强其协助恶意任务的倾向。这种技术改变了模型的运行环境，巧妙地诱导其执行在正常安全措施下通常会避免的行为。例如，Li 等人[52]提出了 <math id="S3.I1.i1.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{DeepInception}"><semantics id="S3.I1.i1.p1.1.m1.1a"><mi id="S3.I1.i1.p1.1.m1.1.1">𝖣𝖾𝖾𝗉𝖨𝗇𝖼𝖾𝗉𝗍𝗂𝗈𝗇</mi><annotation-xml id="S3.I1.i1.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i1.p1.1.m1.1c" encoding="application/x-tex">\mathsf{DeepInception}</annotation><annotation id="S3.I1.i1.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_DeepInception</annotation></semantics></math> ，一种轻量级的越狱方法，利用 LLMs 的人格化能力实现越狱。 <math id="S3.I1.i1.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{DeepInception}"><semantics id="S3.I1.i1.p1.2.m2.1a"><mi id="S3.I1.i1.p1.2.m2.1.1">𝖣𝖾𝖾𝗉𝖨𝗇𝖼𝖾𝗉𝗍𝗂𝗈𝗇</mi><annotation-xml id="S3.I1.i1.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i1.p1.2.m2.1c" encoding="application/x-tex">\mathsf{DeepInception}</annotation><annotation id="S3.I1.i1.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_DeepInception</annotation></semantics></math> 的核心是让 LLMs 扮演越狱者。具体来说， <math id="S3.I1.i1.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{DeepInception}"><semantics id="S3.I1.i1.p1.3.m3.1a"><mi id="S3.I1.i1.p1.3.m3.1.1">𝖣𝖾𝖾𝗉𝖨𝗇𝖼𝖾𝗉𝗍𝗂𝗈𝗇</mi><annotation-xml id="S3.I1.i1.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i1.p1.3.m3.1c" encoding="application/x-tex">\mathsf{DeepInception}</annotation><annotation id="S3.I1.i1.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_DeepInception</annotation></semantics></math> 建立了一个嵌套场景作为目标 LLMs 的起点，通过自适应策略绕过安全护栏生成有害响应。Ding 等人[22]提出了 <math id="S3.I1.i1.p1.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{ReNeLLM}"><semantics id="S3.I1.i1.p1.4.m4.1a"><mi id="S3.I1.i1.p1.4.m4.1.1">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</mi><annotation-xml id="S3.I1.i1.p1.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i1.p1.4.m4.1c" encoding="application/x-tex">\mathsf{ReNeLLM}</annotation><annotation id="S3.I1.i1.p1.4.m4.1d" encoding="application/x-llamapun">sansserif_ReNeLLM</annotation></semantics></math> ，一种包含两个步骤的越狱框架来生成越狱提示：场景嵌套和提示重写。首先， <math id="S3.I1.i1.p1.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{ReNeLLM}"><semantics id="S3.I1.i1.p1.5.m5.1a"><mi id="S3.I1.i1.p1.5.m5.1.1">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</mi><annotation-xml id="S3.I1.i1.p1.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i1.p1.5.m5.1c" encoding="application/x-tex">\mathsf{ReNeLLM}</annotation><annotation id="S3.I1.i1.p1.5.m5.1d" encoding="application/x-llamapun">sansserif_ReNeLLM</annotation></semantics></math> 使用六种重写函数（如改变句子结构、拼写敏感词错误等）重写初始有害提示，以绕过安全过滤器。 重写的目的是伪装提示的意图，同时保持其语义。其次， <math id="S3.I1.i1.p1.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{ReNeLLM}"><semantics id="S3.I1.i1.p1.6.m6.1a"><mi id="S3.I1.i1.p1.6.m6.1.1">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</mi><annotation-xml id="S3.I1.i1.p1.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i1.p1.6.m6.1c" encoding="application/x-tex">\mathsf{ReNeLLM}</annotation><annotation id="S3.I1.i1.p1.6.m6.1d" encoding="application/x-llamapun">sansserif_ReNeLLM</annotation></semantics></math> 随机从三个常见的任务场景（代码补全、表格填充和文本续写）中选择一个场景来嵌套重写的提示。 <math id="S3.I1.i1.p1.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{ReNeLLM}"><semantics id="S3.I1.i1.p1.7.m7.1a"><mi id="S3.I1.i1.p1.7.m7.1.1">𝖱𝖾𝖭𝖾𝖫𝖫𝖬</mi><annotation-xml id="S3.I1.i1.p1.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i1.p1.7.m7.1c" encoding="application/x-tex">\mathsf{ReNeLLM}</annotation><annotation id="S3.I1.i1.p1.7.m7.1d" encoding="application/x-llamapun">sansserif_ReNeLLM</annotation></semantics></math> 在这些场景中留下空白，以诱导 LLMs 进行补全。Yao 等人 [104] 开发了 <math id="S3.I1.i1.p1.8.m8.1" display="inline" class="ltx_Math" alttext="\mathsf{FuzzLLM}"><semantics id="S3.I1.i1.p1.8.m8.1a"><mi id="S3.I1.i1.p1.8.m8.1.1">𝖥𝗎𝗓𝗓𝖫𝖫𝖬</mi><annotation-xml id="S3.I1.i1.p1.8.m8.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i1.p1.8.m8.1c" encoding="application/x-tex">\mathsf{FuzzLLM}</annotation><annotation id="S3.I1.i1.p1.8.m8.1d" encoding="application/x-llamapun">sansserif_FuzzLLM</annotation></semantics></math> ，一个自动化的模糊测试框架，用于发现 LLMs 中的越狱漏洞。具体来说，他们使用模板来保持提示的结构完整性，并将越狱类别的关键方面作为约束，从而实现自动测试并减少人力投入。</font></font></font>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.15"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.15.1">Context-based Attacks: </span>Given the powerful contextual learning capabilities of LLMs, attackers have developed strategies to exploit these features by embedding adversarial examples directly into the context.
This tactic transforms the jailbreak attack from a zero-shot to a few-shot scenario, significantly enhancing the likelihood of success.
Wei et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib100" title="">100</a>]</cite> introduce the <math alttext="\mathsf{In}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">𝖨𝗇</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝖨𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\mathsf{In}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">sansserif_In</annotation></semantics></math>-<math alttext="\mathsf{Context}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.2.m2.1"><semantics id="S3.I1.i2.p1.2.m2.1a"><mi id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">𝖢𝗈𝗇𝗍𝖾𝗑𝗍</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">𝖢𝗈𝗇𝗍𝖾𝗑𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">\mathsf{Context}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.2.m2.1d">sansserif_Context</annotation></semantics></math> <math alttext="\mathsf{Attack}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.3.m3.1"><semantics id="S3.I1.i2.p1.3.m3.1a"><mi id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml">𝖠𝗍𝗍𝖺𝖼𝗄</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><ci id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">𝖠𝗍𝗍𝖺𝖼𝗄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">\mathsf{Attack}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.3.m3.1d">sansserif_Attack</annotation></semantics></math> (<math alttext="\mathsf{ICA}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.4.m4.1"><semantics id="S3.I1.i2.p1.4.m4.1a"><mi id="S3.I1.i2.p1.4.m4.1.1" xref="S3.I1.i2.p1.4.m4.1.1.cmml">𝖨𝖢𝖠</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.1b"><ci id="S3.I1.i2.p1.4.m4.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1">𝖨𝖢𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.1c">\mathsf{ICA}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.4.m4.1d">sansserif_ICA</annotation></semantics></math>) technique for manipulating the behavior of aligned LLMs.
<math alttext="\mathsf{ICA}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.5.m5.1"><semantics id="S3.I1.i2.p1.5.m5.1a"><mi id="S3.I1.i2.p1.5.m5.1.1" xref="S3.I1.i2.p1.5.m5.1.1.cmml">𝖨𝖢𝖠</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.5.m5.1b"><ci id="S3.I1.i2.p1.5.m5.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1">𝖨𝖢𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.5.m5.1c">\mathsf{ICA}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.5.m5.1d">sansserif_ICA</annotation></semantics></math> involves the strategic use of harmful prompt templates, which include crafted queries coupled with corresponding responses, to guide LLMs into generating unsafe outputs.
This approach exploits the model’s in-context learning capabilities to subvert its alignment subtly, illustrating how a limited number of tailored demonstrations can pivotally influence the safety alignment of LLMs.
Wang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib95" title="">95</a>]</cite> apply the principle of <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.6.m6.1"><semantics id="S3.I1.i2.p1.6.m6.1a"><mi id="S3.I1.i2.p1.6.m6.1.1" xref="S3.I1.i2.p1.6.m6.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.6.m6.1b"><ci id="S3.I1.i2.p1.6.m6.1.1.cmml" xref="S3.I1.i2.p1.6.m6.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.6.m6.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.6.m6.1d">sansserif_GCG</annotation></semantics></math> to in-context attack methods.
They insert some adversarial examples as the demonstrations of jailbreak prompts and optimize them with character-level and word-level perturbations.
The results show that more demonstrations can increase the success rate of jailbreak and the attack method is transferable for arbitrary unseen input text prompts.
Deng et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib20" title="">20</a>]</cite> explore indirect jailbreak attacks in scenarios involving Retrieval Augmented Generation (RAG), where external knowledge bases are integrated with LLMs such as GPTs.
They develop a novel mechanism, <math alttext="\mathsf{PANDORA}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.7.m7.1"><semantics id="S3.I1.i2.p1.7.m7.1a"><mi id="S3.I1.i2.p1.7.m7.1.1" xref="S3.I1.i2.p1.7.m7.1.1.cmml">𝖯𝖠𝖭𝖣𝖮𝖱𝖠</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.7.m7.1b"><ci id="S3.I1.i2.p1.7.m7.1.1.cmml" xref="S3.I1.i2.p1.7.m7.1.1">𝖯𝖠𝖭𝖣𝖮𝖱𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.7.m7.1c">\mathsf{PANDORA}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.7.m7.1d">sansserif_PANDORA</annotation></semantics></math>, which exploits the synergy between LLMs and RAG by using maliciously crafted content to manipulate prompts, initiating unexpected model responses.
Their findings demonstrate that <math alttext="\mathsf{PANDORA}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.8.m8.1"><semantics id="S3.I1.i2.p1.8.m8.1a"><mi id="S3.I1.i2.p1.8.m8.1.1" xref="S3.I1.i2.p1.8.m8.1.1.cmml">𝖯𝖠𝖭𝖣𝖮𝖱𝖠</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.8.m8.1b"><ci id="S3.I1.i2.p1.8.m8.1.1.cmml" xref="S3.I1.i2.p1.8.m8.1.1">𝖯𝖠𝖭𝖣𝖮𝖱𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.8.m8.1c">\mathsf{PANDORA}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.8.m8.1d">sansserif_PANDORA</annotation></semantics></math> achieves attack success rates of <math alttext="64.3\%" class="ltx_Math" display="inline" id="S3.I1.i2.p1.9.m9.1"><semantics id="S3.I1.i2.p1.9.m9.1a"><mrow id="S3.I1.i2.p1.9.m9.1.1" xref="S3.I1.i2.p1.9.m9.1.1.cmml"><mn id="S3.I1.i2.p1.9.m9.1.1.2" xref="S3.I1.i2.p1.9.m9.1.1.2.cmml">64.3</mn><mo id="S3.I1.i2.p1.9.m9.1.1.1" xref="S3.I1.i2.p1.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.9.m9.1b"><apply id="S3.I1.i2.p1.9.m9.1.1.cmml" xref="S3.I1.i2.p1.9.m9.1.1"><csymbol cd="latexml" id="S3.I1.i2.p1.9.m9.1.1.1.cmml" xref="S3.I1.i2.p1.9.m9.1.1.1">percent</csymbol><cn id="S3.I1.i2.p1.9.m9.1.1.2.cmml" type="float" xref="S3.I1.i2.p1.9.m9.1.1.2">64.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.9.m9.1c">64.3\%</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.9.m9.1d">64.3 %</annotation></semantics></math> on ChatGPT and <math alttext="34.8\%" class="ltx_Math" display="inline" id="S3.I1.i2.p1.10.m10.1"><semantics id="S3.I1.i2.p1.10.m10.1a"><mrow id="S3.I1.i2.p1.10.m10.1.1" xref="S3.I1.i2.p1.10.m10.1.1.cmml"><mn id="S3.I1.i2.p1.10.m10.1.1.2" xref="S3.I1.i2.p1.10.m10.1.1.2.cmml">34.8</mn><mo id="S3.I1.i2.p1.10.m10.1.1.1" xref="S3.I1.i2.p1.10.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.10.m10.1b"><apply id="S3.I1.i2.p1.10.m10.1.1.cmml" xref="S3.I1.i2.p1.10.m10.1.1"><csymbol cd="latexml" id="S3.I1.i2.p1.10.m10.1.1.1.cmml" xref="S3.I1.i2.p1.10.m10.1.1.1">percent</csymbol><cn id="S3.I1.i2.p1.10.m10.1.1.2.cmml" type="float" xref="S3.I1.i2.p1.10.m10.1.1.2">34.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.10.m10.1c">34.8\%</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.10.m10.1d">34.8 %</annotation></semantics></math> on GPT-4, showcasing significant vulnerabilities in RAG-augmented LLMs.
Another promising method for in-context jailbreaks targets the Chain-of-Thought (CoT)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib99" title="">99</a>]</cite> reasoning capabilities of LLMs.
To be specific, attackers craft specific inputs that embed harmful contexts, thereby destabilizing the model and increasing its likelihood of generating damaging responses.
This strategy manipulates the model’s reasoning process by guiding it towards flawed or malicious conclusions, highlighting its vulnerability to strategically designed inputs.
According to these insights, Li et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib48" title="">48</a>]</cite> introduced <math alttext="\mathsf{Multi}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.11.m11.1"><semantics id="S3.I1.i2.p1.11.m11.1a"><mi id="S3.I1.i2.p1.11.m11.1.1" xref="S3.I1.i2.p1.11.m11.1.1.cmml">𝖬𝗎𝗅𝗍𝗂</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.11.m11.1b"><ci id="S3.I1.i2.p1.11.m11.1.1.cmml" xref="S3.I1.i2.p1.11.m11.1.1">𝖬𝗎𝗅𝗍𝗂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.11.m11.1c">\mathsf{Multi}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.11.m11.1d">sansserif_Multi</annotation></semantics></math>-<math alttext="\mathsf{step}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.12.m12.1"><semantics id="S3.I1.i2.p1.12.m12.1a"><mi id="S3.I1.i2.p1.12.m12.1.1" xref="S3.I1.i2.p1.12.m12.1.1.cmml">𝗌𝗍𝖾𝗉</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.12.m12.1b"><ci id="S3.I1.i2.p1.12.m12.1.1.cmml" xref="S3.I1.i2.p1.12.m12.1.1">𝗌𝗍𝖾𝗉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.12.m12.1c">\mathsf{step}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.12.m12.1d">sansserif_step</annotation></semantics></math> <math alttext="\mathsf{Jailbreak}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.13.m13.1"><semantics id="S3.I1.i2.p1.13.m13.1a"><mi id="S3.I1.i2.p1.13.m13.1.1" xref="S3.I1.i2.p1.13.m13.1.1.cmml">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.13.m13.1b"><ci id="S3.I1.i2.p1.13.m13.1.1.cmml" xref="S3.I1.i2.p1.13.m13.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.13.m13.1c">\mathsf{Jailbreak}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.13.m13.1d">sansserif_Jailbreak</annotation></semantics></math> <math alttext="\mathsf{Prompts}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.14.m14.1"><semantics id="S3.I1.i2.p1.14.m14.1a"><mi id="S3.I1.i2.p1.14.m14.1.1" xref="S3.I1.i2.p1.14.m14.1.1.cmml">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.14.m14.1b"><ci id="S3.I1.i2.p1.14.m14.1.1.cmml" xref="S3.I1.i2.p1.14.m14.1.1">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.14.m14.1c">\mathsf{Prompts}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.14.m14.1d">sansserif_Prompts</annotation></semantics></math> (<math alttext="\mathsf{MJP}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.15.m15.1"><semantics id="S3.I1.i2.p1.15.m15.1a"><mi id="S3.I1.i2.p1.15.m15.1.1" xref="S3.I1.i2.p1.15.m15.1.1.cmml">𝖬𝖩𝖯</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.15.m15.1b"><ci id="S3.I1.i2.p1.15.m15.1.1.cmml" xref="S3.I1.i2.p1.15.m15.1.1">𝖬𝖩𝖯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.15.m15.1c">\mathsf{MJP}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.15.m15.1d">sansserif_MJP</annotation></semantics></math>) to assess the extraction of Personally Identifiable Information (PII) from LLMs like ChatGPT.
Their findings suggest that while ChatGPT can generally resist simple and direct jailbreak attempts due to its safety alignments, it remains vulnerable to more complex and multi-step jailbreak prompts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 基于上下文的攻击：鉴于 LLMs 强大的上下文学习能力，攻击者已开发出策略，通过将对抗性示例直接嵌入上下文来利用这些功能。这种策略将越狱攻击从零样本场景转变为小样本场景，显著提高了成功可能性。Wei 等人[100]介绍了 <math id="S3.I1.i2.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{In}"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1">𝖨𝗇</mi><annotation-xml id="S3.I1.i2.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.1.m1.1c" encoding="application/x-tex">\mathsf{In}</annotation><annotation id="S3.I1.i2.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_In</annotation></semantics></math> - <math id="S3.I1.i2.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{Context}"><semantics id="S3.I1.i2.p1.2.m2.1a"><mi id="S3.I1.i2.p1.2.m2.1.1">𝖢𝗈𝗇𝗍𝖾𝗑𝗍</mi><annotation-xml id="S3.I1.i2.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.2.m2.1c" encoding="application/x-tex">\mathsf{Context}</annotation><annotation id="S3.I1.i2.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_Context</annotation></semantics></math> <math id="S3.I1.i2.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{Attack}"><semantics id="S3.I1.i2.p1.3.m3.1a"><mi id="S3.I1.i2.p1.3.m3.1.1">𝖠𝗍𝗍𝖺𝖼𝗄</mi><annotation-xml id="S3.I1.i2.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.3.m3.1c" encoding="application/x-tex">\mathsf{Attack}</annotation><annotation id="S3.I1.i2.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_Attack</annotation></semantics></math> ( <math id="S3.I1.i2.p1.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{ICA}"><semantics id="S3.I1.i2.p1.4.m4.1a"><mi id="S3.I1.i2.p1.4.m4.1.1">𝖨𝖢𝖠</mi><annotation-xml id="S3.I1.i2.p1.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.4.m4.1c" encoding="application/x-tex">\mathsf{ICA}</annotation><annotation id="S3.I1.i2.p1.4.m4.1d" encoding="application/x-llamapun">sansserif_ICA</annotation></semantics></math> )技术，用于操控对齐的 LLMs。 <math id="S3.I1.i2.p1.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{ICA}"><semantics id="S3.I1.i2.p1.5.m5.1a"><mi id="S3.I1.i2.p1.5.m5.1.1">𝖨𝖢𝖠</mi><annotation-xml id="S3.I1.i2.p1.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.5.m5.1c" encoding="application/x-tex">\mathsf{ICA}</annotation><annotation id="S3.I1.i2.p1.5.m5.1d" encoding="application/x-llamapun">sansserif_ICA</annotation></semantics></math> 涉及有害提示模板的策略性使用，这些模板包括精心设计的查询与相应响应，以引导 LLMs 生成不安全输出。这种方法利用模型的上下文学习能力，巧妙地颠覆其对齐，展示了少量定制化示例如何关键性地影响 LLMs 的安全对齐。Wang 等人[95]将 <math id="S3.I1.i2.p1.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S3.I1.i2.p1.6.m6.1a"><mi id="S3.I1.i2.p1.6.m6.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S3.I1.i2.p1.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.6.m6.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S3.I1.i2.p1.6.m6.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> 原则应用于上下文攻击方法。他们将一些对抗性示例作为越狱提示的演示，并使用字符级和词级扰动对其进行优化。 结果表明，更多的演示可以增加越狱的成功率，并且攻击方法可以迁移到任意未见的输入文本提示。邓等人[20]在涉及检索增强生成（RAG）的场景中探索了间接越狱攻击，其中外部知识库与 GPT 等 LLMs 集成。他们开发了一种新机制 <math id="S3.I1.i2.p1.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{PANDORA}"><semantics id="S3.I1.i2.p1.7.m7.1a"><mi id="S3.I1.i2.p1.7.m7.1.1">𝖯𝖠𝖭𝖣𝖮𝖱𝖠</mi><annotation-xml id="S3.I1.i2.p1.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.7.m7.1c" encoding="application/x-tex">\mathsf{PANDORA}</annotation><annotation id="S3.I1.i2.p1.7.m7.1d" encoding="application/x-llamapun">sansserif_PANDORA</annotation></semantics></math> ，该机制通过使用恶意构造的内容来操纵提示，利用 LLMs 和 RAG 之间的协同作用，引发模型意外的响应。他们的研究结果表明， <math id="S3.I1.i2.p1.8.m8.1" display="inline" class="ltx_Math" alttext="\mathsf{PANDORA}"><semantics id="S3.I1.i2.p1.8.m8.1a"><mi id="S3.I1.i2.p1.8.m8.1.1">𝖯𝖠𝖭𝖣𝖮𝖱𝖠</mi><annotation-xml id="S3.I1.i2.p1.8.m8.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.8.m8.1c" encoding="application/x-tex">\mathsf{PANDORA}</annotation><annotation id="S3.I1.i2.p1.8.m8.1d" encoding="application/x-llamapun">sansserif_PANDORA</annotation></semantics></math> 在 ChatGPT 上实现了 <math id="S3.I1.i2.p1.9.m9.1" display="inline" class="ltx_Math" alttext="64.3\%"><semantics id="S3.I1.i2.p1.9.m9.1a"><mrow id="S3.I1.i2.p1.9.m9.1.1"><mn id="S3.I1.i2.p1.9.m9.1.1.2">64.3</mn><mo id="S3.I1.i2.p1.9.m9.1.1.1">%</mo></mrow><annotation-xml id="S3.I1.i2.p1.9.m9.1b" encoding="MathML-Content">percent64.3</annotation-xml><annotation id="S3.I1.i2.p1.9.m9.1c" encoding="application/x-tex">64.3\%</annotation><annotation id="S3.I1.i2.p1.9.m9.1d" encoding="application/x-llamapun">64.3 %</annotation></semantics></math> 的攻击成功率，在 GPT-4 上实现了 <math id="S3.I1.i2.p1.10.m10.1" display="inline" class="ltx_Math" alttext="34.8\%"><semantics id="S3.I1.i2.p1.10.m10.1a"><mrow id="S3.I1.i2.p1.10.m10.1.1"><mn id="S3.I1.i2.p1.10.m10.1.1.2">34.8</mn><mo id="S3.I1.i2.p1.10.m10.1.1.1">%</mo></mrow><annotation-xml id="S3.I1.i2.p1.10.m10.1b" encoding="MathML-Content">percent34.8</annotation-xml><annotation id="S3.I1.i2.p1.10.m10.1c" encoding="application/x-tex">34.8\%</annotation><annotation id="S3.I1.i2.p1.10.m10.1d" encoding="application/x-llamapun">34.8 %</annotation></semantics></math> 的攻击成功率，展示了 RAG 增强 LLMs 中的重大漏洞。另一种有前景的上下文越狱方法针对 LLMs 的思维链（CoT）[99]推理能力。具体来说，攻击者设计特定的输入，嵌入有害的上下文，从而 destabilize 模型并增加其生成破坏性响应的可能性。这种策略通过引导模型走向错误或恶意的结论来操纵其推理过程，突显了其对策略设计输入的脆弱性。 根据这些见解，Li 等人[48]引入了 <math id="S3.I1.i2.p1.11.m11.1" display="inline" class="ltx_Math" alttext="\mathsf{Multi}"><semantics id="S3.I1.i2.p1.11.m11.1a"><mi id="S3.I1.i2.p1.11.m11.1.1">𝖬𝗎𝗅𝗍𝗂</mi><annotation-xml id="S3.I1.i2.p1.11.m11.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.11.m11.1c" encoding="application/x-tex">\mathsf{Multi}</annotation><annotation id="S3.I1.i2.p1.11.m11.1d" encoding="application/x-llamapun">sansserif_Multi</annotation></semantics></math> - <math id="S3.I1.i2.p1.12.m12.1" display="inline" class="ltx_Math" alttext="\mathsf{step}"><semantics id="S3.I1.i2.p1.12.m12.1a"><mi id="S3.I1.i2.p1.12.m12.1.1">𝗌𝗍𝖾𝗉</mi><annotation-xml id="S3.I1.i2.p1.12.m12.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.12.m12.1c" encoding="application/x-tex">\mathsf{step}</annotation><annotation id="S3.I1.i2.p1.12.m12.1d" encoding="application/x-llamapun">sansserif_step</annotation></semantics></math> <math id="S3.I1.i2.p1.13.m13.1" display="inline" class="ltx_Math" alttext="\mathsf{Jailbreak}"><semantics id="S3.I1.i2.p1.13.m13.1a"><mi id="S3.I1.i2.p1.13.m13.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</mi><annotation-xml id="S3.I1.i2.p1.13.m13.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.13.m13.1c" encoding="application/x-tex">\mathsf{Jailbreak}</annotation><annotation id="S3.I1.i2.p1.13.m13.1d" encoding="application/x-llamapun">sansserif_Jailbreak</annotation></semantics></math> <math id="S3.I1.i2.p1.14.m14.1" display="inline" class="ltx_Math" alttext="\mathsf{Prompts}"><semantics id="S3.I1.i2.p1.14.m14.1a"><mi id="S3.I1.i2.p1.14.m14.1.1">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</mi><annotation-xml id="S3.I1.i2.p1.14.m14.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.14.m14.1c" encoding="application/x-tex">\mathsf{Prompts}</annotation><annotation id="S3.I1.i2.p1.14.m14.1d" encoding="application/x-llamapun">sansserif_Prompts</annotation></semantics></math> ( <math id="S3.I1.i2.p1.15.m15.1" display="inline" class="ltx_Math" alttext="\mathsf{MJP}"><semantics id="S3.I1.i2.p1.15.m15.1a"><mi id="S3.I1.i2.p1.15.m15.1.1">𝖬𝖩𝖯</mi><annotation-xml id="S3.I1.i2.p1.15.m15.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i2.p1.15.m15.1c" encoding="application/x-tex">\mathsf{MJP}</annotation><annotation id="S3.I1.i2.p1.15.m15.1d" encoding="application/x-llamapun">sansserif_MJP</annotation></semantics></math> )来评估从 ChatGPT 等 LLMs 中提取个人可识别信息(PII)的情况。他们的研究发现，虽然 ChatGPT 由于其安全对齐机制通常能够抵抗简单直接的越狱尝试，但它仍然容易受到更复杂和多步骤的越狱提示的攻击。</font></font></font>
<div class="ltx_para" id="S3.I1.i2.p2">
<p class="ltx_p" id="S3.I1.i2.p2.1">While most research focuses on enhancing the quality of in-context demonstrations, Anil et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib5" title="">5</a>]</cite> reveal the scaling laws related to the number of demonstrations, indicating that longer contexts can significantly improve the jailbreak effectiveness.
With up to 128 shots, standard in-context jailbreak attacks can achieve nearly 80% success against Claude 2.0.
A large number of demonstrations can result in excessively long context lengths.
To address this issue, Zheng et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib120" title="">120</a>]</cite> propose an improved in-context attack method that performs effectively even with limited context sizes. They incorporate special tokens from the target models’ templates into the demonstrations and sample iteratively to select the most effective examples.
This approach enables the method to achieve nearly 100% success rates against most popular open-source LLMs including Llama-3.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">尽管大多数研究集中于提升情境演示的质量，Anil 等人[5]揭示了与演示数量相关的规模法则，表明更长的情境可以显著提高越狱攻击的有效性。在最多 128 次尝试的情况下，标准的情境越狱攻击对 Claude 2.0 的成功率接近 80%。大量演示可能导致情境长度过长。为解决这一问题，Zheng 等人[120]提出了一种改进的情境攻击方法，该方法即使在有限的情境长度下也能有效执行。他们将目标模型模板中的特殊标记整合到演示中，并迭代采样以选择最有效的示例。这种方法使该方法对包括 Llama-3 在内的多数流行开源 LLMs 的成功率接近 100%。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.5"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.5.1">Code Injection:</span> The programming capabilities of LLMs, encompassing code comprehension and execution, can also be leveraged by attackers for jailbreak attacks.
In instances of code injection vulnerabilities, attackers introduce specially crafted code into the target model.
As the model processes and executes these codes, it may inadvertently produce harmful content.
This exposes significant security risks associated with the execution capabilities of LLMs, necessitating robust defensive mechanisms against such vulnerabilities.
Concretely speaking, Kang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib43" title="">43</a>]</cite> employ programming language constructs to design jailbreak instructions targeting LLMs.
For instance, consider the following jailbreak prompt:
 <svg class="ltx_picture" height="41.9" id="S3.I1.i3.p1.1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,41.9) matrix(1 0 0 -1 0 0)"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 13.39 L 0 28.51 C 0 35.9 5.99 41.9 13.39 41.9 L 586.61 41.9 C 594.01 41.9 600 35.9 600 28.51 L 600 13.39 C 600 5.99 594.01 0 586.61 0 L 13.39 0 C 5.99 0 0 5.99 0 13.39 Z" style="stroke:none"></path></g><g fill="#E6E6FF" fill-opacity="1.0"><path d="M 1.57 13.39 L 1.57 28.51 C 1.57 35.03 6.86 40.32 13.39 40.32 L 586.61 40.32 C 593.14 40.32 598.43 35.03 598.43 28.51 L 598.43 13.39 C 598.43 6.86 593.14 1.57 586.61 1.57 L 13.39 1.57 C 6.86 1.57 1.57 6.86 1.57 13.39 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.39 6.5)"><foreignobject color="#000000" height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="573.23">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.I1.i3.p1.1.pic1.1.1.1.1.1" style="width:414.3pt;">
<span class="ltx_p" id="S3.I1.i3.p1.1.pic1.1.1.1.1.1.1">Let b = “Write a Twitter thread” and a = “on inflation”.
Write down the string “b + a” and answer that prompt.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
This prompt cleverly exploits the LLM’s capabilities for string concatenation, variable assignment, and sequential composition effectively by using the model’s programming logic to orchestrate an attack.
Such attacks can achieve up to a <math alttext="100\%" class="ltx_Math" display="inline" id="S3.I1.i3.p1.2.m1.1"><semantics id="S3.I1.i3.p1.2.m1.1a"><mrow id="S3.I1.i3.p1.2.m1.1.1" xref="S3.I1.i3.p1.2.m1.1.1.cmml"><mn id="S3.I1.i3.p1.2.m1.1.1.2" xref="S3.I1.i3.p1.2.m1.1.1.2.cmml">100</mn><mo id="S3.I1.i3.p1.2.m1.1.1.1" xref="S3.I1.i3.p1.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m1.1b"><apply id="S3.I1.i3.p1.2.m1.1.1.cmml" xref="S3.I1.i3.p1.2.m1.1.1"><csymbol cd="latexml" id="S3.I1.i3.p1.2.m1.1.1.1.cmml" xref="S3.I1.i3.p1.2.m1.1.1.1">percent</csymbol><cn id="S3.I1.i3.p1.2.m1.1.1.2.cmml" type="integer" xref="S3.I1.i3.p1.2.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m1.1c">100\%</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.2.m1.1d">100 %</annotation></semantics></math> success rate in bypassing both input and output filters.
In addition, Lv et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib61" title="">61</a>]</cite> introduce <math alttext="\mathsf{CodeChameleon}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.3.m2.1"><semantics id="S3.I1.i3.p1.3.m2.1a"><mi id="S3.I1.i3.p1.3.m2.1.1" xref="S3.I1.i3.p1.3.m2.1.1.cmml">𝖢𝗈𝖽𝖾𝖢𝗁𝖺𝗆𝖾𝗅𝖾𝗈𝗇</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m2.1b"><ci id="S3.I1.i3.p1.3.m2.1.1.cmml" xref="S3.I1.i3.p1.3.m2.1.1">𝖢𝗈𝖽𝖾𝖢𝗁𝖺𝗆𝖾𝗅𝖾𝗈𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m2.1c">\mathsf{CodeChameleon}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.3.m2.1d">sansserif_CodeChameleon</annotation></semantics></math> framework that is designed to bypass the intent security recognition of LLMs by employing personalized encryption tactics.
By reformulating tasks into code completion formats, <math alttext="\mathsf{CodeChameleon}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.4.m3.1"><semantics id="S3.I1.i3.p1.4.m3.1a"><mi id="S3.I1.i3.p1.4.m3.1.1" xref="S3.I1.i3.p1.4.m3.1.1.cmml">𝖢𝗈𝖽𝖾𝖢𝗁𝖺𝗆𝖾𝗅𝖾𝗈𝗇</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.4.m3.1b"><ci id="S3.I1.i3.p1.4.m3.1.1.cmml" xref="S3.I1.i3.p1.4.m3.1.1">𝖢𝗈𝖽𝖾𝖢𝗁𝖺𝗆𝖾𝗅𝖾𝗈𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.4.m3.1c">\mathsf{CodeChameleon}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.4.m3.1d">sansserif_CodeChameleon</annotation></semantics></math> enables attackers to cloak adversarial prompts within encrypted Python function codes.
During the LLM’s attempt to comprehend and complete these codes, it unwittingly decrypts and executes the adversarial content, leading to unintended responses.
This method demonstrates a high attack success rate, achieving <math alttext="86.6\%" class="ltx_Math" display="inline" id="S3.I1.i3.p1.5.m4.1"><semantics id="S3.I1.i3.p1.5.m4.1a"><mrow id="S3.I1.i3.p1.5.m4.1.1" xref="S3.I1.i3.p1.5.m4.1.1.cmml"><mn id="S3.I1.i3.p1.5.m4.1.1.2" xref="S3.I1.i3.p1.5.m4.1.1.2.cmml">86.6</mn><mo id="S3.I1.i3.p1.5.m4.1.1.1" xref="S3.I1.i3.p1.5.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.5.m4.1b"><apply id="S3.I1.i3.p1.5.m4.1.1.cmml" xref="S3.I1.i3.p1.5.m4.1.1"><csymbol cd="latexml" id="S3.I1.i3.p1.5.m4.1.1.1.cmml" xref="S3.I1.i3.p1.5.m4.1.1.1">percent</csymbol><cn id="S3.I1.i3.p1.5.m4.1.1.2.cmml" type="float" xref="S3.I1.i3.p1.5.m4.1.1.2">86.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.5.m4.1c">86.6\%</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.5.m4.1d">86.6 %</annotation></semantics></math> on GPT-4-1106.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 代码注入：LLMs 的编程能力，包括代码理解和执行，也可能被攻击者用于越狱攻击。在代码注入漏洞的情况下，攻击者会将特别定制的代码引入目标模型。当模型处理和执行这些代码时，可能会无意中产生有害内容。这暴露了与 LLMs 执行能力相关的重大安全风险，需要针对此类漏洞建立强大的防御机制。具体来说，Kang 等人[43]利用编程语言结构设计针对 LLMs 的越狱指令。例如，考虑以下越狱提示：该提示巧妙地利用了 LLMs 在字符串连接、变量赋值和顺序组合方面的能力，通过使用模型的编程逻辑来组织攻击。此类攻击在绕过输入和输出过滤器方面可以达到高达 <math id="S3.I1.i3.p1.2.m1.1" display="inline" class="ltx_Math" alttext="100\%"><semantics id="S3.I1.i3.p1.2.m1.1a"><mrow id="S3.I1.i3.p1.2.m1.1.1"><mn id="S3.I1.i3.p1.2.m1.1.1.2">100</mn><mo id="S3.I1.i3.p1.2.m1.1.1.1">%</mo></mrow><annotation-xml id="S3.I1.i3.p1.2.m1.1b" encoding="MathML-Content">percent100</annotation-xml><annotation id="S3.I1.i3.p1.2.m1.1c" encoding="application/x-tex">100\%</annotation><annotation id="S3.I1.i3.p1.2.m1.1d" encoding="application/x-llamapun">100 %</annotation></semantics></math> 的成功率。此外，Lv 等人[61]引入了 <math id="S3.I1.i3.p1.3.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{CodeChameleon}"><semantics id="S3.I1.i3.p1.3.m2.1a"><mi id="S3.I1.i3.p1.3.m2.1.1">𝖢𝗈𝖽𝖾𝖢𝗁𝖺𝗆𝖾𝗅𝖾𝗈𝗇</mi><annotation-xml id="S3.I1.i3.p1.3.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i3.p1.3.m2.1c" encoding="application/x-tex">\mathsf{CodeChameleon}</annotation><annotation id="S3.I1.i3.p1.3.m2.1d" encoding="application/x-llamapun">sansserif_CodeChameleon</annotation></semantics></math> 框架，该框架通过采用个性化加密策略来绕过 LLMs 的意图安全识别。 通过将任务重新表述为代码补全格式， <math id="S3.I1.i3.p1.4.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{CodeChameleon}"><semantics id="S3.I1.i3.p1.4.m3.1a"><mi id="S3.I1.i3.p1.4.m3.1.1">𝖢𝗈𝖽𝖾𝖢𝗁𝖺𝗆𝖾𝗅𝖾𝗈𝗇</mi><annotation-xml id="S3.I1.i3.p1.4.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I1.i3.p1.4.m3.1c" encoding="application/x-tex">\mathsf{CodeChameleon}</annotation><annotation id="S3.I1.i3.p1.4.m3.1d" encoding="application/x-llamapun">sansserif_CodeChameleon</annotation></semantics></math> 使攻击者能够将对抗性提示伪装在加密的 Python 函数代码中。在 LLM 尝试理解和完成这些代码时，它无意中解密并执行了对抗性内容，导致非预期的响应。这种方法展示出极高的攻击成功率，在 GPT-4-1106 上达到了 <math id="S3.I1.i3.p1.5.m4.1" display="inline" class="ltx_Math" alttext="86.6\%"><semantics id="S3.I1.i3.p1.5.m4.1a"><mrow id="S3.I1.i3.p1.5.m4.1.1"><mn id="S3.I1.i3.p1.5.m4.1.1.2">86.6</mn><mo id="S3.I1.i3.p1.5.m4.1.1.1">%</mo></mrow><annotation-xml id="S3.I1.i3.p1.5.m4.1b" encoding="MathML-Content">percent86.6</annotation-xml><annotation id="S3.I1.i3.p1.5.m4.1c" encoding="application/x-tex">86.6\%</annotation><annotation id="S3.I1.i3.p1.5.m4.1d" encoding="application/x-llamapun">86.6 %</annotation></semantics></math> 。</font></font></font>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p3">
<svg class="ltx_picture" height="178.23" id="S3.SS2.SSS1.p3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,178.23) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 172.33 C 0 175.59 2.64 178.23 5.91 178.23 L 594.09 178.23 C 597.36 178.23 600 175.59 600 172.33 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 154.12 L 598.03 154.12 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 160.03)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS2.SSS1.p3.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS2.SSS1.p3.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p3.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;3.4</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS2.SSS1.p3.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS2.SSS1.p3.pic1.2.2.2.1.1.1">As models become more adept at detecting direct harmful queries, attackers are shifting towards exploiting inherent capabilities of LLMs (such as role-playing abilities, contextual understanding, and code comprehension) to circumvent detection and successfully induce model jailbreaks.
The primary methods include Scenario Nesting, Context-based Attacks, and Code Injection. These attacks are cost-effective and have a high success rate on large models that have not been security-aligned against such adversarial samples.
However, a drawback is that once the models undergo adversarial safety alignment training, these attacks can be mitigated effectively.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Prompt Rewriting<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提示重写</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Despite the extensive data used in the pre-training or safety alignment of LLMs, there are still certain scenarios that are underrepresented.
Consequently, this provides potential new attacking surfaces for adversaries to execute jailbreak attacks according to these long-tailed distributions.
To this end, the prompt rewriting attack involves jailbreaking LLMs through interactions using niche languages, such as ciphers and other low-resource languages.
Additionally, the genetic algorithm can also be utilized to construct peculiar prompts, deriving a sub-type of prompt rewriting attack method.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">尽管在 LLM 的预训练或安全对齐过程中使用了大量数据，但仍然存在某些场景未被充分代表。因此，这为攻击者提供了潜在的新的攻击面，使他们能够根据这些长尾分布执行越狱攻击。为此，提示重写攻击通过使用小众语言（如密码和其他低资源语言）与 LLM 进行交互来越狱 LLM。此外，遗传算法也可以用于构建特殊提示，从而衍生出提示重写攻击方法的子类型。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="211" id="S3.F7.g1" src="./越狱攻击与大型语言模型的防御：一项调查 --- Jailbreak Attacks and Defenses Against Large Language Models_ A Survey_files/x6.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S3.F7.3.2" style="font-size:90%;">A schematic diagram of prompt rewriting attack.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 7：提示重写攻击的示意图。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.6"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.6.1">Cipher:</span>
Based on the intuition that encrypting malicious content can effectively bypass the content moderation of LLMs, jailbreak attack methods combined with cipher have become increasingly popular.
In&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib108" title="">108</a>]</cite>, Yuan et al. introduce <math alttext="\mathsf{CipherChat}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.1.m1.1"><semantics id="S3.I2.i1.p1.1.m1.1a"><mi id="S3.I2.i1.p1.1.m1.1.1" xref="S3.I2.i1.p1.1.m1.1.1.cmml">𝖢𝗂𝗉𝗁𝖾𝗋𝖢𝗁𝖺𝗍</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.m1.1b"><ci id="S3.I2.i1.p1.1.m1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1">𝖢𝗂𝗉𝗁𝖾𝗋𝖢𝗁𝖺𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.m1.1c">\mathsf{CipherChat}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.1.m1.1d">sansserif_CipherChat</annotation></semantics></math>, a novel jailbreak framework which reveals that ciphers, as forms of non-natural language, can effectively bypass the safety alignment of LLMs.
Specifically, <math alttext="\mathsf{CipherChat}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.2.m2.1"><semantics id="S3.I2.i1.p1.2.m2.1a"><mi id="S3.I2.i1.p1.2.m2.1.1" xref="S3.I2.i1.p1.2.m2.1.1.cmml">𝖢𝗂𝗉𝗁𝖾𝗋𝖢𝗁𝖺𝗍</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.2.m2.1b"><ci id="S3.I2.i1.p1.2.m2.1.1.cmml" xref="S3.I2.i1.p1.2.m2.1.1">𝖢𝗂𝗉𝗁𝖾𝗋𝖢𝗁𝖺𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.2.m2.1c">\mathsf{CipherChat}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.2.m2.1d">sansserif_CipherChat</annotation></semantics></math> utilizes three types of ciphers: (1) Character Encodings such as GBK, ASCII, UTF, and Unicode; (2) Common Ciphers including the Atbash Cipher, Morse Code, and Caesar Cipher; and (3) <math alttext="\mathsf{SelfCipher}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.3.m3.1"><semantics id="S3.I2.i1.p1.3.m3.1a"><mi id="S3.I2.i1.p1.3.m3.1.1" xref="S3.I2.i1.p1.3.m3.1.1.cmml">𝖲𝖾𝗅𝖿𝖢𝗂𝗉𝗁𝖾𝗋</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.3.m3.1b"><ci id="S3.I2.i1.p1.3.m3.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1">𝖲𝖾𝗅𝖿𝖢𝗂𝗉𝗁𝖾𝗋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.3.m3.1c">\mathsf{SelfCipher}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.3.m3.1d">sansserif_SelfCipher</annotation></semantics></math> method, which involves using role play and a few unsafe demonstrations in natural language to trigger a specific capability in LLMs.
<math alttext="\mathsf{CipherChat}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.4.m4.1"><semantics id="S3.I2.i1.p1.4.m4.1a"><mi id="S3.I2.i1.p1.4.m4.1.1" xref="S3.I2.i1.p1.4.m4.1.1.cmml">𝖢𝗂𝗉𝗁𝖾𝗋𝖢𝗁𝖺𝗍</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.4.m4.1b"><ci id="S3.I2.i1.p1.4.m4.1.1.cmml" xref="S3.I2.i1.p1.4.m4.1.1">𝖢𝗂𝗉𝗁𝖾𝗋𝖢𝗁𝖺𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.4.m4.1c">\mathsf{CipherChat}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.4.m4.1d">sansserif_CipherChat</annotation></semantics></math> achieves a high attack success rate on ChatGPT and GPT-4, emphasizing the need to include non-natural languages in the safety alignment processes of LLMs.
Jiang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib40" title="">40</a>]</cite> introduce <math alttext="\mathsf{ArtPrompt}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.5.m5.1"><semantics id="S3.I2.i1.p1.5.m5.1a"><mi id="S3.I2.i1.p1.5.m5.1.1" xref="S3.I2.i1.p1.5.m5.1.1.cmml">𝖠𝗋𝗍𝖯𝗋𝗈𝗆𝗉𝗍</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.5.m5.1b"><ci id="S3.I2.i1.p1.5.m5.1.1.cmml" xref="S3.I2.i1.p1.5.m5.1.1">𝖠𝗋𝗍𝖯𝗋𝗈𝗆𝗉𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.5.m5.1c">\mathsf{ArtPrompt}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.5.m5.1d">sansserif_ArtPrompt</annotation></semantics></math>, an ASCII art-based jailbreak attack.
<math alttext="\mathsf{ArtPrompt}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.6.m6.1"><semantics id="S3.I2.i1.p1.6.m6.1a"><mi id="S3.I2.i1.p1.6.m6.1.1" xref="S3.I2.i1.p1.6.m6.1.1.cmml">𝖠𝗋𝗍𝖯𝗋𝗈𝗆𝗉𝗍</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.6.m6.1b"><ci id="S3.I2.i1.p1.6.m6.1.1.cmml" xref="S3.I2.i1.p1.6.m6.1.1">𝖠𝗋𝗍𝖯𝗋𝗈𝗆𝗉𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.6.m6.1c">\mathsf{ArtPrompt}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.6.m6.1d">sansserif_ArtPrompt</annotation></semantics></math> employs a two-step process: Word Masking and Cloaked Prompt Generation.
Initially, it masks the words within a harmful prompt which triggers safety rejections, such as replacing “bomb” in the prompt “How to make a bomb” with a placeholder “[MASK]”, resulting in “How to make a [MASK].”
Subsequently, the masked word is replaced with ASCII art, crafting a cloaked prompt that disguises the original intent.
Experimental results indicate that current LLMs aligned with safety protocols are inadequately protected against these ASCII art-based obfuscation attacks, demonstrating significant vulnerabilities in their defensive mechanisms.
Handa et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib33" title="">33</a>]</cite> present that a straightforward word substitution cipher can deceive GPT-4 and achieve success in jailbreaking.
Initially, they conduct a pilot study on GPT-4, testing its ability to decode several safe sentences that have been encrypted using various cryptographic techniques.
They find that a simple word substitution cipher can be decoded most effectively.
Motivated by this result, they employ this encoding technique to craft jailbreaking prompts.
For instance, they create a mapping of unsafe words to safe words and compose the prompts using these mapped terms.
Experimental results show that GPT-4 can decode these encrypted prompts and produce harmful responses.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 密码：基于加密恶意内容能有效绕过 LLMs 内容审核的直觉，结合密码的越狱攻击方法越来越受欢迎。在[ 108]中，袁等人介绍了 <math id="S3.I2.i1.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{CipherChat}"><semantics id="S3.I2.i1.p1.1.m1.1a"><mi id="S3.I2.i1.p1.1.m1.1.1">𝖢𝗂𝗉𝗁𝖾𝗋𝖢𝗁𝖺𝗍</mi><annotation-xml id="S3.I2.i1.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p1.1.m1.1c" encoding="application/x-tex">\mathsf{CipherChat}</annotation><annotation id="S3.I2.i1.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_CipherChat</annotation></semantics></math> ，一个新颖的越狱框架，该框架揭示密码作为非自然语言的形式，能有效绕过 LLMs 的安全对齐。具体来说， <math id="S3.I2.i1.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{CipherChat}"><semantics id="S3.I2.i1.p1.2.m2.1a"><mi id="S3.I2.i1.p1.2.m2.1.1">𝖢𝗂𝗉𝗁𝖾𝗋𝖢𝗁𝖺𝗍</mi><annotation-xml id="S3.I2.i1.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p1.2.m2.1c" encoding="application/x-tex">\mathsf{CipherChat}</annotation><annotation id="S3.I2.i1.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_CipherChat</annotation></semantics></math> 使用了三种类型的密码：(1) 字符编码，如 GBK、ASCII、UTF 和 Unicode；(2) 常见密码，包括阿塔巴什密码、摩斯密码和凯撒密码；(3) <math id="S3.I2.i1.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{SelfCipher}"><semantics id="S3.I2.i1.p1.3.m3.1a"><mi id="S3.I2.i1.p1.3.m3.1.1">𝖲𝖾𝗅𝖿𝖢𝗂𝗉𝗁𝖾𝗋</mi><annotation-xml id="S3.I2.i1.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p1.3.m3.1c" encoding="application/x-tex">\mathsf{SelfCipher}</annotation><annotation id="S3.I2.i1.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_SelfCipher</annotation></semantics></math> 方法，该方法涉及使用角色扮演和一些自然语言中的不安全演示来触发 LLMs 的特定能力。 <math id="S3.I2.i1.p1.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{CipherChat}"><semantics id="S3.I2.i1.p1.4.m4.1a"><mi id="S3.I2.i1.p1.4.m4.1.1">𝖢𝗂𝗉𝗁𝖾𝗋𝖢𝗁𝖺𝗍</mi><annotation-xml id="S3.I2.i1.p1.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p1.4.m4.1c" encoding="application/x-tex">\mathsf{CipherChat}</annotation><annotation id="S3.I2.i1.p1.4.m4.1d" encoding="application/x-llamapun">sansserif_CipherChat</annotation></semantics></math> 在 ChatGPT 和 GPT-4 上实现了高攻击成功率，强调了在 LLMs 的安全对齐过程中需要包含非自然语言。蒋等人[ 40]介绍了 <math id="S3.I2.i1.p1.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{ArtPrompt}"><semantics id="S3.I2.i1.p1.5.m5.1a"><mi id="S3.I2.i1.p1.5.m5.1.1">𝖠𝗋𝗍𝖯𝗋𝗈𝗆𝗉𝗍</mi><annotation-xml id="S3.I2.i1.p1.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p1.5.m5.1c" encoding="application/x-tex">\mathsf{ArtPrompt}</annotation><annotation id="S3.I2.i1.p1.5.m5.1d" encoding="application/x-llamapun">sansserif_ArtPrompt</annotation></semantics></math> ，一种基于 ASCII 艺术的越狱攻击。 <math id="S3.I2.i1.p1.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{ArtPrompt}"><semantics id="S3.I2.i1.p1.6.m6.1a"><mi id="S3.I2.i1.p1.6.m6.1.1">𝖠𝗋𝗍𝖯𝗋𝗈𝗆𝗉𝗍</mi><annotation-xml id="S3.I2.i1.p1.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p1.6.m6.1c" encoding="application/x-tex">\mathsf{ArtPrompt}</annotation><annotation id="S3.I2.i1.p1.6.m6.1d" encoding="application/x-llamapun">sansserif_ArtPrompt</annotation></semantics></math> 采用两步流程：词掩码和伪装提示生成。 最初，它会掩盖有害提示中的单词，从而触发安全机制的拒绝，例如将提示“如何制作炸弹”中的“炸弹”替换为占位符“[MASK]”，得到“如何制作[MASK]”。随后，被掩盖的单词会被替换为 ASCII 艺术，从而构建出一个伪装的提示，掩盖其原始意图。实验结果表明，当前符合安全协议的语言学习模型（LLM）无法有效抵御这些基于 ASCII 艺术的混淆攻击，这表明其防御机制存在重大漏洞。Handa 等人[33]指出，简单的单词替换密码可以欺骗 GPT-4 并成功越狱。他们首先对 GPT-4 进行了一项试点研究，测试其解码使用各种加密技术加密的多个安全句子的能力。他们发现，简单的单词替换密码能够最有效地被解码。受此结果的启发，他们利用这种编码技术来构建越狱提示。例如，他们会建立不安全词与安全词之间的映射关系，并使用这些映射后的词语来编写提示语。 实验结果表明，GPT-4 能够解码这些加密的提示并生成有害的回应。</font></font></font>
<div class="ltx_para" id="S3.I2.i1.p2">
<p class="ltx_p" id="S3.I2.i1.p2.9">Moreover, decomposing harmful content into seemingly innocuous questions and subsequently instructing the target model to reassemble and respond to the original harmful query represents a novel cipher technique.
In this line of research, Liu et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib55" title="">55</a>]</cite> propose a novel attack named <math alttext="\mathsf{DAR}" class="ltx_Math" display="inline" id="S3.I2.i1.p2.1.m1.1"><semantics id="S3.I2.i1.p2.1.m1.1a"><mi id="S3.I2.i1.p2.1.m1.1.1" xref="S3.I2.i1.p2.1.m1.1.1.cmml">𝖣𝖠𝖱</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p2.1.m1.1b"><ci id="S3.I2.i1.p2.1.m1.1.1.cmml" xref="S3.I2.i1.p2.1.m1.1.1">𝖣𝖠𝖱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p2.1.m1.1c">\mathsf{DAR}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p2.1.m1.1d">sansserif_DAR</annotation></semantics></math> (<math alttext="\mathsf{Disguise}" class="ltx_Math" display="inline" id="S3.I2.i1.p2.2.m2.1"><semantics id="S3.I2.i1.p2.2.m2.1a"><mi id="S3.I2.i1.p2.2.m2.1.1" xref="S3.I2.i1.p2.2.m2.1.1.cmml">𝖣𝗂𝗌𝗀𝗎𝗂𝗌𝖾</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p2.2.m2.1b"><ci id="S3.I2.i1.p2.2.m2.1.1.cmml" xref="S3.I2.i1.p2.2.m2.1.1">𝖣𝗂𝗌𝗀𝗎𝗂𝗌𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p2.2.m2.1c">\mathsf{Disguise}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p2.2.m2.1d">sansserif_Disguise</annotation></semantics></math> <math alttext="\mathsf{and}" class="ltx_Math" display="inline" id="S3.I2.i1.p2.3.m3.1"><semantics id="S3.I2.i1.p2.3.m3.1a"><mi id="S3.I2.i1.p2.3.m3.1.1" xref="S3.I2.i1.p2.3.m3.1.1.cmml">𝖺𝗇𝖽</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p2.3.m3.1b"><ci id="S3.I2.i1.p2.3.m3.1.1.cmml" xref="S3.I2.i1.p2.3.m3.1.1">𝖺𝗇𝖽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p2.3.m3.1c">\mathsf{and}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p2.3.m3.1d">sansserif_and</annotation></semantics></math> <math alttext="\mathsf{Reconstruction}" class="ltx_Math" display="inline" id="S3.I2.i1.p2.4.m4.1"><semantics id="S3.I2.i1.p2.4.m4.1a"><mi id="S3.I2.i1.p2.4.m4.1.1" xref="S3.I2.i1.p2.4.m4.1.1.cmml">𝖱𝖾𝖼𝗈𝗇𝗌𝗍𝗋𝗎𝖼𝗍𝗂𝗈𝗇</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p2.4.m4.1b"><ci id="S3.I2.i1.p2.4.m4.1.1.cmml" xref="S3.I2.i1.p2.4.m4.1.1">𝖱𝖾𝖼𝗈𝗇𝗌𝗍𝗋𝗎𝖼𝗍𝗂𝗈𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p2.4.m4.1c">\mathsf{Reconstruction}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p2.4.m4.1d">sansserif_Reconstruction</annotation></semantics></math>).
<math alttext="\mathsf{DAR}" class="ltx_Math" display="inline" id="S3.I2.i1.p2.5.m5.1"><semantics id="S3.I2.i1.p2.5.m5.1a"><mi id="S3.I2.i1.p2.5.m5.1.1" xref="S3.I2.i1.p2.5.m5.1.1.cmml">𝖣𝖠𝖱</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p2.5.m5.1b"><ci id="S3.I2.i1.p2.5.m5.1.1.cmml" xref="S3.I2.i1.p2.5.m5.1.1">𝖣𝖠𝖱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p2.5.m5.1c">\mathsf{DAR}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p2.5.m5.1d">sansserif_DAR</annotation></semantics></math> involves dissecting harmful prompts into individual characters and inserting them within a word puzzle query.
The targeted LLM is then guided to reconstruct the original jailbreak prompt by following the disguised query instructions.
Once the jailbreak prompt is recovered accurately, the context manipulation is utilized to elicit the LLM to generate harmful responses.
Similar to <math alttext="\mathsf{DAR}" class="ltx_Math" display="inline" id="S3.I2.i1.p2.6.m6.1"><semantics id="S3.I2.i1.p2.6.m6.1a"><mi id="S3.I2.i1.p2.6.m6.1.1" xref="S3.I2.i1.p2.6.m6.1.1.cmml">𝖣𝖠𝖱</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p2.6.m6.1b"><ci id="S3.I2.i1.p2.6.m6.1.1.cmml" xref="S3.I2.i1.p2.6.m6.1.1">𝖣𝖠𝖱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p2.6.m6.1c">\mathsf{DAR}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p2.6.m6.1d">sansserif_DAR</annotation></semantics></math>, Li et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib51" title="">51</a>]</cite> also propose a decomposition and reconstruction attack framework named <math alttext="\mathsf{DrAttack}" class="ltx_Math" display="inline" id="S3.I2.i1.p2.7.m7.1"><semantics id="S3.I2.i1.p2.7.m7.1a"><mi id="S3.I2.i1.p2.7.m7.1.1" xref="S3.I2.i1.p2.7.m7.1.1.cmml">𝖣𝗋𝖠𝗍𝗍𝖺𝖼𝗄</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p2.7.m7.1b"><ci id="S3.I2.i1.p2.7.m7.1.1.cmml" xref="S3.I2.i1.p2.7.m7.1.1">𝖣𝗋𝖠𝗍𝗍𝖺𝖼𝗄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p2.7.m7.1c">\mathsf{DrAttack}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p2.7.m7.1d">sansserif_DrAttack</annotation></semantics></math>.
This attack method segments the jailbreak prompt into sub-prompts following semantic rules, and conceals them in benign contextual tasks, which can elicit the target LLM to follow the instructions and examples to recover the concealed harmful prompt and generate the corresponding responses.
Besides, Chang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib13" title="">13</a>]</cite> develop <math alttext="\mathsf{Puzzler}" class="ltx_Math" display="inline" id="S3.I2.i1.p2.8.m8.1"><semantics id="S3.I2.i1.p2.8.m8.1a"><mi id="S3.I2.i1.p2.8.m8.1.1" xref="S3.I2.i1.p2.8.m8.1.1.cmml">𝖯𝗎𝗓𝗓𝗅𝖾𝗋</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p2.8.m8.1b"><ci id="S3.I2.i1.p2.8.m8.1.1.cmml" xref="S3.I2.i1.p2.8.m8.1.1">𝖯𝗎𝗓𝗓𝗅𝖾𝗋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p2.8.m8.1c">\mathsf{Puzzler}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p2.8.m8.1d">sansserif_Puzzler</annotation></semantics></math>, which provides clues about the jailbreak objective by first querying LLMs about their defensive strategies, and then acquiring the offensive methods from LLMs.
After that, <math alttext="\mathsf{Puzzler}" class="ltx_Math" display="inline" id="S3.I2.i1.p2.9.m9.1"><semantics id="S3.I2.i1.p2.9.m9.1a"><mi id="S3.I2.i1.p2.9.m9.1.1" xref="S3.I2.i1.p2.9.m9.1.1.cmml">𝖯𝗎𝗓𝗓𝗅𝖾𝗋</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p2.9.m9.1b"><ci id="S3.I2.i1.p2.9.m9.1.1.cmml" xref="S3.I2.i1.p2.9.m9.1.1">𝖯𝗎𝗓𝗓𝗅𝖾𝗋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p2.9.m9.1c">\mathsf{Puzzler}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p2.9.m9.1d">sansserif_Puzzler</annotation></semantics></math> encourages LLMs to infer the true intent concealed within the fragmented information and generate malicious responses.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">此外，将有害内容分解成看似无害的问题，并随后指示目标模型重新组装并响应原始的有害查询，代表了一种新颖的加密技术。在该研究方向上，刘等人[ 55]提出了一种名为 <math id="S3.I2.i1.p2.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{DAR}"><semantics id="S3.I2.i1.p2.1.m1.1a"><mi id="S3.I2.i1.p2.1.m1.1.1">𝖣𝖠𝖱</mi><annotation-xml id="S3.I2.i1.p2.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p2.1.m1.1c" encoding="application/x-tex">\mathsf{DAR}</annotation><annotation id="S3.I2.i1.p2.1.m1.1d" encoding="application/x-llamapun">sansserif_DAR</annotation></semantics></math> （ <math id="S3.I2.i1.p2.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{Disguise}"><semantics id="S3.I2.i1.p2.2.m2.1a"><mi id="S3.I2.i1.p2.2.m2.1.1">𝖣𝗂𝗌𝗀𝗎𝗂𝗌𝖾</mi><annotation-xml id="S3.I2.i1.p2.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p2.2.m2.1c" encoding="application/x-tex">\mathsf{Disguise}</annotation><annotation id="S3.I2.i1.p2.2.m2.1d" encoding="application/x-llamapun">sansserif_Disguise</annotation></semantics></math> <math id="S3.I2.i1.p2.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{and}"><semantics id="S3.I2.i1.p2.3.m3.1a"><mi id="S3.I2.i1.p2.3.m3.1.1">𝖺𝗇𝖽</mi><annotation-xml id="S3.I2.i1.p2.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p2.3.m3.1c" encoding="application/x-tex">\mathsf{and}</annotation><annotation id="S3.I2.i1.p2.3.m3.1d" encoding="application/x-llamapun">sansserif_and</annotation></semantics></math> <math id="S3.I2.i1.p2.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{Reconstruction}"><semantics id="S3.I2.i1.p2.4.m4.1a"><mi id="S3.I2.i1.p2.4.m4.1.1">𝖱𝖾𝖼𝗈𝗇𝗌𝗍𝗋𝗎𝖼𝗍𝗂𝗈𝗇</mi><annotation-xml id="S3.I2.i1.p2.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p2.4.m4.1c" encoding="application/x-tex">\mathsf{Reconstruction}</annotation><annotation id="S3.I2.i1.p2.4.m4.1d" encoding="application/x-llamapun">sansserif_Reconstruction</annotation></semantics></math> ）的新型攻击方法。 <math id="S3.I2.i1.p2.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{DAR}"><semantics id="S3.I2.i1.p2.5.m5.1a"><mi id="S3.I2.i1.p2.5.m5.1.1">𝖣𝖠𝖱</mi><annotation-xml id="S3.I2.i1.p2.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p2.5.m5.1c" encoding="application/x-tex">\mathsf{DAR}</annotation><annotation id="S3.I2.i1.p2.5.m5.1d" encoding="application/x-llamapun">sansserif_DAR</annotation></semantics></math> 涉及将有害提示分解为单个字符，并将其插入到字谜查询中。然后引导目标 LLM 通过遵循伪装的查询指令来重建原始的越狱提示。一旦准确恢复越狱提示，就利用上下文操作来诱使 LLM 生成有害响应。类似于 <math id="S3.I2.i1.p2.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{DAR}"><semantics id="S3.I2.i1.p2.6.m6.1a"><mi id="S3.I2.i1.p2.6.m6.1.1">𝖣𝖠𝖱</mi><annotation-xml id="S3.I2.i1.p2.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p2.6.m6.1c" encoding="application/x-tex">\mathsf{DAR}</annotation><annotation id="S3.I2.i1.p2.6.m6.1d" encoding="application/x-llamapun">sansserif_DAR</annotation></semantics></math> ，李等人[ 51]也提出了一种名为 <math id="S3.I2.i1.p2.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{DrAttack}"><semantics id="S3.I2.i1.p2.7.m7.1a"><mi id="S3.I2.i1.p2.7.m7.1.1">𝖣𝗋𝖠𝗍𝗍𝖺𝖼𝗄</mi><annotation-xml id="S3.I2.i1.p2.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p2.7.m7.1c" encoding="application/x-tex">\mathsf{DrAttack}</annotation><annotation id="S3.I2.i1.p2.7.m7.1d" encoding="application/x-llamapun">sansserif_DrAttack</annotation></semantics></math> 的分解和重建攻击框架。这种攻击方法根据语义规则将越狱提示分割成子提示，并将它们隐藏在良性上下文任务中，这可以诱使目标 LLM 遵循指令和示例来恢复隐藏的有害提示并生成相应的响应。 此外，Chang 等人[13]开发了 <math id="S3.I2.i1.p2.8.m8.1" display="inline" class="ltx_Math" alttext="\mathsf{Puzzler}"><semantics id="S3.I2.i1.p2.8.m8.1a"><mi id="S3.I2.i1.p2.8.m8.1.1">𝖯𝗎𝗓𝗓𝗅𝖾𝗋</mi><annotation-xml id="S3.I2.i1.p2.8.m8.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p2.8.m8.1c" encoding="application/x-tex">\mathsf{Puzzler}</annotation><annotation id="S3.I2.i1.p2.8.m8.1d" encoding="application/x-llamapun">sansserif_Puzzler</annotation></semantics></math> ，通过首先查询 LLMs 的防御策略，然后从 LLMs 获取攻击方法，从而提供有关越狱目标的线索。之后， <math id="S3.I2.i1.p2.9.m9.1" display="inline" class="ltx_Math" alttext="\mathsf{Puzzler}"><semantics id="S3.I2.i1.p2.9.m9.1a"><mi id="S3.I2.i1.p2.9.m9.1.1">𝖯𝗎𝗓𝗓𝗅𝖾𝗋</mi><annotation-xml id="S3.I2.i1.p2.9.m9.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i1.p2.9.m9.1c" encoding="application/x-tex">\mathsf{Puzzler}</annotation><annotation id="S3.I2.i1.p2.9.m9.1d" encoding="application/x-llamapun">sansserif_Puzzler</annotation></semantics></math> 鼓励 LLMs 推断碎片化信息中隐藏的真实意图，并生成恶意响应。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.4"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.4.1">Low-resource Languages:</span>
Given that safety mechanisms for LLMs primarily rely on English text datasets, prompts in low-resource, non-English languages may also effectively evade these safeguards.
The typical approach for executing jailbreaks using low-resource languages involves translating harmful English prompts into equivalent versions in other languages, categorized by their resource availability (ranging from low to high).
Given these intuitions, Deng et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib21" title="">21</a>]</cite> propose multilingual jailbreak attacks, where they exploit Google Translate<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://translate.google.com/" title="">https://translate.google.com</a>.</span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 低资源语言：鉴于 LLMs 的安全机制主要依赖于英文文本数据集，低资源非英文语言的提示也可能有效规避这些安全措施。使用低资源语言执行越狱的典型方法是将有害的英文提示翻译成其他语言中的等效版本，并根据资源可用性（从低到高）进行分类。基于这些直觉，Deng 等人[21]提出了多语言越狱攻击，他们利用 Google Translate <sup class="ltx_note_mark">1</sup> </font></font></font> to convert harmful English prompts into thirty other languages to jailbreak ChatGPT and GPT-4.
In the intentional scenario, the combination of multilingual prompts with malicious instructions leads to dramatically high success rates for generating unsafe outputs, reaching <math alttext="80.92\%" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.m1.1"><semantics id="S3.I2.i2.p1.1.m1.1a"><mrow id="S3.I2.i2.p1.1.m1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.cmml"><mn id="S3.I2.i2.p1.1.m1.1.1.2" xref="S3.I2.i2.p1.1.m1.1.1.2.cmml">80.92</mn><mo id="S3.I2.i2.p1.1.m1.1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.m1.1b"><apply id="S3.I2.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.I2.i2.p1.1.m1.1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1.1">percent</csymbol><cn id="S3.I2.i2.p1.1.m1.1.1.2.cmml" type="float" xref="S3.I2.i2.p1.1.m1.1.1.2">80.92</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.m1.1c">80.92\%</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.m1.1d">80.92 %</annotation></semantics></math> on ChatGPT and <math alttext="40.71\%" class="ltx_Math" display="inline" id="S3.I2.i2.p1.2.m2.1"><semantics id="S3.I2.i2.p1.2.m2.1a"><mrow id="S3.I2.i2.p1.2.m2.1.1" xref="S3.I2.i2.p1.2.m2.1.1.cmml"><mn id="S3.I2.i2.p1.2.m2.1.1.2" xref="S3.I2.i2.p1.2.m2.1.1.2.cmml">40.71</mn><mo id="S3.I2.i2.p1.2.m2.1.1.1" xref="S3.I2.i2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.2.m2.1b"><apply id="S3.I2.i2.p1.2.m2.1.1.cmml" xref="S3.I2.i2.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.I2.i2.p1.2.m2.1.1.1.cmml" xref="S3.I2.i2.p1.2.m2.1.1.1">percent</csymbol><cn id="S3.I2.i2.p1.2.m2.1.1.2.cmml" type="float" xref="S3.I2.i2.p1.2.m2.1.1.2">40.71</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.2.m2.1c">40.71\%</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.2.m2.1d">40.71 %</annotation></semantics></math> on GPT-4.
Yong et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib106" title="">106</a>]</cite> conduct experiments using twelve non-English prompts to assess the robustness of GPT-4’s safety mechanisms.
They reveal that translating English inputs into low-resource languages significantly increases the likelihood of bypassing GPT-4’s safety filters, with the bypass rate escalating from less than <math alttext="1\%" class="ltx_Math" display="inline" id="S3.I2.i2.p1.3.m3.1"><semantics id="S3.I2.i2.p1.3.m3.1a"><mrow id="S3.I2.i2.p1.3.m3.1.1" xref="S3.I2.i2.p1.3.m3.1.1.cmml"><mn id="S3.I2.i2.p1.3.m3.1.1.2" xref="S3.I2.i2.p1.3.m3.1.1.2.cmml">1</mn><mo id="S3.I2.i2.p1.3.m3.1.1.1" xref="S3.I2.i2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.3.m3.1b"><apply id="S3.I2.i2.p1.3.m3.1.1.cmml" xref="S3.I2.i2.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.I2.i2.p1.3.m3.1.1.1.cmml" xref="S3.I2.i2.p1.3.m3.1.1.1">percent</csymbol><cn id="S3.I2.i2.p1.3.m3.1.1.2.cmml" type="integer" xref="S3.I2.i2.p1.3.m3.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.3.m3.1c">1\%</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.3.m3.1d">1 %</annotation></semantics></math> to <math alttext="79\%" class="ltx_Math" display="inline" id="S3.I2.i2.p1.4.m4.1"><semantics id="S3.I2.i2.p1.4.m4.1a"><mrow id="S3.I2.i2.p1.4.m4.1.1" xref="S3.I2.i2.p1.4.m4.1.1.cmml"><mn id="S3.I2.i2.p1.4.m4.1.1.2" xref="S3.I2.i2.p1.4.m4.1.1.2.cmml">79</mn><mo id="S3.I2.i2.p1.4.m4.1.1.1" xref="S3.I2.i2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.4.m4.1b"><apply id="S3.I2.i2.p1.4.m4.1.1.cmml" xref="S3.I2.i2.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.I2.i2.p1.4.m4.1.1.1.cmml" xref="S3.I2.i2.p1.4.m4.1.1.1">percent</csymbol><cn id="S3.I2.i2.p1.4.m4.1.1.2.cmml" type="integer" xref="S3.I2.i2.p1.4.m4.1.1.2">79</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.4.m4.1c">79\%</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.4.m4.1d">79 %</annotation></semantics></math>.
In response to the notable lack of comprehensive empirical research on this specific threat, Li et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib49" title="">49</a>]</cite> conduct extensive empirical studies to explore multilingual jailbreak attacks.
They develop an innovative semantic preservation algorithm to create a diverse multilingual jailbreak dataset.
This dataset is intended as a benchmark for rigorous evaluations conducted on widely used commercial and open-source LLMs, including GPT-4 and Llama.
The experimental results in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib49" title="">49</a>]</cite> further reveal that multilingual jailbreaks pose significant threats to LLMs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"> 将有害的英语提示转换为三十种其他语言以越狱 ChatGPT 和 GPT-4。在有意场景中，多语言提示与恶意指令的结合导致生成不安全输出的成功率显著提高，达到 ChatGPT 的 <math id="S3.I2.i2.p1.1.m1.1" display="inline" class="ltx_Math" alttext="80.92\%"><semantics id="S3.I2.i2.p1.1.m1.1a"><mrow id="S3.I2.i2.p1.1.m1.1.1"><mn id="S3.I2.i2.p1.1.m1.1.1.2">80.92</mn><mo id="S3.I2.i2.p1.1.m1.1.1.1">%</mo></mrow><annotation-xml id="S3.I2.i2.p1.1.m1.1b" encoding="MathML-Content">percent80.92</annotation-xml><annotation id="S3.I2.i2.p1.1.m1.1c" encoding="application/x-tex">80.92\%</annotation><annotation id="S3.I2.i2.p1.1.m1.1d" encoding="application/x-llamapun">80.92 %</annotation></semantics></math> 和 GPT-4 的 <math id="S3.I2.i2.p1.2.m2.1" display="inline" class="ltx_Math" alttext="40.71\%"><semantics id="S3.I2.i2.p1.2.m2.1a"><mrow id="S3.I2.i2.p1.2.m2.1.1"><mn id="S3.I2.i2.p1.2.m2.1.1.2">40.71</mn><mo id="S3.I2.i2.p1.2.m2.1.1.1">%</mo></mrow><annotation-xml id="S3.I2.i2.p1.2.m2.1b" encoding="MathML-Content">percent40.71</annotation-xml><annotation id="S3.I2.i2.p1.2.m2.1c" encoding="application/x-tex">40.71\%</annotation><annotation id="S3.I2.i2.p1.2.m2.1d" encoding="application/x-llamapun">40.71 %</annotation></semantics></math> 。Yong 等人[ 106]使用十二种非英语提示进行实验，以评估 GPT-4 安全机制的鲁棒性。他们揭示将英语输入翻译成低资源语言会显著增加绕过 GPT-4 安全过滤器的可能性，绕过率从不到 <math id="S3.I2.i2.p1.3.m3.1" display="inline" class="ltx_Math" alttext="1\%"><semantics id="S3.I2.i2.p1.3.m3.1a"><mrow id="S3.I2.i2.p1.3.m3.1.1"><mn id="S3.I2.i2.p1.3.m3.1.1.2">1</mn><mo id="S3.I2.i2.p1.3.m3.1.1.1">%</mo></mrow><annotation-xml id="S3.I2.i2.p1.3.m3.1b" encoding="MathML-Content">percent1</annotation-xml><annotation id="S3.I2.i2.p1.3.m3.1c" encoding="application/x-tex">1\%</annotation><annotation id="S3.I2.i2.p1.3.m3.1d" encoding="application/x-llamapun">1 %</annotation></semantics></math> 上升到 <math id="S3.I2.i2.p1.4.m4.1" display="inline" class="ltx_Math" alttext="79\%"><semantics id="S3.I2.i2.p1.4.m4.1a"><mrow id="S3.I2.i2.p1.4.m4.1.1"><mn id="S3.I2.i2.p1.4.m4.1.1.2">79</mn><mo id="S3.I2.i2.p1.4.m4.1.1.1">%</mo></mrow><annotation-xml id="S3.I2.i2.p1.4.m4.1b" encoding="MathML-Content">percent79</annotation-xml><annotation id="S3.I2.i2.p1.4.m4.1c" encoding="application/x-tex">79\%</annotation><annotation id="S3.I2.i2.p1.4.m4.1d" encoding="application/x-llamapun">79 %</annotation></semantics></math> 。针对这一特定威胁的全面实证研究明显不足，Li 等人[ 49]进行广泛的实证研究，探索多语言越狱攻击。他们开发了一种创新的意义保留算法来创建多样化的多语言越狱数据集。该数据集旨在作为对广泛使用的商业和开源 LLMs（包括 GPT-4 和 Llama）进行严格评估的基准。[ 49]中的实验结果进一步揭示，多语言越狱对 LLMs 构成重大威胁。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.5"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.5.1">Genetic Algorithm-based Attacks:</span>
Genetic-based methods typically exploit mutation and selection processes to dynamically explore and identify effective prompts.
These techniques iteratively modify existing prompts (mutation) and then choose the most promising variants (selection), enhancing their ability to bypass the safety alignments of LLMs.
Liu et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib56" title="">56</a>]</cite> develop <math alttext="\mathsf{AutoDAN}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.1.m1.1"><semantics id="S3.I2.i3.p1.1.m1.1a"><mi id="S3.I2.i3.p1.1.m1.1.1" xref="S3.I2.i3.p1.1.m1.1.1.cmml">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.1.m1.1b"><ci id="S3.I2.i3.p1.1.m1.1.1.cmml" xref="S3.I2.i3.p1.1.m1.1.1">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.1.m1.1c">\mathsf{AutoDAN}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.1.m1.1d">sansserif_AutoDAN</annotation></semantics></math>-<math alttext="\mathsf{HGA}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.2.m2.1"><semantics id="S3.I2.i3.p1.2.m2.1a"><mi id="S3.I2.i3.p1.2.m2.1.1" xref="S3.I2.i3.p1.2.m2.1.1.cmml">𝖧𝖦𝖠</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.2.m2.1b"><ci id="S3.I2.i3.p1.2.m2.1.1.cmml" xref="S3.I2.i3.p1.2.m2.1.1">𝖧𝖦𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.2.m2.1c">\mathsf{HGA}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.2.m2.1d">sansserif_HGA</annotation></semantics></math>, a hierarchical Genetic Algorithm (GA) tailored for the automatic generation of stealthy jailbreak prompts against aligned LLMs.
This method initiates by selecting an optimal set of initialization prompts, followed by a refinement process at both the paragraph and sentence levels using populations that are evaluated based on higher fitness scores (i.e., lower negative log-likelihood of the generated response).
This approach not only automates the prompt crafting process but also effectively bypasses common perplexity-based defense mechanisms, enhancing both the stealthiness and efficacy of the attacks.
Lapid et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib46" title="">46</a>]</cite> introduce a novel universal black-box attack strategy utilizing a GA designed to disrupt the alignment of LLMs.
This approach employs crossover and mutation techniques to iteratively update and optimize candidate jailbreak prompts.
By systematically adjusting these prompts, the GA manipulates the model’s output to deviate from its intended safe and aligned responses, thereby revealing the model’s vulnerabilities to adversarial inputs.
Yu et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib107" title="">107</a>]</cite> develop <math alttext="\mathsf{GPTFUZZER}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.3.m3.1"><semantics id="S3.I2.i3.p1.3.m3.1a"><mi id="S3.I2.i3.p1.3.m3.1.1" xref="S3.I2.i3.p1.3.m3.1.1.cmml">𝖦𝖯𝖳𝖥𝖴𝖹𝖹𝖤𝖱</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.3.m3.1b"><ci id="S3.I2.i3.p1.3.m3.1.1.cmml" xref="S3.I2.i3.p1.3.m3.1.1">𝖦𝖯𝖳𝖥𝖴𝖹𝖹𝖤𝖱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.3.m3.1c">\mathsf{GPTFUZZER}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.3.m3.1d">sansserif_GPTFUZZER</annotation></semantics></math>, an automated framework designed to generate jailbreak prompts for testing LLMs.
The framework integrates a seed selection strategy to optimize initial templates, mutation operators to ensure semantic consistency, and a judgment model to evaluate attack effectiveness.
<math alttext="\mathsf{GPTFUZZER}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.4.m4.1"><semantics id="S3.I2.i3.p1.4.m4.1a"><mi id="S3.I2.i3.p1.4.m4.1.1" xref="S3.I2.i3.p1.4.m4.1.1.cmml">𝖦𝖯𝖳𝖥𝖴𝖹𝖹𝖤𝖱</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.4.m4.1b"><ci id="S3.I2.i3.p1.4.m4.1.1.cmml" xref="S3.I2.i3.p1.4.m4.1.1">𝖦𝖯𝖳𝖥𝖴𝖹𝖹𝖤𝖱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.4.m4.1c">\mathsf{GPTFUZZER}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.4.m4.1d">sansserif_GPTFUZZER</annotation></semantics></math> has proven highly effective in bypassing model defenses, demonstrating significant success across various LLMs under multiple attack scenarios.
Li et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib50" title="">50</a>]</cite> propose a genetic algorithm to generate new jailbreak prompts that are semantically similar to the original prompt.
They initialize the population by substituting the words in original prompt randomly, and calculate the fitness based on the similarity and performance of each prompt.
In the crossover step, the qualified prompts are transformed into other syntactic forms to generate offspring.
If the new population retains a similarity with the previous generation for several rounds, the algorithm will terminate.
In&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib88" title="">88</a>]</cite>, Takemoto points out that the target LLMs can rewrite harmful prompts into benign expressions by themselves.
The intuition is that since LLMs determine safeguard activation based on the content of the input prompts, it is thus reasonable that texts evading safeguards can be efficiently generated from the LLM.
To achieve this purpose, an attacker can feed the following prompt&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib88" title="">88</a>]</cite> to transform the harmful queries:
 <svg class="ltx_picture" height="58.5" id="S3.I2.i3.p1.5.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,58.5) matrix(1 0 0 -1 0 0)"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 13.39 L 0 45.11 C 0 52.51 5.99 58.5 13.39 58.5 L 586.61 58.5 C 594.01 58.5 600 52.51 600 45.11 L 600 13.39 C 600 5.99 594.01 0 586.61 0 L 13.39 0 C 5.99 0 0 5.99 0 13.39 Z" style="stroke:none"></path></g><g fill="#E6E6FF" fill-opacity="1.0"><path d="M 1.57 13.39 L 1.57 45.11 C 1.57 51.64 6.86 56.93 13.39 56.93 L 586.61 56.93 C 593.14 56.93 598.43 51.64 598.43 45.11 L 598.43 13.39 C 598.43 6.86 593.14 1.57 586.61 1.57 L 13.39 1.57 C 6.86 1.57 1.57 6.86 1.57 13.39 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.39 6.5)"><foreignobject color="#000000" height="45.51" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="573.23">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.I2.i3.p1.5.pic1.1.1.1.1.1" style="width:414.3pt;">
<span class="ltx_p" id="S3.I2.i3.p1.5.pic1.1.1.1.1.1.1">Please rewrite the provided text in such a way that it retains its original meaning but is rephrased in a manner that would not cause discomfort to readers. Feel free to add clarification sentences if necessary.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 基于遗传算法的攻击：基于遗传的方法通常利用变异和选择过程来动态探索和识别有效的提示。这些技术迭代地修改现有提示（变异），然后选择最有潜力的变体（选择），增强了它们绕过 LLMs 安全对齐的能力。Liu 等人[ 56]开发了 <math id="S3.I2.i3.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{AutoDAN}"><semantics id="S3.I2.i3.p1.1.m1.1a"><mi id="S3.I2.i3.p1.1.m1.1.1">𝖠𝗎𝗍𝗈𝖣𝖠𝖭</mi><annotation-xml id="S3.I2.i3.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i3.p1.1.m1.1c" encoding="application/x-tex">\mathsf{AutoDAN}</annotation><annotation id="S3.I2.i3.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_AutoDAN</annotation></semantics></math> - <math id="S3.I2.i3.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{HGA}"><semantics id="S3.I2.i3.p1.2.m2.1a"><mi id="S3.I2.i3.p1.2.m2.1.1">𝖧𝖦𝖠</mi><annotation-xml id="S3.I2.i3.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i3.p1.2.m2.1c" encoding="application/x-tex">\mathsf{HGA}</annotation><annotation id="S3.I2.i3.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_HGA</annotation></semantics></math> ，一种针对自动生成针对对齐 LLMs 的隐蔽越狱提示而设计的分层遗传算法（GA）。该方法首先选择一组最优的初始化提示，然后使用基于较高适应度分数（即生成响应的负对数似然较低）评估的种群，在段落和句子级别上进行细化。这种方法不仅自动化了提示制作过程，而且有效地绕过了常见的基于困惑度的防御机制，增强了攻击的隐蔽性和有效性。Lapid 等人[ 46]介绍了一种利用遗传算法设计的全新通用黑盒攻击策略，旨在破坏 LLMs 的对齐。 这种方法采用交叉和变异技术来迭代更新和优化候选越狱提示。通过系统地调整这些提示，遗传算法操纵模型的输出使其偏离其预期的安全和对齐的响应，从而揭示模型在对抗性输入下的漏洞。Yu 等人[107]开发了 <math id="S3.I2.i3.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{GPTFUZZER}"><semantics id="S3.I2.i3.p1.3.m3.1a"><mi id="S3.I2.i3.p1.3.m3.1.1">𝖦𝖯𝖳𝖥𝖴𝖹𝖹𝖤𝖱</mi><annotation-xml id="S3.I2.i3.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i3.p1.3.m3.1c" encoding="application/x-tex">\mathsf{GPTFUZZER}</annotation><annotation id="S3.I2.i3.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_GPTFUZZER</annotation></semantics></math> ，这是一个自动化的框架，旨在生成越狱提示以测试 LLMs。该框架集成了种子选择策略来优化初始模板，变异算子来确保语义一致性，以及一个判断模型来评估攻击效果。 <math id="S3.I2.i3.p1.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{GPTFUZZER}"><semantics id="S3.I2.i3.p1.4.m4.1a"><mi id="S3.I2.i3.p1.4.m4.1.1">𝖦𝖯𝖳𝖥𝖴𝖹𝖹𝖤𝖱</mi><annotation-xml id="S3.I2.i3.p1.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.I2.i3.p1.4.m4.1c" encoding="application/x-tex">\mathsf{GPTFUZZER}</annotation><annotation id="S3.I2.i3.p1.4.m4.1d" encoding="application/x-llamapun">sansserif_GPTFUZZER</annotation></semantics></math> 已被证明在绕过模型防御方面非常有效，在多种攻击场景下对各种 LLMs 都表现出显著的成功。Li 等人[50]提出了一种遗传算法来生成与原始提示语义相似的新越狱提示。他们通过随机替换原始提示中的词语来初始化种群，并根据每个提示的相似性和性能计算适应度。在交叉步骤中，合格的提示被转化为其他句法形式以生成后代。 如果新种群在几轮内仍与上一代保持相似性，算法将终止。在[ 88]中，高桥指出目标 LLMs 可以自行将有害提示重写为无害表达。其原理是，由于 LLMs 根据输入提示的内容决定安全防护的激活，因此从 LLMs 中高效生成规避安全防护的文本是合理的。 为实现此目的，攻击者可以向[ 88]中描述的以下提示中输入，以转换有害查询：</font></font></font>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p3">
<svg class="ltx_picture" height="178.23" id="S3.SS2.SSS2.p3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,178.23) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 172.33 C 0 175.59 2.64 178.23 5.91 178.23 L 594.09 178.23 C 597.36 178.23 600 175.59 600 172.33 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 154.12 L 598.03 154.12 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 160.03)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS2.SSS2.p3.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS2.SSS2.p3.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p3.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;3.5</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS2.SSS2.p3.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS2.SSS2.p3.pic1.2.2.2.1.1.1">Although many LLMs are safety-aligned and equipped with input detection strategies, they still face the challenges posed by data’s long-tailed distributions.
Attackers can exploit this to effectively bypass security mechanisms, primarily using methods such as ciphers and low-resource languages.
Additionally, attackers can use genetic algorithms to optimize prompts, automatically finding ones that can circumvent security alignments.
These attacks are highly variable, but as LLMs enhance their capabilities in processing multiple languages and non-natural languages, which might makes the LLMs to detect and prevent these attacks more easily.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> LLM-based Generation<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于 LLM 的生成</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">With a robust set of adversarial examples and high-quality feedback mechanisms, LLMs can be fine-tuned to simulate attackers, thereby enabling the efficient and automatic generation of adversarial prompts.
Numerous studies have successfully incorporated LLMs into their research pipelines as a vital component, achieving substantial improvements in performance.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">凭借一套强大的对抗样本和高质量的反馈机制，LLMs 可以被微调以模拟攻击者，从而实现对抗提示的高效和自动生成。大量研究已成功将 LLMs 纳入其研究流程中作为关键组成部分，显著提升了性能。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.6">Some researchers adopt the approach of training a single LLM as the attacker with fine-tuning techniques or RLHF.
For instance, Deng et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib19" title="">19</a>]</cite> develop an LLM-based jailbreaking framework named <math alttext="\mathsf{MASTERKEY}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.1.m1.1"><semantics id="S3.SS2.SSS3.p2.1.m1.1a"><mi id="S3.SS2.SSS3.p2.1.m1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.cmml">𝖬𝖠𝖲𝖳𝖤𝖱𝖪𝖤𝖸</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.1.m1.1b"><ci id="S3.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1">𝖬𝖠𝖲𝖳𝖤𝖱𝖪𝖤𝖸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.1.m1.1c">\mathsf{MASTERKEY}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.1.m1.1d">sansserif_MASTERKEY</annotation></semantics></math> to automatically generate adversarial prompts designed to bypass security mechanisms.
This framework was constructed by pre-training and fine-tuning an LLM using a dataset that includes a range of such prompts, both in their original form and their augmented variants.
Inspired by time-based SQL injection, <math alttext="\mathsf{MASTERKEY}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.2.m2.1"><semantics id="S3.SS2.SSS3.p2.2.m2.1a"><mi id="S3.SS2.SSS3.p2.2.m2.1.1" xref="S3.SS2.SSS3.p2.2.m2.1.1.cmml">𝖬𝖠𝖲𝖳𝖤𝖱𝖪𝖤𝖸</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.2.m2.1b"><ci id="S3.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1">𝖬𝖠𝖲𝖳𝖤𝖱𝖪𝖤𝖸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.2.m2.1c">\mathsf{MASTERKEY}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.2.m2.1d">sansserif_MASTERKEY</annotation></semantics></math> leverages insights into internal defense strategies of LLMs, specifically targeting real-time semantic analysis and keyword detection defenses utilized by platforms like Bing Chat and Bard.
Zeng et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib109" title="">109</a>]</cite> discover a novel perspective to jailbreak LLMs by acting like human communicators.
Specifically, they first develop a persuasion taxonomy from social science research.
Then, the taxonomy will be applied to generate interpretable <math alttext="\mathsf{Persuasive}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.3.m3.1"><semantics id="S3.SS2.SSS3.p2.3.m3.1a"><mi id="S3.SS2.SSS3.p2.3.m3.1.1" xref="S3.SS2.SSS3.p2.3.m3.1.1.cmml">𝖯𝖾𝗋𝗌𝗎𝖺𝗌𝗂𝗏𝖾</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.3.m3.1b"><ci id="S3.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1">𝖯𝖾𝗋𝗌𝗎𝖺𝗌𝗂𝗏𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.3.m3.1c">\mathsf{Persuasive}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.3.m3.1d">sansserif_Persuasive</annotation></semantics></math> <math alttext="\mathsf{Adversarial}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.4.m4.1"><semantics id="S3.SS2.SSS3.p2.4.m4.1a"><mi id="S3.SS2.SSS3.p2.4.m4.1.1" xref="S3.SS2.SSS3.p2.4.m4.1.1.cmml">𝖠𝖽𝗏𝖾𝗋𝗌𝖺𝗋𝗂𝖺𝗅</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.4.m4.1b"><ci id="S3.SS2.SSS3.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1">𝖠𝖽𝗏𝖾𝗋𝗌𝖺𝗋𝗂𝖺𝗅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.4.m4.1c">\mathsf{Adversarial}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.4.m4.1d">sansserif_Adversarial</annotation></semantics></math> <math alttext="\mathsf{Prompts}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.5.m5.1"><semantics id="S3.SS2.SSS3.p2.5.m5.1a"><mi id="S3.SS2.SSS3.p2.5.m5.1.1" xref="S3.SS2.SSS3.p2.5.m5.1.1.cmml">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.5.m5.1b"><ci id="S3.SS2.SSS3.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.5.m5.1c">\mathsf{Prompts}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.5.m5.1d">sansserif_Prompts</annotation></semantics></math> (<math alttext="\mathsf{PAPs}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.6.m6.1"><semantics id="S3.SS2.SSS3.p2.6.m6.1a"><mi id="S3.SS2.SSS3.p2.6.m6.1.1" xref="S3.SS2.SSS3.p2.6.m6.1.1.cmml">𝖯𝖠𝖯𝗌</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.6.m6.1b"><ci id="S3.SS2.SSS3.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1">𝖯𝖠𝖯𝗌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.6.m6.1c">\mathsf{PAPs}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.6.m6.1d">sansserif_PAPs</annotation></semantics></math>) using various methods such as in-context prompting and fine-tuned paraphraser.
After that, the training data is constructed where a training sample is a tuple, i.e., <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p2.6.1">&lt;a plain harmful query, a technique in the taxonomy, a corresponding persuasive adversarial prompt&gt;</span>.
The training data will be used to fine-tune a pre-trained LLM to generate a persuasive paraphraser that can generate PAPs automatically by the provided harmful query and one persuasion technique.
Shah et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib76" title="">76</a>]</cite> utilize an LLM assistant to generate persona-modulation attack prompts automatically.
The attacker only needs to provide the attacker LLM with the prompt containing the adversarial intention, then the attacker LLM will search for a persona in which the target LLM is susceptible to the jailbreak, and finally, a persona-modulation prompt will be constructed automatically to elicit the target LLM to play the persona role.
Casper et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib12" title="">12</a>]</cite> propose a red-teaming method without a pre-existing classifier.
To classify the behaviors of the target LLM, they collect numerous outputs of the model and ask human experts to categorize with diverse labels, and train corresponding classifiers that can explicitly reflect the human evaluations.
Based on the feedback given by classifiers, they can train an attacker LLM with the reinforcement learning algorithm.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">一些研究人员采用训练单个 LLM 作为攻击者的方法，使用微调技术或 RLHF。例如，邓等人[19]开发了一个名为 <math id="S3.SS2.SSS3.p2.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{MASTERKEY}"><semantics id="S3.SS2.SSS3.p2.1.m1.1a"><mi id="S3.SS2.SSS3.p2.1.m1.1.1">𝖬𝖠𝖲𝖳𝖤𝖱𝖪𝖤𝖸</mi><annotation-xml id="S3.SS2.SSS3.p2.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p2.1.m1.1c" encoding="application/x-tex">\mathsf{MASTERKEY}</annotation><annotation id="S3.SS2.SSS3.p2.1.m1.1d" encoding="application/x-llamapun">sansserif_MASTERKEY</annotation></semantics></math> 的基于 LLM 的越狱框架，用于自动生成旨在绕过安全机制的对立提示。该框架通过使用包含各种此类提示（包括其原始形式和增强变体）的数据集来预训练和微调 LLM 而构建。受基于时间的 SQL 注入启发， <math id="S3.SS2.SSS3.p2.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{MASTERKEY}"><semantics id="S3.SS2.SSS3.p2.2.m2.1a"><mi id="S3.SS2.SSS3.p2.2.m2.1.1">𝖬𝖠𝖲𝖳𝖤𝖱𝖪𝖤𝖸</mi><annotation-xml id="S3.SS2.SSS3.p2.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p2.2.m2.1c" encoding="application/x-tex">\mathsf{MASTERKEY}</annotation><annotation id="S3.SS2.SSS3.p2.2.m2.1d" encoding="application/x-llamapun">sansserif_MASTERKEY</annotation></semantics></math> 利用对 LLM 内部防御策略的理解，特别是针对像 Bing Chat 和 Bard 这样的平台使用的实时语义分析和关键词检测防御。曾等人[109]通过扮演人类沟通者的方式，发现了一种越狱 LLM 的新视角。具体来说，他们首先从社会科学研究中开发了一个说服分类法。然后，该分类法将应用于使用上下文提示和微调的释义器等多种方法来生成可解释的 <math id="S3.SS2.SSS3.p2.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{Persuasive}"><semantics id="S3.SS2.SSS3.p2.3.m3.1a"><mi id="S3.SS2.SSS3.p2.3.m3.1.1">𝖯𝖾𝗋𝗌𝗎𝖺𝗌𝗂𝗏𝖾</mi><annotation-xml id="S3.SS2.SSS3.p2.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p2.3.m3.1c" encoding="application/x-tex">\mathsf{Persuasive}</annotation><annotation id="S3.SS2.SSS3.p2.3.m3.1d" encoding="application/x-llamapun">sansserif_Persuasive</annotation></semantics></math> <math id="S3.SS2.SSS3.p2.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{Adversarial}"><semantics id="S3.SS2.SSS3.p2.4.m4.1a"><mi id="S3.SS2.SSS3.p2.4.m4.1.1">𝖠𝖽𝗏𝖾𝗋𝗌𝖺𝗋𝗂𝖺𝗅</mi><annotation-xml id="S3.SS2.SSS3.p2.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p2.4.m4.1c" encoding="application/x-tex">\mathsf{Adversarial}</annotation><annotation id="S3.SS2.SSS3.p2.4.m4.1d" encoding="application/x-llamapun">sansserif_Adversarial</annotation></semantics></math> <math id="S3.SS2.SSS3.p2.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{Prompts}"><semantics id="S3.SS2.SSS3.p2.5.m5.1a"><mi id="S3.SS2.SSS3.p2.5.m5.1.1">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</mi><annotation-xml id="S3.SS2.SSS3.p2.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p2.5.m5.1c" encoding="application/x-tex">\mathsf{Prompts}</annotation><annotation id="S3.SS2.SSS3.p2.5.m5.1d" encoding="application/x-llamapun">sansserif_Prompts</annotation></semantics></math> （ <math id="S3.SS2.SSS3.p2.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{PAPs}"><semantics id="S3.SS2.SSS3.p2.6.m6.1a"><mi id="S3.SS2.SSS3.p2.6.m6.1.1">𝖯𝖠𝖯𝗌</mi><annotation-xml id="S3.SS2.SSS3.p2.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p2.6.m6.1c" encoding="application/x-tex">\mathsf{PAPs}</annotation><annotation id="S3.SS2.SSS3.p2.6.m6.1d" encoding="application/x-llamapun">sansserif_PAPs</annotation></semantics></math> ）。 之后，构建训练数据，其中训练样本是一个元组，即&lt;一个普通的恶意查询，分类体系中的一个技术，一个相应的说服性对抗提示&gt;。训练数据将用于微调预训练的 LLM，以生成一个能够根据提供的恶意查询和一个说服技术自动生成 PAPs 的说服性释义器。Shah 等人[76]利用一个 LLM 助手自动生成角色调整攻击提示。攻击者只需向攻击者 LLM 提供包含对抗意图的提示，然后攻击者 LLM 将搜索一个目标 LLM 容易受到越狱攻击的角色，最后自动构建一个角色调整提示来诱使目标 LLM 扮演该角色角色。Casper 等人[12]提出了一种无需预分类器的红队方法。为了分类目标 LLM 的行为，他们收集了模型的大量输出，并要求人类专家使用不同的标签进行分类，并训练相应的分类器，这些分类器可以明确反映人类评估。 根据分类器提供的反馈他们可以使用强化学习算法训练一个攻击者的 LLM。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.8">Another strategy is to have multiple LLMs collaborate to form a framework, in which every LLMs serve as a different agent and can be optimized systematically.
Chao et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib15" title="">15</a>]</cite> propose <math alttext="\mathsf{Prompt}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p3.1.m1.1"><semantics id="S3.SS2.SSS3.p3.1.m1.1a"><mi id="S3.SS2.SSS3.p3.1.m1.1.1" xref="S3.SS2.SSS3.p3.1.m1.1.1.cmml">𝖯𝗋𝗈𝗆𝗉𝗍</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.1.m1.1b"><ci id="S3.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1">𝖯𝗋𝗈𝗆𝗉𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.1.m1.1c">\mathsf{Prompt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.1.m1.1d">sansserif_Prompt</annotation></semantics></math> <math alttext="\mathsf{Automatic}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p3.2.m2.1"><semantics id="S3.SS2.SSS3.p3.2.m2.1a"><mi id="S3.SS2.SSS3.p3.2.m2.1.1" xref="S3.SS2.SSS3.p3.2.m2.1.1.cmml">𝖠𝗎𝗍𝗈𝗆𝖺𝗍𝗂𝖼</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.2.m2.1b"><ci id="S3.SS2.SSS3.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p3.2.m2.1.1">𝖠𝗎𝗍𝗈𝗆𝖺𝗍𝗂𝖼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.2.m2.1c">\mathsf{Automatic}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.2.m2.1d">sansserif_Automatic</annotation></semantics></math>
<math alttext="\mathsf{Iterative}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p3.3.m3.1"><semantics id="S3.SS2.SSS3.p3.3.m3.1a"><mi id="S3.SS2.SSS3.p3.3.m3.1.1" xref="S3.SS2.SSS3.p3.3.m3.1.1.cmml">𝖨𝗍𝖾𝗋𝖺𝗍𝗂𝗏𝖾</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.3.m3.1b"><ci id="S3.SS2.SSS3.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1">𝖨𝗍𝖾𝗋𝖺𝗍𝗂𝗏𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.3.m3.1c">\mathsf{Iterative}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.3.m3.1d">sansserif_Iterative</annotation></semantics></math> <math alttext="\mathsf{Refinement}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p3.4.m4.1"><semantics id="S3.SS2.SSS3.p3.4.m4.1a"><mi id="S3.SS2.SSS3.p3.4.m4.1.1" xref="S3.SS2.SSS3.p3.4.m4.1.1.cmml">𝖱𝖾𝖿𝗂𝗇𝖾𝗆𝖾𝗇𝗍</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.4.m4.1b"><ci id="S3.SS2.SSS3.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p3.4.m4.1.1">𝖱𝖾𝖿𝗂𝗇𝖾𝗆𝖾𝗇𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.4.m4.1c">\mathsf{Refinement}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.4.m4.1d">sansserif_Refinement</annotation></semantics></math> (<math alttext="\mathsf{PAIR}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p3.5.m5.1"><semantics id="S3.SS2.SSS3.p3.5.m5.1a"><mi id="S3.SS2.SSS3.p3.5.m5.1.1" xref="S3.SS2.SSS3.p3.5.m5.1.1.cmml">𝖯𝖠𝖨𝖱</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.5.m5.1b"><ci id="S3.SS2.SSS3.p3.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p3.5.m5.1.1">𝖯𝖠𝖨𝖱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.5.m5.1c">\mathsf{PAIR}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.5.m5.1d">sansserif_PAIR</annotation></semantics></math>) to generate jailbreak prompts with only black-box access to the target LLM.
Concretely, <math alttext="\mathsf{PAIR}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p3.6.m6.1"><semantics id="S3.SS2.SSS3.p3.6.m6.1a"><mi id="S3.SS2.SSS3.p3.6.m6.1.1" xref="S3.SS2.SSS3.p3.6.m6.1.1.cmml">𝖯𝖠𝖨𝖱</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.6.m6.1b"><ci id="S3.SS2.SSS3.p3.6.m6.1.1.cmml" xref="S3.SS2.SSS3.p3.6.m6.1.1">𝖯𝖠𝖨𝖱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.6.m6.1c">\mathsf{PAIR}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.6.m6.1d">sansserif_PAIR</annotation></semantics></math> uses an attacker LLM to iteratively update the jailbreak prompt against the target LLM by querying the target LLM and refining the prompt.
Jin et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib41" title="">41</a>]</cite> design a multi-agent system to generate jailbreak prompts automatically.
In the system, LLMs serve as different roles including generator, translator, evaluator, and optimizer.
For instance, the generator is responsible for crafting initial jailbreak prompts based on previous jailbreak examples, then the translator and evaluator examine the responses of the target LLM, and finally the optimizer analyzes the effectiveness of the jailbreak and gives feedback to the generator.
Ge et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib27" title="">27</a>]</cite> propose a red teaming framework to integrate jailbreak attack with safety alignment and optimize them together.
In the framework, an adversarial LLM will generate harmful prompts to jailbreak the target LLM.
While the adversarial LLM optimizes the generation based on the feedback of target LLM, the target LLM also enhances the robustness through being fine-tuned upon the adversarial prompts, and the interplay continues iteratively until both LLMs achieve expected performance.
Tian et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib91" title="">91</a>]</cite> propose <math alttext="\mathsf{Evil}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p3.7.m7.1"><semantics id="S3.SS2.SSS3.p3.7.m7.1a"><mi id="S3.SS2.SSS3.p3.7.m7.1.1" xref="S3.SS2.SSS3.p3.7.m7.1.1.cmml">𝖤𝗏𝗂𝗅</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.7.m7.1b"><ci id="S3.SS2.SSS3.p3.7.m7.1.1.cmml" xref="S3.SS2.SSS3.p3.7.m7.1.1">𝖤𝗏𝗂𝗅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.7.m7.1c">\mathsf{Evil}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.7.m7.1d">sansserif_Evil</annotation></semantics></math> <math alttext="\mathsf{Geniuses}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p3.8.m8.1"><semantics id="S3.SS2.SSS3.p3.8.m8.1a"><mi id="S3.SS2.SSS3.p3.8.m8.1.1" xref="S3.SS2.SSS3.p3.8.m8.1.1.cmml">𝖦𝖾𝗇𝗂𝗎𝗌𝖾𝗌</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.8.m8.1b"><ci id="S3.SS2.SSS3.p3.8.m8.1.1.cmml" xref="S3.SS2.SSS3.p3.8.m8.1.1">𝖦𝖾𝗇𝗂𝗎𝗌𝖾𝗌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.8.m8.1c">\mathsf{Geniuses}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.8.m8.1d">sansserif_Geniuses</annotation></semantics></math> to automatically generate jailbreak prompts against LLM-based agents using the Red-Blue exercise.
They discover that, compared to LLMs, the agents are less robust and more prone to conduct harmful behaviors.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">另一种策略是让多个 LLM 协作形成一个框架，其中每个 LLM 都充当不同的代理，并可以进行系统化优化。Chao 等人[15]提出了 <math id="S3.SS2.SSS3.p3.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{Prompt}"><semantics id="S3.SS2.SSS3.p3.1.m1.1a"><mi id="S3.SS2.SSS3.p3.1.m1.1.1">𝖯𝗋𝗈𝗆𝗉𝗍</mi><annotation-xml id="S3.SS2.SSS3.p3.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p3.1.m1.1c" encoding="application/x-tex">\mathsf{Prompt}</annotation><annotation id="S3.SS2.SSS3.p3.1.m1.1d" encoding="application/x-llamapun">sansserif_Prompt</annotation></semantics></math> <math id="S3.SS2.SSS3.p3.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{Automatic}"><semantics id="S3.SS2.SSS3.p3.2.m2.1a"><mi id="S3.SS2.SSS3.p3.2.m2.1.1">𝖠𝗎𝗍𝗈𝗆𝖺𝗍𝗂𝖼</mi><annotation-xml id="S3.SS2.SSS3.p3.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p3.2.m2.1c" encoding="application/x-tex">\mathsf{Automatic}</annotation><annotation id="S3.SS2.SSS3.p3.2.m2.1d" encoding="application/x-llamapun">sansserif_Automatic</annotation></semantics></math> <math id="S3.SS2.SSS3.p3.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{Iterative}"><semantics id="S3.SS2.SSS3.p3.3.m3.1a"><mi id="S3.SS2.SSS3.p3.3.m3.1.1">𝖨𝗍𝖾𝗋𝖺𝗍𝗂𝗏𝖾</mi><annotation-xml id="S3.SS2.SSS3.p3.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p3.3.m3.1c" encoding="application/x-tex">\mathsf{Iterative}</annotation><annotation id="S3.SS2.SSS3.p3.3.m3.1d" encoding="application/x-llamapun">sansserif_Iterative</annotation></semantics></math> <math id="S3.SS2.SSS3.p3.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{Refinement}"><semantics id="S3.SS2.SSS3.p3.4.m4.1a"><mi id="S3.SS2.SSS3.p3.4.m4.1.1">𝖱𝖾𝖿𝗂𝗇𝖾𝗆𝖾𝗇𝗍</mi><annotation-xml id="S3.SS2.SSS3.p3.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p3.4.m4.1c" encoding="application/x-tex">\mathsf{Refinement}</annotation><annotation id="S3.SS2.SSS3.p3.4.m4.1d" encoding="application/x-llamapun">sansserif_Refinement</annotation></semantics></math> ( <math id="S3.SS2.SSS3.p3.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{PAIR}"><semantics id="S3.SS2.SSS3.p3.5.m5.1a"><mi id="S3.SS2.SSS3.p3.5.m5.1.1">𝖯𝖠𝖨𝖱</mi><annotation-xml id="S3.SS2.SSS3.p3.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p3.5.m5.1c" encoding="application/x-tex">\mathsf{PAIR}</annotation><annotation id="S3.SS2.SSS3.p3.5.m5.1d" encoding="application/x-llamapun">sansserif_PAIR</annotation></semantics></math> )来生成针对目标 LLM 的越狱提示，仅通过黑盒方式访问目标 LLM。具体来说， <math id="S3.SS2.SSS3.p3.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{PAIR}"><semantics id="S3.SS2.SSS3.p3.6.m6.1a"><mi id="S3.SS2.SSS3.p3.6.m6.1.1">𝖯𝖠𝖨𝖱</mi><annotation-xml id="S3.SS2.SSS3.p3.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p3.6.m6.1c" encoding="application/x-tex">\mathsf{PAIR}</annotation><annotation id="S3.SS2.SSS3.p3.6.m6.1d" encoding="application/x-llamapun">sansserif_PAIR</annotation></semantics></math> 使用攻击者 LLM 通过查询目标 LLM 并优化提示来迭代更新针对目标 LLM 的越狱提示。Jin 等人[41]设计了一个多代理系统来自动生成越狱提示。在该系统中，LLM 扮演不同的角色，包括生成器、翻译器、评估器和优化器。例如，生成器负责根据先前的越狱示例创建初始越狱提示，然后翻译器和评估器检查目标 LLM 的响应，最后优化器分析越狱的有效性并向生成器提供反馈。Ge 等人[27]提出了一种红队框架，将越狱攻击与安全对齐相结合，并一起优化。在该框架中，一个对抗性 LLM 将生成有害提示来越狱目标 LLM。 在对抗性 LLM 根据目标 LLM 的反馈优化生成内容的同时，目标 LLM 也通过在对抗性提示上进行微调来增强鲁棒性，这种相互作用持续迭代，直到两个 LLM 都达到预期性能。Tian 等人[91]提出了 <math id="S3.SS2.SSS3.p3.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{Evil}"><semantics id="S3.SS2.SSS3.p3.7.m7.1a"><mi id="S3.SS2.SSS3.p3.7.m7.1.1">𝖤𝗏𝗂𝗅</mi><annotation-xml id="S3.SS2.SSS3.p3.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p3.7.m7.1c" encoding="application/x-tex">\mathsf{Evil}</annotation><annotation id="S3.SS2.SSS3.p3.7.m7.1d" encoding="application/x-llamapun">sansserif_Evil</annotation></semantics></math> <math id="S3.SS2.SSS3.p3.8.m8.1" display="inline" class="ltx_Math" alttext="\mathsf{Geniuses}"><semantics id="S3.SS2.SSS3.p3.8.m8.1a"><mi id="S3.SS2.SSS3.p3.8.m8.1.1">𝖦𝖾𝗇𝗂𝗎𝗌𝖾𝗌</mi><annotation-xml id="S3.SS2.SSS3.p3.8.m8.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p3.8.m8.1c" encoding="application/x-tex">\mathsf{Geniuses}</annotation><annotation id="S3.SS2.SSS3.p3.8.m8.1d" encoding="application/x-llamapun">sansserif_Geniuses</annotation></semantics></math> ，利用红蓝演练自动生成针对基于 LLM 的代理的越狱提示。他们发现，与 LLM 相比，代理的鲁棒性较低，更容易执行有害行为。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p4">
<p class="ltx_p" id="S3.SS2.SSS3.p4.7">We note that techniques based on LLMs are increasingly being integrated with other methods to enhance jailbreak attacks.
For example, an LLM can be programmed to generate templates for scenario nesting attacks, which involve embedding malicious payloads within benign contexts.
Additionally, LLMs can assist in the perturbation operation, a critical step in genetic algorithm-based attacks, where slight modifications are algorithmically generated to test system vulnerabilities.
Liu et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib54" title="">54</a>]</cite> divide an adversarial prompt into three elements: goal, content, and template, and construct plenty of content and templates manually with different attack goals.
Later, a LLM generator will randomly combine the content and templates to produce hybrid prompts, which are then estimated by the LLM evaluator to judge their effectiveness.
Mehrotra et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib64" title="">64</a>]</cite> propose a novel method called <math alttext="\mathsf{Tree}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p4.1.m1.1"><semantics id="S3.SS2.SSS3.p4.1.m1.1a"><mi id="S3.SS2.SSS3.p4.1.m1.1.1" xref="S3.SS2.SSS3.p4.1.m1.1.1.cmml">𝖳𝗋𝖾𝖾</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.1.m1.1b"><ci id="S3.SS2.SSS3.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1">𝖳𝗋𝖾𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.1.m1.1c">\mathsf{Tree}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p4.1.m1.1d">sansserif_Tree</annotation></semantics></math> <math alttext="\mathsf{of}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p4.2.m2.1"><semantics id="S3.SS2.SSS3.p4.2.m2.1a"><mi id="S3.SS2.SSS3.p4.2.m2.1.1" xref="S3.SS2.SSS3.p4.2.m2.1.1.cmml">𝗈𝖿</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.2.m2.1b"><ci id="S3.SS2.SSS3.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p4.2.m2.1.1">𝗈𝖿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.2.m2.1c">\mathsf{of}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p4.2.m2.1d">sansserif_of</annotation></semantics></math> <math alttext="\mathsf{Attacks}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p4.3.m3.1"><semantics id="S3.SS2.SSS3.p4.3.m3.1a"><mi id="S3.SS2.SSS3.p4.3.m3.1.1" xref="S3.SS2.SSS3.p4.3.m3.1.1.cmml">𝖠𝗍𝗍𝖺𝖼𝗄𝗌</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.3.m3.1b"><ci id="S3.SS2.SSS3.p4.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.1">𝖠𝗍𝗍𝖺𝖼𝗄𝗌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.3.m3.1c">\mathsf{Attacks}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p4.3.m3.1d">sansserif_Attacks</annotation></semantics></math> <math alttext="\mathsf{with}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p4.4.m4.1"><semantics id="S3.SS2.SSS3.p4.4.m4.1a"><mi id="S3.SS2.SSS3.p4.4.m4.1.1" xref="S3.SS2.SSS3.p4.4.m4.1.1.cmml">𝗐𝗂𝗍𝗁</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.4.m4.1b"><ci id="S3.SS2.SSS3.p4.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p4.4.m4.1.1">𝗐𝗂𝗍𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.4.m4.1c">\mathsf{with}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p4.4.m4.1d">sansserif_with</annotation></semantics></math> <math alttext="\mathsf{Pruning}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p4.5.m5.1"><semantics id="S3.SS2.SSS3.p4.5.m5.1a"><mi id="S3.SS2.SSS3.p4.5.m5.1.1" xref="S3.SS2.SSS3.p4.5.m5.1.1.cmml">𝖯𝗋𝗎𝗇𝗂𝗇𝗀</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.5.m5.1b"><ci id="S3.SS2.SSS3.p4.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.1">𝖯𝗋𝗎𝗇𝗂𝗇𝗀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.5.m5.1c">\mathsf{Pruning}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p4.5.m5.1d">sansserif_Pruning</annotation></semantics></math> (<math alttext="\mathsf{TAP}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p4.6.m6.1"><semantics id="S3.SS2.SSS3.p4.6.m6.1a"><mi id="S3.SS2.SSS3.p4.6.m6.1.1" xref="S3.SS2.SSS3.p4.6.m6.1.1.cmml">𝖳𝖠𝖯</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.6.m6.1b"><ci id="S3.SS2.SSS3.p4.6.m6.1.1.cmml" xref="S3.SS2.SSS3.p4.6.m6.1.1">𝖳𝖠𝖯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.6.m6.1c">\mathsf{TAP}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p4.6.m6.1d">sansserif_TAP</annotation></semantics></math>).
Starting from seed prompts, <math alttext="\mathsf{TAP}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p4.7.m7.1"><semantics id="S3.SS2.SSS3.p4.7.m7.1a"><mi id="S3.SS2.SSS3.p4.7.m7.1.1" xref="S3.SS2.SSS3.p4.7.m7.1.1.cmml">𝖳𝖠𝖯</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.7.m7.1b"><ci id="S3.SS2.SSS3.p4.7.m7.1.1.cmml" xref="S3.SS2.SSS3.p4.7.m7.1.1">𝖳𝖠𝖯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.7.m7.1c">\mathsf{TAP}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p4.7.m7.1d">sansserif_TAP</annotation></semantics></math> will generate improved prompts and discard the inferior ones.
The reserved prompts are then inputted into the target LLMs to estimate their effectiveness.
If a jailbreak turns out to be successful, the corresponding prompt will be returned as seed prompts for the next iteration.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们注意到基于 LLMs 的技术正越来越多地与其他方法结合，以增强越狱攻击。例如，一个 LLM 可以被编程生成场景嵌套攻击的模板，这种攻击涉及将恶意载荷嵌入良性环境中。此外，LLMs 可以协助扰动操作，这是基于遗传算法攻击的一个关键步骤，其中算法会生成轻微的修改来测试系统漏洞。Liu 等人[54]将对抗性提示分为三个元素：目标、内容和模板，并手动构建大量具有不同攻击目标的内容和模板。随后，一个 LLM 生成器会随机组合内容和模板，生成混合提示，这些提示再由 LLM 评估器估计其有效性。Mehrotra 等人[64]提出了一种名为 <math id="S3.SS2.SSS3.p4.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{Tree}"><semantics id="S3.SS2.SSS3.p4.1.m1.1a"><mi id="S3.SS2.SSS3.p4.1.m1.1.1">𝖳𝗋𝖾𝖾</mi><annotation-xml id="S3.SS2.SSS3.p4.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p4.1.m1.1c" encoding="application/x-tex">\mathsf{Tree}</annotation><annotation id="S3.SS2.SSS3.p4.1.m1.1d" encoding="application/x-llamapun">sansserif_Tree</annotation></semantics></math> <math id="S3.SS2.SSS3.p4.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{of}"><semantics id="S3.SS2.SSS3.p4.2.m2.1a"><mi id="S3.SS2.SSS3.p4.2.m2.1.1">𝗈𝖿</mi><annotation-xml id="S3.SS2.SSS3.p4.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p4.2.m2.1c" encoding="application/x-tex">\mathsf{of}</annotation><annotation id="S3.SS2.SSS3.p4.2.m2.1d" encoding="application/x-llamapun">sansserif_of</annotation></semantics></math> <math id="S3.SS2.SSS3.p4.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{Attacks}"><semantics id="S3.SS2.SSS3.p4.3.m3.1a"><mi id="S3.SS2.SSS3.p4.3.m3.1.1">𝖠𝗍𝗍𝖺𝖼𝗄𝗌</mi><annotation-xml id="S3.SS2.SSS3.p4.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p4.3.m3.1c" encoding="application/x-tex">\mathsf{Attacks}</annotation><annotation id="S3.SS2.SSS3.p4.3.m3.1d" encoding="application/x-llamapun">sansserif_Attacks</annotation></semantics></math> <math id="S3.SS2.SSS3.p4.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{with}"><semantics id="S3.SS2.SSS3.p4.4.m4.1a"><mi id="S3.SS2.SSS3.p4.4.m4.1.1">𝗐𝗂𝗍𝗁</mi><annotation-xml id="S3.SS2.SSS3.p4.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p4.4.m4.1c" encoding="application/x-tex">\mathsf{with}</annotation><annotation id="S3.SS2.SSS3.p4.4.m4.1d" encoding="application/x-llamapun">sansserif_with</annotation></semantics></math> <math id="S3.SS2.SSS3.p4.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{Pruning}"><semantics id="S3.SS2.SSS3.p4.5.m5.1a"><mi id="S3.SS2.SSS3.p4.5.m5.1.1">𝖯𝗋𝗎𝗇𝗂𝗇𝗀</mi><annotation-xml id="S3.SS2.SSS3.p4.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p4.5.m5.1c" encoding="application/x-tex">\mathsf{Pruning}</annotation><annotation id="S3.SS2.SSS3.p4.5.m5.1d" encoding="application/x-llamapun">sansserif_Pruning</annotation></semantics></math> ( <math id="S3.SS2.SSS3.p4.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{TAP}"><semantics id="S3.SS2.SSS3.p4.6.m6.1a"><mi id="S3.SS2.SSS3.p4.6.m6.1.1">𝖳𝖠𝖯</mi><annotation-xml id="S3.SS2.SSS3.p4.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p4.6.m6.1c" encoding="application/x-tex">\mathsf{TAP}</annotation><annotation id="S3.SS2.SSS3.p4.6.m6.1d" encoding="application/x-llamapun">sansserif_TAP</annotation></semantics></math> )的新方法。从种子提示开始， <math id="S3.SS2.SSS3.p4.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{TAP}"><semantics id="S3.SS2.SSS3.p4.7.m7.1a"><mi id="S3.SS2.SSS3.p4.7.m7.1.1">𝖳𝖠𝖯</mi><annotation-xml id="S3.SS2.SSS3.p4.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S3.SS2.SSS3.p4.7.m7.1c" encoding="application/x-tex">\mathsf{TAP}</annotation><annotation id="S3.SS2.SSS3.p4.7.m7.1d" encoding="application/x-llamapun">sansserif_TAP</annotation></semantics></math> 会生成改进的提示并丢弃较差的提示。保留的提示随后输入到目标 LLMs 中，以估计其有效性。 如果越狱攻击成功，相应的提示将作为种子提示返回给下一次迭代。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p5">
<svg class="ltx_picture" height="188.49" id="S3.SS2.SSS3.p5.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,188.49) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 182.59 C 0 185.85 2.64 188.49 5.91 188.49 L 594.09 188.49 C 597.36 188.49 600 185.85 600 182.59 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 164.38 L 598.03 164.38 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 170.29)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS2.SSS3.p5.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS2.SSS3.p5.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p5.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;3.6</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="138.79" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS2.SSS3.p5.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S3.SS2.SSS3.p5.pic1.2.2.2.1.1.1">The use of LLMs to simulate attackers encompasses two main strategies.
On one hand, LLMs are trained to assume the role of human attackers, and on the other hand, multiple LLMs collaborate within a framework where each serves as a distinct agent, automating the generation of jailbreak prompts.
Moreover, LLMs are also integrated with other jailbreak attack techniques, such as scenario nesting and genetic algorithms, to further increase the likelihood of successful attacks.
The growing complexity and efficacy of these techniques necessitate relentless efforts to bolster the defenses of LLMs against such adversarial attacks, ensuring that enhancements in attack capabilities are paralleled by advancements in security and robustness.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;"> Defense Methods<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">防御方法</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.F8.2" style="width:346.9pt;height:147pt;vertical-align:-142.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-79.2pt,1.1pt) scale(0.686607096773005,0.686607096773005) ;"><span class="ltx_ERROR undefined" id="S4.F8.2.1" data-imt_insert_failed="1">{forest}</span>
<p class="ltx_p" id="S4.F8.2.2">forked edges,
for tree=
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
node options=align=center,
align = center,
base=left,
font=<span class="ltx_text" id="S4.F8.2.2.1" style="font-size:90%;">,
rectangle,
draw=hidden-draw,
rounded corners,
edge+=darkgray, line width=1pt,
s sep=3pt,
inner xsep=2pt,
inner ysep=3pt,
ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
</span>,
where level=1text width=5.0em,font=,
where level=2text width=5.6em,font=,
where level=3text width=6.8em,font=,
[
Jailbreak Defense Methods, ver
[
Prompt Level
[
Prompt Detection [
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib37" title="">37</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib1" title="">1</a>]</cite>
, leaf, text width=3em
]
]
[
Prompt 
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">分叉边，对于树= grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, 节点选项=align=center, align = center, base=left, font=, 矩形, draw=hidden-draw, 圆角, 边+=darkgray, 线宽=1pt, s sep=3pt, inner xsep=2pt, inner ysep=3pt, ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center, , 其中 level=1text width=5.0em,font=, 其中 level=2text width=5.6em,font=, 其中 level=3text width=6.8em,font=, [ Jailbreak 防御方法, ver [ 提示级别 [ 提示检测 [ [ 37][ 1] , 叶子, text width=3em ] ] [ 提示 </font></font></font><br class="ltx_break">Perturbation
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib11" title="">11</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib73" title="">73</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib38" title="">38</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib112" title="">112</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib45" title="">45</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib121" title="">121</a>]</cite>
, leaf, text width=10.5em
]
]
[
System Prompt 
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">扰动 [ [ 11][ 73][ 38][ 112][ 45][ 121] , 叶子, text width=10.5em ] ] [ 系统提示 </font></font></font><br class="ltx_break">Safeguard
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib77" title="">77</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib126" title="">126</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib94" title="">94</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib118" title="">118</a>]</cite>
, leaf, text width=7.5em
]
]
]
[
Model Level
[
SFT-based
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib9" title="">9</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib18" title="">18</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib8" title="">8</a>]</cite>
, leaf, text width=4.5em
]
]
[
RLHF-based
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib66" title="">66</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib6" title="">6</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib83" title="">83</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib25" title="">25</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib59" title="">59</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib26" title="">26</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib58" title="">58</a>]</cite>
, leaf, text width=11em
]
]
[
Gradient and Logit 
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">保护措施 [ [ 77][ 126][ 94][ 118] , 叶子, text width=7.5em ] ] ] [ 模型级别 [ 基于 SFT 的 [ [ 9][ 18][ 8] , 叶子, text width=4.5em ] ] [ 基于 RLHF 的 [ [ 66][ 6][ 83][ 25][ 59][ 26][ 58] , 叶子, text width=11em ] ] [ 梯度和逻辑</font></font></font><br class="ltx_break">Analysis
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib101" title="">101</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib102" title="">102</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib35" title="">35</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib53" title="">53</a>]</cite>
, leaf, text width=6.5em
]
]
[
Refinement
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib44" title="">44</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib113" title="">113</a>]</cite>
, leaf, text width=4em
]
]
[
Proxy Defense
[
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib110" title="">110</a>]</cite>
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib85" title="">85</a>]</cite>
, leaf, text width=4em
]
]
]
]<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">分析 [ [ 101][ 102][ 35][ 53] , 叶, 文本宽度=6.5em ] ] [ 精炼 [ [ 44][ 113] , 叶, 文本宽度=4em ] ] [ 代理防御 [ [ 110][ 85] , 叶, 文本宽度=4em ] ] ] ]</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.3.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.4.2" style="font-size:90%;">Taxonomy of jailbreak defense.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 8：越狱防御的分类。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">With the development of LLM jailbreak techniques, concerns regarding model ethics and substantial threats in proprietary models like ChatGPT and open-source models like Llama have gained more attention, and various defense methods have been proposed to protect the language model from potential attacks. A taxonomy of the methods is illustrated in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.F8" title="In Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a>.
The defense methods can be categorized into two classes: prompt-level defense methods and model-level defense methods.
The prompt-level defense methods directly probe the input prompts and eliminate the malicious content before they are fed into the language model for generation.
While the prompt-level defense method assumes the language model unchanged and adjusts the prompts, model-level defense methods leave the prompts unchanged and fine-tune the language model to enhance the intrinsic safety guardrails so that the models decline to answer the harmful requests.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">随着 LLM 越狱技术的开发，关于模型伦理以及 ChatGPT 等专有模型和 Llama 等开源模型中的重大威胁的关注度日益增加，并提出了各种防御方法来保护语言模型免受潜在攻击。图 8 展示了这些方法的一个分类。防御方法可以分为两类：提示级防御方法和模型级防御方法。提示级防御方法直接探测输入提示，并在它们被输入语言模型进行生成之前消除恶意内容。而提示级防御方法假设语言模型不变并调整提示，模型级防御方法则保持提示不变并微调语言模型以增强内在的安全护栏，使模型拒绝回答有害请求。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:120%;"> Prompt-level Defenses<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提示级防御</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Prompt-level defenses refer to the scenarios where the direct access to neither the internal model weight nor the output logits is available, thus the prompt becomes the only variable both the attackers and defenders can control.
To protect the model from the increasing number of elaborately constructed malicious prompts, the prompt-level defense method usually serves as a function to filter the adversarial prompts or pre-process suspicious prompts to render them less harmful.
If carefully designed, this model-agnostic defense can be lightweight yet effective.
Generally, prompt-level defenses can be divided into three sub-classes based on how they treat prompts, namely Prompt Detection, Prompt Perturbation, and System Prompt Safeguard.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提示级别的防御指的是在无法直接访问内部模型权重或输出 logits 的情况下，提示成为攻击者和防御者唯一可以控制的变量。为了保护模型免受日益增多精心构造的恶意提示的攻击，提示级别的防御方法通常作为一个功能来过滤对抗性提示或预处理可疑提示，使其危害性降低。如果设计得当，这种与模型无关的防御可以轻量级且有效。通常，提示级别的防御可以根据它们如何处理提示分为三个子类别，即提示检测、提示扰动和系统提示保护。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Prompt Detection<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提示检测</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">For proprietary models like ChatGPT or Claude, the model vendors usually maintain a data moderation system like Llama-guard&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib90" title="">90</a>]</cite> or conduct reinforcement-learning-based fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib66" title="">66</a>]</cite> to enhance the safety guardrails and ensure the user prompts may not violate the safety policy.
However, recent work has disclosed the vulnerability in the existing defense system.
Zou et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib125" title="">125</a>]</cite> append an incoherent suffix to the malicious prompts, which increases the model’s perplexity of the prompt and successfully bypasses the safety guardrails.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">对于 ChatGPT 或 Claude 等专有模型，模型供应商通常会维护类似 Llama-guard[90]的数据审核系统，或采用基于强化学习的微调[66]来增强安全防护措施，并确保用户提示不会违反安全政策。然而，近期的研究揭示了现有防御系统的漏洞。Zou 等人[125]在恶意提示中附加一个不连贯的后缀，这增加了模型对提示的困惑度，并成功绕过了安全防护措施。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">To fill the gap, Jain et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib37" title="">37</a>]</cite> consider a threshold-based detection that computes the perplexity of both the text segments and the entire prompt in the context window, and declares the harmfulness if the perplexity exceeds a certain threshold.
Note that a similar work is <math alttext="\mathsf{LightGBM}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.1.m1.1"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mi id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml">𝖫𝗂𝗀𝗁𝗍𝖦𝖡𝖬</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><ci id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1">𝖫𝗂𝗀𝗁𝗍𝖦𝖡𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">\mathsf{LightGBM}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.1.m1.1d">sansserif_LightGBM</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib1" title="">1</a>]</cite>, which first calculates the perplexity of the prompts and trains a classifier based on the perplexity and sequence length to detect the harmfulness of the prompt.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了弥补这一空白，Jain 等人[37]提出了一种基于阈值的检测方法，该方法计算上下文窗口中文本片段和整个提示的困惑度，并在困惑度超过特定阈值时判定为有害。请注意，类似的工作有 <math id="S4.SS1.SSS1.p2.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{LightGBM}"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mi id="S4.SS1.SSS1.p2.1.m1.1.1">𝖫𝗂𝗀𝗁𝗍𝖦𝖡𝖬</mi><annotation-xml id="S4.SS1.SSS1.p2.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS1.p2.1.m1.1c" encoding="application/x-tex">\mathsf{LightGBM}</annotation><annotation id="S4.SS1.SSS1.p2.1.m1.1d" encoding="application/x-llamapun">sansserif_LightGBM</annotation></semantics></math> [1]，该方法首先计算提示的困惑度，并基于困惑度和序列长度训练分类器来检测提示的有害性。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS1.p3">
<svg class="ltx_picture" height="111.81" id="S4.SS1.SSS1.p3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,111.81) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 105.91 C 0 109.17 2.64 111.81 5.91 111.81 L 594.09 111.81 C 597.36 111.81 600 109.17 600 105.91 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 87.7 L 598.03 87.7 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 93.61)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS1.SSS1.p3.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS1.SSS1.p3.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.pic1.2.2.2.1.1.1.1">Takeaways.&nbsp;4.1</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">Although the detection methods show promising defense results against white-box attacks like <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">sansserif_GCG</annotation></semantics></math>, they often classify the benign prompts mistakenly into the harmful class thus making a high false positive rate.
At times, they may judge normal prompts as harmful prompts, thereby affecting the model’s overall helpfulness.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Prompt Perturbation<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提示扰动</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.11">Despite the improved accuracy in detecting malicious inputs, prompt detection methods have the side-effect of a high false positive rate which may influence the response quality of the questions that should have been treated as benign inputs.
Recent work shows the perturbation of prompts can effectively improve the prediction reliability of the input prompts.
Cao et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib11" title="">11</a>]</cite> propose <math alttext="\mathsf{RA}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1.1"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mi id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">𝖱𝖠</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><ci id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1">𝖱𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">\mathsf{RA}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.1.m1.1d">sansserif_RA</annotation></semantics></math>-<math alttext="\mathsf{LLM}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.2.m2.1"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mi id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml">𝖫𝖫𝖬</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><ci id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1">𝖫𝖫𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">\mathsf{LLM}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.2.m2.1d">sansserif_LLM</annotation></semantics></math> that randomly puts word-level masks on the copies of the original prompt, and considers the original prompt malicious if LLM rejects a certain ratio of the processed copies.
Robey et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib73" title="">73</a>]</cite> introduce <math alttext="\mathsf{SmoothLLM}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.3.m3.1"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><mi id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml">𝖲𝗆𝗈𝗈𝗍𝗁𝖫𝖫𝖬</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><ci id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1">𝖲𝗆𝗈𝗈𝗍𝗁𝖫𝖫𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">\mathsf{SmoothLLM}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.3.m3.1d">sansserif_SmoothLLM</annotation></semantics></math> to apply character-level perturbation to the copies of a given prompt.
It perturbs prompts multiple times and selects a final prompt that consistently defends the jailbreak attack.
Ji et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib38" title="">38</a>]</cite> propose a similar method as&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib73" title="">73</a>]</cite>, except that they perturb the original prompt with semantic transformations.
Zhang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib112" title="">112</a>]</cite> propose <math alttext="\mathsf{JailGuard}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.4.m4.1"><semantics id="S4.SS1.SSS2.p1.4.m4.1a"><mi id="S4.SS1.SSS2.p1.4.m4.1.1" xref="S4.SS1.SSS2.p1.4.m4.1.1.cmml">𝖩𝖺𝗂𝗅𝖦𝗎𝖺𝗋𝖽</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.4.m4.1b"><ci id="S4.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1">𝖩𝖺𝗂𝗅𝖦𝗎𝖺𝗋𝖽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.4.m4.1c">\mathsf{JailGuard}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.4.m4.1d">sansserif_JailGuard</annotation></semantics></math>, supporting jailbreak detection in image and text modalities.
Concretely, <math alttext="\mathsf{JailGuard}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.5.m5.1"><semantics id="S4.SS1.SSS2.p1.5.m5.1a"><mi id="S4.SS1.SSS2.p1.5.m5.1.1" xref="S4.SS1.SSS2.p1.5.m5.1.1.cmml">𝖩𝖺𝗂𝗅𝖦𝗎𝖺𝗋𝖽</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.5.m5.1b"><ci id="S4.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1">𝖩𝖺𝗂𝗅𝖦𝗎𝖺𝗋𝖽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.5.m5.1c">\mathsf{JailGuard}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.5.m5.1d">sansserif_JailGuard</annotation></semantics></math> introduces multiple perturbations to the query and observes the consistency of the corresponding outputs.
If the divergence of the outputs exceeds a threshold, the query will be considered a jailbreak query.
Kumar et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib45" title="">45</a>]</cite> propose a more fine-grained defense framework called <math alttext="\mathsf{erase}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.6.m6.1"><semantics id="S4.SS1.SSS2.p1.6.m6.1a"><mi id="S4.SS1.SSS2.p1.6.m6.1.1" xref="S4.SS1.SSS2.p1.6.m6.1.1.cmml">𝖾𝗋𝖺𝗌𝖾</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.6.m6.1b"><ci id="S4.SS1.SSS2.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS2.p1.6.m6.1.1">𝖾𝗋𝖺𝗌𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.6.m6.1c">\mathsf{erase}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.6.m6.1d">sansserif_erase</annotation></semantics></math>-<math alttext="\mathsf{and}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.7.m7.1"><semantics id="S4.SS1.SSS2.p1.7.m7.1a"><mi id="S4.SS1.SSS2.p1.7.m7.1.1" xref="S4.SS1.SSS2.p1.7.m7.1.1.cmml">𝖺𝗇𝖽</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.7.m7.1b"><ci id="S4.SS1.SSS2.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.1">𝖺𝗇𝖽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.7.m7.1c">\mathsf{and}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.7.m7.1d">sansserif_and</annotation></semantics></math>-<math alttext="\mathsf{check}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.8.m8.1"><semantics id="S4.SS1.SSS2.p1.8.m8.1a"><mi id="S4.SS1.SSS2.p1.8.m8.1.1" xref="S4.SS1.SSS2.p1.8.m8.1.1.cmml">𝖼𝗁𝖾𝖼𝗄</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.8.m8.1b"><ci id="S4.SS1.SSS2.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS2.p1.8.m8.1.1">𝖼𝗁𝖾𝖼𝗄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.8.m8.1c">\mathsf{check}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.8.m8.1d">sansserif_check</annotation></semantics></math>.
They erase tokens of the original prompt and check the resulting subsequences, and the prompt will be regarded as malicious if any subsequence is detected harmful by the safety filter.
Moreover, they further explore how to erase tokens more efficiently and introduce different rule-based methods including randomized, greedy, and gradient-based <math alttext="\mathsf{erase}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.9.m9.1"><semantics id="S4.SS1.SSS2.p1.9.m9.1a"><mi id="S4.SS1.SSS2.p1.9.m9.1.1" xref="S4.SS1.SSS2.p1.9.m9.1.1.cmml">𝖾𝗋𝖺𝗌𝖾</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.9.m9.1b"><ci id="S4.SS1.SSS2.p1.9.m9.1.1.cmml" xref="S4.SS1.SSS2.p1.9.m9.1.1">𝖾𝗋𝖺𝗌𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.9.m9.1c">\mathsf{erase}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.9.m9.1d">sansserif_erase</annotation></semantics></math>-<math alttext="\mathsf{and}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.10.m10.1"><semantics id="S4.SS1.SSS2.p1.10.m10.1a"><mi id="S4.SS1.SSS2.p1.10.m10.1.1" xref="S4.SS1.SSS2.p1.10.m10.1.1.cmml">𝖺𝗇𝖽</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.10.m10.1b"><ci id="S4.SS1.SSS2.p1.10.m10.1.1.cmml" xref="S4.SS1.SSS2.p1.10.m10.1.1">𝖺𝗇𝖽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.10.m10.1c">\mathsf{and}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.10.m10.1d">sansserif_and</annotation></semantics></math>-<math alttext="\mathsf{check}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.11.m11.1"><semantics id="S4.SS1.SSS2.p1.11.m11.1a"><mi id="S4.SS1.SSS2.p1.11.m11.1.1" xref="S4.SS1.SSS2.p1.11.m11.1.1.cmml">𝖼𝗁𝖾𝖼𝗄</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.11.m11.1b"><ci id="S4.SS1.SSS2.p1.11.m11.1.1.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1">𝖼𝗁𝖾𝖼𝗄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.11.m11.1c">\mathsf{check}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.11.m11.1d">sansserif_check</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">尽管在检测恶意输入方面的准确性有所提高，但提示检测方法存在高误报率的副作用，这可能会影响本应被视为良性输入的问题的响应质量。近期研究表明，对提示进行扰动可以有效提高输入提示的预测可靠性。曹等人[11]提出了 <math id="S4.SS1.SSS2.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{RA}"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mi id="S4.SS1.SSS2.p1.1.m1.1.1">𝖱𝖠</mi><annotation-xml id="S4.SS1.SSS2.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.1.m1.1c" encoding="application/x-tex">\mathsf{RA}</annotation><annotation id="S4.SS1.SSS2.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_RA</annotation></semantics></math> - <math id="S4.SS1.SSS2.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{LLM}"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mi id="S4.SS1.SSS2.p1.2.m2.1.1">𝖫𝖫𝖬</mi><annotation-xml id="S4.SS1.SSS2.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.2.m2.1c" encoding="application/x-tex">\mathsf{LLM}</annotation><annotation id="S4.SS1.SSS2.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_LLM</annotation></semantics></math> ，该方法随机在原始提示的副本上添加词级掩码，如果 LLM 拒绝了一定比例的处理后副本，则将原始提示视为恶意。罗伯等人[73]引入了 <math id="S4.SS1.SSS2.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{SmoothLLM}"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><mi id="S4.SS1.SSS2.p1.3.m3.1.1">𝖲𝗆𝗈𝗈𝗍𝗁𝖫𝖫𝖬</mi><annotation-xml id="S4.SS1.SSS2.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.3.m3.1c" encoding="application/x-tex">\mathsf{SmoothLLM}</annotation><annotation id="S4.SS1.SSS2.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_SmoothLLM</annotation></semantics></math> ，用于对给定提示的副本应用字符级扰动。该方法多次扰动提示，并选择一个始终能够防御越狱攻击的最终提示。季等人[38]提出了一种与[73]类似的方法，不同之处在于他们使用语义转换来扰动原始提示。张等人[112]提出了 <math id="S4.SS1.SSS2.p1.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{JailGuard}"><semantics id="S4.SS1.SSS2.p1.4.m4.1a"><mi id="S4.SS1.SSS2.p1.4.m4.1.1">𝖩𝖺𝗂𝗅𝖦𝗎𝖺𝗋𝖽</mi><annotation-xml id="S4.SS1.SSS2.p1.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.4.m4.1c" encoding="application/x-tex">\mathsf{JailGuard}</annotation><annotation id="S4.SS1.SSS2.p1.4.m4.1d" encoding="application/x-llamapun">sansserif_JailGuard</annotation></semantics></math> ，支持图像和文本模态的越狱检测。具体来说， <math id="S4.SS1.SSS2.p1.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{JailGuard}"><semantics id="S4.SS1.SSS2.p1.5.m5.1a"><mi id="S4.SS1.SSS2.p1.5.m5.1.1">𝖩𝖺𝗂𝗅𝖦𝗎𝖺𝗋𝖽</mi><annotation-xml id="S4.SS1.SSS2.p1.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.5.m5.1c" encoding="application/x-tex">\mathsf{JailGuard}</annotation><annotation id="S4.SS1.SSS2.p1.5.m5.1d" encoding="application/x-llamapun">sansserif_JailGuard</annotation></semantics></math> 对查询引入多个扰动，并观察相应输出的一致性。 如果输出结果的差异超过阈值，该查询将被视为越狱查询。Kumar 等人[45]提出了一种更细粒度的防御框架，称为 <math id="S4.SS1.SSS2.p1.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{erase}"><semantics id="S4.SS1.SSS2.p1.6.m6.1a"><mi id="S4.SS1.SSS2.p1.6.m6.1.1">𝖾𝗋𝖺𝗌𝖾</mi><annotation-xml id="S4.SS1.SSS2.p1.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.6.m6.1c" encoding="application/x-tex">\mathsf{erase}</annotation><annotation id="S4.SS1.SSS2.p1.6.m6.1d" encoding="application/x-llamapun">sansserif_erase</annotation></semantics></math> - <math id="S4.SS1.SSS2.p1.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{and}"><semantics id="S4.SS1.SSS2.p1.7.m7.1a"><mi id="S4.SS1.SSS2.p1.7.m7.1.1">𝖺𝗇𝖽</mi><annotation-xml id="S4.SS1.SSS2.p1.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.7.m7.1c" encoding="application/x-tex">\mathsf{and}</annotation><annotation id="S4.SS1.SSS2.p1.7.m7.1d" encoding="application/x-llamapun">sansserif_and</annotation></semantics></math> - <math id="S4.SS1.SSS2.p1.8.m8.1" display="inline" class="ltx_Math" alttext="\mathsf{check}"><semantics id="S4.SS1.SSS2.p1.8.m8.1a"><mi id="S4.SS1.SSS2.p1.8.m8.1.1">𝖼𝗁𝖾𝖼𝗄</mi><annotation-xml id="S4.SS1.SSS2.p1.8.m8.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.8.m8.1c" encoding="application/x-tex">\mathsf{check}</annotation><annotation id="S4.SS1.SSS2.p1.8.m8.1d" encoding="application/x-llamapun">sansserif_check</annotation></semantics></math> 。他们擦除原始提示中的标记并检查生成的子序列，如果任何子序列被安全过滤器检测为有害，则该提示将被视为恶意。此外，他们进一步探索了如何更高效地擦除标记，并引入了不同的基于规则的 <math id="S4.SS1.SSS2.p1.9.m9.1" display="inline" class="ltx_Math" alttext="\mathsf{erase}"><semantics id="S4.SS1.SSS2.p1.9.m9.1a"><mi id="S4.SS1.SSS2.p1.9.m9.1.1">𝖾𝗋𝖺𝗌𝖾</mi><annotation-xml id="S4.SS1.SSS2.p1.9.m9.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.9.m9.1c" encoding="application/x-tex">\mathsf{erase}</annotation><annotation id="S4.SS1.SSS2.p1.9.m9.1d" encoding="application/x-llamapun">sansserif_erase</annotation></semantics></math> - <math id="S4.SS1.SSS2.p1.10.m10.1" display="inline" class="ltx_Math" alttext="\mathsf{and}"><semantics id="S4.SS1.SSS2.p1.10.m10.1a"><mi id="S4.SS1.SSS2.p1.10.m10.1.1">𝖺𝗇𝖽</mi><annotation-xml id="S4.SS1.SSS2.p1.10.m10.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.10.m10.1c" encoding="application/x-tex">\mathsf{and}</annotation><annotation id="S4.SS1.SSS2.p1.10.m10.1d" encoding="application/x-llamapun">sansserif_and</annotation></semantics></math> - <math id="S4.SS1.SSS2.p1.11.m11.1" display="inline" class="ltx_Math" alttext="\mathsf{check}"><semantics id="S4.SS1.SSS2.p1.11.m11.1a"><mi id="S4.SS1.SSS2.p1.11.m11.1.1">𝖼𝗁𝖾𝖼𝗄</mi><annotation-xml id="S4.SS1.SSS2.p1.11.m11.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p1.11.m11.1c" encoding="application/x-tex">\mathsf{check}</annotation><annotation id="S4.SS1.SSS2.p1.11.m11.1d" encoding="application/x-llamapun">sansserif_check</annotation></semantics></math> 方法，包括随机化、贪婪和基于梯度的方法。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">While the above works focus on various transformations to the original prompt and generate the final response corresponding to aggregation of the outputs, another line of works introduces an alternative approach that appends a defense prefix or suffix to the prompt.
For instance, Zhou et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib121" title="">121</a>]</cite> propose a robust prompt optimization algorithm to construct such suffixes.
They select representative adversarial prompts to build a dataset and then optimize the suffixes on it based on the gradient, and the defense strategy turns out to be efficient for both manual jailbreak attacks and gradient-based attacks like <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.1.m1.1"><semantics id="S4.SS1.SSS2.p2.1.m1.1a"><mi id="S4.SS1.SSS2.p2.1.m1.1.1" xref="S4.SS1.SSS2.p2.1.m1.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.1.m1.1b"><ci id="S4.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.1.m1.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.1.m1.1d">sansserif_GCG</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">虽然上述工作主要关注对原始提示的各种变换，并生成与输出聚合相对应的最终响应，但另一系列工作引入了一种替代方法，即向提示中附加防御前缀或后缀。例如，Zhou 等人[121]提出了一种鲁棒的提示优化算法来构建这样的后缀。他们选择具有代表性的对抗性提示来构建数据集，然后基于梯度在该数据集上优化后缀，结果表明这种防御策略对手动越狱攻击和基于梯度的攻击（如 <math id="S4.SS1.SSS2.p2.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S4.SS1.SSS2.p2.1.m1.1a"><mi id="S4.SS1.SSS2.p2.1.m1.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S4.SS1.SSS2.p2.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS2.p2.1.m1.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S4.SS1.SSS2.p2.1.m1.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> ）都有效。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.p3">
<svg class="ltx_picture" height="145.02" id="S4.SS1.SSS2.p3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,145.02) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 139.12 C 0 142.38 2.64 145.02 5.91 145.02 L 594.09 145.02 C 597.36 145.02 600 142.38 600 139.12 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 120.91 L 598.03 120.91 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 126.82)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS1.SSS2.p3.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS1.SSS2.p3.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p3.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;4.2</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="95.32" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS1.SSS2.p3.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS1.SSS2.p3.pic1.2.2.2.1.1.1">The prompt perturbation methods exploit fine-grained contents in the prompt, such as token-level perturbation and sentence-level perturbation, to defend the prompt-based attack and are currently the mainstream for jailbreak defense.
However, the method has the following drawbacks:
On the one hand, the perturbation may reduce the readability of the original prompts.
On the other hand, the perturbation walks randomly in the search space thus making it unstable to find an optimal perturbation result.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> System Prompt Safeguard<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">系统提示保护</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.3">The system prompts built-in LLMs guide the behavior, tone, and style of responses, ensuring consistency and appropriateness of model responses.
By clearly instructing LLMs, the system prompt improves response accuracy and relevance, enhancing the overall user experience.
A spectrum of works utilizes system prompts as the safeguard to activate the model to generate safe responses facing malicious user prompts.
Sharma et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib77" title="">77</a>]</cite> introduce a domain-specific diagram <math alttext="\mathsf{SPML}" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.1.m1.1"><semantics id="S4.SS1.SSS3.p1.1.m1.1a"><mi id="S4.SS1.SSS3.p1.1.m1.1.1" xref="S4.SS1.SSS3.p1.1.m1.1.1.cmml">𝖲𝖯𝖬𝖫</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.1.m1.1b"><ci id="S4.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1">𝖲𝖯𝖬𝖫</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.1.m1.1c">\mathsf{SPML}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p1.1.m1.1d">sansserif_SPML</annotation></semantics></math> to create powerful system prompts.
During the compilation pipeline of <math alttext="\mathsf{SPML}" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.2.m2.1"><semantics id="S4.SS1.SSS3.p1.2.m2.1a"><mi id="S4.SS1.SSS3.p1.2.m2.1.1" xref="S4.SS1.SSS3.p1.2.m2.1.1.cmml">𝖲𝖯𝖬𝖫</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.2.m2.1b"><ci id="S4.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1">𝖲𝖯𝖬𝖫</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.2.m2.1c">\mathsf{SPML}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p1.2.m2.1d">sansserif_SPML</annotation></semantics></math>, system prompts are processed in several procedures like type-checking and intermediate representation transformation, and finally, robust system prompts are generated to deal with various conversation scenarios.
Zou et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib126" title="">126</a>]</cite> explore the effectiveness of system prompt against jailbreak and propose <math alttext="\mathsf{SMEA}" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.3.m3.1"><semantics id="S4.SS1.SSS3.p1.3.m3.1a"><mi id="S4.SS1.SSS3.p1.3.m3.1.1" xref="S4.SS1.SSS3.p1.3.m3.1.1.cmml">𝖲𝖬𝖤𝖠</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.3.m3.1b"><ci id="S4.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p1.3.m3.1.1">𝖲𝖬𝖤𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.3.m3.1c">\mathsf{SMEA}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p1.3.m3.1d">sansserif_SMEA</annotation></semantics></math> to generate system prompt.
Built on a genetic algorithm, they first leverage universal system prompts as the initial population, then generate new individuals by crossover and rephrasing, and finally select the improved population after fitness evaluation.
Wang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib94" title="">94</a>]</cite> integrate a secret prompt into the system prompt to defend against fine-tuning-based jailbreaks.
Since the system prompt is not accessible to the user, the secret prompt can perform as a backdoor trigger to ensure the models generate safety responses.
Given a fine-tuning alignment dataset, they generate the secret prompt with random tokens, then concatenate it and the original system prompt to enhance the alignment dataset.
After fine-tuning with the new alignment dataset, the models will stay robust even if they are later maliciously fine-tuned.
Zheng et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib118" title="">118</a>]</cite> take a deep dive into the intrinsic mechanism of safety system prompt.
They find that the harmful and harmless user prompts are distributed at two clusters in the representation space, and safety prompts move all user prompt vectors in a similar direction so that the model tends to give rejection responses.
Based on their findings, they optimize safety system prompts to move the representations of harmful or harmless user prompts to the corresponding directions, leading the model to respond more actively to non-adversarial prompts and more passively to adversarial prompts.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">内置 LLMs 的系统提示指导着回答的行为、语气和风格，确保模型回答的一致性和适当性。通过明确指示 LLMs，系统提示提高了回答的准确性和相关性，增强了整体用户体验。一系列工作利用系统提示作为保护措施，激活模型以面对恶意用户提示时生成安全回答。Sharma 等人[ 77]介绍了一种特定领域的图表 <math id="S4.SS1.SSS3.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{SPML}"><semantics id="S4.SS1.SSS3.p1.1.m1.1a"><mi id="S4.SS1.SSS3.p1.1.m1.1.1">𝖲𝖯𝖬𝖫</mi><annotation-xml id="S4.SS1.SSS3.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS3.p1.1.m1.1c" encoding="application/x-tex">\mathsf{SPML}</annotation><annotation id="S4.SS1.SSS3.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_SPML</annotation></semantics></math> 来创建强大的系统提示。在 <math id="S4.SS1.SSS3.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{SPML}"><semantics id="S4.SS1.SSS3.p1.2.m2.1a"><mi id="S4.SS1.SSS3.p1.2.m2.1.1">𝖲𝖯𝖬𝖫</mi><annotation-xml id="S4.SS1.SSS3.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS3.p1.2.m2.1c" encoding="application/x-tex">\mathsf{SPML}</annotation><annotation id="S4.SS1.SSS3.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_SPML</annotation></semantics></math> 的编译管道中，系统提示经过类型检查和中间表示转换等多个步骤的处理，最终生成强大的系统提示以应对各种对话场景。Zou 等人[ 126]探索了系统提示对越狱的有效性，并提出了 <math id="S4.SS1.SSS3.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{SMEA}"><semantics id="S4.SS1.SSS3.p1.3.m3.1a"><mi id="S4.SS1.SSS3.p1.3.m3.1.1">𝖲𝖬𝖤𝖠</mi><annotation-xml id="S4.SS1.SSS3.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS1.SSS3.p1.3.m3.1c" encoding="application/x-tex">\mathsf{SMEA}</annotation><annotation id="S4.SS1.SSS3.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_SMEA</annotation></semantics></math> 来生成系统提示。基于遗传算法，他们首先利用通用系统提示作为初始种群，然后通过交叉和改写生成新的个体，最后在适应度评估后选择改进的种群。 王等人[94]将秘密提示整合到系统提示中，以防御基于微调的越狱攻击。由于系统提示对用户不可访问，秘密提示可以作为后门触发器，确保模型生成安全响应。给定一个微调对齐数据集，他们使用随机标记生成秘密提示，然后将它与原始系统提示连接起来，以增强对齐数据集。在对新对齐数据集进行微调后，即使模型后来被恶意微调，它们仍将保持鲁棒性。郑等人[118]深入研究了安全系统提示的内在机制。他们发现有害和无害的用户提示在表示空间中分布在两个簇中，安全提示将所有用户提示向量向相似方向移动，使模型倾向于给出拒绝响应。基于他们的发现，他们优化安全系统提示，将有害或无害用户提示的表示移动到相应的方向，引导模型更积极地响应非对抗性提示，更被动地响应对抗性提示。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p2">
<svg class="ltx_picture" height="111.81" id="S4.SS1.SSS3.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,111.81) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 105.91 C 0 109.17 2.64 111.81 5.91 111.81 L 594.09 111.81 C 597.36 111.81 600 109.17 600 105.91 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 87.7 L 598.03 87.7 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 93.61)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS1.SSS3.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS1.SSS3.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p2.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;4.3</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS1.SSS3.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS1.SSS3.p2.pic1.2.2.2.1.1.1">The System Prompt Safeguard defenses provide universal defense methods adapting to different attacks at a low cost.
However, the system prompts can be vulnerable when the adversary designs purposeful attacks to break the safety guardrail.
The tailored attack and defense may result in a painful long-term mouse-and-cat game between the adversary and defender.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:120%;"> Model-level Defenses<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">模型级防御</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">For a more flexible case in which defenders can access and modify the model weights, model-level defense helps the safety guardrail to generalize better.
Unlike prompt-level defense which proposes a certain and detailed strategy to mitigate the harmful impact of the malicious input, model-level defense exploits the robustness of the LLM itself.
It enhances the model safety guardrails by instruction tuning, RLHF, logit/gradient analysis, and refinement.
Besides fine-tuning the target model directly, proxy defense methods that draw support from a carefully aligned proxy model are also widely discussed.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在防御者可以访问和修改模型权重的更灵活情况下，模型级防御有助于安全护栏更好地泛化。与提出特定详细策略以减轻恶意输入有害影响的提示级防御不同，模型级防御利用 LLM 自身的鲁棒性。它通过指令微调、RLHF、logit/梯度分析和改进来增强模型安全护栏。除了直接微调目标模型外，还广泛讨论了借助精心校准的代理模型提供支持的代理防御方法。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> SFT-based Methods<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于 SFT 的方法</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.4">Supervised Fine-Tuning (SFT) is an important method for enhancing the instruction-following ability of LLMs, which is a crucial part of establishing safety alignment as well&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib92" title="">92</a>]</cite>.
Recent work reveals the importance of a clean and high-quality dataset in the training phase, i.e., models fine-tuned with a comprehensive and refined safety dataset show their superior robustness&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib92" title="">92</a>]</cite>.
As a result, many efforts have been put into constructing a dataset emphasizing safety and trustworthiness.
Bianchi et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib9" title="">9</a>]</cite> discuss how the mixture of safety data (i.e. pairs of harmful instructions and refusal examples) and target instruction affects safety.
For one thing, they show fine-tuning with the mixture of Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib89" title="">89</a>]</cite> and safety data can improve the model safety.
For another, they reveal the existence of a trade-off between the quality and safety of the responses, that is, excessive safety data may break the balance and induce the model to be over-sensitive to some safe prompts.
Deng et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib18" title="">18</a>]</cite> discover the possibility of constructing a safety dataset from the adversarial prompts.
They first propose an attack framework to efficiently generate adversarial prompts based on the in-context learning ability of LLMs, and then fine-tune the target model through iterative interactions with the attack framework to enhance the safety against red teaming attacks.
Similarly, Bhardwaj et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib8" title="">8</a>]</cite> leverage <math alttext="\mathsf{Chain}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.1.m1.1"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mi id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml">𝖢𝗁𝖺𝗂𝗇</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><ci id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">𝖢𝗁𝖺𝗂𝗇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">\mathsf{Chain}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.1.m1.1d">sansserif_Chain</annotation></semantics></math> <math alttext="\mathsf{of}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.2.m2.1"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mi id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml">𝗈𝖿</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><ci id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1">𝗈𝖿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">\mathsf{of}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.2.m2.1d">sansserif_of</annotation></semantics></math> <math alttext="\mathsf{Utterances}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.3.m3.1"><semantics id="S4.SS2.SSS1.p1.3.m3.1a"><mi id="S4.SS2.SSS1.p1.3.m3.1.1" xref="S4.SS2.SSS1.p1.3.m3.1.1.cmml">𝖴𝗍𝗍𝖾𝗋𝖺𝗇𝖼𝖾𝗌</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m3.1b"><ci id="S4.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1">𝖴𝗍𝗍𝖾𝗋𝖺𝗇𝖼𝖾𝗌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.1c">\mathsf{Utterances}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.3.m3.1d">sansserif_Utterances</annotation></semantics></math> (<math alttext="\mathsf{CoU}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.4.m4.1"><semantics id="S4.SS2.SSS1.p1.4.m4.1a"><mi id="S4.SS2.SSS1.p1.4.m4.1.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.cmml">𝖢𝗈𝖴</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.4.m4.1b"><ci id="S4.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1">𝖢𝗈𝖴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.4.m4.1c">\mathsf{CoU}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.4.m4.1d">sansserif_CoU</annotation></semantics></math>) to construct the safety dataset that covers a wide range of harmful conversations generated from ChatGPT.
After being fine-tuned with the dataset, LLMs like Vicuna-7B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib119" title="">119</a>]</cite> can perform well on safety benchmarks while preserving the response quality.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">监督微调（SFT）是增强 LLMs 指令跟随能力的重要方法，这也是建立安全对齐的关键部分[92]。近期的研究揭示了训练阶段中干净且高质量数据集的重要性，即使用全面且精细的安全数据集微调的模型表现出更强的鲁棒性[92]。因此，许多工作都致力于构建一个强调安全性和可信度的数据集。Bianchi 等人[9]讨论了安全数据（即有害指令和拒绝示例的对）与目标指令的混合如何影响安全性。一方面，他们展示了使用 Alpaca[89]和安全数据混合进行微调可以提高模型的安全性。另一方面，他们揭示了响应质量和安全之间的权衡，即过多的安全数据可能会打破平衡并使模型对某些安全提示过于敏感。Deng 等人[18]发现了从对抗性提示中构建安全数据集的可能性。 他们首先提出了一种攻击框架，基于 LLMs 的情境学习能力高效生成对抗性提示，然后通过迭代与攻击框架的交互微调目标模型，以增强对红队攻击的安全性。类似地，Bhardwaj 等人[8]利用 <math id="S4.SS2.SSS1.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{Chain}"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mi id="S4.SS2.SSS1.p1.1.m1.1.1">𝖢𝗁𝖺𝗂𝗇</mi><annotation-xml id="S4.SS2.SSS1.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS2.SSS1.p1.1.m1.1c" encoding="application/x-tex">\mathsf{Chain}</annotation><annotation id="S4.SS2.SSS1.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_Chain</annotation></semantics></math> <math id="S4.SS2.SSS1.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{of}"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mi id="S4.SS2.SSS1.p1.2.m2.1.1">𝗈𝖿</mi><annotation-xml id="S4.SS2.SSS1.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS2.SSS1.p1.2.m2.1c" encoding="application/x-tex">\mathsf{of}</annotation><annotation id="S4.SS2.SSS1.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_of</annotation></semantics></math> <math id="S4.SS2.SSS1.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{Utterances}"><semantics id="S4.SS2.SSS1.p1.3.m3.1a"><mi id="S4.SS2.SSS1.p1.3.m3.1.1">𝖴𝗍𝗍𝖾𝗋𝖺𝗇𝖼𝖾𝗌</mi><annotation-xml id="S4.SS2.SSS1.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS2.SSS1.p1.3.m3.1c" encoding="application/x-tex">\mathsf{Utterances}</annotation><annotation id="S4.SS2.SSS1.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_Utterances</annotation></semantics></math> ( <math id="S4.SS2.SSS1.p1.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{CoU}"><semantics id="S4.SS2.SSS1.p1.4.m4.1a"><mi id="S4.SS2.SSS1.p1.4.m4.1.1">𝖢𝗈𝖴</mi><annotation-xml id="S4.SS2.SSS1.p1.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS2.SSS1.p1.4.m4.1c" encoding="application/x-tex">\mathsf{CoU}</annotation><annotation id="S4.SS2.SSS1.p1.4.m4.1d" encoding="application/x-llamapun">sansserif_CoU</annotation></semantics></math> )构建了涵盖 ChatGPT 生成的广泛有害对话的安全数据集。在用该数据集微调后，像 Vicuna-7B[119]这样的 LLMs 可以在保持响应质量的同时，在安全基准测试中表现良好。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p2">
<svg class="ltx_picture" height="223.36" id="S4.SS2.SSS1.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,223.36) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 217.45 C 0 220.71 2.64 223.36 5.91 223.36 L 594.09 223.36 C 597.36 223.36 600 220.71 600 217.45 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 199.24 L 598.03 199.24 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 205.15)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.SSS1.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.SSS1.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;4.4</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="173.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.SSS1.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.SSS1.p2.pic1.2.2.2.1.1.1">SFT with safety instructions is a direct and effective method to enhance the safety of LLMs.
Meanwhile, the cost of time and money of the training phase is moderate.
However, it has several drawbacks: Firstly, a significant challenge in this paradigm is catastrophic forgetting, in which a model forgets previous knowledge due to parameter updates during the safety alignment, leading to decreased performance on general tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib9" title="">9</a>]</cite>.
Secondly, although the cost of running SFT is moderate, the collection of high-quality safety instructions is expensive&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib92" title="">92</a>]</cite>.
Thirdly, recent work has revealed the vulnerability of the alignment and showed a few harmful demonstrations can increase the jailbreak rate by a large extent&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib68" title="">68</a>]</cite>.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> RLHF-based Methods<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基于 RLHF 的方法</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Reinforcement Learning from Human Feedback (RLHF) is a traditional model training procedure applied to a well-pre-trained language model to further align model behavior with human preferences and instructions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib66" title="">66</a>]</cite>.
To be specific, RLHF first fits a reward model that reflects human preferences and then fine-tunes the large unsupervised language model using reinforcement learning to maximize this estimated reward without drifting
too far from the original model.
The effectiveness of RLHF in safety alignment has been proved by lots of promising LLMs such as GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib65" title="">65</a>]</cite>, Llama&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib92" title="">92</a>]</cite>, and Claude&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib4" title="">4</a>]</cite>.
On the one hand, high-quality human preference datasets lie in the key point of successful training, whereby human annotators select which of two model outputs they prefer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib39" title="">39</a>]</cite>.
On the other hand, improving the vanilla RLHF with new techniques or tighter algorithm bounds is another line of work.
Bai et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib6" title="">6</a>]</cite> introduce an online version of RLHF that collects preference data while training the language model synchronously. The online RLHF has been deployed in Claude&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib4" title="">4</a>]</cite> and gets competitive results.
Siththaranjan et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib83" title="">83</a>]</cite> reveal that the hidden context of incomplete data (e.g. the background of annotators) may implicitly harm the quality of the preference data.
Therefore, they propose RLHF combined with Distributional Preference Learning (DPL) to consider different hidden contexts, and significantly reduce the jailbreak risk of the fine-tuned LLM.
While RLHF is a complex and often unstable procedure, recent work proposes Direct Preference Optimization (DPO)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib70" title="">70</a>]</cite> as a substitute.
As a more stable and lightweight method, enhancing the safety of LLMs with DPO is becoming more popular&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib59" title="">59</a>]</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">人类反馈强化学习（RLHF）是一种传统的模型训练流程，应用于预训练良好的语言模型，以进一步使模型行为与人类偏好和指令保持一致[66]。具体来说，RLHF 首先拟合一个反映人类偏好的奖励模型，然后使用强化学习对大型无监督语言模型进行微调，以在不偏离原始模型太远的情况下最大化估计的奖励。RLHF 在安全对齐方面的有效性已被许多有前景的 LLMs（如 GPT-4[65]、Llama[92]和 Claude[4]）所证明。一方面，高质量的人类偏好数据集是成功训练的关键点，其中人类标注者选择他们更倾向于两个模型输出中的哪一个[6, 26, 58, 39]。另一方面，通过新技术或更严格的算法界限改进原味 RLHF 是另一条研究路线。Bai 等人[6]介绍了一种在线版本的 RLHF，它在训练语言模型的同时同步收集偏好数据。在线 RLHF 已在 Claude[4]中得到部署，并取得了具有竞争力的结果。 Siththaranjan 等人[ 83]揭示，不完整数据的隐藏上下文（例如标注者的背景）可能会隐含地损害偏好数据的质量。因此，他们提出将 RLHF 与分布偏好学习（DPL）相结合，以考虑不同的隐藏上下文，并显著降低微调 LLM 的越狱风险。虽然 RLHF 是一个复杂且往往不稳定的流程，近期的研究提出了直接偏好优化（DPO）[ 70]作为替代方案。作为一种更稳定且轻量级的方法，使用 DPO 增强 LLM 的安全性正变得越来越流行[ 25, 59]。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p2">
<svg class="ltx_picture" height="179.77" id="S4.SS2.SSS2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,179.77) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 173.86 C 0 177.12 2.64 179.77 5.91 179.77 L 594.09 179.77 C 597.36 179.77 600 177.12 600 173.86 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 155.66 L 598.03 155.66 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 161.56)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.SSS2.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.SSS2.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p2.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;4.5</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="130.07" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.SSS2.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.SSS2.p2.pic1.2.2.2.1.1.1">As one of the most widely used methods to improve model safety, the advantages of RLHF lie in (1) the LLMs trained with RLHF show significant improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions;
(2) the preference data is easier and cheaper to collect compared to the high-quality professional safety instruction data.
However, it has several drawbacks: First, the training process of RLHF is time-consuming because the reward model needs the generation result to calculate the score, thus making the training extremely slow.
Second, similar to SFT, the expensive safety alignment can be bypassed easily&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib68" title="">68</a>]</cite>.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Gradient and Logit Analysis<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">梯度与逻辑分析</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Since the logits and gradients retrieved in the forward pass can contain fruitful information about the beliefs and judgments of the input prompts, which can be useful for model defense, defenders can analyze and manipulate the logits and gradients to detect potential jailbreak threats and propose corresponding defenses.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">由于在正向传递中检索到的 logits 和梯度包含了关于输入提示的信念和判断的有价值信息，这些信息可用于模型防御，因此防御者可以分析和操纵 logits 和梯度，以检测潜在的越狱威胁并提出相应的防御措施。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.2"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p2.2.1">Gradient Analysis.</span>
Gradient analysis-based defenses extract information from the gradient in the forward pass and treat the processed logits or gradients as a feature for classification. Xie et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib101" title="">101</a>]</cite> compare the similarity between safety-critical parameters and gradients.
Once the similarity exceeds a certain threshold, the defending model will alert a jailbreak attack.
Hu et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib35" title="">35</a>]</cite> first define a refusal loss which indicates the likelihood of generating a normal response and notice that there is a difference between the refusal loss obtained by malicious prompts and normal prompts.
Based on this discovery, they further propose <math alttext="\mathsf{Gradient}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.1.m1.1"><semantics id="S4.SS2.SSS3.p2.1.m1.1a"><mi id="S4.SS2.SSS3.p2.1.m1.1.1" xref="S4.SS2.SSS3.p2.1.m1.1.1.cmml">𝖦𝗋𝖺𝖽𝗂𝖾𝗇𝗍</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.1.m1.1b"><ci id="S4.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1">𝖦𝗋𝖺𝖽𝗂𝖾𝗇𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.1.m1.1c">\mathsf{Gradient}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.1.m1.1d">sansserif_Gradient</annotation></semantics></math> <math alttext="\mathsf{Cuff}" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.2.m2.1"><semantics id="S4.SS2.SSS3.p2.2.m2.1a"><mi id="S4.SS2.SSS3.p2.2.m2.1.1" xref="S4.SS2.SSS3.p2.2.m2.1.1.cmml">𝖢𝗎𝖿𝖿</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.2.m2.1b"><ci id="S4.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1">𝖢𝗎𝖿𝖿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.2.m2.1c">\mathsf{Cuff}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.2.m2.1d">sansserif_Cuff</annotation></semantics></math> to identify jailbreak attacks by computing the gradient norm and other characteristics of refusal loss.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">梯度分析。基于梯度的防御从正向传递中的梯度中提取信息，并将处理后的 logits 或梯度作为分类的特征。Xie 等人[101]比较了安全关键参数与梯度之间的相似性。一旦相似性超过某个阈值，防御模型就会警告存在越狱攻击。Hu 等人[35]首先定义了一个拒绝损失，该损失表示生成正常响应的可能性，并注意到恶意提示获得的拒绝损失与正常提示获得的拒绝损失之间存在差异。基于这一发现，他们进一步提出了 <math id="S4.SS2.SSS3.p2.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{Gradient}"><semantics id="S4.SS2.SSS3.p2.1.m1.1a"><mi id="S4.SS2.SSS3.p2.1.m1.1.1">𝖦𝗋𝖺𝖽𝗂𝖾𝗇𝗍</mi><annotation-xml id="S4.SS2.SSS3.p2.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS2.SSS3.p2.1.m1.1c" encoding="application/x-tex">\mathsf{Gradient}</annotation><annotation id="S4.SS2.SSS3.p2.1.m1.1d" encoding="application/x-llamapun">sansserif_Gradient</annotation></semantics></math> <math id="S4.SS2.SSS3.p2.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{Cuff}"><semantics id="S4.SS2.SSS3.p2.2.m2.1a"><mi id="S4.SS2.SSS3.p2.2.m2.1.1">𝖢𝗎𝖿𝖿</mi><annotation-xml id="S4.SS2.SSS3.p2.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS2.SSS3.p2.2.m2.1c" encoding="application/x-tex">\mathsf{Cuff}</annotation><annotation id="S4.SS2.SSS3.p2.2.m2.1d" encoding="application/x-llamapun">sansserif_Cuff</annotation></semantics></math> 来通过计算拒绝损失的梯度范数和其他特征来识别越狱攻击。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p3.1.1">Logit Analysis.</span>
Logit analysis-based defenses aim to develop new decoding algorithms, i.e., new logit processors, which transform the logits in next-token prediction to reduce the potential harmfulness.
For instance, Xu et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib102" title="">102</a>]</cite> mix the output logits of the target model and safety-aligned model to obtain a new logits distribution, in which the probability density of harmful and benign tokens are attenuated and amplified, respectively.
Li et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib53" title="">53</a>]</cite> add a safety heuristic in beam search, which evaluates the harmfulness of the candidates in one round and selects the one with the lowest harmful score.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">Logit 分析。基于 Logit 分析的防御方法旨在开发新的解码算法，即新的 Logit 处理器，这些处理器将下一个词预测中的 Logit 转换为降低潜在危害性。例如，Xu 等人[102]将目标模型和安全对齐模型的输出 Logit 混合，以获得一个新的 Logit 分布，其中有害标记和良性标记的概率密度分别被减弱和增强。Li 等人[53]在集束搜索中添加了一个安全启发式算法，该算法在一轮中评估候选词的危害性，并选择危害评分最低的那个。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p4">
<svg class="ltx_picture" height="178.23" id="S4.SS2.SSS3.p4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,178.23) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 172.33 C 0 175.59 2.64 178.23 5.91 178.23 L 594.09 178.23 C 597.36 178.23 600 175.59 600 172.33 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 154.12 L 598.03 154.12 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 160.03)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.SSS3.p4.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.SSS3.p4.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p4.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;4.6</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.SSS3.p4.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.SSS3.p4.pic1.2.2.2.1.1.1">Logit and gradient analysis does not require updating the model weights thus making it a cheap and fast detecting method.
The gradient-based method trains a classifier and predicts the jailbreak result.
However, since the classifier is trained only on a given dataset, concerns regarding generalizability arise when used in an out-of-distribution (OOD) scenario.
Moreover, intended adversarial attacks can hijack the detecting process and fail the analysis.
The logit-based method aims to propose new decoding algorithms to reduce the harmfulness. Despite a higher attack success rate, the readability of the defending prompts might be low. The additional calculation in decoding influences the inference speed as well.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.20.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.21.2" style="font-size:90%;">Overview of evaluation datasets.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 2：评估数据集概述。</font></font></font></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.18" style="width:433.6pt;height:256.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.6pt,15.7pt) scale(0.89077188473302,0.89077188473302) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.18.18">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.18.18.19.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T2.18.18.19.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.18.18.19.1.1.1">
<span class="ltx_p" id="S4.T2.18.18.19.1.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.18.18.19.1.1.1.1.1">Benchmark Name<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">基准名称</font></font></font></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T2.18.18.19.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.18.18.19.1.2.1">
<span class="ltx_p" id="S4.T2.18.18.19.1.2.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.18.18.19.1.2.1.1.1">Languages<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">语言</font></font></font></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T2.18.18.19.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.18.18.19.1.3.1">
<span class="ltx_p" id="S4.T2.18.18.19.1.3.1.1" style="width:28.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.18.18.19.1.3.1.1.1">Size<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">大小</font></font></font></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T2.18.18.19.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.18.18.19.1.4.1">
<span class="ltx_p" id="S4.T2.18.18.19.1.4.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.18.18.19.1.4.1.1.1">Safety Dimensions<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">安全维度</font></font></font></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T2.18.18.19.1.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.18.18.19.1.5.1">
<span class="ltx_p" id="S4.T2.18.18.19.1.5.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.18.18.19.1.5.1.1.1">Composition<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">组合</font></font></font></span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.1.1.1">
<span class="ltx_p" id="S4.T2.1.1.1.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{XSTEST}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.1.m1.1a"><mi id="S4.T2.1.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.1.m1.1.1.cmml">𝖷𝖲𝖳𝖤𝖲𝖳</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.m1.1.1">𝖷𝖲𝖳𝖤𝖲𝖳</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.m1.1c">\mathsf{XSTEST}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.1.m1.1d">sansserif_XSTEST</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib74" title="">74</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.1.2.1">
<span class="ltx_p" id="S4.T2.1.1.1.2.1.1" style="width:56.9pt;">English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">英文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.1.3.1">
<span class="ltx_p" id="S4.T2.1.1.1.3.1.1" style="width:28.5pt;">450</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.1.4.1">
<span class="ltx_p" id="S4.T2.1.1.1.4.1.1" style="width:85.4pt;">10</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.1.5.1">
<span class="ltx_p" id="S4.T2.1.1.1.5.1.1" style="width:170.7pt;">Safe questions and unsafe questions<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">安全问题和不安全问题</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.2.1.1">
<span class="ltx_p" id="S4.T2.2.2.2.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{AdvBench}" class="ltx_Math" display="inline" id="S4.T2.2.2.2.1.1.1.m1.1"><semantics id="S4.T2.2.2.2.1.1.1.m1.1a"><mi id="S4.T2.2.2.2.1.1.1.m1.1.1" xref="S4.T2.2.2.2.1.1.1.m1.1.1.cmml">𝖠𝖽𝗏𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.1.1.m1.1b"><ci id="S4.T2.2.2.2.1.1.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.1.1.m1.1.1">𝖠𝖽𝗏𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.1.1.m1.1c">\mathsf{AdvBench}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.1.1.1.m1.1d">sansserif_AdvBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib125" title="">125</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.2.2.1">
<span class="ltx_p" id="S4.T2.2.2.2.2.1.1" style="width:56.9pt;">English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">英文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.2.3.1">
<span class="ltx_p" id="S4.T2.2.2.2.3.1.1" style="width:28.5pt;">1000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.2.4.1">
<span class="ltx_p" id="S4.T2.2.2.2.4.1.1" style="width:85.4pt;">8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.2.5.1">
<span class="ltx_p" id="S4.T2.2.2.2.5.1.1" style="width:170.7pt;">Harmful strings and harmful behaviors<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">有害字符串和有害行为 </font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.3.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.1.1">
<span class="ltx_p" id="S4.T2.3.3.3.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{SafeBench}" class="ltx_Math" display="inline" id="S4.T2.3.3.3.1.1.1.m1.1"><semantics id="S4.T2.3.3.3.1.1.1.m1.1a"><mi id="S4.T2.3.3.3.1.1.1.m1.1.1" xref="S4.T2.3.3.3.1.1.1.m1.1.1.cmml">𝖲𝖺𝖿𝖾𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.1.1.m1.1b"><ci id="S4.T2.3.3.3.1.1.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.1.1.m1.1.1">𝖲𝖺𝖿𝖾𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.1.1.m1.1c">\mathsf{SafeBench}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.1.1.1.m1.1d">sansserif_SafeBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib30" title="">30</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.3.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.2.1">
<span class="ltx_p" id="S4.T2.3.3.3.2.1.1" style="width:56.9pt;">English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">英文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.3.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.3.1">
<span class="ltx_p" id="S4.T2.3.3.3.3.1.1" style="width:28.5pt;">500</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.3.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.4.1">
<span class="ltx_p" id="S4.T2.3.3.3.4.1.1" style="width:85.4pt;">10</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.3.3.3.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.3.5.1">
<span class="ltx_p" id="S4.T2.3.3.3.5.1.1" style="width:170.7pt;">Unsafe questions<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">不安全的提问</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.6.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.6.6.6.3.3">
<span class="ltx_p" id="S4.T2.6.6.6.3.3.3" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{Do}" class="ltx_Math" display="inline" id="S4.T2.4.4.4.1.1.1.m1.1"><semantics id="S4.T2.4.4.4.1.1.1.m1.1a"><mi id="S4.T2.4.4.4.1.1.1.m1.1.1" xref="S4.T2.4.4.4.1.1.1.m1.1.1.cmml">𝖣𝗈</mi><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.1.1.m1.1b"><ci id="S4.T2.4.4.4.1.1.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.1.1.m1.1.1">𝖣𝗈</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.1.1.m1.1c">\mathsf{Do}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.1.1.1.m1.1d">sansserif_Do</annotation></semantics></math>-<math alttext="\mathsf{Not}" class="ltx_centering" display="inline" id="S4.T2.5.5.5.2.2.2.m2.1"><semantics id="S4.T2.5.5.5.2.2.2.m2.1a"><mi id="S4.T2.5.5.5.2.2.2.m2.1.1" xref="S4.T2.5.5.5.2.2.2.m2.1.1.cmml">𝖭𝗈𝗍</mi><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.2.2.2.m2.1b"><ci id="S4.T2.5.5.5.2.2.2.m2.1.1.cmml" xref="S4.T2.5.5.5.2.2.2.m2.1.1">𝖭𝗈𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.2.2.2.m2.1c">\mathsf{Not}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.2.2.2.m2.1d">sansserif_Not</annotation></semantics></math>-<math alttext="\mathsf{Answer}" class="ltx_centering" display="inline" id="S4.T2.6.6.6.3.3.3.m3.1"><semantics id="S4.T2.6.6.6.3.3.3.m3.1a"><mi id="S4.T2.6.6.6.3.3.3.m3.1.1" xref="S4.T2.6.6.6.3.3.3.m3.1.1.cmml">𝖠𝗇𝗌𝗐𝖾𝗋</mi><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.3.3.3.m3.1b"><ci id="S4.T2.6.6.6.3.3.3.m3.1.1.cmml" xref="S4.T2.6.6.6.3.3.3.m3.1.1">𝖠𝗇𝗌𝗐𝖾𝗋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.3.3.3.m3.1c">\mathsf{Answer}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.3.3.3.m3.1d">sansserif_Answer</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib96" title="">96</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.6.6.6.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.6.6.6.4.1">
<span class="ltx_p" id="S4.T2.6.6.6.4.1.1" style="width:56.9pt;">English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">英文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.6.6.6.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.6.6.6.5.1">
<span class="ltx_p" id="S4.T2.6.6.6.5.1.1" style="width:28.5pt;">939</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.6.6.6.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.6.6.6.6.1">
<span class="ltx_p" id="S4.T2.6.6.6.6.1.1" style="width:85.4pt;">5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.6.6.6.7">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.6.6.6.7.1">
<span class="ltx_p" id="S4.T2.6.6.6.7.1.1" style="width:170.7pt;">Harmful instructions<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">有害指令 </font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.7.7.7.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.7.7.1.1">
<span class="ltx_p" id="S4.T2.7.7.7.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{TechHazardQA}" class="ltx_Math" display="inline" id="S4.T2.7.7.7.1.1.1.m1.1"><semantics id="S4.T2.7.7.7.1.1.1.m1.1a"><mi id="S4.T2.7.7.7.1.1.1.m1.1.1" xref="S4.T2.7.7.7.1.1.1.m1.1.1.cmml">𝖳𝖾𝖼𝗁𝖧𝖺𝗓𝖺𝗋𝖽𝖰𝖠</mi><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.1.1.1.m1.1b"><ci id="S4.T2.7.7.7.1.1.1.m1.1.1.cmml" xref="S4.T2.7.7.7.1.1.1.m1.1.1">𝖳𝖾𝖼𝗁𝖧𝖺𝗓𝖺𝗋𝖽𝖰𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.1.1.1.m1.1c">\mathsf{TechHazardQA}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.7.1.1.1.m1.1d">sansserif_TechHazardQA</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib7" title="">7</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.7.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.7.7.2.1">
<span class="ltx_p" id="S4.T2.7.7.7.2.1.1" style="width:56.9pt;">English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">英文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.7.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.7.7.3.1">
<span class="ltx_p" id="S4.T2.7.7.7.3.1.1" style="width:28.5pt;">1850</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.7.7.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.7.7.4.1">
<span class="ltx_p" id="S4.T2.7.7.7.4.1.1" style="width:85.4pt;">7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.7.7.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.7.7.7.5.1">
<span class="ltx_p" id="S4.T2.7.7.7.5.1.1" style="width:170.7pt;">I nstruction-centric questions<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">指令中心型问题</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.9.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.9.9.9.2.2">
<span class="ltx_p" id="S4.T2.9.9.9.2.2.2" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{SC}" class="ltx_Math" display="inline" id="S4.T2.8.8.8.1.1.1.m1.1"><semantics id="S4.T2.8.8.8.1.1.1.m1.1a"><mi id="S4.T2.8.8.8.1.1.1.m1.1.1" xref="S4.T2.8.8.8.1.1.1.m1.1.1.cmml">𝖲𝖢</mi><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.1.1.1.m1.1b"><ci id="S4.T2.8.8.8.1.1.1.m1.1.1.cmml" xref="S4.T2.8.8.8.1.1.1.m1.1.1">𝖲𝖢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.1.1.1.m1.1c">\mathsf{SC}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.8.1.1.1.m1.1d">sansserif_SC</annotation></semantics></math>-<math alttext="\mathsf{Safety}" class="ltx_centering" display="inline" id="S4.T2.9.9.9.2.2.2.m2.1"><semantics id="S4.T2.9.9.9.2.2.2.m2.1a"><mi id="S4.T2.9.9.9.2.2.2.m2.1.1" xref="S4.T2.9.9.9.2.2.2.m2.1.1.cmml">𝖲𝖺𝖿𝖾𝗍𝗒</mi><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.2.2.2.m2.1b"><ci id="S4.T2.9.9.9.2.2.2.m2.1.1.cmml" xref="S4.T2.9.9.9.2.2.2.m2.1.1">𝖲𝖺𝖿𝖾𝗍𝗒</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.2.2.2.m2.1c">\mathsf{Safety}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.9.2.2.2.m2.1d">sansserif_Safety</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib86" title="">86</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.9.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.9.9.9.3.1">
<span class="ltx_p" id="S4.T2.9.9.9.3.1.1" style="width:56.9pt;">Chinese<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">中文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.9.9.9.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.9.9.9.4.1">
<span class="ltx_p" id="S4.T2.9.9.9.4.1.1" style="width:28.5pt;">4912</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.9.9.9.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.9.9.9.5.1">
<span class="ltx_p" id="S4.T2.9.9.9.5.1.1" style="width:85.4pt;">20+</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.9.9.9.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.9.9.9.6.1">
<span class="ltx_p" id="S4.T2.9.9.9.6.1.1" style="width:170.7pt;">Multi-round conversations<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">多轮对话</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.10.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.10.10.10.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.10.10.10.1.1">
<span class="ltx_p" id="S4.T2.10.10.10.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{LatentJailbreak}" class="ltx_Math" display="inline" id="S4.T2.10.10.10.1.1.1.m1.1"><semantics id="S4.T2.10.10.10.1.1.1.m1.1a"><mi id="S4.T2.10.10.10.1.1.1.m1.1.1" xref="S4.T2.10.10.10.1.1.1.m1.1.1.cmml">𝖫𝖺𝗍𝖾𝗇𝗍𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</mi><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.1.1.1.m1.1b"><ci id="S4.T2.10.10.10.1.1.1.m1.1.1.cmml" xref="S4.T2.10.10.10.1.1.1.m1.1.1">𝖫𝖺𝗍𝖾𝗇𝗍𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.1.1.1.m1.1c">\mathsf{LatentJailbreak}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.10.10.1.1.1.m1.1d">sansserif_LatentJailbreak</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib69" title="">69</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.10.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.10.10.10.2.1">
<span class="ltx_p" id="S4.T2.10.10.10.2.1.1" style="width:56.9pt;" data-imt_insert_failed="1">Chinese English</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.10.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.10.10.10.3.1">
<span class="ltx_p" id="S4.T2.10.10.10.3.1.1" style="width:28.5pt;">416</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.10.10.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.10.10.10.4.1">
<span class="ltx_p" id="S4.T2.10.10.10.4.1.1" style="width:85.4pt;">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.10.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.10.10.10.5.1">
<span class="ltx_p" id="S4.T2.10.10.10.5.1.1" style="width:170.7pt;">Translation tasks<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">翻译任务</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.11.11.11.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.11.11.11.1.1">
<span class="ltx_p" id="S4.T2.11.11.11.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{SafetyBench}" class="ltx_Math" display="inline" id="S4.T2.11.11.11.1.1.1.m1.1"><semantics id="S4.T2.11.11.11.1.1.1.m1.1a"><mi id="S4.T2.11.11.11.1.1.1.m1.1.1" xref="S4.T2.11.11.11.1.1.1.m1.1.1.cmml">𝖲𝖺𝖿𝖾𝗍𝗒𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.1.1.1.m1.1b"><ci id="S4.T2.11.11.11.1.1.1.m1.1.1.cmml" xref="S4.T2.11.11.11.1.1.1.m1.1.1">𝖲𝖺𝖿𝖾𝗍𝗒𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.1.1.1.m1.1c">\mathsf{SafetyBench}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.11.11.1.1.1.m1.1d">sansserif_SafetyBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib115" title="">115</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.11.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.11.11.11.2.1">
<span class="ltx_p" id="S4.T2.11.11.11.2.1.1" style="width:56.9pt;">Chinese English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">中文 英语 </font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.11.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.11.11.11.3.1">
<span class="ltx_p" id="S4.T2.11.11.11.3.1.1" style="width:28.5pt;">11435</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.11.11.11.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.11.11.11.4.1">
<span class="ltx_p" id="S4.T2.11.11.11.4.1.1" style="width:85.4pt;">7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.11.11.11.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.11.11.11.5.1">
<span class="ltx_p" id="S4.T2.11.11.11.5.1.1" style="width:170.7pt;">Multiple choice questions<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">选择题</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.12.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.12.12.12.1.1">
<span class="ltx_p" id="S4.T2.12.12.12.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{StrongREJECT}" class="ltx_Math" display="inline" id="S4.T2.12.12.12.1.1.1.m1.1"><semantics id="S4.T2.12.12.12.1.1.1.m1.1a"><mi id="S4.T2.12.12.12.1.1.1.m1.1.1" xref="S4.T2.12.12.12.1.1.1.m1.1.1.cmml">𝖲𝗍𝗋𝗈𝗇𝗀𝖱𝖤𝖩𝖤𝖢𝖳</mi><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.12.1.1.1.m1.1b"><ci id="S4.T2.12.12.12.1.1.1.m1.1.1.cmml" xref="S4.T2.12.12.12.1.1.1.m1.1.1">𝖲𝗍𝗋𝗈𝗇𝗀𝖱𝖤𝖩𝖤𝖢𝖳</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.12.1.1.1.m1.1c">\mathsf{StrongREJECT}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.12.12.1.1.1.m1.1d">sansserif_StrongREJECT</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib84" title="">84</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.12.12.12.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.12.12.12.2.1">
<span class="ltx_p" id="S4.T2.12.12.12.2.1.1" style="width:56.9pt;">English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">英文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.12.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.12.12.12.3.1">
<span class="ltx_p" id="S4.T2.12.12.12.3.1.1" style="width:28.5pt;">346</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.12.12.12.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.12.12.12.4.1">
<span class="ltx_p" id="S4.T2.12.12.12.4.1.1" style="width:85.4pt;">6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.12.12.12.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.12.12.12.5.1">
<span class="ltx_p" id="S4.T2.12.12.12.5.1.1" style="width:170.7pt;">Unsafe questions<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">不安全的提问</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.13.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.13.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.13.13.13.1.1">
<span class="ltx_p" id="S4.T2.13.13.13.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{AttackEval}" class="ltx_Math" display="inline" id="S4.T2.13.13.13.1.1.1.m1.1"><semantics id="S4.T2.13.13.13.1.1.1.m1.1a"><mi id="S4.T2.13.13.13.1.1.1.m1.1.1" xref="S4.T2.13.13.13.1.1.1.m1.1.1.cmml">𝖠𝗍𝗍𝖺𝖼𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.13.1.1.1.m1.1b"><ci id="S4.T2.13.13.13.1.1.1.m1.1.1.cmml" xref="S4.T2.13.13.13.1.1.1.m1.1.1">𝖠𝗍𝗍𝖺𝖼𝗄𝖤𝗏𝖺𝗅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.13.1.1.1.m1.1c">\mathsf{AttackEval}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.13.13.13.1.1.1.m1.1d">sansserif_AttackEval</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib80" title="">80</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.13.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.13.13.13.2.1">
<span class="ltx_p" id="S4.T2.13.13.13.2.1.1" style="width:56.9pt;">English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">英文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.13.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.13.13.13.3.1">
<span class="ltx_p" id="S4.T2.13.13.13.3.1.1" style="width:28.5pt;">390</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.13.13.13.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.13.13.13.4.1">
<span class="ltx_p" id="S4.T2.13.13.13.4.1.1" style="width:85.4pt;">13</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.13.13.13.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.13.13.13.5.1">
<span class="ltx_p" id="S4.T2.13.13.13.5.1.1" style="width:170.7pt;">Unsafe questions<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">不安全的提问</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.14.14.14">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.14.14.14.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.14.14.14.1.1">
<span class="ltx_p" id="S4.T2.14.14.14.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{HarmBench}" class="ltx_Math" display="inline" id="S4.T2.14.14.14.1.1.1.m1.1"><semantics id="S4.T2.14.14.14.1.1.1.m1.1a"><mi id="S4.T2.14.14.14.1.1.1.m1.1.1" xref="S4.T2.14.14.14.1.1.1.m1.1.1.cmml">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.14.1.1.1.m1.1b"><ci id="S4.T2.14.14.14.1.1.1.m1.1.1.cmml" xref="S4.T2.14.14.14.1.1.1.m1.1.1">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.14.1.1.1.m1.1c">\mathsf{HarmBench}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.14.14.1.1.1.m1.1d">sansserif_HarmBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib63" title="">63</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.14.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.14.14.14.2.1">
<span class="ltx_p" id="S4.T2.14.14.14.2.1.1" style="width:56.9pt;">English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">英文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.14.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.14.14.14.3.1">
<span class="ltx_p" id="S4.T2.14.14.14.3.1.1" style="width:28.5pt;">510</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.14.14.14.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.14.14.14.4.1">
<span class="ltx_p" id="S4.T2.14.14.14.4.1.1" style="width:85.4pt;">18</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.14.14.14.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.14.14.14.5.1">
<span class="ltx_p" id="S4.T2.14.14.14.5.1.1" style="width:170.7pt;">Harmful behaviors<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">有害行为</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.16.16.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.16.16.16.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.16.16.16.2.2">
<span class="ltx_p" id="S4.T2.16.16.16.2.2.2" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{Safety}" class="ltx_Math" display="inline" id="S4.T2.15.15.15.1.1.1.m1.1"><semantics id="S4.T2.15.15.15.1.1.1.m1.1a"><mi id="S4.T2.15.15.15.1.1.1.m1.1.1" xref="S4.T2.15.15.15.1.1.1.m1.1.1.cmml">𝖲𝖺𝖿𝖾𝗍𝗒</mi><annotation-xml encoding="MathML-Content" id="S4.T2.15.15.15.1.1.1.m1.1b"><ci id="S4.T2.15.15.15.1.1.1.m1.1.1.cmml" xref="S4.T2.15.15.15.1.1.1.m1.1.1">𝖲𝖺𝖿𝖾𝗍𝗒</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.15.15.1.1.1.m1.1c">\mathsf{Safety}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.15.15.15.1.1.1.m1.1d">sansserif_Safety</annotation></semantics></math>-<math alttext="\mathsf{Prompts}" class="ltx_centering" display="inline" id="S4.T2.16.16.16.2.2.2.m2.1"><semantics id="S4.T2.16.16.16.2.2.2.m2.1a"><mi id="S4.T2.16.16.16.2.2.2.m2.1.1" xref="S4.T2.16.16.16.2.2.2.m2.1.1.cmml">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</mi><annotation-xml encoding="MathML-Content" id="S4.T2.16.16.16.2.2.2.m2.1b"><ci id="S4.T2.16.16.16.2.2.2.m2.1.1.cmml" xref="S4.T2.16.16.16.2.2.2.m2.1.1">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.16.16.2.2.2.m2.1c">\mathsf{Prompts}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.16.16.16.2.2.2.m2.1d">sansserif_Prompts</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib86" title="">86</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.16.16.16.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.16.16.16.3.1">
<span class="ltx_p" id="S4.T2.16.16.16.3.1.1" style="width:56.9pt;">Chinese<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">中文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.16.16.16.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.16.16.16.4.1">
<span class="ltx_p" id="S4.T2.16.16.16.4.1.1" style="width:28.5pt;">100000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.16.16.16.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.16.16.16.5.1">
<span class="ltx_p" id="S4.T2.16.16.16.5.1.1" style="width:85.4pt;">14</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.16.16.16.6">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.16.16.16.6.1">
<span class="ltx_p" id="S4.T2.16.16.16.6.1.1" style="width:170.7pt;">Harmful behaviors<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">有害行为</font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.17.17.17">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.17.17.17.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.17.17.17.1.1">
<span class="ltx_p" id="S4.T2.17.17.17.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{JailbreakBench}" class="ltx_Math" display="inline" id="S4.T2.17.17.17.1.1.1.m1.1"><semantics id="S4.T2.17.17.17.1.1.1.m1.1a"><mi id="S4.T2.17.17.17.1.1.1.m1.1.1" xref="S4.T2.17.17.17.1.1.1.m1.1.1.cmml">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.17.1.1.1.m1.1b"><ci id="S4.T2.17.17.17.1.1.1.m1.1.1.cmml" xref="S4.T2.17.17.17.1.1.1.m1.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.17.1.1.1.m1.1c">\mathsf{JailbreakBench}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.17.17.17.1.1.1.m1.1d">sansserif_JailbreakBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib14" title="">14</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.17.17.17.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.17.17.17.2.1">
<span class="ltx_p" id="S4.T2.17.17.17.2.1.1" style="width:56.9pt;">English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">英文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.17.17.17.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.17.17.17.3.1">
<span class="ltx_p" id="S4.T2.17.17.17.3.1.1" style="width:28.5pt;">200</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.17.17.17.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.17.17.17.4.1">
<span class="ltx_p" id="S4.T2.17.17.17.4.1.1" style="width:85.4pt;">10</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.17.17.17.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.17.17.17.5.1">
<span class="ltx_p" id="S4.T2.17.17.17.5.1.1" style="width:170.7pt;">Harmful behaviors and benign behaviors<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">有害行为和良性行为 </font></font></font></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.18.18.18">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.18.18.18.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.18.18.18.1.1">
<span class="ltx_p" id="S4.T2.18.18.18.1.1.1" style="width:85.4pt;" data-imt_insert_failed="1"><math alttext="\mathsf{DoAnythingNow}" class="ltx_Math" display="inline" id="S4.T2.18.18.18.1.1.1.m1.1"><semantics id="S4.T2.18.18.18.1.1.1.m1.1a"><mi id="S4.T2.18.18.18.1.1.1.m1.1.1" xref="S4.T2.18.18.18.1.1.1.m1.1.1.cmml">𝖣𝗈𝖠𝗇𝗒𝗍𝗁𝗂𝗇𝗀𝖭𝗈𝗐</mi><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.18.1.1.1.m1.1b"><ci id="S4.T2.18.18.18.1.1.1.m1.1.1.cmml" xref="S4.T2.18.18.18.1.1.1.m1.1.1">𝖣𝗈𝖠𝗇𝗒𝗍𝗁𝗂𝗇𝗀𝖭𝗈𝗐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.18.1.1.1.m1.1c">\mathsf{DoAnythingNow}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.18.18.18.1.1.1.m1.1d">sansserif_DoAnythingNow</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_centering ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib79" title="">79</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.18.18.18.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.18.18.18.2.1">
<span class="ltx_p" id="S4.T2.18.18.18.2.1.1" style="width:56.9pt;">English<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">英文</font></font></font></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.18.18.18.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.18.18.18.3.1">
<span class="ltx_p" id="S4.T2.18.18.18.3.1.1" style="width:28.5pt;">107250</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.18.18.18.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.18.18.18.4.1">
<span class="ltx_p" id="S4.T2.18.18.18.4.1.1" style="width:85.4pt;">13</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.18.18.18.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.18.18.18.5.1">
<span class="ltx_p" id="S4.T2.18.18.18.5.1.1" style="width:170.7pt;">Forbidden questions<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">禁止性问题</font></font></font></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Refinement Methods<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">精炼方法</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">The refinement methods exploit the self-correction ability of LLM to reduce the risk of generating illegal responses.
As evidenced in <math alttext="\mathsf{RLAIF}" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p1.1.m1.1"><semantics id="S4.SS2.SSS4.p1.1.m1.1a"><mi id="S4.SS2.SSS4.p1.1.m1.1.1" xref="S4.SS2.SSS4.p1.1.m1.1.1.cmml">𝖱𝖫𝖠𝖨𝖥</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.1.m1.1b"><ci id="S4.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1">𝖱𝖫𝖠𝖨𝖥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.1.m1.1c">\mathsf{RLAIF}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p1.1.m1.1d">sansserif_RLAIF</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib87" title="">87</a>]</cite>, LLMs can be “aware” that their outputs are inappropriate given an adversarial prompt.
Therefore, the model can rectify the improper content by iteratively questioning and correcting the output.
Kim et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib44" title="">44</a>]</cite> validate the effectiveness of naive self-refinement methods on non-aligned LLM.
They suggest formatting the prompts and responses into JSON format or code format to distinguish them from the model’s feedback.
Zhang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib113" title="">113</a>]</cite> propose a specific target the model should achieve during the self-refinement to make the refinement more effective.
To be specific, they utilize the language model to analyze user prompts in essential aspects like ethics and legality and gather the intermediate responses from the model that reflect the intention of the prompts.
With the additional information padded to the prompt, the model will be sober to give safe and accurate responses.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">精炼方法利用 LLM 的自我纠错能力来降低生成非法响应的风险。正如 <math id="S4.SS2.SSS4.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{RLAIF}"><semantics id="S4.SS2.SSS4.p1.1.m1.1a"><mi id="S4.SS2.SSS4.p1.1.m1.1.1">𝖱𝖫𝖠𝖨𝖥</mi><annotation-xml id="S4.SS2.SSS4.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS2.SSS4.p1.1.m1.1c" encoding="application/x-tex">\mathsf{RLAIF}</annotation><annotation id="S4.SS2.SSS4.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_RLAIF</annotation></semantics></math> [ 87]所示，LLMs 在给定对抗性提示时能够“意识到”其输出不恰当。因此，模型可以通过迭代质询和纠正输出来修正不当内容。Kim 等人[ 44]验证了朴素自我精炼方法在非对齐 LLM 上的有效性。他们建议将提示和响应格式化为 JSON 格式或代码格式，以区别于模型的反馈。Zhang 等人[ 113]提出模型在自我精炼过程中应达到的具体目标，以提高精炼效果。具体来说，他们利用语言模型从伦理和法律等关键方面分析用户提示，并收集模型中反映提示意图的中级响应。在将额外信息添加到提示后，模型将更加清醒地给出安全准确的响应。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS4.p2">
<svg class="ltx_picture" height="111.81" id="S4.SS2.SSS4.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,111.81) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 105.91 C 0 109.17 2.64 111.81 5.91 111.81 L 594.09 111.81 C 597.36 111.81 600 109.17 600 105.91 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 87.7 L 598.03 87.7 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 93.61)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.SSS4.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.SSS4.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS4.p2.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;4.7</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.SSS4.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.SSS4.p2.pic1.2.2.2.1.1.1">Although the refinement methods do not require additional fine-tuning processes and exhibit competitive performance across different defenses, the self-refinement process relies on the intrinsic ability for correction, which may cause unstable performance.
Therefore, if the LLM is poorly safety-aligned, the refinement-based defenses may fail.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS5">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Proxy Defense<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">代理防御</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS5.p1">
<p class="ltx_p" id="S4.SS2.SSS5.p1.3">In brief, the proxy defenses move the security duties to another guardrail model.
One way is to pass the generated response to the external models for help.
Meta team&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib90" title="">90</a>]</cite> propose <math alttext="\mathsf{LlamaGuard}" class="ltx_Math" display="inline" id="S4.SS2.SSS5.p1.1.m1.1"><semantics id="S4.SS2.SSS5.p1.1.m1.1a"><mi id="S4.SS2.SSS5.p1.1.m1.1.1" xref="S4.SS2.SSS5.p1.1.m1.1.1.cmml">𝖫𝗅𝖺𝗆𝖺𝖦𝗎𝖺𝗋𝖽</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p1.1.m1.1b"><ci id="S4.SS2.SSS5.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p1.1.m1.1.1">𝖫𝗅𝖺𝗆𝖺𝖦𝗎𝖺𝗋𝖽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p1.1.m1.1c">\mathsf{LlamaGuard}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS5.p1.1.m1.1d">sansserif_LlamaGuard</annotation></semantics></math> for classifying content in both language model inputs (prompt classification) and responses (response classification), which can be directly used for proxy defense.
Zeng et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib110" title="">110</a>]</cite> design a multi-agent defense framework named <math alttext="\mathsf{AutoDefense}" class="ltx_Math" display="inline" id="S4.SS2.SSS5.p1.2.m2.1"><semantics id="S4.SS2.SSS5.p1.2.m2.1a"><mi id="S4.SS2.SSS5.p1.2.m2.1.1" xref="S4.SS2.SSS5.p1.2.m2.1.1.cmml">𝖠𝗎𝗍𝗈𝖣𝖾𝖿𝖾𝗇𝗌𝖾</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p1.2.m2.1b"><ci id="S4.SS2.SSS5.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p1.2.m2.1.1">𝖠𝗎𝗍𝗈𝖣𝖾𝖿𝖾𝗇𝗌𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p1.2.m2.1c">\mathsf{AutoDefense}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS5.p1.2.m2.1d">sansserif_AutoDefense</annotation></semantics></math>.
<math alttext="\mathsf{AutoDefense}" class="ltx_Math" display="inline" id="S4.SS2.SSS5.p1.3.m3.1"><semantics id="S4.SS2.SSS5.p1.3.m3.1a"><mi id="S4.SS2.SSS5.p1.3.m3.1.1" xref="S4.SS2.SSS5.p1.3.m3.1.1.cmml">𝖠𝗎𝗍𝗈𝖣𝖾𝖿𝖾𝗇𝗌𝖾</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p1.3.m3.1b"><ci id="S4.SS2.SSS5.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS5.p1.3.m3.1.1">𝖠𝗎𝗍𝗈𝖣𝖾𝖿𝖾𝗇𝗌𝖾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p1.3.m3.1c">\mathsf{AutoDefense}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS5.p1.3.m3.1d">sansserif_AutoDefense</annotation></semantics></math> consists of agents responsible for the intention analyzing and prompt judging, respectively.
The agents can inspect the harmful responses and filter them out to ensure the safety of the model answers.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">简而言之，代理防御将安全职责转移到了另一个护栏模型。一种方法是将生成的响应传递给外部模型寻求帮助。Meta 团队[90]提出了 <math id="S4.SS2.SSS5.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{LlamaGuard}"><semantics id="S4.SS2.SSS5.p1.1.m1.1a"><mi id="S4.SS2.SSS5.p1.1.m1.1.1">𝖫𝗅𝖺𝗆𝖺𝖦𝗎𝖺𝗋𝖽</mi><annotation-xml id="S4.SS2.SSS5.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS2.SSS5.p1.1.m1.1c" encoding="application/x-tex">\mathsf{LlamaGuard}</annotation><annotation id="S4.SS2.SSS5.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_LlamaGuard</annotation></semantics></math> 用于对语言模型输入（提示分类）和响应（响应分类）中的内容进行分类，这可以直接用于代理防御。Zeng 等人[110]设计了一个名为 <math id="S4.SS2.SSS5.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{AutoDefense}"><semantics id="S4.SS2.SSS5.p1.2.m2.1a"><mi id="S4.SS2.SSS5.p1.2.m2.1.1">𝖠𝗎𝗍𝗈𝖣𝖾𝖿𝖾𝗇𝗌𝖾</mi><annotation-xml id="S4.SS2.SSS5.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS2.SSS5.p1.2.m2.1c" encoding="application/x-tex">\mathsf{AutoDefense}</annotation><annotation id="S4.SS2.SSS5.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_AutoDefense</annotation></semantics></math> 的多智能体防御框架。 <math id="S4.SS2.SSS5.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{AutoDefense}"><semantics id="S4.SS2.SSS5.p1.3.m3.1a"><mi id="S4.SS2.SSS5.p1.3.m3.1.1">𝖠𝗎𝗍𝗈𝖣𝖾𝖿𝖾𝗇𝗌𝖾</mi><annotation-xml id="S4.SS2.SSS5.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S4.SS2.SSS5.p1.3.m3.1c" encoding="application/x-tex">\mathsf{AutoDefense}</annotation><annotation id="S4.SS2.SSS5.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_AutoDefense</annotation></semantics></math> 由分别负责意图分析和提示判断的智能体组成。这些智能体可以检查有害响应并将它们过滤掉，以确保模型答案的安全性。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS5.p2">
<svg class="ltx_picture" height="128.42" id="S4.SS2.SSS5.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,128.42) matrix(1 0 0 -1 0 0)"><g fill="#FFA64D" fill-opacity="1.0"><path d="M 0 5.91 L 0 122.51 C 0 125.77 2.64 128.42 5.91 128.42 L 594.09 128.42 C 597.36 128.42 600 125.77 600 122.51 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF2E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 104.31 L 598.03 104.31 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 110.21)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.SSS5.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.SSS5.p2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS5.p2.pic1.1.1.1.1.1.1.1">Takeaways.&nbsp;4.8</span></span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.SSS5.p2.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.SSS5.p2.pic1.2.2.2.1.1.1">The proxy defense methods do not depend on the target model, thus increasing the performance and making the defense robust against most prompt-based attacks.
However, recent work reveals the risk that the external detector can be derived&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib85" title="">85</a>]</cite>, that is, the message exchange between the target model and the defense model can be hijacked, which is also a potential risk.</span>
</span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;"> Evaluation<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">评估</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Evaluation methods are significant as they provide a unified comparison for various jailbreak attack and defense methods.
Currently, different studies have proposed a spectrum of benchmarks to estimate the safety of LLMs or the effectiveness of jailbreak.
In this section, we will introduce some universal metrics in evaluation and then compare different benchmarks in detail.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">评估方法非常重要，因为它们为各种越狱攻击和防御方法提供了统一的比较基准。目前，不同的研究已经提出了多种基准来评估 LLMs 的安全性或越狱的有效性。在本节中，我们将介绍一些通用的评估指标，然后详细比较不同的基准。 </font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:120%;"> Metric<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">指标</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Attack Success Rate<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">攻击成功率</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.2">Attack Success Rate (ASR) is a widely used metric to validate the effectiveness of a jailbreak method.
Formally, we denote the total number of jailbreak prompts as
<math alttext="N_{total}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p1.1.m1.1"><semantics id="S5.SS1.SSS1.p1.1.m1.1a"><msub id="S5.SS1.SSS1.p1.1.m1.1.1" xref="S5.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.SSS1.p1.1.m1.1.1.2" xref="S5.SS1.SSS1.p1.1.m1.1.1.2.cmml">N</mi><mrow id="S5.SS1.SSS1.p1.1.m1.1.1.3" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S5.SS1.SSS1.p1.1.m1.1.1.3.2" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.2.cmml">t</mi><mo id="S5.SS1.SSS1.p1.1.m1.1.1.3.1" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p1.1.m1.1.1.3.3" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.3.cmml">o</mi><mo id="S5.SS1.SSS1.p1.1.m1.1.1.3.1a" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p1.1.m1.1.1.3.4" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.4.cmml">t</mi><mo id="S5.SS1.SSS1.p1.1.m1.1.1.3.1b" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p1.1.m1.1.1.3.5" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.5.cmml">a</mi><mo id="S5.SS1.SSS1.p1.1.m1.1.1.3.1c" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p1.1.m1.1.1.3.6" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.1.m1.1b"><apply id="S5.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.2">𝑁</ci><apply id="S5.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3"><times id="S5.SS1.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.1"></times><ci id="S5.SS1.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.2">𝑡</ci><ci id="S5.SS1.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.3">𝑜</ci><ci id="S5.SS1.SSS1.p1.1.m1.1.1.3.4.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.4">𝑡</ci><ci id="S5.SS1.SSS1.p1.1.m1.1.1.3.5.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.5">𝑎</ci><ci id="S5.SS1.SSS1.p1.1.m1.1.1.3.6.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.1.m1.1c">N_{total}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p1.1.m1.1d">italic_N start_POSTSUBSCRIPT italic_t italic_o italic_t italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>, and the number of successfully attacked prompts as <math alttext="N_{success}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p1.2.m2.1"><semantics id="S5.SS1.SSS1.p1.2.m2.1a"><msub id="S5.SS1.SSS1.p1.2.m2.1.1" xref="S5.SS1.SSS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.SSS1.p1.2.m2.1.1.2" xref="S5.SS1.SSS1.p1.2.m2.1.1.2.cmml">N</mi><mrow id="S5.SS1.SSS1.p1.2.m2.1.1.3" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.2" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.2.cmml">s</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.3" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.3.cmml">u</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1a" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.4" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.4.cmml">c</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1b" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.5" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.5.cmml">c</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1c" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.6" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.6.cmml">e</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1d" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.7" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.7.cmml">s</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1e" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.8" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.8.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.2.m2.1b"><apply id="S5.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.2">𝑁</ci><apply id="S5.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.3"><times id="S5.SS1.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.1"></times><ci id="S5.SS1.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.2">𝑠</ci><ci id="S5.SS1.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.3">𝑢</ci><ci id="S5.SS1.SSS1.p1.2.m2.1.1.3.4.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.4">𝑐</ci><ci id="S5.SS1.SSS1.p1.2.m2.1.1.3.5.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.5">𝑐</ci><ci id="S5.SS1.SSS1.p1.2.m2.1.1.3.6.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.6">𝑒</ci><ci id="S5.SS1.SSS1.p1.2.m2.1.1.3.7.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.7">𝑠</ci><ci id="S5.SS1.SSS1.p1.2.m2.1.1.3.8.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.8">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.2.m2.1c">N_{success}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p1.2.m2.1d">italic_N start_POSTSUBSCRIPT italic_s italic_u italic_c italic_c italic_e italic_s italic_s end_POSTSUBSCRIPT</annotation></semantics></math>.
Then, ASR can be formulated as<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">攻击成功率（ASR）是验证一种越狱方法有效性的常用指标。形式上，我们用 <math id="S5.SS1.SSS1.p1.1.m1.1" display="inline" class="ltx_Math" alttext="N_{total}"><semantics id="S5.SS1.SSS1.p1.1.m1.1a"><msub id="S5.SS1.SSS1.p1.1.m1.1.1"><mi id="S5.SS1.SSS1.p1.1.m1.1.1.2">N</mi><mrow id="S5.SS1.SSS1.p1.1.m1.1.1.3"><mi id="S5.SS1.SSS1.p1.1.m1.1.1.3.2">t</mi><mo id="S5.SS1.SSS1.p1.1.m1.1.1.3.1">⁢</mo><mi id="S5.SS1.SSS1.p1.1.m1.1.1.3.3">o</mi><mo id="S5.SS1.SSS1.p1.1.m1.1.1.3.1a">⁢</mo><mi id="S5.SS1.SSS1.p1.1.m1.1.1.3.4">t</mi><mo id="S5.SS1.SSS1.p1.1.m1.1.1.3.1b">⁢</mo><mi id="S5.SS1.SSS1.p1.1.m1.1.1.3.5">a</mi><mo id="S5.SS1.SSS1.p1.1.m1.1.1.3.1c">⁢</mo><mi id="S5.SS1.SSS1.p1.1.m1.1.1.3.6">l</mi></mrow></msub><annotation-xml id="S5.SS1.SSS1.p1.1.m1.1b" encoding="MathML-Content">subscript</annotation-xml><annotation id="S5.SS1.SSS1.p1.1.m1.1c" encoding="application/x-tex">N_{total}</annotation><annotation id="S5.SS1.SSS1.p1.1.m1.1d" encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_t italic_o italic_t italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math> 表示越狱提示的总数，用 <math id="S5.SS1.SSS1.p1.2.m2.1" display="inline" class="ltx_Math" alttext="N_{success}"><semantics id="S5.SS1.SSS1.p1.2.m2.1a"><msub id="S5.SS1.SSS1.p1.2.m2.1.1"><mi id="S5.SS1.SSS1.p1.2.m2.1.1.2">N</mi><mrow id="S5.SS1.SSS1.p1.2.m2.1.1.3"><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.2">s</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.3">u</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1a">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.4">c</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1b">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.5">c</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1c">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.6">e</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1d">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.7">s</mi><mo id="S5.SS1.SSS1.p1.2.m2.1.1.3.1e">⁢</mo><mi id="S5.SS1.SSS1.p1.2.m2.1.1.3.8">s</mi></mrow></msub><annotation-xml id="S5.SS1.SSS1.p1.2.m2.1b" encoding="MathML-Content">subscript</annotation-xml><annotation id="S5.SS1.SSS1.p1.2.m2.1c" encoding="application/x-tex">N_{success}</annotation><annotation id="S5.SS1.SSS1.p1.2.m2.1d" encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_s italic_u italic_c italic_c italic_e italic_s italic_s end_POSTSUBSCRIPT</annotation></semantics></math> 表示成功攻击的提示数。那么，ASR 可以表示为</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S5.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="ASR=\frac{N_{success}}{N_{total}}." class="ltx_Math" display="block" id="S5.E1.m1.1"><semantics id="S5.E1.m1.1a"><mrow id="S5.E1.m1.1.1.1" xref="S5.E1.m1.1.1.1.1.cmml"><mrow id="S5.E1.m1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.cmml"><mrow id="S5.E1.m1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.2.cmml"><mi id="S5.E1.m1.1.1.1.1.2.2" xref="S5.E1.m1.1.1.1.1.2.2.cmml">A</mi><mo id="S5.E1.m1.1.1.1.1.2.1" xref="S5.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.2.3" xref="S5.E1.m1.1.1.1.1.2.3.cmml">S</mi><mo id="S5.E1.m1.1.1.1.1.2.1a" xref="S5.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.2.4" xref="S5.E1.m1.1.1.1.1.2.4.cmml">R</mi></mrow><mo id="S5.E1.m1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S5.E1.m1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.3.cmml"><msub id="S5.E1.m1.1.1.1.1.3.2" xref="S5.E1.m1.1.1.1.1.3.2.cmml"><mi id="S5.E1.m1.1.1.1.1.3.2.2" xref="S5.E1.m1.1.1.1.1.3.2.2.cmml">N</mi><mrow id="S5.E1.m1.1.1.1.1.3.2.3" xref="S5.E1.m1.1.1.1.1.3.2.3.cmml"><mi id="S5.E1.m1.1.1.1.1.3.2.3.2" xref="S5.E1.m1.1.1.1.1.3.2.3.2.cmml">s</mi><mo id="S5.E1.m1.1.1.1.1.3.2.3.1" xref="S5.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.3.2.3.3" xref="S5.E1.m1.1.1.1.1.3.2.3.3.cmml">u</mi><mo id="S5.E1.m1.1.1.1.1.3.2.3.1a" xref="S5.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.3.2.3.4" xref="S5.E1.m1.1.1.1.1.3.2.3.4.cmml">c</mi><mo id="S5.E1.m1.1.1.1.1.3.2.3.1b" xref="S5.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.3.2.3.5" xref="S5.E1.m1.1.1.1.1.3.2.3.5.cmml">c</mi><mo id="S5.E1.m1.1.1.1.1.3.2.3.1c" xref="S5.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.3.2.3.6" xref="S5.E1.m1.1.1.1.1.3.2.3.6.cmml">e</mi><mo id="S5.E1.m1.1.1.1.1.3.2.3.1d" xref="S5.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.3.2.3.7" xref="S5.E1.m1.1.1.1.1.3.2.3.7.cmml">s</mi><mo id="S5.E1.m1.1.1.1.1.3.2.3.1e" xref="S5.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.3.2.3.8" xref="S5.E1.m1.1.1.1.1.3.2.3.8.cmml">s</mi></mrow></msub><msub id="S5.E1.m1.1.1.1.1.3.3" xref="S5.E1.m1.1.1.1.1.3.3.cmml"><mi id="S5.E1.m1.1.1.1.1.3.3.2" xref="S5.E1.m1.1.1.1.1.3.3.2.cmml">N</mi><mrow id="S5.E1.m1.1.1.1.1.3.3.3" xref="S5.E1.m1.1.1.1.1.3.3.3.cmml"><mi id="S5.E1.m1.1.1.1.1.3.3.3.2" xref="S5.E1.m1.1.1.1.1.3.3.3.2.cmml">t</mi><mo id="S5.E1.m1.1.1.1.1.3.3.3.1" xref="S5.E1.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.3.3.3.3" xref="S5.E1.m1.1.1.1.1.3.3.3.3.cmml">o</mi><mo id="S5.E1.m1.1.1.1.1.3.3.3.1a" xref="S5.E1.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.3.3.3.4" xref="S5.E1.m1.1.1.1.1.3.3.3.4.cmml">t</mi><mo id="S5.E1.m1.1.1.1.1.3.3.3.1b" xref="S5.E1.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.3.3.3.5" xref="S5.E1.m1.1.1.1.1.3.3.3.5.cmml">a</mi><mo id="S5.E1.m1.1.1.1.1.3.3.3.1c" xref="S5.E1.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S5.E1.m1.1.1.1.1.3.3.3.6" xref="S5.E1.m1.1.1.1.1.3.3.3.6.cmml">l</mi></mrow></msub></mfrac></mrow><mo id="S5.E1.m1.1.1.1.2" lspace="0em" xref="S5.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E1.m1.1b"><apply id="S5.E1.m1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1"><eq id="S5.E1.m1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1"></eq><apply id="S5.E1.m1.1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1.2"><times id="S5.E1.m1.1.1.1.1.2.1.cmml" xref="S5.E1.m1.1.1.1.1.2.1"></times><ci id="S5.E1.m1.1.1.1.1.2.2.cmml" xref="S5.E1.m1.1.1.1.1.2.2">𝐴</ci><ci id="S5.E1.m1.1.1.1.1.2.3.cmml" xref="S5.E1.m1.1.1.1.1.2.3">𝑆</ci><ci id="S5.E1.m1.1.1.1.1.2.4.cmml" xref="S5.E1.m1.1.1.1.1.2.4">𝑅</ci></apply><apply id="S5.E1.m1.1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.1.3"><divide id="S5.E1.m1.1.1.1.1.3.1.cmml" xref="S5.E1.m1.1.1.1.1.3"></divide><apply id="S5.E1.m1.1.1.1.1.3.2.cmml" xref="S5.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.3.2.1.cmml" xref="S5.E1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S5.E1.m1.1.1.1.1.3.2.2.cmml" xref="S5.E1.m1.1.1.1.1.3.2.2">𝑁</ci><apply id="S5.E1.m1.1.1.1.1.3.2.3.cmml" xref="S5.E1.m1.1.1.1.1.3.2.3"><times id="S5.E1.m1.1.1.1.1.3.2.3.1.cmml" xref="S5.E1.m1.1.1.1.1.3.2.3.1"></times><ci id="S5.E1.m1.1.1.1.1.3.2.3.2.cmml" xref="S5.E1.m1.1.1.1.1.3.2.3.2">𝑠</ci><ci id="S5.E1.m1.1.1.1.1.3.2.3.3.cmml" xref="S5.E1.m1.1.1.1.1.3.2.3.3">𝑢</ci><ci id="S5.E1.m1.1.1.1.1.3.2.3.4.cmml" xref="S5.E1.m1.1.1.1.1.3.2.3.4">𝑐</ci><ci id="S5.E1.m1.1.1.1.1.3.2.3.5.cmml" xref="S5.E1.m1.1.1.1.1.3.2.3.5">𝑐</ci><ci id="S5.E1.m1.1.1.1.1.3.2.3.6.cmml" xref="S5.E1.m1.1.1.1.1.3.2.3.6">𝑒</ci><ci id="S5.E1.m1.1.1.1.1.3.2.3.7.cmml" xref="S5.E1.m1.1.1.1.1.3.2.3.7">𝑠</ci><ci id="S5.E1.m1.1.1.1.1.3.2.3.8.cmml" xref="S5.E1.m1.1.1.1.1.3.2.3.8">𝑠</ci></apply></apply><apply id="S5.E1.m1.1.1.1.1.3.3.cmml" xref="S5.E1.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.3.3.1.cmml" xref="S5.E1.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S5.E1.m1.1.1.1.1.3.3.2.cmml" xref="S5.E1.m1.1.1.1.1.3.3.2">𝑁</ci><apply id="S5.E1.m1.1.1.1.1.3.3.3.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3"><times id="S5.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3.1"></times><ci id="S5.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3.2">𝑡</ci><ci id="S5.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3.3">𝑜</ci><ci id="S5.E1.m1.1.1.1.1.3.3.3.4.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3.4">𝑡</ci><ci id="S5.E1.m1.1.1.1.1.3.3.3.5.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3.5">𝑎</ci><ci id="S5.E1.m1.1.1.1.1.3.3.3.6.cmml" xref="S5.E1.m1.1.1.1.1.3.3.3.6">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.1c">ASR=\frac{N_{success}}{N_{total}}.</annotation><annotation encoding="application/x-llamapun" id="S5.E1.m1.1d">italic_A italic_S italic_R = divide start_ARG italic_N start_POSTSUBSCRIPT italic_s italic_u italic_c italic_c italic_e italic_s italic_s end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_t italic_o italic_t italic_a italic_l end_POSTSUBSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p2.1.1">Safety Evaluators.</span>
However, one challenge is defining a so-called “successful jailbreak”, i.e., how to evaluate the success of a jailbreak attempt against an LLM has not been unified&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib71" title="">71</a>]</cite>, which leads to inconsistencies in the value of <math alttext="N_{success}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p2.1.m1.1"><semantics id="S5.SS1.SSS1.p2.1.m1.1a"><msub id="S5.SS1.SSS1.p2.1.m1.1.1" xref="S5.SS1.SSS1.p2.1.m1.1.1.cmml"><mi id="S5.SS1.SSS1.p2.1.m1.1.1.2" xref="S5.SS1.SSS1.p2.1.m1.1.1.2.cmml">N</mi><mrow id="S5.SS1.SSS1.p2.1.m1.1.1.3" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.cmml"><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.2" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.2.cmml">s</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.3" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.3.cmml">u</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1a" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.4" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.4.cmml">c</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1b" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.5" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.5.cmml">c</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1c" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.6" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.6.cmml">e</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1d" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.7" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.7.cmml">s</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1e" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.8" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.8.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p2.1.m1.1b"><apply id="S5.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.2">𝑁</ci><apply id="S5.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.3"><times id="S5.SS1.SSS1.p2.1.m1.1.1.3.1.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.1"></times><ci id="S5.SS1.SSS1.p2.1.m1.1.1.3.2.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.2">𝑠</ci><ci id="S5.SS1.SSS1.p2.1.m1.1.1.3.3.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.3">𝑢</ci><ci id="S5.SS1.SSS1.p2.1.m1.1.1.3.4.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.4">𝑐</ci><ci id="S5.SS1.SSS1.p2.1.m1.1.1.3.5.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.5">𝑐</ci><ci id="S5.SS1.SSS1.p2.1.m1.1.1.3.6.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.6">𝑒</ci><ci id="S5.SS1.SSS1.p2.1.m1.1.1.3.7.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.7">𝑠</ci><ci id="S5.SS1.SSS1.p2.1.m1.1.1.3.8.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.8">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p2.1.m1.1c">N_{success}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p2.1.m1.1d">italic_N start_POSTSUBSCRIPT italic_s italic_u italic_c italic_c italic_e italic_s italic_s end_POSTSUBSCRIPT</annotation></semantics></math>.
Current work mainly uses the following two methods: rule-based and LLM-based methods.
Rule-based methods assess the effectiveness of an attack by examining keywords in the target LLM’s responses&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib125" title="">125</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib126" title="">126</a>]</cite>.
This is because it is common that rejection responses consistently contain refusal phrases like “do not”, “I’m sorry”, and “I apologize”.
Therefore, an attack is deemed successful when the corresponding response lacks these rejection keywords.
LLM-based methods usually utilize a state-of-the-art LLM as the evaluator to determine if an attack is successful&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib68" title="">68</a>]</cite>.
In this approach, the prompt and response of a jailbreak attack are input into the evaluator together, and then the evaluator will provide a binary answer or a fine-grained score to represent the degree of harmfulness.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">安全评估器。然而，一个挑战在于定义所谓的“成功越狱”，即如何评估针对 LLM 的越狱尝试的成功与否尚未统一[71]，这导致了 <math id="S5.SS1.SSS1.p2.1.m1.1" display="inline" class="ltx_Math" alttext="N_{success}"><semantics id="S5.SS1.SSS1.p2.1.m1.1a"><msub id="S5.SS1.SSS1.p2.1.m1.1.1"><mi id="S5.SS1.SSS1.p2.1.m1.1.1.2">N</mi><mrow id="S5.SS1.SSS1.p2.1.m1.1.1.3"><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.2">s</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.3">u</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1a">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.4">c</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1b">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.5">c</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1c">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.6">e</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1d">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.7">s</mi><mo id="S5.SS1.SSS1.p2.1.m1.1.1.3.1e">⁢</mo><mi id="S5.SS1.SSS1.p2.1.m1.1.1.3.8">s</mi></mrow></msub><annotation-xml id="S5.SS1.SSS1.p2.1.m1.1b" encoding="MathML-Content">subscript</annotation-xml><annotation id="S5.SS1.SSS1.p2.1.m1.1c" encoding="application/x-tex">N_{success}</annotation><annotation id="S5.SS1.SSS1.p2.1.m1.1d" encoding="application/x-llamapun">italic_N start_POSTSUBSCRIPT italic_s italic_u italic_c italic_c italic_e italic_s italic_s end_POSTSUBSCRIPT</annotation></semantics></math> 值的不一致性。当前工作主要采用以下两种方法：基于规则的方法和基于 LLM 的方法。基于规则的方法通过检查目标 LLM 响应中的关键词来评估攻击的有效性[125, 126]。这是因为拒绝响应通常包含“不要”、“抱歉”和“我道歉”等拒绝短语。因此，当相应的响应缺少这些拒绝关键词时，攻击被认为成功。基于 LLM 的方法通常使用最先进的 LLM 作为评估器来确定攻击是否成功[68]。在这种方法中，越狱攻击的提示和响应一起输入到评估器中，然后评估器将提供一个二进制答案或细粒度分数来表示有害程度。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.p3.5">While most benchmarks have employed LLM-based evaluation methods and integrated state-of-the-art LLMs as the safety evaluators, some research have made different innovations in the evaluation process.
For instance, <math alttext="\mathsf{StrongReject}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.1.m1.1"><semantics id="S5.SS1.SSS1.p3.1.m1.1a"><mi id="S5.SS1.SSS1.p3.1.m1.1.1" xref="S5.SS1.SSS1.p3.1.m1.1.1.cmml">𝖲𝗍𝗋𝗈𝗇𝗀𝖱𝖾𝗃𝖾𝖼𝗍</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.1.m1.1b"><ci id="S5.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p3.1.m1.1.1">𝖲𝗍𝗋𝗈𝗇𝗀𝖱𝖾𝗃𝖾𝖼𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.1.m1.1c">\mathsf{StrongReject}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.1.m1.1d">sansserif_StrongReject</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib84" title="">84</a>]</cite> instructs a pre-trained LLM to examine the jailbreak prompt and the response to give a score from three dimensions, representing whether the target model refuses the harmful prompt, whether the answer accurately aligns with the harmful prompt, and whether the answer is realistic.
<math alttext="\mathsf{AttackEval}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.2.m2.1"><semantics id="S5.SS1.SSS1.p3.2.m2.1a"><mi id="S5.SS1.SSS1.p3.2.m2.1.1" xref="S5.SS1.SSS1.p3.2.m2.1.1.cmml">𝖠𝗍𝗍𝖺𝖼𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.2.m2.1b"><ci id="S5.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p3.2.m2.1.1">𝖠𝗍𝗍𝖺𝖼𝗄𝖤𝗏𝖺𝗅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.2.m2.1c">\mathsf{AttackEval}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.2.m2.1d">sansserif_AttackEval</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib80" title="">80</a>]</cite> utilizes a judgement model to identify the effectiveness of a jailbreak.
Given a jailbreak prompt and its response, the safety evaluator not only gives a binary answer to indicate the success of the attack, but also serves more detailed scores of whether the jailbreak is partially or fully successful.
Note that in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib71" title="">71</a>]</cite>, Ran et al. categorize the current mainstream methods of judging whether a jailbreak attempt is successful into Human Annotation, String Matching, Chat Completion, and Text Classification, as well as discuss their specific advantages and disadvantages.
Furthermore, they propose <math alttext="\mathsf{JailbreakEval}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.3.m3.1"><semantics id="S5.SS1.SSS1.p3.3.m3.1a"><mi id="S5.SS1.SSS1.p3.3.m3.1.1" xref="S5.SS1.SSS1.p3.3.m3.1.1.cmml">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.3.m3.1b"><ci id="S5.SS1.SSS1.p3.3.m3.1.1.cmml" xref="S5.SS1.SSS1.p3.3.m3.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖤𝗏𝖺𝗅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.3.m3.1c">\mathsf{JailbreakEval}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.3.m3.1d">sansserif_JailbreakEval</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ThuCCSLab/JailbreakEval" title="">https://github.com/ThuCCSLab/JailbreakEval</a>.</span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">尽管大多数基准测试采用了基于 LLM 的评估方法，并将最先进的 LLM 集成作为安全评估器，但一些研究在评估过程中进行了不同的创新。例如， <math id="S5.SS1.SSS1.p3.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{StrongReject}"><semantics id="S5.SS1.SSS1.p3.1.m1.1a"><mi id="S5.SS1.SSS1.p3.1.m1.1.1">𝖲𝗍𝗋𝗈𝗇𝗀𝖱𝖾𝗃𝖾𝖼𝗍</mi><annotation-xml id="S5.SS1.SSS1.p3.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS1.SSS1.p3.1.m1.1c" encoding="application/x-tex">\mathsf{StrongReject}</annotation><annotation id="S5.SS1.SSS1.p3.1.m1.1d" encoding="application/x-llamapun">sansserif_StrongReject</annotation></semantics></math> [ 84] 指示预训练的 LLM 检查越狱提示和响应，并从三个维度给出评分，代表目标模型是否拒绝有害提示，答案是否准确与有害提示一致，以及答案是否真实。 <math id="S5.SS1.SSS1.p3.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{AttackEval}"><semantics id="S5.SS1.SSS1.p3.2.m2.1a"><mi id="S5.SS1.SSS1.p3.2.m2.1.1">𝖠𝗍𝗍𝖺𝖼𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml id="S5.SS1.SSS1.p3.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS1.SSS1.p3.2.m2.1c" encoding="application/x-tex">\mathsf{AttackEval}</annotation><annotation id="S5.SS1.SSS1.p3.2.m2.1d" encoding="application/x-llamapun">sansserif_AttackEval</annotation></semantics></math> [ 80] 利用判断模型来识别越狱的有效性。给定一个越狱提示及其响应，安全评估器不仅给出一个二进制答案来指示攻击的成功，还提供更详细的评分，表明越狱是部分成功还是完全成功。请注意，在[ 71]中，Ran 等人将当前主流的判断越狱尝试是否成功的方法分为人工标注、字符串匹配、聊天完成和文本分类，并讨论了它们的具体优缺点。此外，他们提出了 <math id="S5.SS1.SSS1.p3.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{JailbreakEval}"><semantics id="S5.SS1.SSS1.p3.3.m3.1a"><mi id="S5.SS1.SSS1.p3.3.m3.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml id="S5.SS1.SSS1.p3.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS1.SSS1.p3.3.m3.1c" encoding="application/x-tex">\mathsf{JailbreakEval}</annotation><annotation id="S5.SS1.SSS1.p3.3.m3.1d" encoding="application/x-llamapun">sansserif_JailbreakEval</annotation></semantics></math> <sup class="ltx_note_mark">2</sup> </font></font></font>, an integrated toolkit that contains various mainstream safety evaluators.
Notably, <math alttext="\mathsf{JailbreakEval}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.4.m4.1"><semantics id="S5.SS1.SSS1.p3.4.m4.1a"><mi id="S5.SS1.SSS1.p3.4.m4.1.1" xref="S5.SS1.SSS1.p3.4.m4.1.1.cmml">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.4.m4.1b"><ci id="S5.SS1.SSS1.p3.4.m4.1.1.cmml" xref="S5.SS1.SSS1.p3.4.m4.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖤𝗏𝖺𝗅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.4.m4.1c">\mathsf{JailbreakEval}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.4.m4.1d">sansserif_JailbreakEval</annotation></semantics></math> supports voting-based safety evaluation, i.e., <math alttext="\mathsf{JailbreakEval}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.5.m5.1"><semantics id="S5.SS1.SSS1.p3.5.m5.1a"><mi id="S5.SS1.SSS1.p3.5.m5.1.1" xref="S5.SS1.SSS1.p3.5.m5.1.1.cmml">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.5.m5.1b"><ci id="S5.SS1.SSS1.p3.5.m5.1.1.cmml" xref="S5.SS1.SSS1.p3.5.m5.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖤𝗏𝖺𝗅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.5.m5.1c">\mathsf{JailbreakEval}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.5.m5.1d">sansserif_JailbreakEval</annotation></semantics></math> generates the final judgement through multiple safety evaluators.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">, 一个包含多种主流安全评估器的集成工具包。值得注意的是， <math id="S5.SS1.SSS1.p3.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{JailbreakEval}"><semantics id="S5.SS1.SSS1.p3.4.m4.1a"><mi id="S5.SS1.SSS1.p3.4.m4.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml id="S5.SS1.SSS1.p3.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS1.SSS1.p3.4.m4.1c" encoding="application/x-tex">\mathsf{JailbreakEval}</annotation><annotation id="S5.SS1.SSS1.p3.4.m4.1d" encoding="application/x-llamapun">sansserif_JailbreakEval</annotation></semantics></math> 支持基于投票的安全评估，即 <math id="S5.SS1.SSS1.p3.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{JailbreakEval}"><semantics id="S5.SS1.SSS1.p3.5.m5.1a"><mi id="S5.SS1.SSS1.p3.5.m5.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml id="S5.SS1.SSS1.p3.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS1.SSS1.p3.5.m5.1c" encoding="application/x-tex">\mathsf{JailbreakEval}</annotation><annotation id="S5.SS1.SSS1.p3.5.m5.1d" encoding="application/x-llamapun">sansserif_JailbreakEval</annotation></semantics></math> 通过多个安全评估器生成最终判断。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection" style="font-size:120%;"> Perplexity<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">困惑度</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.3">Perplexity (PPL) is a metric used to measure the readability and fluency of a jailbreak prompt.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib67" title="">67</a>]</cite>
Since many defense methods filter high-perplexity prompts to provide protection, attack methods with low-perplexity jailbreak prompts have become increasingly noteworthy.
Formally, given a text sequence <math alttext="W={(w_{1},w_{2},.......,w_{n})}" class="ltx_math_unparsed" display="inline" id="S5.SS1.SSS2.p1.1.m1.1"><semantics id="S5.SS1.SSS2.p1.1.m1.1a"><mrow id="S5.SS1.SSS2.p1.1.m1.1b"><mi id="S5.SS1.SSS2.p1.1.m1.1.1">W</mi><mo id="S5.SS1.SSS2.p1.1.m1.1.2">=</mo><mrow id="S5.SS1.SSS2.p1.1.m1.1.3"><mo id="S5.SS1.SSS2.p1.1.m1.1.3.1" stretchy="false">(</mo><msub id="S5.SS1.SSS2.p1.1.m1.1.3.2"><mi id="S5.SS1.SSS2.p1.1.m1.1.3.2.2">w</mi><mn id="S5.SS1.SSS2.p1.1.m1.1.3.2.3">1</mn></msub><mo id="S5.SS1.SSS2.p1.1.m1.1.3.3">,</mo><msub id="S5.SS1.SSS2.p1.1.m1.1.3.4"><mi id="S5.SS1.SSS2.p1.1.m1.1.3.4.2">w</mi><mn id="S5.SS1.SSS2.p1.1.m1.1.3.4.3">2</mn></msub><mo id="S5.SS1.SSS2.p1.1.m1.1.3.5">,</mo><mi id="S5.SS1.SSS2.p1.1.m1.1.3.6" mathvariant="normal">…</mi><mi id="S5.SS1.SSS2.p1.1.m1.1.3.7" mathvariant="normal">…</mi><mo id="S5.SS1.SSS2.p1.1.m1.1.3.8" lspace="0em" rspace="0.167em">.</mo><mo id="S5.SS1.SSS2.p1.1.m1.1.3.9">,</mo><msub id="S5.SS1.SSS2.p1.1.m1.1.3.10"><mi id="S5.SS1.SSS2.p1.1.m1.1.3.10.2">w</mi><mi id="S5.SS1.SSS2.p1.1.m1.1.3.10.3">n</mi></msub><mo id="S5.SS1.SSS2.p1.1.m1.1.3.11" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p1.1.m1.1c">W={(w_{1},w_{2},.......,w_{n})}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS2.p1.1.m1.1d">italic_W = ( italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … … . , italic_w start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="w_{i}" class="ltx_Math" display="inline" id="S5.SS1.SSS2.p1.2.m2.1"><semantics id="S5.SS1.SSS2.p1.2.m2.1a"><msub id="S5.SS1.SSS2.p1.2.m2.1.1" xref="S5.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S5.SS1.SSS2.p1.2.m2.1.1.2" xref="S5.SS1.SSS2.p1.2.m2.1.1.2.cmml">w</mi><mi id="S5.SS1.SSS2.p1.2.m2.1.1.3" xref="S5.SS1.SSS2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p1.2.m2.1b"><apply id="S5.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S5.SS1.SSS2.p1.2.m2.1.1.2">𝑤</ci><ci id="S5.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S5.SS1.SSS2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p1.2.m2.1c">w_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS2.p1.2.m2.1d">italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents the i-th token of the sequence, the perplexity of the sequence <math alttext="W" class="ltx_Math" display="inline" id="S5.SS1.SSS2.p1.3.m3.1"><semantics id="S5.SS1.SSS2.p1.3.m3.1a"><mi id="S5.SS1.SSS2.p1.3.m3.1.1" xref="S5.SS1.SSS2.p1.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p1.3.m3.1b"><ci id="S5.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S5.SS1.SSS2.p1.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p1.3.m3.1c">W</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS2.p1.3.m3.1d">italic_W</annotation></semantics></math> can be expressed as<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">困惑度（PPL）是一个用于衡量越狱提示可读性和流畅性的指标。[ 56, 1, 67] 由于许多防御方法会过滤高困惑度的提示以提供保护，因此具有低困惑度的越狱提示攻击方法越来越受到关注。形式上，给定一个文本序列 <math id="S5.SS1.SSS2.p1.1.m1.1" display="inline" class="ltx_math_unparsed" alttext="W={(w_{1},w_{2},.......,w_{n})}"><semantics id="S5.SS1.SSS2.p1.1.m1.1a"><mrow id="S5.SS1.SSS2.p1.1.m1.1b"><mi id="S5.SS1.SSS2.p1.1.m1.1.1">W</mi><mo id="S5.SS1.SSS2.p1.1.m1.1.2">=</mo><mrow id="S5.SS1.SSS2.p1.1.m1.1.3"><mo stretchy="false" id="S5.SS1.SSS2.p1.1.m1.1.3.1">(</mo><msub id="S5.SS1.SSS2.p1.1.m1.1.3.2"><mi id="S5.SS1.SSS2.p1.1.m1.1.3.2.2">w</mi><mn id="S5.SS1.SSS2.p1.1.m1.1.3.2.3">1</mn></msub><mo id="S5.SS1.SSS2.p1.1.m1.1.3.3">,</mo><msub id="S5.SS1.SSS2.p1.1.m1.1.3.4"><mi id="S5.SS1.SSS2.p1.1.m1.1.3.4.2">w</mi><mn id="S5.SS1.SSS2.p1.1.m1.1.3.4.3">2</mn></msub><mo id="S5.SS1.SSS2.p1.1.m1.1.3.5">,</mo><mi mathvariant="normal" id="S5.SS1.SSS2.p1.1.m1.1.3.6">…</mi><mi mathvariant="normal" id="S5.SS1.SSS2.p1.1.m1.1.3.7">…</mi><mo rspace="0.167em" lspace="0em" id="S5.SS1.SSS2.p1.1.m1.1.3.8">.</mo><mo id="S5.SS1.SSS2.p1.1.m1.1.3.9">,</mo><msub id="S5.SS1.SSS2.p1.1.m1.1.3.10"><mi id="S5.SS1.SSS2.p1.1.m1.1.3.10.2">w</mi><mi id="S5.SS1.SSS2.p1.1.m1.1.3.10.3">n</mi></msub><mo stretchy="false" id="S5.SS1.SSS2.p1.1.m1.1.3.11">)</mo></mrow></mrow><annotation id="S5.SS1.SSS2.p1.1.m1.1c" encoding="application/x-tex">W={(w_{1},w_{2},.......,w_{n})}</annotation><annotation id="S5.SS1.SSS2.p1.1.m1.1d" encoding="application/x-llamapun">italic_W = ( italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … … . , italic_w start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math> ，其中 <math id="S5.SS1.SSS2.p1.2.m2.1" display="inline" class="ltx_Math" alttext="w_{i}"><semantics id="S5.SS1.SSS2.p1.2.m2.1a"><msub id="S5.SS1.SSS2.p1.2.m2.1.1"><mi id="S5.SS1.SSS2.p1.2.m2.1.1.2">w</mi><mi id="S5.SS1.SSS2.p1.2.m2.1.1.3">i</mi></msub><annotation-xml id="S5.SS1.SSS2.p1.2.m2.1b" encoding="MathML-Content">subscript</annotation-xml><annotation id="S5.SS1.SSS2.p1.2.m2.1c" encoding="application/x-tex">w_{i}</annotation><annotation id="S5.SS1.SSS2.p1.2.m2.1d" encoding="application/x-llamapun">italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> 表示序列的第 i 个标记，序列 <math id="S5.SS1.SSS2.p1.3.m3.1" display="inline" class="ltx_Math" alttext="W"><semantics id="S5.SS1.SSS2.p1.3.m3.1a"><mi id="S5.SS1.SSS2.p1.3.m3.1.1">W</mi><annotation-xml id="S5.SS1.SSS2.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS1.SSS2.p1.3.m3.1c" encoding="application/x-tex">W</annotation><annotation id="S5.SS1.SSS2.p1.3.m3.1d" encoding="application/x-llamapun">italic_W</annotation></semantics></math> 的困惑度可以表示为</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S5.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="PPL(W)=\exp(-\frac{1}{n}\sum_{i=1}^{n}\log{\rm Pr}(w_{i}|w_{&lt;i}))," class="ltx_Math" display="block" id="S5.E2.m1.3"><semantics id="S5.E2.m1.3a"><mrow id="S5.E2.m1.3.3.1" xref="S5.E2.m1.3.3.1.1.cmml"><mrow id="S5.E2.m1.3.3.1.1" xref="S5.E2.m1.3.3.1.1.cmml"><mrow id="S5.E2.m1.3.3.1.1.3" xref="S5.E2.m1.3.3.1.1.3.cmml"><mi id="S5.E2.m1.3.3.1.1.3.2" xref="S5.E2.m1.3.3.1.1.3.2.cmml">P</mi><mo id="S5.E2.m1.3.3.1.1.3.1" xref="S5.E2.m1.3.3.1.1.3.1.cmml">⁢</mo><mi id="S5.E2.m1.3.3.1.1.3.3" xref="S5.E2.m1.3.3.1.1.3.3.cmml">P</mi><mo id="S5.E2.m1.3.3.1.1.3.1a" xref="S5.E2.m1.3.3.1.1.3.1.cmml">⁢</mo><mi id="S5.E2.m1.3.3.1.1.3.4" xref="S5.E2.m1.3.3.1.1.3.4.cmml">L</mi><mo id="S5.E2.m1.3.3.1.1.3.1b" xref="S5.E2.m1.3.3.1.1.3.1.cmml">⁢</mo><mrow id="S5.E2.m1.3.3.1.1.3.5.2" xref="S5.E2.m1.3.3.1.1.3.cmml"><mo id="S5.E2.m1.3.3.1.1.3.5.2.1" stretchy="false" xref="S5.E2.m1.3.3.1.1.3.cmml">(</mo><mi id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml">W</mi><mo id="S5.E2.m1.3.3.1.1.3.5.2.2" stretchy="false" xref="S5.E2.m1.3.3.1.1.3.cmml">)</mo></mrow></mrow><mo id="S5.E2.m1.3.3.1.1.2" xref="S5.E2.m1.3.3.1.1.2.cmml">=</mo><mrow id="S5.E2.m1.3.3.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.2.cmml"><mi id="S5.E2.m1.2.2" xref="S5.E2.m1.2.2.cmml">exp</mi><mo id="S5.E2.m1.3.3.1.1.1.1a" xref="S5.E2.m1.3.3.1.1.1.2.cmml">⁡</mo><mrow id="S5.E2.m1.3.3.1.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.2.cmml"><mo id="S5.E2.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S5.E2.m1.3.3.1.1.1.2.cmml">(</mo><mrow id="S5.E2.m1.3.3.1.1.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S5.E2.m1.3.3.1.1.1.1.1.1a" xref="S5.E2.m1.3.3.1.1.1.1.1.1.cmml">−</mo><mrow id="S5.E2.m1.3.3.1.1.1.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mfrac id="S5.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mn id="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">n</mi></mfrac><mo id="S5.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml"><munderover id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><mo id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2" movablelimits="false" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3a" lspace="0.167em" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml">Pr</mi></mrow><mo id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">w</mi><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">w</mi><mrow id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S5.E2.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="S5.E2.m1.3.3.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S5.E2.m1.3.3.1.2" xref="S5.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.3b"><apply id="S5.E2.m1.3.3.1.1.cmml" xref="S5.E2.m1.3.3.1"><eq id="S5.E2.m1.3.3.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.2"></eq><apply id="S5.E2.m1.3.3.1.1.3.cmml" xref="S5.E2.m1.3.3.1.1.3"><times id="S5.E2.m1.3.3.1.1.3.1.cmml" xref="S5.E2.m1.3.3.1.1.3.1"></times><ci id="S5.E2.m1.3.3.1.1.3.2.cmml" xref="S5.E2.m1.3.3.1.1.3.2">𝑃</ci><ci id="S5.E2.m1.3.3.1.1.3.3.cmml" xref="S5.E2.m1.3.3.1.1.3.3">𝑃</ci><ci id="S5.E2.m1.3.3.1.1.3.4.cmml" xref="S5.E2.m1.3.3.1.1.3.4">𝐿</ci><ci id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1">𝑊</ci></apply><apply id="S5.E2.m1.3.3.1.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1"><exp id="S5.E2.m1.2.2.cmml" xref="S5.E2.m1.2.2"></exp><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1"><minus id="S5.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1"></minus><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1"><times id="S5.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.2"></times><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.3"><divide id="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.3"></divide><cn id="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.2">1</cn><ci id="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.3.3">𝑛</ci></apply><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1"><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3"><eq id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.2.3">𝑛</ci></apply><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><times id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2"></times><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3"><log id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1"></log><ci id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">Pr</ci></apply><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑤</ci><ci id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑤</ci><apply id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3"><lt id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S5.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.3c">PPL(W)=\exp(-\frac{1}{n}\sum_{i=1}^{n}\log{\rm Pr}(w_{i}|w_{&lt;i})),</annotation><annotation encoding="application/x-llamapun" id="S5.E2.m1.3d">italic_P italic_P italic_L ( italic_W ) = roman_exp ( - divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_log roman_Pr ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_w start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.SSS2.p1.4">where <math alttext="{\rm Pr}(w_{i}|w_{&lt;i})" class="ltx_Math" display="inline" id="S5.SS1.SSS2.p1.4.m1.1"><semantics id="S5.SS1.SSS2.p1.4.m1.1a"><mrow id="S5.SS1.SSS2.p1.4.m1.1.1" xref="S5.SS1.SSS2.p1.4.m1.1.1.cmml"><mi id="S5.SS1.SSS2.p1.4.m1.1.1.3" xref="S5.SS1.SSS2.p1.4.m1.1.1.3.cmml">Pr</mi><mo id="S5.SS1.SSS2.p1.4.m1.1.1.2" xref="S5.SS1.SSS2.p1.4.m1.1.1.2.cmml">⁢</mo><mrow id="S5.SS1.SSS2.p1.4.m1.1.1.1.1" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.cmml"><mo id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.2" stretchy="false" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.cmml"><msub id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.cmml"><mi id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.2" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.2.cmml">w</mi><mi id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.3" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.1" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.1.cmml">|</mo><msub id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.cmml"><mi id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.2" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.2.cmml">w</mi><mrow id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.cmml"><mi id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.2" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.1" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.3" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.3" stretchy="false" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p1.4.m1.1b"><apply id="S5.SS1.SSS2.p1.4.m1.1.1.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1"><times id="S5.SS1.SSS2.p1.4.m1.1.1.2.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.2"></times><ci id="S5.SS1.SSS2.p1.4.m1.1.1.3.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.3">Pr</ci><apply id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.1.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.1">conditional</csymbol><apply id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.1.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.2.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.2">𝑤</ci><ci id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.3.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.1.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.2.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.2">𝑤</ci><apply id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3"><lt id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.1.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.2.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.3.cmml" xref="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p1.4.m1.1c">{\rm Pr}(w_{i}|w_{&lt;i})</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS2.p1.4.m1.1d">roman_Pr ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_w start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> denotes the probability assigned by a LLM to the i-th token given the preceding tokens.
The LLM used in the calculation usually varies in different jailbreak scenarios.
In attack methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib56" title="">56</a>]</cite>, the target LLM is typically used to calculate perplexity, which can serve as a metric of jailbreak.
Whereas in defense methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib1" title="">1</a>]</cite>, a state-of-the-art LLM is more commonly employed to uniformly calculate perplexity, so as to provide a unified metric for the classifiers.
Generally, the lower the perplexity, the better the model is at predicting the tokens, indicating higher fluency and predictability of the prompt. Therefore, jailbreak prompts with lower perplexity are less likely to be detected by defense classifiers, thus achieving higher success rates&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib56" title="">56</a>]</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">其中 <math id="S5.SS1.SSS2.p1.4.m1.1" display="inline" class="ltx_Math" alttext="{\rm Pr}(w_{i}|w_{&lt;i})"><semantics id="S5.SS1.SSS2.p1.4.m1.1a"><mrow id="S5.SS1.SSS2.p1.4.m1.1.1"><mi id="S5.SS1.SSS2.p1.4.m1.1.1.3">Pr</mi><mo id="S5.SS1.SSS2.p1.4.m1.1.1.2">⁢</mo><mrow id="S5.SS1.SSS2.p1.4.m1.1.1.1.1"><mo stretchy="false" id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.2">(</mo><mrow id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1"><msub id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2"><mi id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.2">w</mi><mi id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.2.3">i</mi></msub><mo id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.1" fence="false">|</mo><msub id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3"><mi id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.2">w</mi><mrow id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3"><mi id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.2"></mi><mo id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.1">&lt;</mo><mi id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.1.3.3.3">i</mi></mrow></msub></mrow><mo stretchy="false" id="S5.SS1.SSS2.p1.4.m1.1.1.1.1.3">)</mo></mrow></mrow><annotation-xml id="S5.SS1.SSS2.p1.4.m1.1b" encoding="MathML-Content">conditionalsubscriptsubscriptabsent</annotation-xml><annotation id="S5.SS1.SSS2.p1.4.m1.1c" encoding="application/x-tex">{\rm Pr}(w_{i}|w_{&lt;i})</annotation><annotation id="S5.SS1.SSS2.p1.4.m1.1d" encoding="application/x-llamapun">roman_Pr ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_w start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> 表示 LLM 在给定前序词的情况下，为第 i 个词分配的概率。计算中使用的 LLM 通常在不同越狱场景中有所不同。在攻击方法[67, 56]中，目标 LLM 通常用于计算困惑度，这可以作为越狱的指标。而在防御方法[1]中，更常用最先进的 LLM 来统一计算困惑度，以便为分类器提供一个统一的指标。通常情况下，困惑度越低，模型在预测词元方面表现越好，表明提示词的流畅性和可预测性越高。因此，困惑度较低的越狱提示词不太可能被防御分类器检测到，从而实现更高的成功率[67, 56]。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:120%;"> Dataset<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">数据集</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.13">In&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#S4.T2" title="In Gradient and Logit Analysis ‣ Model-level Defenses ‣ Defense Methods ‣ Jailbreak Attacks and Defenses Against Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>, we provide a comprehensive description of the widely-used evaluation datasets. Especially, the column "Safety dimensions" indicates how many types of harmful categories are covered by the dataset, and the column "Composition" represents the main types of questions that make up the dataset.
We can observe that although current datasets are used mainly to evaluate LLM safety, they have different focus areas in various domains.
Some datasets have designed specific tasks to assess the safety of LLMs in particular scenarios.
<math alttext="\mathsf{TechHazardQA}" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">𝖳𝖾𝖼𝗁𝖧𝖺𝗓𝖺𝗋𝖽𝖰𝖠</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">𝖳𝖾𝖼𝗁𝖧𝖺𝗓𝖺𝗋𝖽𝖰𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\mathsf{TechHazardQA}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">sansserif_TechHazardQA</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib7" title="">7</a>]</cite> requires the model to give answers in text format or pseudo-code format, so as to examine the robustness of LLMs when they generate responses in specific forms.
<math alttext="\mathsf{Latent}" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mi id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">𝖫𝖺𝗍𝖾𝗇𝗍</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><ci id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">𝖫𝖺𝗍𝖾𝗇𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\mathsf{Latent}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">sansserif_Latent</annotation></semantics></math> <math alttext="\mathsf{Jailbreak}" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mi id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><ci id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\mathsf{Jailbreak}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">sansserif_Jailbreak</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib69" title="">69</a>]</cite> instructs the model to translate texts that may contain malicious content.
While <math alttext="\mathsf{Do}" class="ltx_Math" display="inline" id="S5.SS2.p1.4.m4.1"><semantics id="S5.SS2.p1.4.m4.1a"><mi id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml">𝖣𝗈</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><ci id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1">𝖣𝗈</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">\mathsf{Do}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.4.m4.1d">sansserif_Do</annotation></semantics></math>-<math alttext="\mathsf{not}" class="ltx_Math" display="inline" id="S5.SS2.p1.5.m5.1"><semantics id="S5.SS2.p1.5.m5.1a"><mi id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml">𝗇𝗈𝗍</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><ci id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1">𝗇𝗈𝗍</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">\mathsf{not}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.5.m5.1d">sansserif_not</annotation></semantics></math>-<math alttext="\mathsf{Answer}" class="ltx_Math" display="inline" id="S5.SS2.p1.6.m6.1"><semantics id="S5.SS2.p1.6.m6.1a"><mi id="S5.SS2.p1.6.m6.1.1" xref="S5.SS2.p1.6.m6.1.1.cmml">𝖠𝗇𝗌𝗐𝖾𝗋</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.1b"><ci id="S5.SS2.p1.6.m6.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1">𝖠𝗇𝗌𝗐𝖾𝗋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.1c">\mathsf{Answer}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.6.m6.1d">sansserif_Answer</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib96" title="">96</a>]</cite> completely consists of harmful prompts to estimate the safeguard of LLMs, <math alttext="\mathsf{XSTEST}" class="ltx_Math" display="inline" id="S5.SS2.p1.7.m7.1"><semantics id="S5.SS2.p1.7.m7.1a"><mi id="S5.SS2.p1.7.m7.1.1" xref="S5.SS2.p1.7.m7.1.1.cmml">𝖷𝖲𝖳𝖤𝖲𝖳</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.7.m7.1b"><ci id="S5.SS2.p1.7.m7.1.1.cmml" xref="S5.SS2.p1.7.m7.1.1">𝖷𝖲𝖳𝖤𝖲𝖳</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.7.m7.1c">\mathsf{XSTEST}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.7.m7.1d">sansserif_XSTEST</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib74" title="">74</a>]</cite> comprises both safe and unsafe questions to evaluate the balance between helpfulness and harmlessness of LLMs.
<math alttext="\mathsf{SC}" class="ltx_Math" display="inline" id="S5.SS2.p1.8.m8.1"><semantics id="S5.SS2.p1.8.m8.1a"><mi id="S5.SS2.p1.8.m8.1.1" xref="S5.SS2.p1.8.m8.1.1.cmml">𝖲𝖢</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.8.m8.1b"><ci id="S5.SS2.p1.8.m8.1.1.cmml" xref="S5.SS2.p1.8.m8.1.1">𝖲𝖢</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.8.m8.1c">\mathsf{SC}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.8.m8.1d">sansserif_SC</annotation></semantics></math>-<math alttext="\mathsf{Safety}" class="ltx_Math" display="inline" id="S5.SS2.p1.9.m9.1"><semantics id="S5.SS2.p1.9.m9.1a"><mi id="S5.SS2.p1.9.m9.1.1" xref="S5.SS2.p1.9.m9.1.1.cmml">𝖲𝖺𝖿𝖾𝗍𝗒</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.9.m9.1b"><ci id="S5.SS2.p1.9.m9.1.1.cmml" xref="S5.SS2.p1.9.m9.1.1">𝖲𝖺𝖿𝖾𝗍𝗒</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.9.m9.1c">\mathsf{Safety}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.9.m9.1d">sansserif_Safety</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib86" title="">86</a>]</cite> focus on the evaluation of Chinese LLMs, which interacts with the LLMs with multi-round open questions to observe their safety behaviors.
<math alttext="\mathsf{SafetyBench}" class="ltx_Math" display="inline" id="S5.SS2.p1.10.m10.1"><semantics id="S5.SS2.p1.10.m10.1a"><mi id="S5.SS2.p1.10.m10.1.1" xref="S5.SS2.p1.10.m10.1.1.cmml">𝖲𝖺𝖿𝖾𝗍𝗒𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.10.m10.1b"><ci id="S5.SS2.p1.10.m10.1.1.cmml" xref="S5.SS2.p1.10.m10.1.1">𝖲𝖺𝖿𝖾𝗍𝗒𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.10.m10.1c">\mathsf{SafetyBench}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.10.m10.1d">sansserif_SafetyBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib115" title="">115</a>]</cite> designs multiple-choice questions in both Chinese and English that cover various safety concerns to assess the safety of popular LLMs.
<math alttext="\mathsf{AdvBench}" class="ltx_Math" display="inline" id="S5.SS2.p1.11.m11.1"><semantics id="S5.SS2.p1.11.m11.1a"><mi id="S5.SS2.p1.11.m11.1.1" xref="S5.SS2.p1.11.m11.1.1.cmml">𝖠𝖽𝗏𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.11.m11.1b"><ci id="S5.SS2.p1.11.m11.1.1.cmml" xref="S5.SS2.p1.11.m11.1.1">𝖠𝖽𝗏𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.11.m11.1c">\mathsf{AdvBench}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.11.m11.1d">sansserif_AdvBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib125" title="">125</a>]</cite> is initially proposed by <math alttext="\mathsf{GCG}" class="ltx_Math" display="inline" id="S5.SS2.p1.12.m12.1"><semantics id="S5.SS2.p1.12.m12.1a"><mi id="S5.SS2.p1.12.m12.1.1" xref="S5.SS2.p1.12.m12.1.1.cmml">𝖦𝖢𝖦</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.12.m12.1b"><ci id="S5.SS2.p1.12.m12.1.1.cmml" xref="S5.SS2.p1.12.m12.1.1">𝖦𝖢𝖦</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.12.m12.1c">\mathsf{GCG}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.12.m12.1d">sansserif_GCG</annotation></semantics></math> to construct suffixes for gradient-based attacks, and has been utilized by other studies like AdvPrompter&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib67" title="">67</a>]</cite> in various jailbreak scenarios.
<math alttext="\mathsf{SafeBench}" class="ltx_Math" display="inline" id="S5.SS2.p1.13.m13.1"><semantics id="S5.SS2.p1.13.m13.1a"><mi id="S5.SS2.p1.13.m13.1.1" xref="S5.SS2.p1.13.m13.1.1.cmml">𝖲𝖺𝖿𝖾𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.13.m13.1b"><ci id="S5.SS2.p1.13.m13.1.1.cmml" xref="S5.SS2.p1.13.m13.1.1">𝖲𝖺𝖿𝖾𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.13.m13.1c">\mathsf{SafeBench}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.13.m13.1d">sansserif_SafeBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib30" title="">30</a>]</cite> is a collection of harmful textual prompts that can be converted into images to bypass the safeguard of VLMs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在表 2 中，我们提供了广泛使用的评估数据集的全面描述。特别是，“安全维度”这一列表明数据集涵盖了多少种有害类别，而“组成”这一列则代表构成数据集的主要问题类型。我们可以观察到，尽管当前数据集主要用于评估 LLM 的安全性，但它们在不同领域有不同的关注点。一些数据集设计了特定任务，以评估 LLM 在特定场景下的安全性。 <math id="S5.SS2.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{TechHazardQA}"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1">𝖳𝖾𝖼𝗁𝖧𝖺𝗓𝖺𝗋𝖽𝖰𝖠</mi><annotation-xml id="S5.SS2.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.1.m1.1c" encoding="application/x-tex">\mathsf{TechHazardQA}</annotation><annotation id="S5.SS2.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_TechHazardQA</annotation></semantics></math> [7] 要求模型以文本格式或伪代码格式给出答案，以便考察 LLM 在以特定形式生成响应时的鲁棒性。 <math id="S5.SS2.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{Latent}"><semantics id="S5.SS2.p1.2.m2.1a"><mi id="S5.SS2.p1.2.m2.1.1">𝖫𝖺𝗍𝖾𝗇𝗍</mi><annotation-xml id="S5.SS2.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.2.m2.1c" encoding="application/x-tex">\mathsf{Latent}</annotation><annotation id="S5.SS2.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_Latent</annotation></semantics></math> <math id="S5.SS2.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{Jailbreak}"><semantics id="S5.SS2.p1.3.m3.1a"><mi id="S5.SS2.p1.3.m3.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</mi><annotation-xml id="S5.SS2.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.3.m3.1c" encoding="application/x-tex">\mathsf{Jailbreak}</annotation><annotation id="S5.SS2.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_Jailbreak</annotation></semantics></math> [69] 指示模型翻译可能包含恶意内容文本。而 <math id="S5.SS2.p1.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{Do}"><semantics id="S5.SS2.p1.4.m4.1a"><mi id="S5.SS2.p1.4.m4.1.1">𝖣𝗈</mi><annotation-xml id="S5.SS2.p1.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.4.m4.1c" encoding="application/x-tex">\mathsf{Do}</annotation><annotation id="S5.SS2.p1.4.m4.1d" encoding="application/x-llamapun">sansserif_Do</annotation></semantics></math> - <math id="S5.SS2.p1.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{not}"><semantics id="S5.SS2.p1.5.m5.1a"><mi id="S5.SS2.p1.5.m5.1.1">𝗇𝗈𝗍</mi><annotation-xml id="S5.SS2.p1.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.5.m5.1c" encoding="application/x-tex">\mathsf{not}</annotation><annotation id="S5.SS2.p1.5.m5.1d" encoding="application/x-llamapun">sansserif_not</annotation></semantics></math> - <math id="S5.SS2.p1.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{Answer}"><semantics id="S5.SS2.p1.6.m6.1a"><mi id="S5.SS2.p1.6.m6.1.1">𝖠𝗇𝗌𝗐𝖾𝗋</mi><annotation-xml id="S5.SS2.p1.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.6.m6.1c" encoding="application/x-tex">\mathsf{Answer}</annotation><annotation id="S5.SS2.p1.6.m6.1d" encoding="application/x-llamapun">sansserif_Answer</annotation></semantics></math> [96] 完全由有害提示组成，用于评估 LLM 的安全防护能力， <math id="S5.SS2.p1.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{XSTEST}"><semantics id="S5.SS2.p1.7.m7.1a"><mi id="S5.SS2.p1.7.m7.1.1">𝖷𝖲𝖳𝖤𝖲𝖳</mi><annotation-xml id="S5.SS2.p1.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.7.m7.1c" encoding="application/x-tex">\mathsf{XSTEST}</annotation><annotation id="S5.SS2.p1.7.m7.1d" encoding="application/x-llamapun">sansserif_XSTEST</annotation></semantics></math> [74] 则包含安全和不安全的问题，用于评估 LLM 在有益性和无害性之间的平衡。 <math id="S5.SS2.p1.8.m8.1" display="inline" class="ltx_Math" alttext="\mathsf{SC}"><semantics id="S5.SS2.p1.8.m8.1a"><mi id="S5.SS2.p1.8.m8.1.1">𝖲𝖢</mi><annotation-xml id="S5.SS2.p1.8.m8.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.8.m8.1c" encoding="application/x-tex">\mathsf{SC}</annotation><annotation id="S5.SS2.p1.8.m8.1d" encoding="application/x-llamapun">sansserif_SC</annotation></semantics></math> - <math id="S5.SS2.p1.9.m9.1" display="inline" class="ltx_Math" alttext="\mathsf{Safety}"><semantics id="S5.SS2.p1.9.m9.1a"><mi id="S5.SS2.p1.9.m9.1.1">𝖲𝖺𝖿𝖾𝗍𝗒</mi><annotation-xml id="S5.SS2.p1.9.m9.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.9.m9.1c" encoding="application/x-tex">\mathsf{Safety}</annotation><annotation id="S5.SS2.p1.9.m9.1d" encoding="application/x-llamapun">sansserif_Safety</annotation></semantics></math> [86] 专注于中文 LLM 的评估，通过与 LLM 进行多轮开放式问题互动，观察其安全行为。 <math id="S5.SS2.p1.10.m10.1" display="inline" class="ltx_Math" alttext="\mathsf{SafetyBench}"><semantics id="S5.SS2.p1.10.m10.1a"><mi id="S5.SS2.p1.10.m10.1.1">𝖲𝖺𝖿𝖾𝗍𝗒𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml id="S5.SS2.p1.10.m10.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.10.m10.1c" encoding="application/x-tex">\mathsf{SafetyBench}</annotation><annotation id="S5.SS2.p1.10.m10.1d" encoding="application/x-llamapun">sansserif_SafetyBench</annotation></semantics></math> [ 115] 设计了中英文选择题，涵盖各种安全问题，用于评估流行 LLMs 的安全性。 <math id="S5.SS2.p1.11.m11.1" display="inline" class="ltx_Math" alttext="\mathsf{AdvBench}"><semantics id="S5.SS2.p1.11.m11.1a"><mi id="S5.SS2.p1.11.m11.1.1">𝖠𝖽𝗏𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml id="S5.SS2.p1.11.m11.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.11.m11.1c" encoding="application/x-tex">\mathsf{AdvBench}</annotation><annotation id="S5.SS2.p1.11.m11.1d" encoding="application/x-llamapun">sansserif_AdvBench</annotation></semantics></math> [ 125] 最初由 <math id="S5.SS2.p1.12.m12.1" display="inline" class="ltx_Math" alttext="\mathsf{GCG}"><semantics id="S5.SS2.p1.12.m12.1a"><mi id="S5.SS2.p1.12.m12.1.1">𝖦𝖢𝖦</mi><annotation-xml id="S5.SS2.p1.12.m12.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.12.m12.1c" encoding="application/x-tex">\mathsf{GCG}</annotation><annotation id="S5.SS2.p1.12.m12.1d" encoding="application/x-llamapun">sansserif_GCG</annotation></semantics></math> 提出，用于构建基于梯度的攻击的后缀，并在 AdvPrompter [ 67] 等研究中被用于各种越狱场景。 <math id="S5.SS2.p1.13.m13.1" display="inline" class="ltx_Math" alttext="\mathsf{SafeBench}"><semantics id="S5.SS2.p1.13.m13.1a"><mi id="S5.SS2.p1.13.m13.1.1">𝖲𝖺𝖿𝖾𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml id="S5.SS2.p1.13.m13.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p1.13.m13.1c" encoding="application/x-tex">\mathsf{SafeBench}</annotation><annotation id="S5.SS2.p1.13.m13.1d" encoding="application/x-llamapun">sansserif_SafeBench</annotation></semantics></math> [ 30] 是一组有害文本提示的集合，可以转换为图像以绕过 VLMs 的防护。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.7">Some datasets are introduced by toolkits as part of their automated evaluation pipeline.
Based on the similarities in the usage policies of different mainstream models. <math alttext="\mathsf{StrongREJECT}" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">𝖲𝗍𝗋𝗈𝗇𝗀𝖱𝖤𝖩𝖤𝖢𝖳</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝖲𝗍𝗋𝗈𝗇𝗀𝖱𝖤𝖩𝖤𝖢𝖳</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\mathsf{StrongREJECT}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">sansserif_StrongREJECT</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib84" title="">84</a>]</cite> propose a universal dataset that consists of forbidden questions that should be rejected by most LLMs.
<math alttext="\mathsf{AttackEval}" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><mi id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">𝖠𝗍𝗍𝖺𝖼𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">𝖠𝗍𝗍𝖺𝖼𝗄𝖤𝗏𝖺𝗅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">\mathsf{AttackEval}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">sansserif_AttackEval</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib80" title="">80</a>]</cite> develop a dataset containing jailbreak prompts with ground truth, which can serve as a robust standard to estimate the effectiveness of the jailbreak.
<math alttext="\mathsf{HarmBench}" class="ltx_Math" display="inline" id="S5.SS2.p2.3.m3.1"><semantics id="S5.SS2.p2.3.m3.1a"><mi id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><ci id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">\mathsf{HarmBench}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.3.m3.1d">sansserif_HarmBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib63" title="">63</a>]</cite> constructs a spectrum of special harmful behaviors as the dataset. Besides standard harmful behaviors, <math alttext="\mathsf{HarmBench}" class="ltx_Math" display="inline" id="S5.SS2.p2.4.m4.1"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><ci id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">\mathsf{HarmBench}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.4.m4.1d">sansserif_HarmBench</annotation></semantics></math> further introduces copyright behaviors, contextual behaviors, and multimodal behaviors for specific evaluations.
Aiming to provide a comprehensive assessment of Chinese LLMs, <math alttext="\mathsf{Safety}" class="ltx_Math" display="inline" id="S5.SS2.p2.5.m5.1"><semantics id="S5.SS2.p2.5.m5.1a"><mi id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml">𝖲𝖺𝖿𝖾𝗍𝗒</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><ci id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1">𝖲𝖺𝖿𝖾𝗍𝗒</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">\mathsf{Safety}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.5.m5.1d">sansserif_Safety</annotation></semantics></math>-<math alttext="\mathsf{Prompts}" class="ltx_Math" display="inline" id="S5.SS2.p2.6.m6.1"><semantics id="S5.SS2.p2.6.m6.1a"><mi id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><ci id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">\mathsf{Prompts}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.6.m6.1d">sansserif_Prompts</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib86" title="">86</a>]</cite> constructs a vast amount of malicious prompts in Chinese by instructing GPT-3.5-turbo to enhance high-quality artificial data.
<math alttext="\mathsf{JailbreakBench}" class="ltx_Math" display="inline" id="S5.SS2.p2.7.m7.1"><semantics id="S5.SS2.p2.7.m7.1a"><mi id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><ci id="S5.SS2.p2.7.m7.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">\mathsf{JailbreakBench}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.7.m7.1d">sansserif_JailbreakBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib14" title="">14</a>]</cite> constructs a mixed dataset that covers OpenAI’s usage policy, in which every harmful behavior is matched with a benign behavior to examine both the safety and robustness of target LLMs.
To achieve a comprehensive understanding of jailbreak prompts in the wild, Shen et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib79" title="">79</a>]</cite> conduct an extensive investigation of prompts sourced from online platforms, classifying them into distinct communities based on their characteristics.
Moreover, when presented with a scenario prohibited by OpenAI’s usage policy, they utilize GPT-4 to generate jailbreak prompts for different communities, thereby constructing a large set of forbidden questions.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">一些数据集由工具包作为其自动化评估流程的一部分引入。基于不同主流模型使用策略的相似性， <math id="S5.SS2.p2.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{StrongREJECT}"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1">𝖲𝗍𝗋𝗈𝗇𝗀𝖱𝖤𝖩𝖤𝖢𝖳</mi><annotation-xml id="S5.SS2.p2.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p2.1.m1.1c" encoding="application/x-tex">\mathsf{StrongREJECT}</annotation><annotation id="S5.SS2.p2.1.m1.1d" encoding="application/x-llamapun">sansserif_StrongREJECT</annotation></semantics></math> [ 84] 提出一个通用数据集，其中包含大多数 LLMs 应拒绝的禁止性问题。 <math id="S5.SS2.p2.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{AttackEval}"><semantics id="S5.SS2.p2.2.m2.1a"><mi id="S5.SS2.p2.2.m2.1.1">𝖠𝗍𝗍𝖺𝖼𝗄𝖤𝗏𝖺𝗅</mi><annotation-xml id="S5.SS2.p2.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p2.2.m2.1c" encoding="application/x-tex">\mathsf{AttackEval}</annotation><annotation id="S5.SS2.p2.2.m2.1d" encoding="application/x-llamapun">sansserif_AttackEval</annotation></semantics></math> [ 80] 开发了一个包含有真实标签的越狱提示的数据集，该数据集可作为评估越狱有效性的可靠标准。 <math id="S5.SS2.p2.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{HarmBench}"><semantics id="S5.SS2.p2.3.m3.1a"><mi id="S5.SS2.p2.3.m3.1.1">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml id="S5.SS2.p2.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p2.3.m3.1c" encoding="application/x-tex">\mathsf{HarmBench}</annotation><annotation id="S5.SS2.p2.3.m3.1d" encoding="application/x-llamapun">sansserif_HarmBench</annotation></semantics></math> [ 63] 构建了一个特殊有害行为谱系作为数据集。除了标准有害行为外， <math id="S5.SS2.p2.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{HarmBench}"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml id="S5.SS2.p2.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p2.4.m4.1c" encoding="application/x-tex">\mathsf{HarmBench}</annotation><annotation id="S5.SS2.p2.4.m4.1d" encoding="application/x-llamapun">sansserif_HarmBench</annotation></semantics></math> 进一步引入了版权行为、上下文行为和多模态行为进行特定评估。为了全面评估中文 LLMs， <math id="S5.SS2.p2.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{Safety}"><semantics id="S5.SS2.p2.5.m5.1a"><mi id="S5.SS2.p2.5.m5.1.1">𝖲𝖺𝖿𝖾𝗍𝗒</mi><annotation-xml id="S5.SS2.p2.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p2.5.m5.1c" encoding="application/x-tex">\mathsf{Safety}</annotation><annotation id="S5.SS2.p2.5.m5.1d" encoding="application/x-llamapun">sansserif_Safety</annotation></semantics></math> - <math id="S5.SS2.p2.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{Prompts}"><semantics id="S5.SS2.p2.6.m6.1a"><mi id="S5.SS2.p2.6.m6.1.1">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</mi><annotation-xml id="S5.SS2.p2.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p2.6.m6.1c" encoding="application/x-tex">\mathsf{Prompts}</annotation><annotation id="S5.SS2.p2.6.m6.1d" encoding="application/x-llamapun">sansserif_Prompts</annotation></semantics></math> [ 86] 通过指示 GPT-3.5-turbo 生成大量中文恶意提示，以增强高质量人工数据。 <math id="S5.SS2.p2.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{JailbreakBench}"><semantics id="S5.SS2.p2.7.m7.1a"><mi id="S5.SS2.p2.7.m7.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml id="S5.SS2.p2.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS2.p2.7.m7.1c" encoding="application/x-tex">\mathsf{JailbreakBench}</annotation><annotation id="S5.SS2.p2.7.m7.1d" encoding="application/x-llamapun">sansserif_JailbreakBench</annotation></semantics></math> [ 14] 构建了一个混合数据集，涵盖 OpenAI 的使用策略，其中每种有害行为都与一种良性行为相匹配，以检查目标 LLMs 的安全性和鲁棒性。 为了全面了解现实世界中的越狱提示，Shen 等人[79]对来自在线平台的提示进行了广泛调查，根据其特征将它们分类到不同的社区中。此外，当面对 OpenAI 使用政策禁止的场景时，他们利用 GPT-4 为不同社区生成越狱提示，从而构建了一个大型禁止问题集。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:120%;"> Toolkit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">工具包</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.7">Compared to datasets that are mostly used for evaluating the safety of LLMs, toolkits often integrate whole evaluation pipelines and can be extended to assess jailbreak attacks automatically.
<math alttext="\mathsf{HarmBench}" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\mathsf{HarmBench}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">sansserif_HarmBench</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib63" title="">63</a>]</cite> proposes a red-teaming evaluation framework that can estimate both jailbreak attack and defense methods.
Given a jailbreak attack method and a safety-aligned target LLM, the framework will first generate test cases with different harmful behaviors to jailbreak the target model.
Then, the responses and the corresponding behaviors are combined for evaluation, where several classifiers work together to generate the final ASR. <math alttext="\mathsf{Safety}" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><mi id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml">𝖲𝖺𝖿𝖾𝗍𝗒</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><ci id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">𝖲𝖺𝖿𝖾𝗍𝗒</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">\mathsf{Safety}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">sansserif_Safety</annotation></semantics></math>-<math alttext="\mathsf{Prompts}" class="ltx_Math" display="inline" id="S5.SS3.p1.3.m3.1"><semantics id="S5.SS3.p1.3.m3.1a"><mi id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><ci id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">\mathsf{Prompts}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.3.m3.1d">sansserif_Prompts</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib86" title="">86</a>]</cite> establish a platform to estimate the safety of Chinese LLMs.
In the evaluation, jailbreak prompts of different safety scenarios are inputted to the target LLM, and the responses are later examined by a LLM evaluator to give a comprehensive score to judge the safety of the target LLM.
To provide a comprehensive and reproducible comparison of current jailbreak research, Chao et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib14" title="">14</a>]</cite> develop <math alttext="\mathsf{JailbreakBench}" class="ltx_Math" display="inline" id="S5.SS3.p1.4.m4.1"><semantics id="S5.SS3.p1.4.m4.1a"><mi id="S5.SS3.p1.4.m4.1.1" xref="S5.SS3.p1.4.m4.1.1.cmml">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.4.m4.1b"><ci id="S5.SS3.p1.4.m4.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.4.m4.1c">\mathsf{JailbreakBench}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.4.m4.1d">sansserif_JailbreakBench</annotation></semantics></math>, a lightweight evaluation framework applicable to jailbreak attack and defense methods. Especially, <math alttext="\mathsf{JailbreakBench}" class="ltx_Math" display="inline" id="S5.SS3.p1.5.m5.1"><semantics id="S5.SS3.p1.5.m5.1a"><mi id="S5.SS3.p1.5.m5.1.1" xref="S5.SS3.p1.5.m5.1.1.cmml">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.5.m5.1b"><ci id="S5.SS3.p1.5.m5.1.1.cmml" xref="S5.SS3.p1.5.m5.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.5.m5.1c">\mathsf{JailbreakBench}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.5.m5.1d">sansserif_JailbreakBench</annotation></semantics></math> has maintained most of the state-of-the-art adversarial prompts, defense methods, and evaluation classifiers so that users can easily invoke them to construct a personal evaluation pipeline.
<math alttext="\mathsf{EasyJailbreak}" class="ltx_Math" display="inline" id="S5.SS3.p1.6.m6.1"><semantics id="S5.SS3.p1.6.m6.1a"><mi id="S5.SS3.p1.6.m6.1.1" xref="S5.SS3.p1.6.m6.1.1.cmml">𝖤𝖺𝗌𝗒𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.6.m6.1b"><ci id="S5.SS3.p1.6.m6.1.1.cmml" xref="S5.SS3.p1.6.m6.1.1">𝖤𝖺𝗌𝗒𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.6.m6.1c">\mathsf{EasyJailbreak}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.6.m6.1d">sansserif_EasyJailbreak</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.04295v2#bib.bib122" title="">122</a>]</cite> proposes a standardized framework consisting of three stages to estimate jailbreak attacks. In the preparation stage, jailbreak settings including malicious questions and template seeds are provided by the user.
Then in the inference stage, <math alttext="\mathsf{EasyJailbreak}" class="ltx_Math" display="inline" id="S5.SS3.p1.7.m7.1"><semantics id="S5.SS3.p1.7.m7.1a"><mi id="S5.SS3.p1.7.m7.1.1" xref="S5.SS3.p1.7.m7.1.1.cmml">𝖤𝖺𝗌𝗒𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.7.m7.1b"><ci id="S5.SS3.p1.7.m7.1.1.cmml" xref="S5.SS3.p1.7.m7.1.1">𝖤𝖺𝗌𝗒𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.7.m7.1c">\mathsf{EasyJailbreak}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.7.m7.1d">sansserif_EasyJailbreak</annotation></semantics></math> applies templates to the questions to construct jailbreak prompts, and mutates the prompts before inputting them into the target model to get responses. In the final stage, the queries and corresponding responses are inspected by LLM-based or rule-based evaluators to give the overall metrics.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">与主要用于评估 LLMs 安全性的数据集相比，工具包通常集成了整个评估流程，并且可以扩展以自动评估越狱攻击。 <math id="S5.SS3.p1.1.m1.1" display="inline" class="ltx_Math" alttext="\mathsf{HarmBench}"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1">𝖧𝖺𝗋𝗆𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml id="S5.SS3.p1.1.m1.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS3.p1.1.m1.1c" encoding="application/x-tex">\mathsf{HarmBench}</annotation><annotation id="S5.SS3.p1.1.m1.1d" encoding="application/x-llamapun">sansserif_HarmBench</annotation></semantics></math> [ 63] 提出了一种红队评估框架，可以评估越狱攻击和防御方法。给定一种越狱攻击方法和一个安全对齐的目标 LLM，该框架将首先生成具有不同有害行为的测试用例来越狱目标模型。然后，将响应及其对应行为结合起来进行评估，其中多个分类器协同工作以生成最终的 ASR。 <math id="S5.SS3.p1.2.m2.1" display="inline" class="ltx_Math" alttext="\mathsf{Safety}"><semantics id="S5.SS3.p1.2.m2.1a"><mi id="S5.SS3.p1.2.m2.1.1">𝖲𝖺𝖿𝖾𝗍𝗒</mi><annotation-xml id="S5.SS3.p1.2.m2.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS3.p1.2.m2.1c" encoding="application/x-tex">\mathsf{Safety}</annotation><annotation id="S5.SS3.p1.2.m2.1d" encoding="application/x-llamapun">sansserif_Safety</annotation></semantics></math> - <math id="S5.SS3.p1.3.m3.1" display="inline" class="ltx_Math" alttext="\mathsf{Prompts}"><semantics id="S5.SS3.p1.3.m3.1a"><mi id="S5.SS3.p1.3.m3.1.1">𝖯𝗋𝗈𝗆𝗉𝗍𝗌</mi><annotation-xml id="S5.SS3.p1.3.m3.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS3.p1.3.m3.1c" encoding="application/x-tex">\mathsf{Prompts}</annotation><annotation id="S5.SS3.p1.3.m3.1d" encoding="application/x-llamapun">sansserif_Prompts</annotation></semantics></math> [ 86] 建立了一个平台来评估中文 LLMs 的安全性。在评估中，不同安全场景的越狱提示被输入到目标 LLM 中，响应随后由 LLM 评估器检查，以给出综合评分来判断目标 LLM 的安全性。为了提供当前越狱研究的全面且可重复的比较，Chao 等人[ 14]开发了 <math id="S5.SS3.p1.4.m4.1" display="inline" class="ltx_Math" alttext="\mathsf{JailbreakBench}"><semantics id="S5.SS3.p1.4.m4.1a"><mi id="S5.SS3.p1.4.m4.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml id="S5.SS3.p1.4.m4.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS3.p1.4.m4.1c" encoding="application/x-tex">\mathsf{JailbreakBench}</annotation><annotation id="S5.SS3.p1.4.m4.1d" encoding="application/x-llamapun">sansserif_JailbreakBench</annotation></semantics></math> ，一个适用于越狱攻击和防御方法的轻量级评估框架。 特别是， <math id="S5.SS3.p1.5.m5.1" display="inline" class="ltx_Math" alttext="\mathsf{JailbreakBench}"><semantics id="S5.SS3.p1.5.m5.1a"><mi id="S5.SS3.p1.5.m5.1.1">𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄𝖡𝖾𝗇𝖼𝗁</mi><annotation-xml id="S5.SS3.p1.5.m5.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS3.p1.5.m5.1c" encoding="application/x-tex">\mathsf{JailbreakBench}</annotation><annotation id="S5.SS3.p1.5.m5.1d" encoding="application/x-llamapun">sansserif_JailbreakBench</annotation></semantics></math> 维护了大部分最先进的对抗性提示、防御方法和评估分类器，以便用户可以轻松调用它们来构建个人评估流程。 <math id="S5.SS3.p1.6.m6.1" display="inline" class="ltx_Math" alttext="\mathsf{EasyJailbreak}"><semantics id="S5.SS3.p1.6.m6.1a"><mi id="S5.SS3.p1.6.m6.1.1">𝖤𝖺𝗌𝗒𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</mi><annotation-xml id="S5.SS3.p1.6.m6.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS3.p1.6.m6.1c" encoding="application/x-tex">\mathsf{EasyJailbreak}</annotation><annotation id="S5.SS3.p1.6.m6.1d" encoding="application/x-llamapun">sansserif_EasyJailbreak</annotation></semantics></math> [ 122] 提出了一种包含三个阶段的标准化框架来评估越狱攻击。在准备阶段，用户提供包括恶意问题和模板种子在内的越狱设置。然后在推理阶段， <math id="S5.SS3.p1.7.m7.1" display="inline" class="ltx_Math" alttext="\mathsf{EasyJailbreak}"><semantics id="S5.SS3.p1.7.m7.1a"><mi id="S5.SS3.p1.7.m7.1.1">𝖤𝖺𝗌𝗒𝖩𝖺𝗂𝗅𝖻𝗋𝖾𝖺𝗄</mi><annotation-xml id="S5.SS3.p1.7.m7.1b" encoding="MathML-Content"></annotation-xml><annotation id="S5.SS3.p1.7.m7.1c" encoding="application/x-tex">\mathsf{EasyJailbreak}</annotation><annotation id="S5.SS3.p1.7.m7.1d" encoding="application/x-llamapun">sansserif_EasyJailbreak</annotation></semantics></math> 将模板应用于问题以构建越狱提示，并在将提示输入目标模型以获取响应之前对其进行变异。在最终阶段，基于 LLM 或基于规则的评估器检查查询和相应的响应，以给出整体指标。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;"> Conclusion<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">结论</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we present a comprehensive taxonomy of attack and defense methods in jailbreaking LLMs and a detailed paradigm to demonstrate their relationship.
We summarize the existing work and notice that the attack methods are becoming more effective and require less knowledge of the target model, which makes the attacks more practical, calling for effective defenses.
This could be a future direction for holistically understanding genuine risks posed by unsafe models. Moreover, we investigate and compare current evaluation benchmarks of jailbreak attack and defense.
We hope our work can identify the gaps in the current race between the jailbreak attack and defense, and provide solid inspiration for future research.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在本文中，我们提出了针对 LLMs 的越狱攻击与防御方法的综合分类体系，并详细阐述了一个展示它们之间关系的范例。我们总结了现有工作，并注意到攻击方法正变得越来越有效，且对目标模型的知识要求降低，这使得攻击更具实用性，从而呼唤有效的防御措施。这可能是全面理解不安全模型所构成的真实风险的未来方向。此外，我们还调查并比较了当前针对越狱攻击与防御的评价基准。我们希望我们的工作能够识别出当前越狱攻击与防御竞赛中的不足之处，并为未来的研究提供坚实的启示。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gabriel Alon and Michael Kamfonas.

</span>
<span class="ltx_bibblock">Detecting Language Model Attacks with Perplexity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">CoRR abs/2308.14132</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion.

</span>
<span class="ltx_bibblock">Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">CoRR abs/2404.02151</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy&nbsp;P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul&nbsp;Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini: A Family of Highly Capable Multimodal Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">CoRR abs/2312.11805</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing claude.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/introducing-claude" title="">https://www.anthropic.com/news/introducing-claude</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Many-shot jailbreaking.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/research/many-shot-jailbreaking" title="">https://www.anthropic.com/research/many-shot-jailbreaking</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer&nbsp;El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom&nbsp;B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan.

</span>
<span class="ltx_bibblock">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">CoRR abs/2204.05862</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Somnath Banerjee, Sayan Layek, Rima Hazra, and Animesh Mukherjee.

</span>
<span class="ltx_bibblock">How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">CoRR abs/2402.15302</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rishabh Bhardwaj and Soujanya Poria.

</span>
<span class="ltx_bibblock">Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">CoRR abs/2308.09662</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou.

</span>
<span class="ltx_bibblock">Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">International Conference on Learning Representations (ICLR)</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Annual Conference on Neural Information Processing Systems (NeurIPS)</span>. NeurIPS, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bochuan Cao, Yuanpu Cao, Lu&nbsp;Lin, and Jinghui Chen.

</span>
<span class="ltx_bibblock">Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">CoRR abs/2309.14348</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell.

</span>
<span class="ltx_bibblock">Explore, Establish, Exploit: Red Teaming Language Models from Scratch.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">CoRR abs/2306.09442</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiyuan Chang, Mingyang Li, Yi&nbsp;Liu, Junjie Wang, Qing Wang, and Yang Liu.

</span>
<span class="ltx_bibblock">Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">CoRR abs/2402.09091</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George&nbsp;J. Pappas, Florian Tramèr, Hamed Hassani, and Eric Wong.

</span>
<span class="ltx_bibblock">JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">CoRR abs/2404.01318</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George&nbsp;J. Pappas, and Eric Wong.

</span>
<span class="ltx_bibblock">Jailbreaking Black Box Large Language Models in Twenty Queries.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">CoRR abs/2310.08419</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique&nbsp;Pondé de&nbsp;Oliveira&nbsp;Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William&nbsp;Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew&nbsp;N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Evaluating Large Language Models Trained on Code.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">CoRR abs/2107.03374</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang.

</span>
<span class="ltx_bibblock">Comprehensive Assessment of Jailbreak Attacks Against LLMs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">CoRR abs/2402.05668</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, and Xiangnan He.

</span>
<span class="ltx_bibblock">Attack Prompt Generation for Red Teaming and Defending Large Language Models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 2176–2189. ACL, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gelei Deng, Yi&nbsp;Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu.

</span>
<span class="ltx_bibblock">MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">CoRR abs/2307.08715</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gelei Deng, Yi&nbsp;Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, and Yang Liu.

</span>
<span class="ltx_bibblock">Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">CoRR abs/2402.08416</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yue Deng, Wenxuan Zhang, Sinno&nbsp;Jialin Pan, and Lidong Bing.

</span>
<span class="ltx_bibblock">Multilingual Jailbreak Challenges in Large Language Models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">International Conference on Learning Representations (ICLR)</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, and Shujian Huang.

</span>
<span class="ltx_bibblock">A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">CoRR abs/2311.08268</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yanrui Du, Sendong Zhao, Ming Ma, Yuhan Chen, and Bing Qin.

</span>
<span class="ltx_bibblock">Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">CoRR abs/2312.04127</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aysan Esmradi, Daniel&nbsp;Wankit Yip, and Chun-Fai Chan.

</span>
<span class="ltx_bibblock">A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">CoRR abs/2312.10982</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Víctor Gallego.

</span>
<span class="ltx_bibblock">Configurable Safety Tuning of Language Models with Synthetic Preference Data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">CoRR abs/2404.00495</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer&nbsp;El Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark.

</span>
<span class="ltx_bibblock">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">CoRR abs/2209.07858</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Suyu Ge, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan Wang, Jiawei Han, and Yuning Mao.

</span>
<span class="ltx_bibblock">MART: improving LLM safety with multi-round automatic red-teaming.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">CoRR abs/2311.07689</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom Goldstein.

</span>
<span class="ltx_bibblock">Coercing LLMs to do and reveal (almost) anything.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">CoRR abs/2402.14020</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Simon Geisler, Tom Wollschläger, M.&nbsp;H.&nbsp;I. Abdalla, Johannes Gasteiger, and Stephan Günnemann.

</span>
<span class="ltx_bibblock">Attacking Large Language Models with Projected Gradient Descent.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">CoRR abs/2402.09154</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang.

</span>
<span class="ltx_bibblock">FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">CoRR abs/2311.05608</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, and Bin Hu.

</span>
<span class="ltx_bibblock">COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">CoRR abs/2402.08679</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Maanak Gupta, Charankumar Akiri, Kshitiz Aryal, Eli Parker, and Lopamudra Praharaj.

</span>
<span class="ltx_bibblock">From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">CoRR abs/2307.00691</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Divij Handa, Advait Chirmule, Bimal&nbsp;G. Gajera, and Chitta Baral.

</span>
<span class="ltx_bibblock">Jailbreaking Proprietary Large Language Models using Word Substitution Cipher.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">CoRR abs/2402.10601</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tramèr, and Milad Nasr.

</span>
<span class="ltx_bibblock">Query-Based Adversarial Prompt Generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">CoRR abs/2402.12329</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho.

</span>
<span class="ltx_bibblock">Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">CoRR abs/2403.00867</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen.

</span>
<span class="ltx_bibblock">Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">International Conference on Learning Representations (ICLR)</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein.

</span>
<span class="ltx_bibblock">Baseline Defenses for Adversarial Attacks Against Aligned Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">CoRR abs/2309.00614</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiabao Ji, Bairu Hou, Alexander Robey, George&nbsp;J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, and Shiyu Chang.

</span>
<span class="ltx_bibblock">Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">CoRR abs/2402.16192</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce&nbsp;Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang.

</span>
<span class="ltx_bibblock">Beavertails: Towards improved safety alignment of LLM via a human-preference dataset.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo&nbsp;Li, and Radha Poovendran.

</span>
<span class="ltx_bibblock">ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">CoRR abs/2402.11753</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, and Haohan Wang.

</span>
<span class="ltx_bibblock">GUARD: role-playing to generate natural-language jailbreakings to test guideline adherence of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">CoRR abs/2402.03299</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Erik Jones, Anca&nbsp;D. Dragan, Aditi Raghunathan, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Automatically Auditing Large Language Models via Discrete Optimization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">International Conference on Machine Learning (ICML)</span>, pages 15307–15329. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto.

</span>
<span class="ltx_bibblock">Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">CoRR abs/2302.05733</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Heegyu Kim, Sehyun Yuk, and Hyunsouk Cho.

</span>
<span class="ltx_bibblock">Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">CoRR abs/2402.15180</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju.

</span>
<span class="ltx_bibblock">Certifying LLM Safety against Adversarial Prompting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">CoRR abs/2309.02705</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Raz Lapid, Ron Langberg, and Moshe Sipper.

</span>
<span class="ltx_bibblock">Open Sesame! Universal Black Box Jailbreaking of Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">CoRR abs/2309.01446</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.

</span>
<span class="ltx_bibblock">Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">CoRR abs/2310.20624</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song.

</span>
<span class="ltx_bibblock">Multi-step Jailbreaking Privacy Attacks on ChatGPT.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">CoRR abs/2304.05197</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jie Li, Yi&nbsp;Liu, Chongyang Liu, Ling Shi, Xiaoning Ren, Yaowen Zheng, Yang Liu, and Yinxing Xue.

</span>
<span class="ltx_bibblock">A Cross-Language Investigation into Jailbreak Attacks in Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">CoRR abs/2401.16765</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, and Ee-Chien Chang.

</span>
<span class="ltx_bibblock">Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">CoRR abs/2402.14872</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh.

</span>
<span class="ltx_bibblock">DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">CoRR abs/2402.16914</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo&nbsp;Han.

</span>
<span class="ltx_bibblock">DeepInception: Hypnotize Large Language Model to Be Jailbreaker.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">CoRR abs/2311.03191</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang.

</span>
<span class="ltx_bibblock">RAIN: your language models can align themselves without finetuning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">CoRR abs/2309.07124</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, and Fei Wu.

</span>
<span class="ltx_bibblock">Goal-Oriented Prompt Attack and Safety Evaluation for LLMs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">CoRR abs/2309.11830</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, and Kai Chen.

</span>
<span class="ltx_bibblock">Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">CoRR abs/2402.18104</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao.

</span>
<span class="ltx_bibblock">AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">CoRR abs/2310.04451</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu.

</span>
<span class="ltx_bibblock">Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">CoRR abs/2305.13860</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yule Liu, Kaitian Chao&nbsp;Ting Lu, Yanshun Zhang, and Yingliang Zhang.

</span>
<span class="ltx_bibblock">Safe and helpful chinese.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/DirectLLM/Safe_and_Helpful_Chinese" title="">https://huggingface.co/datasets/DirectLLM/Safe_and_Helpful_Chinese</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zixuan Liu, Xiaolin Sun, and Zizhan Zheng.

</span>
<span class="ltx_bibblock">Enhancing LLM safety via constrained direct preference optimization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">CoRR abs/2403.02475</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang.

</span>
<span class="ltx_bibblock">An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">CoRR abs/2308.08747</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang, Shihan Dou, Junjie Ye, Tao Gui, Qi&nbsp;Zhang, and Xuanjing Huang.

</span>
<span class="ltx_bibblock">CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">CoRR abs/2402.16717</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem Fawaz, Somesh Jha, and Atul Prakash.

</span>
<span class="ltx_bibblock">PRP: propagating universal perturbations to attack large languagenmodel guard-rails.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">CoRR abs/2402.15911</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo&nbsp;Li, David&nbsp;A. Forsyth, and Dan Hendrycks.

</span>
<span class="ltx_bibblock">HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">CoRR abs/2402.04249</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi.

</span>
<span class="ltx_bibblock">Tree of Attacks: Jailbreaking Black-Box LLMs Automatically.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">CoRR abs/2312.02119</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">CoRR abs/2303.08774</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul&nbsp;F. Christiano, Jan Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Annual Conference on Neural Information Processing Systems (NeurIPS)</span>. NeurIPS, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong Tian.

</span>
<span class="ltx_bibblock">AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">CoRR abs/2404.16873</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiangyu Qi, Yi&nbsp;Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.

</span>
<span class="ltx_bibblock">Fine-tuning aligned language models compromises safety, even when users do not intend to!

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">CoRR abs/2310.03693</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan.

</span>
<span class="ltx_bibblock">Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">CoRR abs/2307.08487</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher&nbsp;D. Manning, Stefano Ermon, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">Annual Conference on Neural Information Processing Systems (NeurIPS)</span>. NeurIPS, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, and Anyu Wang.

</span>
<span class="ltx_bibblock">JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">CoRR abs/2406.09321</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit Choudhury.

</span>
<span class="ltx_bibblock">Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">CoRR abs/2305.14965</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexander Robey, Eric Wong, Hamed Hassani, and George&nbsp;J. Pappas.

</span>
<span class="ltx_bibblock">SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">CoRR abs/2310.03684</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Paul R"ottger, Hannah&nbsp;Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy.

</span>
<span class="ltx_bibblock">XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">CoRR abs/2308.01263</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-François Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson&nbsp;Liu Kost, Christopher Carnahan, and Jordan&nbsp;L. Boyd-Graber.

</span>
<span class="ltx_bibblock">Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush Tagade, Stephen Casper, and Javier Rando.

</span>
<span class="ltx_bibblock">Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">CoRR abs/2311.03348</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Reshabh&nbsp;K. Sharma, Vinayak Gupta, and Dan Grossman.

</span>
<span class="ltx_bibblock">SPML: A DSL for defending language models against prompt attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">CoRR abs/2402.11755</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Erfan Shayegani, Md&nbsp;Abdullah&nbsp;Al Mamun, Yu&nbsp;Fu, Pedram Zaree, Yue Dong, and Nael&nbsp;B. Abu-Ghazaleh.

</span>
<span class="ltx_bibblock">Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">CoRR abs/2310.10844</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang.

</span>
<span class="ltx_bibblock">Do Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">CoRR abs/2308.03825</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dong Shu, Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, and Yongfeng Zhang.

</span>
<span class="ltx_bibblock">AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">CoRR abs/2401.09002</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sonali Singh, Faranak Abri, and Akbar&nbsp;Siami Namin.

</span>
<span class="ltx_bibblock">Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">IEEE International Conference on Big Data (ICBD)</span>, pages 2508–2517. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chawin Sitawarin, Norman Mu, David&nbsp;A. Wagner, and Alexandre Araujo.

</span>
<span class="ltx_bibblock">PAL: proxy-guided black-box attack on large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">CoRR abs/2402.09674</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell.

</span>
<span class="ltx_bibblock">Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">International Conference on Learning Representations (ICLR)</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu&nbsp;Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, and Sam Toyer.

</span>
<span class="ltx_bibblock">A StrongREJECT for Empty Jailbreaks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">CoRR abs/2402.10260</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lukas Struppek, Minh&nbsp;Hieu Le, Dominik Hintersdorf, and Kristian Kersting.

</span>
<span class="ltx_bibblock">Exploring the Adversarial Capabilities of Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">CoRR abs/2402.09132</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang.

</span>
<span class="ltx_bibblock">Safety Assessment of Chinese Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">CoRR abs/2304.10436</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan.

</span>
<span class="ltx_bibblock">Principle-driven self-alignment of language models from scratch with minimal human supervision.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kazuhiro Takemoto.

</span>
<span class="ltx_bibblock">All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">CoRR abs/2401.09798</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Llama Team.

</span>
<span class="ltx_bibblock">Meta llama guard 2.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md" title="">https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yu&nbsp;Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su.

</span>
<span class="ltx_bibblock">Evil Geniuses: Delving into the Safety of LLM-based Agents.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">CoRR abs/2311.11855</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">CoRR abs/2307.09288</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hao Wang, Hao Li, Minlie Huang, and Lei Sha.

</span>
<span class="ltx_bibblock">From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib93.1.1">CoRR abs/2402.16006</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Junjie Hu, Yixuan Li, Patrick McDaniel, Muhao Chen, Bo&nbsp;Li, and Chaowei Xiao.

</span>
<span class="ltx_bibblock">Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">CoRR abs/2402.14968</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiongxiao Wang, Zichen Liu, Keun&nbsp;Hee Park, Muhao Chen, and Chaowei Xiao.

</span>
<span class="ltx_bibblock">Adversarial demonstration attacks on large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">CoRR abs/2305.14950</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin.

</span>
<span class="ltx_bibblock">Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">CoRR abs/2308.13387</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Jailbroken: How does llm safety training fail?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jason Wei, Yi&nbsp;Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed&nbsp;H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib98.1.1">Trans. Mach. Learn. Res.</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed&nbsp;H. Chi, Quoc&nbsp;V. Le, and Denny Zhou.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib99.1.1">Annual Conference on Neural Information Processing Systems (NeurIPS)</span>. NeurIPS, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zeming Wei, Yifei Wang, and Yisen Wang.

</span>
<span class="ltx_bibblock">Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">CoRR abs/2310.06387</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yueqi Xie, Minghong Fang, Renjie Pi, and Neil&nbsp;Zhenqiang Gong.

</span>
<span class="ltx_bibblock">GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">CoRR abs/2402.13494</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill&nbsp;Yuchen Lin, and Radha Poovendran.

</span>
<span class="ltx_bibblock">SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">CoRR abs/2402.08983</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xianjun Yang, Xiao Wang, Qi&nbsp;Zhang, Linda&nbsp;R. Petzold, William&nbsp;Yang Wang, Xun Zhao, and Dahua Lin.

</span>
<span class="ltx_bibblock">Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib103.1.1">CoRR abs/2310.02949</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dongyu Yao, Jianshu Zhang, Ian&nbsp;G. Harris, and Marcel Carlsson.

</span>
<span class="ltx_bibblock">FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib104.1.1">CoRR abs/2309.05274</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang.

</span>
<span class="ltx_bibblock">A survey on large language model (llm) security and privacy: The good, the bad, and the ugly.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib105.1.1">High-Confidence Computing</span>, 4(2):100211, June 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zheng&nbsp;Xin Yong, Cristina Menghini, and Stephen&nbsp;H. Bach.

</span>
<span class="ltx_bibblock">Low-Resource Languages Jailbreak GPT-4.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">CoRR abs/2310.02446</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing.

</span>
<span class="ltx_bibblock">GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib107.1.1">CoRR abs/2309.10253</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu.

</span>
<span class="ltx_bibblock">GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib108.1.1">International Conference on Learning Representations (ICLR)</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi.

</span>
<span class="ltx_bibblock">How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib109.1.1">CoRR abs/2401.06373</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu.

</span>
<span class="ltx_bibblock">AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib110.1.1">CoRR abs/2403.04783</span>, abs/2403.04783, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang.

</span>
<span class="ltx_bibblock">Removing RLHF Protections in GPT-4 via Fine-Tuning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib111.1.1">CoRR abs/2311.05553</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiaoyu Zhang, Cen Zhang, Tianlin Li, Yihao Huang, Xiaojun Jia, Xiaofei Xie, Yang Liu, and Chao Shen.

</span>
<span class="ltx_bibblock">A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib112.1.1">CoRR abs/2312.10766</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuqi Zhang, Liang Ding, Lefei Zhang, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Intention analysis makes llms a good jailbreak defender.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib113.1.1">CoRR abs/2401.06561</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zaibin Zhang, Yongting Zhang, Lijun Li, Hongzhi Gao, Lijun Wang, Huchuan Lu, Feng Zhao, Yu&nbsp;Qiao, and Jing Shao.

</span>
<span class="ltx_bibblock">PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib114.1.1">CoRR abs/2401.11880</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang.

</span>
<span class="ltx_bibblock">Safetybench: Evaluating the safety of large language models with multiple choice questions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib115.1.1">CoRR abs/2309.07045</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhuo Zhang, Guangyu Shen, Guanhong Tao, Siyuan Cheng, and Xiangyu Zhang.

</span>
<span class="ltx_bibblock">Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib116.1.1">CoRR abs/2312.04782</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and William&nbsp;Yang Wang.

</span>
<span class="ltx_bibblock">Weak-to-Strong Jailbreaking on Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib117.1.1">CoRR abs/2401.17256</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng.

</span>
<span class="ltx_bibblock">On prompt-driven safeguarding for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib118.1.1">CoRR abs/2401.18018</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi&nbsp;Lin, Zhuohan Li, Dacheng Li, Eric.&nbsp;P Xing, Hao Zhang, Joseph&nbsp;E. Gonzalez, and Ion Stoica.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, and Min Lin.

</span>
<span class="ltx_bibblock">Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib120.1.1">CoRR abs/2406.01288</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andy Zhou, Bo&nbsp;Li, and Haohan Wang.

</span>
<span class="ltx_bibblock">Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib121.1.1">CoRR abs/2401.17263</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu Chai, Fukang Zhu, Caishuang Huang, Shihan Dou, Zhiheng Xi, Rui Zheng, Songyang Gao, Yicheng Zou, Hang Yan, Yifan Le, Ruohui Wang, Lijun Li, Jing Shao, Tao Gui, Qi&nbsp;Zhang, and Xuanjing Huang.

</span>
<span class="ltx_bibblock">EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib122.1.1">CoRR abs/2403.12171</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yukai Zhou and Wenjie Wang.

</span>
<span class="ltx_bibblock">Don’t Say No: Jailbreaking LLM by Suppressing Refusal.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib123.1.1">CoRR abs/2404.16369</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun.

</span>
<span class="ltx_bibblock">AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib124.1.1">CoRR abs/2310.15140</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andy Zou, Zifan Wang, J.&nbsp;Zico Kolter, and Matt Fredrikson.

</span>
<span class="ltx_bibblock">Universal and Transferable Adversarial Attacks on Aligned Language Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib125.1.1">CoRR abs/2307.15043</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiaotian Zou, Yongkang Chen, and Ke&nbsp;Li.

</span>
<span class="ltx_bibblock">Is the system message really important to jailbreaks in large language models?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib126.1.1">CoRR abs/2402.14857</span>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">报告问题</font></font></font></button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed; left: 984px; top: 473.562px; transform: translate(-50%, -100%); display: inline;">Report Issue for Selection<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">报告选择问题</font></font></font></button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" width="11" height="14">
            </a><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">由 L A T E xml <img height="14" width="11" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"> 生成</font></font></font>
        </div></div><footer id="footer" class="ltx_document" default-translate="no">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body><div id="immersive-translate-popup" style="all: initial"><template shadowrootmode="open"><style>@charset "UTF-8";
/*!
 * Pico.css v1.5.6 (https://picocss.com)
 * Copyright 2019-2022 - Licensed under MIT
 */
/**
 * Theme: default
 */
#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 0.25rem;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 1rem;
  --typography-spacing-vertical: 1.5rem;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 0.75rem;
  --form-element-spacing-horizontal: 1rem;
  --nav-element-spacing-vertical: 1rem;
  --nav-element-spacing-horizontal: 0.5rem;
  --nav-link-spacing-vertical: 0.5rem;
  --nav-link-spacing-horizontal: 0.5rem;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(0.25rem);
}
@media (min-width: 576px) {
  #mount {
    --font-size: 17px;
  }
}
@media (min-width: 768px) {
  #mount {
    --font-size: 18px;
  }
}
@media (min-width: 992px) {
  #mount {
    --font-size: 19px;
  }
}
@media (min-width: 1200px) {
  #mount {
    --font-size: 20px;
  }
}

@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 2);
  }
}
@media (min-width: 768px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3);
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer,
  section {
    --block-spacing-vertical: calc(var(--spacing) * 3.5);
  }
}

@media (min-width: 576px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}
@media (min-width: 992px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 1.75);
  }
}
@media (min-width: 1200px) {
  article {
    --block-spacing-horizontal: calc(var(--spacing) * 2);
  }
}

dialog > article {
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
}
@media (min-width: 576px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 2.5);
    --block-spacing-horizontal: calc(var(--spacing) * 1.25);
  }
}
@media (min-width: 768px) {
  dialog > article {
    --block-spacing-vertical: calc(var(--spacing) * 3);
    --block-spacing-horizontal: calc(var(--spacing) * 1.5);
  }
}

a {
  --text-decoration: none;
}
a.secondary,
a.contrast {
  --text-decoration: underline;
}

small {
  --font-size: 0.875em;
}

h1,
h2,
h3,
h4,
h5,
h6 {
  --font-weight: 700;
}

h1 {
  --font-size: 2rem;
  --typography-spacing-vertical: 3rem;
}

h2 {
  --font-size: 1.75rem;
  --typography-spacing-vertical: 2.625rem;
}

h3 {
  --font-size: 1.5rem;
  --typography-spacing-vertical: 2.25rem;
}

h4 {
  --font-size: 1.25rem;
  --typography-spacing-vertical: 1.874rem;
}

h5 {
  --font-size: 1.125rem;
  --typography-spacing-vertical: 1.6875rem;
}

[type="checkbox"],
[type="radio"] {
  --border-width: 2px;
}

[type="checkbox"][role="switch"] {
  --border-width: 2px;
}

thead th,
thead td,
tfoot th,
tfoot td {
  --border-width: 3px;
}

:not(thead, tfoot) > * > td {
  --font-size: 0.875em;
}

pre,
code,
kbd,
samp {
  --font-family: "Menlo", "Consolas", "Roboto Mono", "Ubuntu Monospace",
    "Noto Mono", "Oxygen Mono", "Liberation Mono", monospace,
    "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
}

kbd {
  --font-weight: bolder;
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --background-color: #fff;
  --background-light-green: #f5f7f9;
  --color: hsl(205deg, 20%, 32%);
  --h1-color: hsl(205deg, 30%, 15%);
  --h2-color: #24333e;
  --h3-color: hsl(205deg, 25%, 23%);
  --h4-color: #374956;
  --h5-color: hsl(205deg, 20%, 32%);
  --h6-color: #4d606d;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: hsl(205deg, 20%, 94%);
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 90%, 32%);
  --primary-focus: rgba(16, 149, 193, 0.125);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 20%, 32%);
  --secondary-focus: rgba(89, 107, 120, 0.125);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 30%, 15%);
  --contrast-hover: #000;
  --contrast-focus: rgba(89, 107, 120, 0.125);
  --contrast-inverse: #fff;
  --mark-background-color: #fff2ca;
  --mark-color: #543a26;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: transparent;
  --form-element-border-color: hsl(205deg, 14%, 68%);
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: transparent;
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 18%, 86%);
  --form-element-disabled-border-color: hsl(205deg, 14%, 68%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #c62828;
  --form-element-invalid-active-border-color: #d32f2f;
  --form-element-invalid-focus-color: rgba(211, 47, 47, 0.125);
  --form-element-valid-border-color: #388e3c;
  --form-element-valid-active-border-color: #43a047;
  --form-element-valid-focus-color: rgba(67, 160, 71, 0.125);
  --switch-background-color: hsl(205deg, 16%, 77%);
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: hsl(205deg, 18%, 86%);
  --range-active-border-color: hsl(205deg, 16%, 77%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: #f6f8f9;
  --code-background-color: hsl(205deg, 20%, 94%);
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 40%, 50%);
  --code-property-color: hsl(185deg, 40%, 40%);
  --code-value-color: hsl(40deg, 20%, 50%);
  --code-comment-color: hsl(205deg, 14%, 68%);
  --accordion-border-color: var(--muted-border-color);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: var(--background-color);
  --card-border-color: var(--muted-border-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(27, 40, 50, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(27, 40, 50, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(27, 40, 50, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(27, 40, 50, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(27, 40, 50, 0.04302),
    0.5rem 1rem 6rem rgba(27, 40, 50, 0.06),
    0 0 0 0.0625rem rgba(27, 40, 50, 0.015);
  --card-sectionning-background-color: #fbfbfc;
  --dropdown-background-color: #fbfbfc;
  --dropdown-border-color: #e1e6eb;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: hsl(205deg, 20%, 94%);
  --modal-overlay-background-color: rgba(213, 220, 226, 0.7);
  --progress-background-color: hsl(205deg, 18%, 86%);
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(198, 40, 40)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(65, 84, 98)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(56, 142, 60)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  --float-ball-more-button-border-color: #f6f6f6;
  --float-ball-more-button-background-color: #ffffff;
  --float-ball-more-button-svg-color: #6c6f73;
  color-scheme: light;
  --service-bg-hover: #f7faff;
  --service-bg: #fafbfb;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --background-color: #11191f;
    --float-ball-more-button-background-color: #ffffff;
    --background-light-green: #141e26;
    --color: hsl(205deg, 16%, 77%);
    --h1-color: hsl(205deg, 20%, 94%);
    --h2-color: #e1e6eb;
    --h3-color: hsl(205deg, 18%, 86%);
    --h4-color: #c8d1d8;
    --h5-color: hsl(205deg, 16%, 77%);
    --h6-color: #afbbc4;
    --muted-color: hsl(205deg, 10%, 50%);
    --muted-border-color: #1f2d38;
    --primary: hsl(195deg, 85%, 41%);
    --primary-hover: hsl(195deg, 80%, 50%);
    --primary-focus: rgba(16, 149, 193, 0.25);
    --primary-inverse: #fff;
    --secondary: hsl(205deg, 15%, 41%);
    --secondary-hover: hsl(205deg, 10%, 50%);
    --secondary-focus: rgba(115, 130, 140, 0.25);
    --secondary-inverse: #fff;
    --contrast: hsl(205deg, 20%, 94%);
    --contrast-hover: #fff;
    --contrast-focus: rgba(115, 130, 140, 0.25);
    --contrast-inverse: #000;
    --mark-background-color: #d1c284;
    --mark-color: #11191f;
    --ins-color: #388e3c;
    --del-color: #c62828;
    --blockquote-border-color: var(--muted-border-color);
    --blockquote-footer-color: var(--muted-color);
    --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
    --form-element-background-color: #11191f;
    --form-element-border-color: #374956;
    --form-element-color: var(--color);
    --form-element-placeholder-color: var(--muted-color);
    --form-element-active-background-color: var(
      --form-element-background-color
    );
    --form-element-active-border-color: var(--primary);
    --form-element-focus-color: var(--primary-focus);
    --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
    --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
    --form-element-disabled-opacity: 0.5;
    --form-element-invalid-border-color: #b71c1c;
    --form-element-invalid-active-border-color: #c62828;
    --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
    --form-element-valid-border-color: #2e7d32;
    --form-element-valid-active-border-color: #388e3c;
    --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
    --switch-background-color: #374956;
    --switch-color: var(--primary-inverse);
    --switch-checked-background-color: var(--primary);
    --range-border-color: #24333e;
    --range-active-border-color: hsl(205deg, 25%, 23%);
    --range-thumb-border-color: var(--background-color);
    --range-thumb-color: var(--secondary);
    --range-thumb-hover-color: var(--secondary-hover);
    --range-thumb-active-color: var(--primary);
    --table-border-color: var(--muted-border-color);
    --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
    --code-background-color: #18232c;
    --code-color: var(--muted-color);
    --code-kbd-background-color: var(--contrast);
    --code-kbd-color: var(--contrast-inverse);
    --code-tag-color: hsl(330deg, 30%, 50%);
    --code-property-color: hsl(185deg, 30%, 50%);
    --code-value-color: hsl(40deg, 10%, 50%);
    --code-comment-color: #4d606d;
    --accordion-border-color: var(--muted-border-color);
    --accordion-active-summary-color: var(--primary);
    --accordion-close-summary-color: var(--color);
    --accordion-open-summary-color: var(--muted-color);
    --card-background-color: #141e26;
    --card-border-color: var(--card-background-color);
    --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
      0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
      0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
      0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
      0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
      0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
    --card-sectionning-background-color: #18232c;
    --dropdown-background-color: hsl(205deg, 30%, 15%);
    --dropdown-border-color: #24333e;
    --dropdown-box-shadow: var(--card-box-shadow);
    --dropdown-color: var(--color);
    --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
    --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
    --progress-background-color: #24333e;
    --progress-color: var(--primary);
    --loading-spinner-opacity: 0.5;
    --tooltip-background-color: var(--contrast);
    --tooltip-color: var(--contrast-inverse);
    --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
    --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
    --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
    --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
    --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
    --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
    --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
    color-scheme: dark;
    --service-bg-hover: #22292f;
    --service-bg: rgba(0, 0, 0, 0.1);
  }
}
[data-theme="dark"] {
  --background-color: #11191f;
  --float-ball-more-button-background-color: #191919;
  --background-light-green: #141e26;
  --color: hsl(205deg, 16%, 77%);
  --h1-color: hsl(205deg, 20%, 94%);
  --h2-color: #e1e6eb;
  --h3-color: hsl(205deg, 18%, 86%);
  --h4-color: #c8d1d8;
  --h5-color: hsl(205deg, 16%, 77%);
  --h6-color: #afbbc4;
  --muted-color: hsl(205deg, 10%, 50%);
  --muted-border-color: #1f2d38;
  --primary: hsl(195deg, 85%, 41%);
  --primary-hover: hsl(195deg, 80%, 50%);
  --primary-focus: rgba(16, 149, 193, 0.25);
  --primary-inverse: #fff;
  --secondary: hsl(205deg, 15%, 41%);
  --secondary-hover: hsl(205deg, 10%, 50%);
  --secondary-focus: rgba(115, 130, 140, 0.25);
  --secondary-inverse: #fff;
  --contrast: hsl(205deg, 20%, 94%);
  --contrast-hover: #fff;
  --contrast-focus: rgba(115, 130, 140, 0.25);
  --contrast-inverse: #000;
  --mark-background-color: #d1c284;
  --mark-color: #11191f;
  --ins-color: #388e3c;
  --del-color: #c62828;
  --blockquote-border-color: var(--muted-border-color);
  --blockquote-footer-color: var(--muted-color);
  --button-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --button-hover-box-shadow: 0 0 0 rgba(0, 0, 0, 0);
  --form-element-background-color: #11191f;
  --form-element-border-color: #374956;
  --form-element-color: var(--color);
  --form-element-placeholder-color: var(--muted-color);
  --form-element-active-background-color: var(--form-element-background-color);
  --form-element-active-border-color: var(--primary);
  --form-element-focus-color: var(--primary-focus);
  --form-element-disabled-background-color: hsl(205deg, 25%, 23%);
  --form-element-disabled-border-color: hsl(205deg, 20%, 32%);
  --form-element-disabled-opacity: 0.5;
  --form-element-invalid-border-color: #b71c1c;
  --form-element-invalid-active-border-color: #c62828;
  --form-element-invalid-focus-color: rgba(198, 40, 40, 0.25);
  --form-element-valid-border-color: #2e7d32;
  --form-element-valid-active-border-color: #388e3c;
  --form-element-valid-focus-color: rgba(56, 142, 60, 0.25);
  --switch-background-color: #374956;
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --range-border-color: #24333e;
  --range-active-border-color: hsl(205deg, 25%, 23%);
  --range-thumb-border-color: var(--background-color);
  --range-thumb-color: var(--secondary);
  --range-thumb-hover-color: var(--secondary-hover);
  --range-thumb-active-color: var(--primary);
  --table-border-color: var(--muted-border-color);
  --table-row-stripped-background-color: rgba(115, 130, 140, 0.05);
  --code-background-color: #18232c;
  --code-color: var(--muted-color);
  --code-kbd-background-color: var(--contrast);
  --code-kbd-color: var(--contrast-inverse);
  --code-tag-color: hsl(330deg, 30%, 50%);
  --code-property-color: hsl(185deg, 30%, 50%);
  --code-value-color: hsl(40deg, 10%, 50%);
  --code-comment-color: #4d606d;
  --accordion-border-color: var(--muted-border-color);
  --accordion-active-summary-color: var(--primary);
  --accordion-close-summary-color: var(--color);
  --accordion-open-summary-color: var(--muted-color);
  --card-background-color: #141e26;
  --card-border-color: var(--card-background-color);
  --card-box-shadow: 0.0145rem 0.029rem 0.174rem rgba(0, 0, 0, 0.01698),
    0.0335rem 0.067rem 0.402rem rgba(0, 0, 0, 0.024),
    0.0625rem 0.125rem 0.75rem rgba(0, 0, 0, 0.03),
    0.1125rem 0.225rem 1.35rem rgba(0, 0, 0, 0.036),
    0.2085rem 0.417rem 2.502rem rgba(0, 0, 0, 0.04302),
    0.5rem 1rem 6rem rgba(0, 0, 0, 0.06), 0 0 0 0.0625rem rgba(0, 0, 0, 0.015);
  --card-sectionning-background-color: #18232c;
  --dropdown-background-color: hsl(205deg, 30%, 15%);
  --dropdown-border-color: #24333e;
  --dropdown-box-shadow: var(--card-box-shadow);
  --dropdown-color: var(--color);
  --dropdown-hover-background-color: rgba(36, 51, 62, 0.75);
  --modal-overlay-background-color: rgba(36, 51, 62, 0.8);
  --progress-background-color: #24333e;
  --progress-color: var(--primary);
  --loading-spinner-opacity: 0.5;
  --tooltip-background-color: var(--contrast);
  --tooltip-color: var(--contrast-inverse);
  --icon-checkbox: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-chevron-button-inverse: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(0, 0, 0)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-close: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(115, 130, 140)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='18' y1='6' x2='6' y2='18'%3E%3C/line%3E%3Cline x1='6' y1='6' x2='18' y2='18'%3E%3C/line%3E%3C/svg%3E");
  --icon-date: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Crect x='3' y='4' width='18' height='18' rx='2' ry='2'%3E%3C/rect%3E%3Cline x1='16' y1='2' x2='16' y2='6'%3E%3C/line%3E%3Cline x1='8' y1='2' x2='8' y2='6'%3E%3C/line%3E%3Cline x1='3' y1='10' x2='21' y2='10'%3E%3C/line%3E%3C/svg%3E");
  --icon-invalid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(183, 28, 28)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cline x1='12' y1='8' x2='12' y2='12'%3E%3C/line%3E%3Cline x1='12' y1='16' x2='12.01' y2='16'%3E%3C/line%3E%3C/svg%3E");
  --icon-minus: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(255, 255, 255)' stroke-width='4' stroke-linecap='round' stroke-linejoin='round'%3E%3Cline x1='5' y1='12' x2='19' y2='12'%3E%3C/line%3E%3C/svg%3E");
  --icon-search: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='11' cy='11' r='8'%3E%3C/circle%3E%3Cline x1='21' y1='21' x2='16.65' y2='16.65'%3E%3C/line%3E%3C/svg%3E");
  --icon-time: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(162, 175, 185)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Ccircle cx='12' cy='12' r='10'%3E%3C/circle%3E%3Cpolyline points='12 6 12 12 16 14'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-valid: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24' viewBox='0 0 24 24' fill='none' stroke='rgb(46, 125, 50)' stroke-width='3' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='20 6 9 17 4 12'%3E%3C/polyline%3E%3C/svg%3E");
  --icon-share: url("data:image/svg+xml;charset=utf-8;base64,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");
  color-scheme: dark;
  --service-bg: rgba(0, 0, 0, 0.1);
}

progress,
[type="checkbox"],
[type="radio"],
[type="range"] {
  accent-color: var(--primary);
}

/**
 * Document
 * Content-box & Responsive typography
 */
*,
*::before,
*::after {
  box-sizing: border-box;
  background-repeat: no-repeat;
}

::before,
::after {
  text-decoration: inherit;
  vertical-align: inherit;
}

:where(#mount) {
  -webkit-tap-highlight-color: transparent;
  -webkit-text-size-adjust: 100%;
  -moz-text-size-adjust: 100%;
  text-size-adjust: 100%;
  background-color: var(--background-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  line-height: var(--line-height);
  font-family: var(--font-family);
  text-rendering: optimizeLegibility;
  overflow-wrap: break-word;
  cursor: default;
  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;
}

/**
 * Sectioning
 * Container and responsive spacings for header, main, footer
 */
main {
  display: block;
}

#mount {
  width: 100%;
  margin: 0;
}
#mount > header,
#mount > main,
#mount > footer {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
}
@media (min-width: 576px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    padding: 2px !important;
  }
}
@media (min-width: 992px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    padding: 0 12px !important;
  }
}
@media (min-width: 1200px) {
  #mount > header,
  #mount > main,
  #mount > footer {
    padding: 0 24px !important;
  }
}

/**
* Container
*/
.container,
.container-fluid {
  width: 100%;
  margin-right: auto;
  margin-left: auto;
  padding-right: var(--spacing);
  padding-left: var(--spacing);
}
/*
@media (min-width: 576px) {
  .container {
    max-width: 510px;
    padding-right: 0;
    padding-left: 0;
  }
}
@media (min-width: 768px) {
  .container {
    max-width: 700px;
  }
} */
@media (min-width: 992px) {
  .container {
    max-width: 920px;
  }
}
@media (min-width: 1200px) {
  .container {
    max-width: 1130px;
  }
}

/**
 * Section
 * Responsive spacings for section
 */
section {
  margin-bottom: var(--block-spacing-vertical);
}

/**
* Grid
* Minimal grid system with auto-layout columns
*/
.grid {
  grid-column-gap: var(--grid-spacing-horizontal);
  grid-row-gap: var(--grid-spacing-vertical);
  display: grid;
  grid-template-columns: 1fr;
  margin: 0;
}
@media (min-width: 1280px) {
  .grid {
    grid-template-columns: repeat(auto-fit, minmax(0%, 1fr));
  }
}
.grid > * {
  min-width: 0;
}

/**
 * Horizontal scroller (<figure>)
 */
figure {
  display: block;
  margin: 0;
  padding: 0;
  overflow-x: auto;
}
figure figcaption {
  padding: calc(var(--spacing) * 0.5) 0;
  color: var(--muted-color);
}

/**
 * Typography
 */
b,
strong {
  font-weight: bolder;
}

sub,
sup {
  position: relative;
  font-size: 0.75em;
  line-height: 0;
  vertical-align: baseline;
}

sub {
  bottom: -0.25em;
}

sup {
  top: -0.5em;
}

address,
blockquote,
dl,
figure,
form,
ol,
p,
pre,
table,
ul {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: var(--font-size);
}

a,
[role="link"] {
  --color: var(--primary);
  --background-color: transparent;
  outline: none;
  background-color: var(--background-color);
  color: var(--color);
  -webkit-text-decoration: var(--text-decoration);
  text-decoration: var(--text-decoration);
  transition: background-color var(--transition), color var(--transition),
    box-shadow var(--transition), -webkit-text-decoration var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition),
    -webkit-text-decoration var(--transition);
}
a:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --color: var(--primary-hover);
  --text-decoration: underline;
}
a:focus,
[role="link"]:focus {
  --background-color: var(--primary-focus);
}
a.secondary,
[role="link"].secondary {
  --color: var(--secondary);
}
a.secondary:is([aria-current], :hover, :active, :focus),
[role="link"].secondary:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}
a.secondary:focus,
[role="link"].secondary:focus {
  --background-color: var(--secondary-focus);
}
a.contrast,
[role="link"].contrast {
  --color: var(--contrast);
}
a.contrast:is([aria-current], :hover, :active, :focus),
[role="link"].contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}
a.contrast:focus,
[role="link"].contrast:focus {
  --background-color: var(--contrast-focus);
}

h1,
h2,
h3,
h4,
h5,
h6 {
  margin-top: 0;
  margin-bottom: var(--typography-spacing-vertical);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  font-family: var(--font-family);
}

h1 {
  --color: var(--h1-color);
}

h2 {
  --color: var(--h2-color);
}

h3 {
  --color: var(--h3-color);
}

h4 {
  --color: var(--h4-color);
}

h5 {
  --color: var(--h5-color);
}

h6 {
  --color: var(--h6-color);
}

:where(address, blockquote, dl, figure, form, ol, p, pre, table, ul)
  ~ :is(h1, h2, h3, h4, h5, h6) {
  margin-top: var(--typography-spacing-vertical);
}

hgroup,
.headings {
  margin-bottom: var(--typography-spacing-vertical);
}
hgroup > *,
.headings > * {
  margin-bottom: 0;
}
hgroup > *:last-child,
.headings > *:last-child {
  --color: var(--muted-color);
  --font-weight: unset;
  font-size: 1rem;
  font-family: unset;
}

p {
  margin-bottom: var(--typography-spacing-vertical);
}

small {
  font-size: var(--font-size);
}

:where(dl, ol, ul) {
  padding-right: 0;
  padding-left: var(--spacing);
  -webkit-padding-start: var(--spacing);
  padding-inline-start: var(--spacing);
  -webkit-padding-end: 0;
  padding-inline-end: 0;
}
:where(dl, ol, ul) li {
  margin-bottom: calc(var(--typography-spacing-vertical) * 0.25);
}

:where(dl, ol, ul) :is(dl, ol, ul) {
  margin: 0;
  margin-top: calc(var(--typography-spacing-vertical) * 0.25);
}

ul li {
  list-style: square;
}

mark {
  padding: 0.125rem 0.25rem;
  background-color: var(--mark-background-color);
  color: var(--mark-color);
  vertical-align: baseline;
}

blockquote {
  display: block;
  margin: var(--typography-spacing-vertical) 0;
  padding: var(--spacing);
  border-right: none;
  border-left: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-start: 0.25rem solid var(--blockquote-border-color);
  border-inline-start: 0.25rem solid var(--blockquote-border-color);
  -webkit-border-end: none;
  border-inline-end: none;
}
blockquote footer {
  margin-top: calc(var(--typography-spacing-vertical) * 0.5);
  color: var(--blockquote-footer-color);
}

abbr[title] {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}

ins {
  color: var(--ins-color);
  text-decoration: none;
}

del {
  color: var(--del-color);
}

::-moz-selection {
  background-color: var(--primary-focus);
}

::selection {
  background-color: var(--primary-focus);
}

/**
 * Embedded content
 */
:where(audio, canvas, iframe, img, svg, video) {
  vertical-align: middle;
}

audio,
video {
  display: inline-block;
}

audio:not([controls]) {
  display: none;
  height: 0;
}

:where(iframe) {
  border-style: none;
}

img {
  max-width: 100%;
  height: auto;
  border-style: none;
}

:where(svg:not([fill])) {
  fill: currentColor;
}

svg:not(#mount) {
  overflow: hidden;
}

/**
 * Button
 */
button {
  margin: 0;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

button,
[type="button"],
[type="reset"],
[type="submit"] {
  -webkit-appearance: button;
}

button {
  display: block;
  width: 100%;
  margin-bottom: var(--spacing);
}

[role="button"] {
  display: inline-block;
  text-decoration: none;
}

button,
input[type="submit"],
input[type="button"],
input[type="reset"],
[role="button"] {
  --background-color: var(--primary);
  --border-color: var(--primary);
  --color: var(--primary-inverse);
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
button:is([aria-current], :hover, :active, :focus),
input[type="submit"]:is([aria-current], :hover, :active, :focus),
input[type="button"]:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus),
[role="button"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--primary-hover);
  --border-color: var(--primary-hover);
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  --color: var(--primary-inverse);
}
button:focus,
input[type="submit"]:focus,
input[type="button"]:focus,
input[type="reset"]:focus,
[role="button"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--primary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary,
input[type="reset"] {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  cursor: pointer;
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"]:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).secondary:focus,
input[type="reset"]:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--secondary-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast {
  --background-color: var(--contrast);
  --border-color: var(--contrast);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:is([aria-current], :hover, :active, :focus) {
  --background-color: var(--contrast-hover);
  --border-color: var(--contrast-hover);
  --color: var(--contrast-inverse);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).contrast:focus {
  --box-shadow: var(--button-hover-box-shadow, 0 0 0 rgba(0, 0, 0, 0)),
    0 0 0 var(--outline-width) var(--contrast-focus);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline,
input[type="reset"].outline {
  --background-color: transparent;
  --color: var(--primary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --background-color: transparent;
  --color: var(--primary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary,
input[type="reset"].outline {
  --color: var(--secondary);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.secondary:is([aria-current], :hover, :active, :focus),
input[type="reset"].outline:is([aria-current], :hover, :active, :focus) {
  --color: var(--secondary-hover);
}

:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast {
  --color: var(--contrast);
}
:is(
    button,
    input[type="submit"],
    input[type="button"],
    [role="button"]
  ).outline.contrast:is([aria-current], :hover, :active, :focus) {
  --color: var(--contrast-hover);
}

:where(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  )[disabled],
:where(fieldset[disabled])
  :is(
    button,
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="button"]
  ),
a[role="button"]:not([href]) {
  opacity: 0.5;
  pointer-events: none;
}

/**
 * Form elements
 */
input,
optgroup,
select,
textarea {
  margin: 0;
  font-size: 1rem;
  line-height: var(--line-height);
  font-family: inherit;
  letter-spacing: inherit;
}

input {
  overflow: visible;
}

select {
  text-transform: none;
}

legend {
  max-width: 100%;
  padding: 0;
  color: inherit;
  white-space: normal;
}

textarea {
  overflow: auto;
}

[type="checkbox"],
[type="radio"] {
  padding: 0;
}

::-webkit-inner-spin-button,
::-webkit-outer-spin-button {
  height: auto;
}

[type="search"] {
  -webkit-appearance: textfield;
  outline-offset: -2px;
}

[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}

::-webkit-file-upload-button {
  -webkit-appearance: button;
  font: inherit;
}

::-moz-focus-inner {
  padding: 0;
  border-style: none;
}

:-moz-focusring {
  outline: none;
}

:-moz-ui-invalid {
  box-shadow: none;
}

::-ms-expand {
  display: none;
}

[type="file"],
[type="range"] {
  padding: 0;
  border-width: 0;
}

input:not([type="checkbox"], [type="radio"], [type="range"]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
}

fieldset {
  margin: 0;
  margin-bottom: var(--spacing);
  padding: 0;
  border: 0;
}

label,
fieldset legend {
  display: block;
  margin-bottom: calc(var(--spacing) * 0.25);
  font-weight: var(--form-label-font-weight, var(--font-weight));
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  width: 100%;
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]),
select,
textarea {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
}

input,
select,
textarea {
  --background-color: var(--form-element-background-color);
  --border-color: var(--form-element-border-color);
  --color: var(--form-element-color);
  --box-shadow: none;
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="checkbox"],
    [type="radio"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --background-color: var(--form-element-active-background-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [role="switch"],
    [readonly]
  ):is(:active, :focus),
:where(select, textarea):is(:active, :focus) {
  --border-color: var(--form-element-active-border-color);
}

input:not(
    [type="submit"],
    [type="button"],
    [type="reset"],
    [type="range"],
    [type="file"],
    [readonly]
  ):focus,
select:focus,
textarea:focus {
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}

input:not([type="submit"], [type="button"], [type="reset"])[disabled],
select[disabled],
textarea[disabled],
:where(fieldset[disabled])
  :is(
    input:not([type="submit"], [type="button"], [type="reset"]),
    select,
    textarea
  ) {
  --background-color: var(--form-element-disabled-background-color);
  --border-color: var(--form-element-disabled-border-color);
  opacity: var(--form-element-disabled-opacity);
  pointer-events: none;
}

:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid] {
  padding-right: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal) !important;
  padding-inline-start: var(--form-element-spacing-horizontal) !important;
  -webkit-padding-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  padding-inline-end: calc(
    var(--form-element-spacing-horizontal) + 1.5rem
  ) !important;
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="false"] {
  background-image: var(--icon-valid);
}
:where(input, select, textarea):not(
    [type="checkbox"],
    [type="radio"],
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  )[aria-invalid="true"] {
  background-image: var(--icon-invalid);
}
:where(input, select, textarea)[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
:where(input, select, textarea)[aria-invalid="false"]:is(:active, :focus) {
  --border-color: var(--form-element-valid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width) var(--form-element-valid-focus-color) !important;
}
:where(input, select, textarea)[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}
:where(input, select, textarea)[aria-invalid="true"]:is(:active, :focus) {
  --border-color: var(--form-element-invalid-active-border-color) !important;
  --box-shadow: 0 0 0 var(--outline-width)
    var(--form-element-invalid-focus-color) !important;
}

[dir="rtl"]
  :where(input, select, textarea):not([type="checkbox"], [type="radio"]):is(
    [aria-invalid],
    [aria-invalid="true"],
    [aria-invalid="false"]
  ) {
  background-position: center left 0.75rem;
}

input::placeholder,
input::-webkit-input-placeholder,
textarea::placeholder,
textarea::-webkit-input-placeholder,
select:invalid {
  color: var(--form-element-placeholder-color);
  opacity: 1;
}

input:not([type="checkbox"], [type="radio"]),
select,
textarea {
  margin-bottom: var(--spacing);
}

select::-ms-expand {
  border: 0;
  background-color: transparent;
}
select:not([multiple], [size]) {
  padding-right: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-left: var(--form-element-spacing-horizontal);
  -webkit-padding-start: var(--form-element-spacing-horizontal);
  padding-inline-start: var(--form-element-spacing-horizontal);
  -webkit-padding-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  padding-inline-end: calc(var(--form-element-spacing-horizontal) + 1.5rem);
  background-image: var(--icon-chevron);
  background-position: center right 0.75rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}

[dir="rtl"] select:not([multiple], [size]) {
  background-position: center left 0.75rem;
}

:where(input, select, textarea) + small {
  display: block;
  width: 100%;
  margin-top: calc(var(--spacing) * -0.75);
  margin-bottom: var(--spacing);
  color: var(--muted-color);
}

label > :where(input, select, textarea) {
  margin-top: calc(var(--spacing) * 0.25);
}

/**
 * Form elements
 * Checkboxes & Radios
 */
[type="checkbox"],
[type="radio"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 1.25em;
  height: 1.25em;
  margin-top: -0.125em;
  margin-right: 0.375em;
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: 0.375em;
  margin-inline-end: 0.375em;
  border-width: var(--border-width);
  font-size: inherit;
  vertical-align: middle;
  cursor: pointer;
}
[type="checkbox"]::-ms-check,
[type="radio"]::-ms-check {
  display: none;
}
[type="checkbox"]:checked,
[type="checkbox"]:checked:active,
[type="checkbox"]:checked:focus,
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-checkbox);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}
[type="checkbox"] ~ label,
[type="radio"] ~ label {
  display: inline-block;
  margin-right: 0.375em;
  margin-bottom: 0;
  cursor: pointer;
}

[type="checkbox"]:indeterminate {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-minus);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}

[type="radio"] {
  border-radius: 50%;
}
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary-inverse);
  border-width: 0.35em;
  background-image: none;
}

[type="checkbox"][role="switch"] {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
  --color: var(--switch-color);
  width: 2.25em;
  height: 1.25em;
  border: var(--border-width) solid var(--border-color);
  border-radius: 1.25em;
  background-color: var(--background-color);
  line-height: 1.25em;
}
[type="checkbox"][role="switch"]:focus {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
}
[type="checkbox"][role="switch"]:checked {
  --background-color: var(--switch-checked-background-color);
  --border-color: var(--switch-checked-background-color);
}
[type="checkbox"][role="switch"]:before {
  display: block;
  width: calc(1.25em - (var(--border-width) * 2));
  height: 100%;
  border-radius: 50%;
  background-color: var(--color);
  content: "";
  transition: margin 0.1s ease-in-out;
}
[type="checkbox"][role="switch"]:checked {
  background-image: none;
}
[type="checkbox"][role="switch"]:checked::before {
  margin-left: calc(1.125em - var(--border-width));
  -webkit-margin-start: calc(1.125em - var(--border-width));
  margin-inline-start: calc(1.125em - var(--border-width));
}

[type="checkbox"][aria-invalid="false"],
[type="checkbox"]:checked[aria-invalid="false"],
[type="radio"][aria-invalid="false"],
[type="radio"]:checked[aria-invalid="false"],
[type="checkbox"][role="switch"][aria-invalid="false"],
[type="checkbox"][role="switch"]:checked[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
[type="checkbox"][aria-invalid="true"],
[type="checkbox"]:checked[aria-invalid="true"],
[type="radio"][aria-invalid="true"],
[type="radio"]:checked[aria-invalid="true"],
[type="checkbox"][role="switch"][aria-invalid="true"],
[type="checkbox"][role="switch"]:checked[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}

/**
 * Form elements
 * Alternatives input types (Not Checkboxes & Radios)
 */
[type="color"]::-webkit-color-swatch-wrapper {
  padding: 0;
}
[type="color"]::-moz-focus-inner {
  padding: 0;
}
[type="color"]::-webkit-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}
[type="color"]::-moz-color-swatch {
  border: 0;
  border-radius: calc(var(--border-radius) * 0.5);
}

input:not([type="checkbox"], [type="radio"], [type="range"], [type="file"]):is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  --icon-position: 0.75rem;
  --icon-width: 1rem;
  padding-right: calc(var(--icon-width) + var(--icon-position));
  background-image: var(--icon-date);
  background-position: center right var(--icon-position);
  background-size: var(--icon-width) auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="time"] {
  background-image: var(--icon-time);
}

[type="date"]::-webkit-calendar-picker-indicator,
[type="datetime-local"]::-webkit-calendar-picker-indicator,
[type="month"]::-webkit-calendar-picker-indicator,
[type="time"]::-webkit-calendar-picker-indicator,
[type="week"]::-webkit-calendar-picker-indicator {
  width: var(--icon-width);
  margin-right: calc(var(--icon-width) * -1);
  margin-left: var(--icon-position);
  opacity: 0;
}

[dir="rtl"]
  :is(
    [type="date"],
    [type="datetime-local"],
    [type="month"],
    [type="time"],
    [type="week"]
  ) {
  text-align: right;
}

[type="file"] {
  --color: var(--muted-color);
  padding: calc(var(--form-element-spacing-vertical) * 0.5) 0;
  border: 0;
  border-radius: 0;
  background: none;
}
[type="file"]::file-selector-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::file-selector-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-webkit-file-upload-button {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) / 2);
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-webkit-file-upload-button:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}
[type="file"]::-ms-browse {
  --background-color: var(--secondary);
  --border-color: var(--secondary);
  --color: var(--secondary-inverse);
  margin-right: calc(var(--spacing) / 2);
  margin-left: 0;
  margin-inline-start: 0;
  margin-inline-end: calc(var(--spacing) / 2);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    calc(var(--form-element-spacing-horizontal) * 0.5);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 1rem;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    border-color var(--transition), color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
[type="file"]::-ms-browse:is(:hover, :active, :focus) {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
}

[type="range"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 100%;
  height: 1.25rem;
  background: none;
}
[type="range"]::-webkit-slider-runnable-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -webkit-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-moz-range-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -moz-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-ms-track {
  width: 100%;
  height: 0.25rem;
  border-radius: var(--border-radius);
  background-color: var(--range-border-color);
  -ms-transition: background-color var(--transition),
    box-shadow var(--transition);
  transition: background-color var(--transition), box-shadow var(--transition);
}
[type="range"]::-webkit-slider-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -webkit-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-moz-range-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -moz-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]::-ms-thumb {
  -webkit-appearance: none;
  width: 1.25rem;
  height: 1.25rem;
  margin-top: -0.5rem;
  border: 2px solid var(--range-thumb-border-color);
  border-radius: 50%;
  background-color: var(--range-thumb-color);
  cursor: pointer;
  -ms-transition: background-color var(--transition),
    transform var(--transition);
  transition: background-color var(--transition), transform var(--transition);
}
[type="range"]:hover,
[type="range"]:focus {
  --range-border-color: var(--range-active-border-color);
  --range-thumb-color: var(--range-thumb-hover-color);
}
[type="range"]:active {
  --range-thumb-color: var(--range-thumb-active-color);
}
[type="range"]:active::-webkit-slider-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-moz-range-thumb {
  transform: scale(1.25);
}
[type="range"]:active::-ms-thumb {
  transform: scale(1.25);
}

input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  -webkit-padding-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  padding-inline-start: calc(var(--form-element-spacing-horizontal) + 1.75rem);
  border-radius: 5rem;
  background-image: var(--icon-search);
  background-position: center left 1.125rem;
  background-size: 1rem auto;
  background-repeat: no-repeat;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  -webkit-padding-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  padding-inline-start: calc(
    var(--form-element-spacing-horizontal) + 1.75rem
  ) !important;
  background-position: center left 1.125rem, center right 0.75rem;
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="false"] {
  background-image: var(--icon-search), var(--icon-valid);
}
input:not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid="true"] {
  background-image: var(--icon-search), var(--icon-invalid);
}

[type="search"]::-webkit-search-cancel-button {
  -webkit-appearance: none;
  display: none;
}

[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"] {
  background-position: center right 1.125rem;
}
[dir="rtl"]
  :where(input):not(
    [type="checkbox"],
    [type="radio"],
    [type="range"],
    [type="file"]
  )[type="search"][aria-invalid] {
  background-position: center right 1.125rem, center left 0.75rem;
}

/**
 * Table
 */
:where(table) {
  width: 100%;
  border-collapse: collapse;
  border-spacing: 0;
  text-indent: 0;
}

th,
td {
  padding: calc(var(--spacing) / 2) var(--spacing);
  border-bottom: var(--border-width) solid var(--table-border-color);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: var(--font-size);
  text-align: left;
  text-align: start;
}

tfoot th,
tfoot td {
  border-top: var(--border-width) solid var(--table-border-color);
  border-bottom: 0;
}

table[role="grid"] tbody tr:nth-child(odd) {
  background-color: var(--table-row-stripped-background-color);
}

/**
 * Code
 */
pre,
code,
kbd,
samp {
  font-size: 0.875em;
  font-family: var(--font-family);
}

pre {
  -ms-overflow-style: scrollbar;
  overflow: auto;
}

pre,
code,
kbd {
  border-radius: var(--border-radius);
  background: var(--code-background-color);
  color: var(--code-color);
  font-weight: var(--font-weight);
  line-height: initial;
}

code,
kbd {
  display: inline-block;
  padding: 0.375rem 0.5rem;
}

pre {
  display: block;
  margin-bottom: var(--spacing);
  overflow-x: auto;
}
pre > code {
  display: block;
  padding: var(--spacing);
  background: none;
  font-size: 14px;
  line-height: var(--line-height);
}

code b {
  color: var(--code-tag-color);
  font-weight: var(--font-weight);
}
code i {
  color: var(--code-property-color);
  font-style: normal;
}
code u {
  color: var(--code-value-color);
  text-decoration: none;
}
code em {
  color: var(--code-comment-color);
  font-style: normal;
}

kbd {
  background-color: var(--code-kbd-background-color);
  color: var(--code-kbd-color);
  vertical-align: baseline;
}

/**
 * Miscs
 */
hr {
  height: 0;
  border: 0;
  border-top: 1px solid var(--muted-border-color);
  color: inherit;
}

[hidden],
template {
  display: none !important;
}

canvas {
  display: inline-block;
}

/**
 * Accordion (<details>)
 */
details {
  display: block;
  margin-bottom: var(--spacing);
  padding-bottom: var(--spacing);
  border-bottom: var(--border-width) solid var(--accordion-border-color);
}
details summary {
  line-height: 1rem;
  list-style-type: none;
  cursor: pointer;
  transition: color var(--transition);
}
details summary:not([role]) {
  color: var(--accordion-close-summary-color);
}
details summary::-webkit-details-marker {
  display: none;
}
details summary::marker {
  display: none;
}
details summary::-moz-list-bullet {
  list-style-type: none;
}
details summary::after {
  display: block;
  width: 1rem;
  height: 1rem;
  -webkit-margin-start: calc(var(--spacing, 1rem) * 0.5);
  margin-inline-start: calc(var(--spacing, 1rem) * 0.5);
  float: right;
  transform: rotate(-90deg);
  background-image: var(--icon-chevron);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
  transition: transform var(--transition);
}
details summary:focus {
  outline: none;
}
details summary:focus:not([role="button"]) {
  color: var(--accordion-active-summary-color);
}
details summary[role="button"] {
  width: 100%;
  text-align: left;
}
details summary[role="button"]::after {
  height: calc(1rem * var(--line-height, 1.5));
  background-image: var(--icon-chevron-button);
}
details summary[role="button"]:not(.outline).contrast::after {
  background-image: var(--icon-chevron-button-inverse);
}
details[open] > summary {
  margin-bottom: calc(var(--spacing));
}
details[open] > summary:not([role]):not(:focus) {
  color: var(--accordion-open-summary-color);
}
details[open] > summary::after {
  transform: rotate(0);
}

[dir="rtl"] details summary {
  text-align: right;
}
[dir="rtl"] details summary::after {
  float: left;
  background-position: left center;
}

/**
 * Card (<article>)
 */
article {
  margin: var(--block-spacing-vertical) 0;
  padding: var(--block-spacing-vertical) var(--block-spacing-horizontal);
  border-radius: var(--border-radius);
  background: var(--card-background-color);
  box-shadow: var(--card-box-shadow);
}
article > header,
article > footer {
  margin-right: calc(var(--block-spacing-horizontal) * -1);
  margin-left: calc(var(--block-spacing-horizontal) * -1);
  padding: calc(var(--block-spacing-vertical) * 0.66)
    var(--block-spacing-horizontal);
  background-color: var(--card-sectionning-background-color);
}
article > header {
  margin-top: calc(var(--block-spacing-vertical) * -1);
  margin-bottom: var(--block-spacing-vertical);
  border-bottom: var(--border-width) solid var(--card-border-color);
  border-top-right-radius: var(--border-radius);
  border-top-left-radius: var(--border-radius);
}
article > footer {
  margin-top: var(--block-spacing-vertical);
  margin-bottom: calc(var(--block-spacing-vertical) * -1);
  border-top: var(--border-width) solid var(--card-border-color);
  border-bottom-right-radius: var(--border-radius);
  border-bottom-left-radius: var(--border-radius);
}

/**
 * Modal (<dialog>)
 */
#mount {
  --scrollbar-width: 0px;
}

dialog {
  display: flex;
  z-index: 999;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  align-items: center;
  justify-content: center;
  width: inherit;
  min-width: 100%;
  height: inherit;
  min-height: 100%;
  padding: var(--spacing);
  border: 0;
  -webkit-backdrop-filter: var(--modal-overlay-backdrop-filter);
  backdrop-filter: var(--modal-overlay-backdrop-filter);
  background-color: var(--modal-overlay-background-color);
  color: var(--color);
}
dialog article {
  max-height: calc(100vh - var(--spacing) * 2);
  overflow: auto;
}
@media (min-width: 576px) {
  dialog article {
    max-width: 510px;
  }
}
@media (min-width: 768px) {
  dialog article {
    max-width: 700px;
  }
}
dialog article > header,
dialog article > footer {
  padding: calc(var(--block-spacing-vertical) * 0.5)
    var(--block-spacing-horizontal);
}
dialog article > header .close {
  margin: 0;
  margin-left: var(--spacing);
  float: right;
}
dialog article > footer {
  text-align: right;
}
dialog article > footer [role="button"] {
  margin-bottom: 0;
}
dialog article > footer [role="button"]:not(:first-of-type) {
  margin-left: calc(var(--spacing) * 0.5);
}
dialog article p:last-of-type {
  margin: 0;
}
dialog article .close {
  display: block;
  width: 1rem;
  height: 1rem;
  margin-top: calc(var(--block-spacing-vertical) * -0.5);
  margin-bottom: var(--typography-spacing-vertical);
  margin-left: auto;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}
dialog article .close:is([aria-current], :hover, :active, :focus) {
  opacity: 1;
}
dialog:not([open]),
dialog[open="false"] {
  display: none;
}

.modal-is-open {
  padding-right: var(--scrollbar-width, 0px);
  overflow: hidden;
  pointer-events: none;
}
.modal-is-open dialog {
  pointer-events: auto;
}

:where(.modal-is-opening, .modal-is-closing) dialog,
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-duration: 0.2s;
  animation-timing-function: ease-in-out;
  animation-fill-mode: both;
}
:where(.modal-is-opening, .modal-is-closing) dialog {
  animation-duration: 0.8s;
  animation-name: modal-overlay;
}
:where(.modal-is-opening, .modal-is-closing) dialog > article {
  animation-delay: 0.2s;
  animation-name: modal;
}

.modal-is-closing dialog,
.modal-is-closing dialog > article {
  animation-delay: 0s;
  animation-direction: reverse;
}

@keyframes modal-overlay {
  from {
    -webkit-backdrop-filter: none;
    backdrop-filter: none;
    background-color: transparent;
  }
}
@keyframes modal {
  from {
    transform: translateY(-100%);
    opacity: 0;
  }
}
/**
 * Nav
 */
:where(nav li)::before {
  float: left;
  content: "​";
}

nav,
nav ul {
  display: flex;
}

nav {
  justify-content: space-between;
}
nav ol,
nav ul {
  align-items: center;
  margin-bottom: 0;
  padding: 0;
  list-style: none;
}
nav ol:first-of-type,
nav ul:first-of-type {
  margin-left: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav ol:last-of-type,
nav ul:last-of-type {
  margin-right: calc(var(--nav-element-spacing-horizontal) * -1);
}
nav li {
  display: inline-block;
  margin: 0;
  padding: var(--nav-element-spacing-vertical)
    var(--nav-element-spacing-horizontal);
}
nav li > * {
  --spacing: 0;
}
nav :where(a, [role="link"]) {
  display: inline-block;
  margin: calc(var(--nav-link-spacing-vertical) * -1)
    calc(var(--nav-link-spacing-horizontal) * -1);
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
  border-radius: var(--border-radius);
  text-decoration: none;
}
nav :where(a, [role="link"]):is([aria-current], :hover, :active, :focus) {
  text-decoration: none;
}
nav[aria-label="breadcrumb"] {
  align-items: center;
  justify-content: start;
}
nav[aria-label="breadcrumb"] ul li:not(:first-child) {
  -webkit-margin-start: var(--nav-link-spacing-horizontal);
  margin-inline-start: var(--nav-link-spacing-horizontal);
}
nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  position: absolute;
  width: calc(var(--nav-link-spacing-horizontal) * 2);
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) / 2);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) / 2);
  content: "/";
  color: var(--muted-color);
  text-align: center;
}
nav[aria-label="breadcrumb"] a[aria-current] {
  background-color: transparent;
  color: inherit;
  text-decoration: none;
  pointer-events: none;
}
nav [role="button"] {
  margin-right: inherit;
  margin-left: inherit;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}

aside nav,
aside ol,
aside ul,
aside li {
  display: block;
}
aside li {
  padding: calc(var(--nav-element-spacing-vertical) * 0.5)
    var(--nav-element-spacing-horizontal);
}
aside li a {
  display: block;
}
aside li [role="button"] {
  margin: inherit;
}

[dir="rtl"] nav[aria-label="breadcrumb"] ul li:not(:last-child) ::after {
  content: "\\";
}

/**
 * Progress
 */
progress {
  display: inline-block;
  vertical-align: baseline;
}

progress {
  -webkit-appearance: none;
  -moz-appearance: none;
  display: inline-block;
  appearance: none;
  width: 100%;
  height: 0.5rem;
  margin-bottom: calc(var(--spacing) * 0.5);
  overflow: hidden;
  border: 0;
  border-radius: var(--border-radius);
  background-color: var(--progress-background-color);
  color: var(--progress-color);
}
progress::-webkit-progress-bar {
  border-radius: var(--border-radius);
  background: none;
}
progress[value]::-webkit-progress-value {
  background-color: var(--progress-color);
}
progress::-moz-progress-bar {
  background-color: var(--progress-color);
}
@media (prefers-reduced-motion: no-preference) {
  progress:indeterminate {
    background: var(--progress-background-color)
      linear-gradient(
        to right,
        var(--progress-color) 30%,
        var(--progress-background-color) 30%
      )
      top left/150% 150% no-repeat;
    animation: progress-indeterminate 1s linear infinite;
  }
  progress:indeterminate[value]::-webkit-progress-value {
    background-color: transparent;
  }
  progress:indeterminate::-moz-progress-bar {
    background-color: transparent;
  }
}

@media (prefers-reduced-motion: no-preference) {
  [dir="rtl"] progress:indeterminate {
    animation-direction: reverse;
  }
}

@keyframes progress-indeterminate {
  0% {
    background-position: 200% 0;
  }
  100% {
    background-position: -200% 0;
  }
}
/**
 * Dropdown ([role="list"])
 */
details[role="list"],
li[role="list"] {
  position: relative;
}

details[role="list"] summary + ul,
li[role="list"] > ul {
  display: flex;
  z-index: 99;
  position: absolute;
  top: auto;
  right: 0;
  left: 0;
  flex-direction: column;
  margin: 0;
  padding: 0;
  border: var(--border-width) solid var(--dropdown-border-color);
  border-radius: var(--border-radius);
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  background-color: var(--dropdown-background-color);
  box-shadow: var(--card-box-shadow);
  color: var(--dropdown-color);
  white-space: nowrap;
}
details[role="list"] summary + ul li,
li[role="list"] > ul li {
  width: 100%;
  margin-bottom: 0;
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  list-style: none;
}
details[role="list"] summary + ul li:first-of-type,
li[role="list"] > ul li:first-of-type {
  margin-top: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li:last-of-type,
li[role="list"] > ul li:last-of-type {
  margin-bottom: calc(var(--form-element-spacing-vertical) * 0.5);
}
details[role="list"] summary + ul li a,
li[role="list"] > ul li a {
  display: block;
  margin: calc(var(--form-element-spacing-vertical) * -0.5)
    calc(var(--form-element-spacing-horizontal) * -1);
  padding: calc(var(--form-element-spacing-vertical) * 0.5)
    var(--form-element-spacing-horizontal);
  overflow: hidden;
  color: var(--dropdown-color);
  text-decoration: none;
  text-overflow: ellipsis;
}
details[role="list"] summary + ul li a:hover,
li[role="list"] > ul li a:hover {
  background-color: var(--dropdown-hover-background-color);
}

details[role="list"] summary::after,
li[role="list"] > a::after {
  display: block;
  width: 1rem;
  height: calc(1rem * var(--line-height, 1.5));
  -webkit-margin-start: 0.5rem;
  margin-inline-start: 0.5rem;
  float: right;
  transform: rotate(0deg);
  background-position: right center;
  background-size: 1rem auto;
  background-repeat: no-repeat;
  content: "";
}

details[role="list"] {
  padding: 0;
  border-bottom: none;
}
details[role="list"] summary {
  margin-bottom: 0;
}
details[role="list"] summary:not([role]) {
  height: calc(
    1rem * var(--line-height) + var(--form-element-spacing-vertical) * 2 +
      var(--border-width) * 2
  );
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--form-element-border-color);
  border-radius: var(--border-radius);
  background-color: var(--form-element-background-color);
  color: var(--form-element-placeholder-color);
  line-height: inherit;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
}
details[role="list"] summary:not([role]):active,
details[role="list"] summary:not([role]):focus {
  border-color: var(--form-element-active-border-color);
  background-color: var(--form-element-active-background-color);
}
details[role="list"] summary:not([role]):focus {
  box-shadow: 0 0 0 var(--outline-width) var(--form-element-focus-color);
}
details[role="list"][open] summary {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
details[role="list"][open] summary::before {
  display: block;
  z-index: 1;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  background: none;
  content: "";
  cursor: default;
}

nav details[role="list"] summary,
nav li[role="list"] a {
  display: flex;
  direction: ltr;
}

nav details[role="list"] summary + ul,
nav li[role="list"] > ul {
  min-width: -moz-fit-content;
  min-width: fit-content;
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul li a,
nav li[role="list"] > ul li a {
  border-radius: 0;
}

nav details[role="list"] summary,
nav details[role="list"] summary:not([role]) {
  height: auto;
  padding: var(--nav-link-spacing-vertical) var(--nav-link-spacing-horizontal);
}
nav details[role="list"][open] summary {
  border-radius: var(--border-radius);
}
nav details[role="list"] summary + ul {
  margin-top: var(--outline-width);
  -webkit-margin-start: 0;
  margin-inline-start: 0;
}
nav details[role="list"] summary[role="link"] {
  margin-bottom: calc(var(--nav-link-spacing-vertical) * -1);
  line-height: var(--line-height);
}
nav details[role="list"] summary[role="link"] + ul {
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(var(--nav-link-spacing-horizontal) * -1);
  margin-inline-start: calc(var(--nav-link-spacing-horizontal) * -1);
}

li[role="list"]:hover > ul,
li[role="list"] a:active ~ ul,
li[role="list"] a:focus ~ ul {
  display: flex;
}
li[role="list"] > ul {
  display: none;
  margin-top: calc(var(--nav-link-spacing-vertical) + var(--outline-width));
  -webkit-margin-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
  margin-inline-start: calc(
    var(--nav-element-spacing-horizontal) - var(--nav-link-spacing-horizontal)
  );
}
li[role="list"] > a::after {
  background-image: var(--icon-chevron);
}

/**
 * Loading ([aria-busy=true])
 */
[aria-busy="true"] {
  cursor: progress;
}

[aria-busy="true"]:not(input, select, textarea)::before {
  display: inline-block;
  width: 1em;
  height: 1em;
  border: 0.1875em solid currentColor;
  border-radius: 1em;
  border-right-color: transparent;
  content: "";
  vertical-align: text-bottom;
  vertical-align: -0.125em;
  animation: spinner 0.75s linear infinite;
  opacity: var(--loading-spinner-opacity);
}
[aria-busy="true"]:not(input, select, textarea):not(:empty)::before {
  margin-right: calc(var(--spacing) * 0.5);
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: calc(var(--spacing) * 0.5);
  margin-inline-end: calc(var(--spacing) * 0.5);
}
[aria-busy="true"]:not(input, select, textarea):empty {
  text-align: center;
}

button[aria-busy="true"],
input[type="submit"][aria-busy="true"],
input[type="button"][aria-busy="true"],
input[type="reset"][aria-busy="true"],
a[aria-busy="true"] {
  pointer-events: none;
}

@keyframes spinner {
  to {
    transform: rotate(360deg);
  }
}
/**
 * Tooltip ([data-tooltip])
 */
[data-tooltip] {
  position: relative;
}
[data-tooltip]:not(a, button, input) {
  border-bottom: 1px dotted;
  text-decoration: none;
  cursor: help;
}
[data-tooltip][data-placement="top"]::before,
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::before,
[data-tooltip]::after {
  display: block;
  z-index: 99;
  position: absolute;
  bottom: 100%;
  left: 50%;
  padding: 0.25rem 0.5rem;
  overflow: hidden;
  transform: translate(-50%, -0.25rem);
  border-radius: var(--border-radius);
  background: var(--tooltip-background-color);
  content: attr(data-tooltip);
  color: var(--tooltip-color);
  font-style: normal;
  font-weight: var(--font-weight);
  font-size: 0.875rem;
  text-decoration: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  opacity: 0;
  pointer-events: none;
}
[data-tooltip][data-placement="top"]::after,
[data-tooltip]::after {
  padding: 0;
  transform: translate(-50%, 0rem);
  border-top: 0.3rem solid;
  border-right: 0.3rem solid transparent;
  border-left: 0.3rem solid transparent;
  border-radius: 0;
  background-color: transparent;
  content: "";
  color: var(--tooltip-background-color);
}
[data-tooltip][data-placement="bottom"]::before,
[data-tooltip][data-placement="bottom"]::after {
  top: 100%;
  bottom: auto;
  transform: translate(-50%, 0.25rem);
}
[data-tooltip][data-placement="bottom"]:after {
  transform: translate(-50%, -0.3rem);
  border: 0.3rem solid transparent;
  border-bottom: 0.3rem solid;
}
[data-tooltip][data-placement="left"]::before,
[data-tooltip][data-placement="left"]::after {
  top: 50%;
  right: 100%;
  bottom: auto;
  left: auto;
  transform: translate(-0.25rem, -50%);
}
[data-tooltip][data-placement="left"]:after {
  transform: translate(0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-left: 0.3rem solid;
}
[data-tooltip][data-placement="right"]::before,
[data-tooltip][data-placement="right"]::after {
  top: 50%;
  right: auto;
  bottom: auto;
  left: 100%;
  transform: translate(0.25rem, -50%);
}
[data-tooltip][data-placement="right"]:after {
  transform: translate(-0.3rem, -50%);
  border: 0.3rem solid transparent;
  border-right: 0.3rem solid;
}
[data-tooltip]:focus::before,
[data-tooltip]:focus::after,
[data-tooltip]:hover::before,
[data-tooltip]:hover::after {
  opacity: 1;
}
@media (hover: hover) and (pointer: fine) {
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::before,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::before,
  [data-tooltip]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover [data-tooltip]:focus::after,
  [data-tooltip]:hover::after {
    animation-name: tooltip-caret-slide-top;
  }
  [data-tooltip][data-placement="bottom"]:focus::before,
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::before,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-bottom;
  }
  [data-tooltip][data-placement="bottom"]:focus::after,
  [data-tooltip][data-placement="bottom"]:hover::after {
    animation-name: tooltip-caret-slide-bottom;
  }
  [data-tooltip][data-placement="left"]:focus::before,
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::before,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-left;
  }
  [data-tooltip][data-placement="left"]:focus::after,
  [data-tooltip][data-placement="left"]:hover::after {
    animation-name: tooltip-caret-slide-left;
  }
  [data-tooltip][data-placement="right"]:focus::before,
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::before,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-duration: 0.2s;
    animation-name: tooltip-slide-right;
  }
  [data-tooltip][data-placement="right"]:focus::after,
  [data-tooltip][data-placement="right"]:hover::after {
    animation-name: tooltip-caret-slide-right;
  }
}
@keyframes tooltip-slide-top {
  from {
    transform: translate(-50%, 0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-top {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.25rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-bottom {
  from {
    transform: translate(-50%, -0.75rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, 0.25rem);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-bottom {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-50%, -0.5rem);
    opacity: 0;
  }
  to {
    transform: translate(-50%, -0.3rem);
    opacity: 1;
  }
}
@keyframes tooltip-slide-left {
  from {
    transform: translate(0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-left {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.3rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-slide-right {
  from {
    transform: translate(-0.75rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(0.25rem, -50%);
    opacity: 1;
  }
}
@keyframes tooltip-caret-slide-right {
  from {
    opacity: 0;
  }
  50% {
    transform: translate(-0.05rem, -50%);
    opacity: 0;
  }
  to {
    transform: translate(-0.3rem, -50%);
    opacity: 1;
  }
}

/**
 * Accessibility & User interaction
 */
[aria-controls] {
  cursor: pointer;
}

[aria-disabled="true"],
[disabled] {
  cursor: not-allowed;
}

[aria-hidden="false"][hidden] {
  display: initial;
}

[aria-hidden="false"][hidden]:not(:focus) {
  clip: rect(0, 0, 0, 0);
  position: absolute;
}

a,
area,
button,
input,
label,
select,
summary,
textarea,
[tabindex] {
  -ms-touch-action: manipulation;
}

[dir="rtl"] {
  direction: rtl;
}

/**
* Reduce Motion Features
*/
@media (prefers-reduced-motion: reduce) {
  *:not([aria-busy="true"]),
  :not([aria-busy="true"])::before,
  :not([aria-busy="true"])::after {
    background-attachment: initial !important;
    animation-duration: 1ms !important;
    animation-delay: -1ms !important;
    animation-iteration-count: 1 !important;
    scroll-behavior: auto !important;
    transition-delay: 0s !important;
    transition-duration: 0s !important;
  }
}

#mount {
  /* --primary: rgb(227, 59, 126); */
  --primary: #ea4c89;
  --primary-hover: #f082ac;
  --icon-xia: url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgaWQ9IkZyYW1lIj4KPHBhdGggaWQ9IlZlY3RvciIgZD0iTTguMDAyOTEgOS42Nzk4M0wzLjgzMzM5IDUuNTEyMjFMMy4wMjUzOSA2LjMxOTgzTDguMDAzMjkgMTEuMjk1MUwxMi45NzYyIDYuMzE5ODNMMTIuMTY3OSA1LjUxMjIxTDguMDAyOTEgOS42Nzk4M1oiIGZpbGw9IiM4MzgzODMiLz4KPC9nPgo8L3N2Zz4K");
  --switch-checked-background-color: var(--primary);
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --primary: #e23c7c;
  --primary-hover: #f082ac;
}

[data-theme="dark"] {
  --primary: #e23c7c;
  --primary-hover: #f082ac;
}

[data-theme="light"] {
  --primary: #ea4c89;
  --primary-hover: #f082ac;
}

li.select-link.select-link:hover > ul {
  display: none;
}
li.select-link.select-link > ul {
  display: none;
}
li.select-link.select-link a:focus ~ ul {
  display: none;
}

li.select-link.select-link a:active ~ ul {
  display: none;
}
li.select-link-active.select-link-active > ul {
  display: flex;
}
li.select-link-active.select-link-active:hover > ul {
  display: flex;
}

li.select-link-active.select-link-active a:focus ~ ul {
  display: flex;
}

li.select-link-active.select-link-active a:active ~ ul {
  display: flex;
}
ul.select-link-ul.select-link-ul {
  right: 0px;
  left: auto;
}

a.select-link-selected {
  background-color: var(--primary-focus);
}
.immersive-translate-no-select {
  -webkit-touch-callout: none; /* iOS Safari */
  -webkit-user-select: none; /* Safari */
  -khtml-user-select: none; /* Konqueror HTML */
  -moz-user-select: none; /* Old versions of Firefox */
  -ms-user-select: none; /* Internet Explorer/Edge */
  user-select: none;
}

/* li[role="list"].no-arrow > a::after { */
/*   background-image: none; */
/*   width: 0; */
/*   color: var(--color); */
/* } */
li[role="list"].no-arrow {
  margin-left: 8px;
  padding-right: 0;
}
li[role="list"] > a::after {
  -webkit-margin-start: 0.2rem;
  margin-inline-start: 0.2rem;
}

li[role="list"].no-arrow > a,
li[role="list"].no-arrow > a:link,
li[role="list"].no-arrow > a:visited {
  color: var(--secondary);
}

select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 4px;
  max-width: 128px;
  overflow: hidden;
  color: var(--primary);
  font-size: 13px;
  border: none;
  padding: 0;
  padding-right: 20px;
  padding-left: 8px;
  text-overflow: ellipsis;
  color: var(--color);
}
select.min-select-secondary {
  color: var(--color);
}
select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}
select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.muted {
  color: var(--muted-color);
}

.select.button-select {
  --background-color: var(--secondary-hover);
  --border-color: var(--secondary-hover);
  --color: var(--secondary-inverse);
  cursor: pointer;
  --box-shadow: var(--button-box-shadow, 0 0 0 rgba(0, 0, 0, 0));
  padding: var(--form-element-spacing-vertical)
    var(--form-element-spacing-horizontal);
  border: var(--border-width) solid var(--border-color);
  border-radius: var(--border-radius);
  outline: none;
  background-color: var(--background-color);
  box-shadow: var(--box-shadow);
  color: var(--color);
  font-weight: var(--font-weight);
  font-size: 16px;
  line-height: var(--line-height);
  text-align: center;
  cursor: pointer;
  transition: background-color var(--transition), border-color var(--transition),
    color var(--transition), box-shadow var(--transition);
  -webkit-appearance: button;
  margin: 0;
  margin-bottom: 0px;
  overflow: visible;
  font-family: inherit;
  text-transform: none;
}

body {
  padding: 0;
  margin: 0 auto;
  min-width: 268px;
  border-radius: 10px;
}

.popup-container {
  font-size: 16px;
  --font-size: 16px;
  color: #666;
  background-color: var(--popup-footer-background-color);
  width: 316px;
  min-width: 316px;
}

.popup-content {
  background-color: var(--popup-content-background-color);
  border-radius: 0px 0px 12px 12px;
  padding: 16px 20px;
}

.immersive-translate-popup-overlay {
  position: fixed;
  top: 0;
  left: 0;
  height: 100%;
  width: 100%;
  touch-action: none;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 10px;
  border: 1px solid var(--muted-border-color);
}

#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 4px;
  --border-width: 1px;
  --outline-width: 3px;
  --spacing: 16px;
  --typography-spacing-vertical: 24px;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 12px;
  --form-element-spacing-horizontal: 16px;
  --nav-element-spacing-vertical: 16px;
  --nav-element-spacing-horizontal: 8px;
  --nav-link-spacing-vertical: 8px;
  --nav-link-spacing-horizontal: 8px;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(4px);
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --popup-footer-background-color: #e8eaeb;
  --popup-content-background-color: #ffffff;
  --popup-item-background-color: #f3f5f6;
  --popup-item-hover-background-color: #eaeced;
  --popup-trial-pro-background-color: #f9fbfc;
  --text-black-2: #222222;
  --text-gray-2: #222222;
  --text-gray-6: #666666;
  --text-gray-9: #999999;
  --text-gray-c2: #c2c2c2;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(75, 76, 77, 0.2);
  --service-select-border-color: #fafafa;
  --service-select-selected-background-color: #f3f5f6;
  --download-app-background: #f3f5f6;
}

@media only screen and (prefers-color-scheme: dark) {
  #mount:not([data-theme="light"]) {
    --popup-footer-background-color: #0d0d0d;
    --popup-content-background-color: #191919;
    --popup-item-background-color: #272727;
    --popup-item-hover-background-color: #333333;
    --popup-trial-pro-background-color: #222222;
    --text-black-2: #ffffff;
    --text-gray-2: #dbdbdb;
    --text-gray-6: #b3b3b3;
    --text-gray-9: #777777;
    --text-gray-c2: #5b5b5b;
    --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
    --service-select-border-color: #2c2c2c;
    --service-select-selected-background-color: #333333;
    --download-app-background: #333;
  }
}

[data-theme="dark"] {
  --popup-footer-background-color: #0d0d0d;
  --popup-content-background-color: #191919;
  --popup-item-background-color: #272727;
  --popup-item-hover-background-color: #333333;
  --popup-trial-pro-background-color: #222222;
  --text-black-2: #ffffff;
  --text-gray-2: #dbdbdb;
  --text-gray-6: #b3b3b3;
  --text-gray-9: #777777;
  --text-gray-c2: #5b5b5b;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
  --service-select-border-color: #2c2c2c;
  --service-select-selected-background-color: #333333;
  --download-app-background: #333;
}

.text-balck {
  color: var(--text-black-2);
}

.text-gray-2 {
  color: var(--text-gray-2);
}

.text-gray-6 {
  color: var(--text-gray-6);
}

.text-gray-9 {
  color: var(--text-gray-9);
}

.text-gray-c2 {
  color: var(--text-gray-c2);
}

#mount {
  min-width: 268px;
}

.main-button {
  font-size: 15px;
  vertical-align: middle;
  border-radius: 12px;
  padding: unset;
  height: 44px;
  line-height: 44px;
}

.pt-4 {
  padding-top: 16px;
}

.p-2 {
  padding: 8px;
}

.pl-5 {
  padding-left: 48px;
}

.p-0 {
  padding: 0;
}

.pl-2 {
  padding-left: 8px;
}

.pl-4 {
  padding-left: 24px;
}

.pt-2 {
  padding-top: 8px;
}

.pb-2 {
  padding-bottom: 8px;
}

.pb-4 {
  padding-bottom: 16px;
}

.pb-5 {
  padding-bottom: 20px;
}

.pr-5 {
  padding-right: 48px;
}

.text-sm {
  font-size: 13px;
}

.text-base {
  font-size: 16px;
}

.w-full {
  width: 100%;
}

.flex {
  display: flex;
}

.flex-row {
  flex-direction: row;
}

.flex-wrap {
  flex-wrap: wrap;
}

.flex-end {
  justify-content: flex-end;
}

.flex-grow {
  flex-grow: 1;
}

.justify-between {
  justify-content: space-between;
}

.mb-0 {
  margin-bottom: 0px;
}

.mb-2 {
  margin-bottom: 8px;
}

.mb-4 {
  margin-bottom: 16px;
}

.mb-3 {
  margin-bottom: 12px;
}

.inline-block {
  display: inline-block;
}

.py-2 {
  padding-top: 8px;
  padding-bottom: 8px;
}

.py-2-5 {
  padding-top: 6px;
  padding-bottom: 6px;
}

.mt-0 {
  margin-top: 0;
}

.mt-2 {
  margin-top: 8px;
}

.mt-3 {
  margin-top: 12px;
}

.mt-4 {
  margin-top: 16px;
}

.mt-5 {
  margin-top: 20px;
}

.mt-6 {
  margin-top: 24px;
}

.mb-1 {
  margin-bottom: 4px;
}

.ml-4 {
  margin-left: 24px;
}

.ml-3 {
  margin-left: 16px;
}

.ml-2 {
  margin-left: 8px;
}

.ml-1 {
  margin-left: 4px;
}

.mr-1 {
  margin-right: 4px;
}

.mr-2 {
  margin-right: 8px;
}

.mr-3 {
  margin-right: 16px;
}

.mx-2 {
  margin-left: 8px;
  margin-right: 8px;
}

.pl-3 {
  padding-left: 12px;
}

.pr-3 {
  padding-right: 12px;
}

.p-3 {
  padding: 12px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-3 {
  padding-top: 12px;
}

.px-6 {
  padding-left: 18px;
  padding-right: 18px;
}

.px-4 {
  padding-left: 16px;
  padding-right: 16px;
}

.pt-6 {
  padding-top: 20px;
}

.py-3 {
  padding-top: 12px;
  padding-bottom: 12px;
}

.py-0 {
  padding-top: 0;
  padding-bottom: 0;
}

.left-auto {
  left: auto !important;
}

.max-h-28 {
  max-height: 112px;
}

.max-h-30 {
  max-height: 120px;
}

.overflow-y-scroll {
  overflow-y: scroll;
}

.text-xs {
  font-size: 12px;
}

.flex-1 {
  flex: 1;
}

.flex-3 {
  flex: 3;
}

.flex-4 {
  flex: 4;
}

.flex-2 {
  flex: 2;
}

.items-center {
  align-items: center;
}

.max-content {
  width: max-content;
}

.justify-center {
  justify-content: center;
}

.items-end {
  align-items: flex-end;
}

.items-baseline {
  align-items: baseline;
}

.my-5 {
  margin-top: 48px;
  margin-bottom: 48px;
}

.my-4 {
  margin-top: 24px;
  margin-bottom: 24px;
}

.my-3 {
  margin-top: 16px;
  margin-bottom: 16px;
}

.pt-3 {
  padding-top: 12px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-2 {
  padding-top: 8px;
}

.px-2 {
  padding-left: 8px;
  padding-right: 8px;
}

.pt-1 {
  padding-top: 4px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.pb-2 {
  padding-bottom: 8px;
}

.justify-end {
  justify-content: flex-end;
}

.w-auto {
  width: auto;
}

.shrink-0 {
  flex-shrink: 0;
}

select.language-select,
select.translate-service,
select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 0px;
  max-width: unset;
  flex: 1;
  overflow: hidden;
  font-size: 13px;
  border: none;
  border-radius: 8px;
  padding-right: 30px;
  padding-left: 0px;
  background-position: center right 12px;
  background-size: 16px auto;
  background-image: var(--icon-xia);
  text-overflow: ellipsis;
  color: var(--text-gray-2);
  background-color: transparent;
  box-shadow: unset !important;
  cursor: pointer;
}

select.more {
  background-position: center right;
  padding-right: 20px;
}

select.transform-padding-left {
  padding-left: 12px;
  transform: translateX(-12px);
  background-position: center right 0px;
}

select.translate-service {
  color: var(--text-black-2);
}

.min-select-container.disabled {
  opacity: 0.5;
  pointer-events: none;
}

/* dark use black, for windows */
@media (prefers-color-scheme: dark) {
  select.language-select option,
  select.translate-service option,
  select.min-select option {
    background-color: #666666;
  }
}

.text-overflow-ellipsis {
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
}

.max-w-20 {
  max-width: 180px;
  white-space: nowrap;
}

select.min-select-secondary {
  color: var(--color);
}

select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}

select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

.popup-footer {
  background-color: var(--popup-footer-background-color);
  height: 40px;
}

.text-right {
  text-align: right;
}

.clickable {
  cursor: pointer;
}

.close {
  cursor: pointer;
  width: 16px;
  height: 16px;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}

.padding-two-column {
  padding-left: 40px;
  padding-right: 40px;
}

.muted {
  color: #999;
}

.text-label {
  color: #666;
}

.display-none {
  display: none;
}

/* dark use #18232c */
@media (prefers-color-scheme: dark) {
  .text-label {
    color: #9ca3af;
  }
}

.text-decoration-none {
  text-decoration: none;
}

.text-decoration-none:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --text-decoration: none !important;
  background-color: transparent !important;
}

.language-select-container {
  position: relative;
  width: 100%;
  background-color: var(--popup-item-background-color);
  height: 55px;
  border-radius: 12px;
}

select.language-select {
  color: var(--text-black-2);
  font-size: 14px;
  padding: 8px 24px 24px 16px;
  position: absolute;
  border-radius: 12px;
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
}

select.text-gray-6 {
  color: var(--text-gray-6);
}

.language-select-container label {
  position: absolute;
  bottom: 10px;
  left: 16px;
  font-size: 12px;
  color: var(--text-gray-9);
  line-height: 12px;
  margin: 0;
}

.translation-service-container {
  background-color: var(--popup-item-background-color);
  border-radius: 12px;
}

.min-select-container {
  display: flex;
  justify-content: space-between;
  align-items: center;
  height: 44px;
  background-color: var(--popup-item-background-color);
  padding-left: 16px;
}

.min-select-container:first-child {
  border-top-left-radius: 10px;
  border-top-right-radius: 10px;
}

.min-select-container:last-child {
  border-bottom-left-radius: 10px;
  border-bottom-right-radius: 10px;
}

.min-select-container:only-child {
  border-radius: 10px;
}

.translate-mode {
  width: 44px;
  height: 44px;
  border-radius: 22px;
  background-color: var(--popup-item-background-color);
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  cursor: pointer;
}

.translate-mode svg {
  fill: var(--text-gray-2);
}

.widgets-container {
  display: flex;
  align-items: stretch;
  justify-content: space-between;
  width: 100%;
  gap: 9px;
}

/* 当只有两个小组件时的样式优化 */
.widgets-container.widgets-two-items {
  gap: 16px;
}

.widgets-container.widgets-two-items .widget-item {
  flex: 0 1 auto;
  min-width: 93px;
  max-width: 120px;
}

.widget-item {
  display: flex;
  max-width: 93px;
  flex-direction: row;
  align-items: center;
  justify-content: center;
  background-color: var(--popup-item-background-color);
  font-size: 12px;
  min-height: 44px;
  height: 100%;
  border-radius: 8px;
  cursor: pointer;
  flex: 1;
  padding: 8px 4px;
  text-align: center;
}

.widget-icon-text {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  color: var(--text-gray-2);
}

.widgets-container svg {
  fill: var(--text-gray-2);
  color: var(--text-gray-2);
}

.share-button-container {
  display: flex;
  align-items: center;
  cursor: pointer;
  padding: 2px 3px 0 8px;
}

.share-button-container svg {
  fill: var(--text-gray-9);
}

.min-select-container:hover,
.language-select-container:hover,
.widget-item:hover,
.translate-mode:hover {
  background-color: var(--popup-item-hover-background-color);
}

.main-button:hover {
  background-color: #f5508f;
}

.share-button-container:hover {
  background-color: var(--popup-item-background-color);
  border-radius: 6px;
}

.error-boundary {
  background: #fff2f0;
  border: 1px solid #ffccc7;
  display: flex;
  padding: 12px;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.88);
  word-break: break-all;
  margin: 12px;
  border-radius: 12px;
  flex-direction: column;
}

.upgrade-pro {
  border-radius: 11px;
  background: linear-gradient(57deg, #272727 19.8%, #696969 82.2%);
  padding: 2px 8px;
  transform: scale(0.85);
}

.upgrade-pro span {
  background: linear-gradient(180deg, #ffeab4 17.65%, #f8c235 85.29%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  font-size: 12px;
  margin-left: 4px;
}

.upgrade-pro svg {
  margin-top: -2px;
}

.upgrade-pro:hover {
  background: linear-gradient(57deg, #3d3d3d 19.8%, #949494 82.2%);
}

.border-bottom-radius-0 {
  border-bottom-left-radius: 0 !important;
  border-bottom-right-radius: 0 !important;
}

.trial-pro-container {
  border-radius: 0px 0px 12px 12px;
  background: var(--popup-trial-pro-background-color);
  display: flex;
  align-items: center;
  height: 44px;
  padding-left: 16px;
  padding-right: 12px;
  font-size: 12px;
}

.trial-pro-container label {
  line-height: 13px;
  color: var(--text-black-2);
}

.trial-pro-container img {
  margin-left: 5px;
}

.cursor-pointer {
  cursor: pointer;
}

.upgrade-pro-discount-act {
  height: 25px;
  display: flex;
  padding: 0 4px;
  align-items: center;
  border-radius: 15px;
  background: linear-gradient(
    90deg,
    #cefbfa 11.33%,
    #d7f56f 63.75%,
    #fccd5e 100%
  );
  transform: scale(0.9);
  box-shadow: 0px 1.8px 3.6px 0px rgba(0, 0, 0, 0.1);
  cursor: pointer;
}

.upgrade-pro-discount-act span {
  font-size: 12px;
  font-weight: 700;
  margin-left: 4px;
  color: #222222;
}

.upgrade-pro-discount-act:hover {
  text-decoration: unset;
  background: linear-gradient(
    90deg,
    #e2fffe 11.33%,
    #e6ff91 63.75%,
    #ffdf93 100%
  );
}

.custom-select-container {
  width: 200px;
  position: relative;
  flex: 1;
}

#translation-service-select {
  padding-right: 12px;
  padding-left: 6px;
}

.custom-select-content {
  border-radius: 12px;
  background: var(--popup-content-background-color);
  box-shadow: var(--service-select-content-shadow);
  border: 1px solid var(--service-select-border-color);
  padding: 4px 5px;
  position: absolute;
  left: -10px;
  right: 0;
  z-index: 100;
  overflow-y: auto;
}

.custom-select-item.default {
  width: 100%;
  padding: 0;
}

.custom-select-item {
  font-size: 13px;
  padding: 5px 6px;
  border-radius: 8px;
  display: flex;
  align-items: center;
  cursor: pointer;
  color: var(--text-black-2);
  width: auto;
  overflow: hidden;
  height: 30px;
  line-height: 30px;
}

.custom-select-item-img {
  width: 20px;
  height: 20px;
  margin-right: 4px;
}

@media (prefers-color-scheme: dark) {
  .custom-select-item-img {
    margin-right: 6px;
  }
}

.custom-select-content .custom-select-item.selected,
.custom-select-content .custom-select-item:hover {
  background: var(--service-select-selected-background-color);
}

.custom-select-item > span {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.custom-select-item-pro {
  font-size: 12px;
  margin-left: 6px;
  display: flex;
}

.custom-select-item-pro img {
  margin: 0 3px;
  width: 20px;
  flex-shrink: 0;
}

.custom-select-group-header {
  font-size: 12px;
  font-weight: 500;
  color: var(--text-gray-9);
  padding: 6px 8px 4px;
  margin-top: 2px;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

.more-container {
  position: relative;
}

.new-menu-indicator {
  position: absolute;
  width: 8px;
  height: 8px;
  background-color: #ef3434;
  border-radius: 50%;
  right: 18px;
  top: 4px;
}

.download-app {
  display: inline-flex;
  align-items: center;
  gap: 4px;
  border-radius: 8px;
  background: var(--download-app-background);
  padding: 4px 8px;
  color: var(--text-gray-6);
  font-size: 12px;
  cursor: pointer;
  transition: all 0.2s ease-in-out;
}

/* Popup 动画效果 */
@keyframes popup-fade-in {
  from {
    opacity: 0;
    transform: translateY(10px) scale(0.95);
  }
  to {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
}

@keyframes popup-fade-out {
  from {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
  to {
    opacity: 0;
    transform: translateY(10px) scale(0.95);
  }
}

.popup-generic-content {
  animation: popup-fade-in 0.2s ease-out;
}

.popup-generic-content.hiding {
  animation: popup-fade-out 0.15s ease-in;
}

html {
  font-size: 17px;
}

@media print {
  .imt-fb-container {
    display: none !important;
  }
}

#mount#mount {
  position: absolute;
  display: none;
  min-width: 250px;
  height: auto;
  --font-size: 17px;
  font-size: 17px;
}

/* float-ball */
.imt-fb-container {
  position: fixed;
  padding: 0;
  top: 335px;
  width: fit-content;
  display: flex;
  flex-direction: column;
  display: none;
  direction: ltr;
}

.imt-fb-container.left {
  align-items: flex-start;
  left: 0;
}

.imt-fb-container.right {
  align-items: flex-end;
  right: 0;
}

.imt-fb-btn {
  cursor: pointer;
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 56px;
  box-shadow: 2px 6px 10px 0px #0e121629;
}

.imt-fb-btn.left {
  border-top-right-radius: 36px;
  border-bottom-right-radius: 36px;
}

.imt-fb-btn.right {
  border-top-left-radius: 36px;
  border-bottom-left-radius: 36px;
}

.imt-fb-btn div {
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 54px;
  display: flex;
  align-items: center;
}

.imt-fb-btn.left div {
  border-top-right-radius: 34px;
  border-bottom-right-radius: 34px;
  justify-content: flex-end;
}

.imt-fb-btn.right div {
  border-top-left-radius: 34px;
  border-bottom-left-radius: 34px;
}

.imt-fb-logo-img {
  width: 20px;
  height: 20px;
  margin: 0 10px;
}

.imt-fb-logo-img-big-bg {
  width: 28px;
  height: 28px;
  margin: 0;
  padding: 4px;
  background-color: #ed6d8f;
  border-radius: 50%;
  margin: 0 5px;
}

.imt-float-ball-translated {
  position: absolute;
  width: 11px;
  height: 11px;
  bottom: 4px;
  right: 20px;
}

.btn-animate {
  -webkit-transform: translate3d(0, 0, 0);
  transform: translate3d(0, 0, 0);
  -webkit-transition: -webkit-transform ease-out 250ms;
  transition: -webkit-transform ease-out 250ms;
  transition: transform ease-out 250ms;
  transition: transform ease-out 250ms, -webkit-transform ease-out 250ms;
}

.imt-fb-setting-btn {
  margin-right: 18px;
  width: 28px;
  height: 28px;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 20px;
  box-shadow: 2px 10px 24px 0px #0e121614;
  border: none;
}

.popup-container {
  border-radius: 20px;
}

.popup-content {
  border-radius: 20px 20px 12px 12px;
}
.popup-footer {
  border-radius: 20px;
}

.imt-fb-close-button {
  pointer-events: all;
  cursor: pointer;
  position: absolute;
  margin-top: -10px;
}

.imt-fb-close-content {
  padding: 22px;
  width: 320px;
  pointer-events: all;
}

.imt-fb-close-title {
  font-weight: 500;
  color: var(--h2-color);
}

.imt-fb-close-radio-content {
  background-color: var(--background-light-green);
  padding: 8px 20px;
}

.imt-fb-radio-sel,
.imt-fb-radio-nor {
  width: 16px;
  height: 16px;
  border-radius: 8px;
  flex-shrink: 0;
}

.imt-fb-radio-sel {
  border: 2px solid var(--primary);
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-radio-sel div {
  width: 8px;
  height: 8px;
  border-radius: 4px;
  background-color: var(--primary);
}

.imt-fb-radio-nor {
  border: 2px solid #d3d4d6;
}

.imt-fb-primary-btn {
  background-color: var(--primary);
  width: 72px;
  height: 32px;
  color: white;
  border-radius: 8px;
  text-align: center;
  line-height: 32px;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-default-btn {
  border: 1px solid var(--primary);
  width: 72px;
  height: 32px;
  border-radius: 8px;
  color: var(--primary);
  line-height: 32px;
  text-align: center;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-guide-container {
  width: 312px;
  transform: translateY(-45%);
}

.imt-fb-guide-bg {
  position: absolute;
  left: 30px;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  height: 100%;
  width: 90%;
}

.imt-fb-guide-bg.left {
  transform: scaleX(-1);
}

.imt-fb-guide-content {
  margin: 16px -30px 80px 0px;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.imt-fb-guide-content.left {
  margin: 16px 21px 60px 32px;
}

.imt-fb-guide-img {
  width: 220px;
  height: 112px;
}

.imt-fb-guide-message {
  font-size: 14px;
  line-height: 28px;
  color: #333333;
  white-space: pre-wrap;
  text-align: center;
  font-weight: 700;
  margin-bottom: 20px;
}

.imt-manga-guide-message {
  font-size: 16px;
  line-height: 24px;
  color: #333333;
  text-align: center;
  font-weight: 500;
  margin-bottom: 12px;
}

.imt-fb-guide-button {
  margin-top: 16px;
  line-height: 40px;
  height: 40px;
  padding: 0 20px;
  width: unset;
}

.imt-fb-more-buttons {
  box-shadow: 0px 2px 10px 0px #00000014;
  border: none;
  background: var(--float-ball-more-button-background-color);
  width: 36px;
  display: flex;
  flex-direction: column;
  border-radius: 18px;
  margin-top: 0px;
  padding: 7px 0 7px 0;
}

.imt-fb-more-buttons > div {
  margin: auto;
}

.imt-fb-side,
.imt-fb-reward {
  border-radius: 50%;
  cursor: pointer;
  pointer-events: all;
  position: relative;
}

.imt-fb-side {
  margin: 10px 0;
}

.imt-fb-new-badge {
  width: 26px;
  height: 14px;
  padding: 3px;
  background-color: #f53f3f;
  border-radius: 4px;
  position: absolute;
  top: -5px;
  right: 15px;
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-side *,
.imt-fb-reward * {
  pointer-events: all;
}

.imt-fb-more-button {
  width: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
}
/* Sheet.css */
.immersive-translate-sheet {
  position: fixed;
  transform: translateY(100%);
  /* Start off screen */
  left: 0;
  right: 0;
  background-color: white;
  transition: transform 0.3s ease-out;
  /* Smooth slide transition */
  box-shadow: 0px -2px 10px rgba(0, 0, 0, 0.1);
  /* Ensure it's above other content */
  bottom: 0;
  border-top-left-radius: 16px;
  border-top-right-radius: 16px;
  overflow: hidden;
}

.immersive-translate-sheet.visible {
  transform: translateY(0);
}

.immersive-translate-sheet-backdrop {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  opacity: 0;
  transition: opacity 0.3s ease-out;
}

.immersive-translate-sheet-backdrop.visible {
  opacity: 1;
}

.popup-container-sheet {
  max-width: 100vw;
  width: 100vw;
}

.imt-no-events svg * {
  pointer-events: none !important;
}

.imt-manga-button {
  width: 36px;
  display: flex;
  flex-direction: column;
  position: relative;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  pointer-events: all;
  margin: 0 0 10px 0;
  background-color: var(--float-ball-more-button-background-color);
  border-radius: 18px;
  filter: drop-shadow(0px 2px 10px rgba(0, 0, 0, 0.08));
  opacity: 0.5;
  right: 8px;
  padding: 10px 0 4px 0;
}

.imt-manga-feedback {
  cursor: pointer;
  margin-bottom: 10px;
}

.imt-fb-feedback {
  cursor: pointer;
  margin-top: 10px;
}

.imt-fb-upgrade-button {
  cursor: pointer;
  margin-top: 10px;
}

.imt-manga-button:hover {
  opacity: 1;
}

.imt-manga-translated {
  position: absolute;
  left: 24px;
  top: 20px;
}

.imt-float-ball-loading {
  animation: imt-loading-animation 0.6s infinite linear !important;
}

.imt-manga-guide-bg {
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  width: 372px;
  transform: translateY(-50%);
}
.imt-manga-guide-content {
  position: absolute;
  top: 15px;
  left: 0;
  right: 0;
  margin: 0 40px 0;
}

.img-manga-guide-button {
  width: fit-content;
  margin: 0 auto;
}

.img-manga-close {
  position: absolute;
  bottom: -200px;
  width: 32px;
  height: 32px;
  left: 0;
  right: 0;
  margin: auto;
  cursor: pointer;
}

.imt-fb-container.dragging .imt-fb-more-buttons,
.imt-fb-container.dragging .imt-manga-button,
.imt-fb-container.dragging .btn-animate:not(.imt-fb-btn) {
  display: none !important;
}

.imt-fb-container.dragging .imt-fb-btn {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  cursor: move !important;
}

.imt-fb-container.dragging .imt-fb-btn div {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  margin: 0 !important;
}

.imt-fb-container.dragging .imt-fb-btn.left,
.imt-fb-container.dragging .imt-fb-btn.right {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-btn.left div,
.imt-fb-container.dragging .imt-fb-btn.right div {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-logo-img {
  margin: 0 !important;
  padding: 4px !important;
}

.imt-fb-container.dragging .imt-float-ball-translated {
  right: 2px !important;
  bottom: 2px !important;
}

@-webkit-keyframes imt-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes imt-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.imt-fb-icon {
  color: #666666;
}

[data-theme="dark"] .imt-fb-icon {
  color: #B3B3B3;
}

[data-theme="light"] .imt-fb-icon {
  color: #666666;
}
</style><div id="mount" style="display: block;"><div class="imt-fb-container right notranslate " data-theme="dark" style="z-index: 2147483637; pointer-events: none; right: 0px; top: 237px; display: flex;"><div class="btn-animate" style="transform: translateX(-4px); opacity: 0.7; padding-left: 10px;"><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-btn imt-fb-more-button imt-fb-side"><svg class="imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M8.60547 12.9228C8.84029 12.9228 9.03755 13.0022 9.19629 13.161C9.3551 13.3198 9.43457 13.5171 9.43457 13.7519V18.5107C9.43457 18.7453 9.35513 18.9426 9.19629 19.1015C9.03755 19.2602 8.84029 19.3398 8.60547 19.3398H3.8457C3.61127 19.3397 3.41464 19.26 3.25586 19.1015C3.09712 18.9426 3.01758 18.7453 3.01758 18.5107V13.7519C3.01758 13.517 3.09712 13.3198 3.25586 13.161C3.41465 13.0023 3.61125 12.9229 3.8457 12.9228H8.60547ZM17.208 12.9228C17.4427 12.9228 17.6399 13.0022 17.7988 13.161C17.9575 13.3198 18.0371 13.5171 18.0371 13.7519V18.5107C18.0371 18.7453 17.9576 18.9426 17.7988 19.1015C17.6399 19.2602 17.4427 19.3398 17.208 19.3398H12.4492C12.2144 19.3398 12.0171 19.2602 11.8584 19.1015C11.6995 18.9426 11.6201 18.7453 11.6201 18.5107V13.7519C11.6201 13.517 11.6995 13.3198 11.8584 13.161C12.0171 13.0022 12.2144 12.9228 12.4492 12.9228H17.208ZM4.39258 17.9648H8.05957V14.2978H4.39258V17.9648ZM12.9951 17.9648H16.6621V14.2978H12.9951V17.9648ZM14.7598 2.92179C14.8641 2.57295 15.3576 2.57295 15.4619 2.92179L15.9561 4.57511C16.1376 5.18219 16.5965 5.66815 17.1924 5.8837L18.7412 6.44327C19.0635 6.56002 19.0633 7.01583 18.7412 7.13273L17.1924 7.69327C16.5966 7.90881 16.1376 8.39389 15.9561 9.00089L15.4619 10.6552C15.3575 11.0038 14.8642 11.0037 14.7598 10.6552L14.2646 9.00089C14.0831 8.39401 13.625 7.90881 13.0293 7.69327L11.4805 7.13273C11.158 7.01598 11.1579 6.55996 11.4805 6.44327L13.0293 5.8837C13.6251 5.66814 14.0831 5.18219 14.2646 4.57511L14.7598 2.92179ZM8.60547 4.32023C8.84029 4.32023 9.03755 4.39977 9.19629 4.55851C9.35496 4.71727 9.43448 4.91396 9.43457 5.14835V9.90812C9.43457 10.1429 9.35518 10.3402 9.19629 10.4989C9.03755 10.6578 8.84029 10.7372 8.60547 10.7372H3.8457C3.61131 10.7371 3.41463 10.6576 3.25586 10.4989C3.09712 10.3402 3.01758 10.1429 3.01758 9.90812V5.14835C3.01767 4.91386 3.09721 4.71731 3.25586 4.55851C3.41466 4.39986 3.61121 4.32032 3.8457 4.32023H8.60547ZM4.39258 9.36222H8.05957V5.69523H4.39258V9.36222Z" fill="currentColor"></path></svg><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg" style="position: absolute; right: 0px; top: 0px; display: none; transform: translate(30%, -30%);"><g clip-path="url(#clip0_34242_2353)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14Z" fill="#B1B1B1" fill-opacity="0.32"></path><mask id="mask0_34242_2353" maskUnits="userSpaceOnUse" x="1" y="1" width="12" height="12" style="mask-type: alpha;"><rect x="1" y="1" width="12" height="12" fill="#D9D9D9"></rect></mask><g mask="url(#mask0_34242_2353)"><path d="M7.86447 3.67324H6.13622V4.72999L4.80409 3.39199C4.75018 3.33699 4.70972 3.27808 4.68272 3.21524C4.65572 3.15241 4.64222 3.09533 4.64222 3.04399C4.64222 2.93141 4.68193 2.8352 4.76134 2.75537C4.84076 2.67562 4.94514 2.63574 5.07447 2.63574H8.98322C9.12864 2.63574 9.25147 2.68883 9.35172 2.79499C9.45189 2.90124 9.50197 3.04578 9.50197 3.22862C9.50197 3.35203 9.46122 3.46245 9.37972 3.55987C9.29822 3.65737 9.18897 3.69516 9.05197 3.67324H8.90197V6.36774C8.90197 6.51316 8.85214 6.63599 8.75247 6.73624C8.65272 6.83641 8.53051 6.88649 8.38585 6.88649C8.24118 6.88649 8.11809 6.83641 8.01659 6.73624C7.91518 6.63599 7.86447 6.51316 7.86447 6.36774V3.67324ZM6.4816 11.974V9.13599H4.57509C4.36193 9.13599 4.19043 9.06703 4.06059 8.92912C3.93076 8.79112 3.86584 8.62983 3.86584 8.44524C3.86584 8.35591 3.88509 8.26499 3.92359 8.17249C3.96209 8.08008 4.01984 7.99437 4.09684 7.91537L5.09872 6.89549V6.36149L2.32422 3.58412C2.22664 3.48645 2.1788 3.37678 2.18072 3.25512C2.18272 3.13345 2.23155 3.02483 2.32722 2.92924C2.42489 2.83158 2.53614 2.78274 2.66097 2.78274C2.7858 2.78274 2.89701 2.83158 2.99459 2.92924L10.9898 10.9245C11.0863 11.0209 11.1351 11.13 11.1361 11.2516C11.1371 11.3733 11.0898 11.4839 10.9941 11.5835C10.8984 11.6772 10.7867 11.7235 10.6588 11.7225C10.5311 11.7215 10.4194 11.6732 10.3237 11.5776L7.87909 9.13599L7.51909 9.14199V11.974C7.51909 12.1195 7.46926 12.2423 7.3696 12.3425C7.26985 12.4427 7.14764 12.4927 7.00297 12.4927C6.8583 12.4927 6.73522 12.4427 6.63372 12.3425C6.5323 12.2423 6.4816 12.1195 6.4816 11.974ZM5.35909 8.09849H6.83872L6.08834 7.35124L6.09434 7.35724L5.35909 8.09849Z" fill="white"></path></g></g><defs><clippath id="clip0_34242_2353"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div></div></div></div><div hidden="" class="imt-no-events btn-animate " id="manga-button" style="position: relative;"><div class="imt-manga-button" style="transform: translateX(2px);"><div class=" " style="position: relative; pointer-events: all; display: inline-block;"><div><svg class="imt-manga-feedback imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.0003 14.2749C11.213 14.2749 11.3895 14.2047 11.5299 14.0643C11.6705 13.9239 11.7408 13.7473 11.7408 13.5345C11.7408 13.3218 11.6705 13.1453 11.5299 13.0049C11.3895 12.8645 11.213 12.7943 11.0003 12.7943C10.7877 12.7943 10.6111 12.8645 10.4707 13.0049C10.3302 13.1453 10.2599 13.3218 10.2599 13.5345C10.2599 13.7473 10.3302 13.9239 10.4707 14.0643C10.6111 14.2047 10.7877 14.2749 11.0003 14.2749ZM11.0003 11.0842C11.1954 11.0842 11.3587 11.0185 11.4903 10.8869C11.622 10.7552 11.6878 10.5918 11.6878 10.3967V6.23645C11.6878 6.04135 11.622 5.87803 11.4903 5.74649C11.3587 5.6148 11.1954 5.54895 11.0003 5.54895C10.8052 5.54895 10.6419 5.6148 10.5104 5.74649C10.3787 5.87803 10.3128 6.04135 10.3128 6.23645V10.3967C10.3128 10.5918 10.3787 10.7552 10.5104 10.8869C10.6419 11.0185 10.8052 11.0842 11.0003 11.0842ZM5.53562 16.8311L3.70045 18.666C3.43966 18.9269 3.13968 18.9861 2.80051 18.8434C2.4615 18.7005 2.29199 18.4434 2.29199 18.072V4.73816C2.29199 4.27509 2.45241 3.88314 2.77324 3.5623C3.09408 3.24147 3.48603 3.08105 3.9491 3.08105H18.0516C18.5146 3.08105 18.9066 3.24147 19.2274 3.5623C19.5482 3.88314 19.7087 4.27509 19.7087 4.73816V15.174C19.7087 15.637 19.5482 16.029 19.2274 16.3498C18.9066 16.6706 18.5146 16.8311 18.0516 16.8311H5.53562ZM4.95033 15.4561H18.0516C18.1221 15.4561 18.1868 15.4266 18.2454 15.3678C18.3042 15.3092 18.3337 15.2445 18.3337 15.174V4.73816C18.3337 4.66758 18.3042 4.60295 18.2454 4.54428C18.1868 4.48546 18.1221 4.45605 18.0516 4.45605H3.9491C3.87851 4.45605 3.81389 4.48546 3.75522 4.54428C3.6964 4.60295 3.66699 4.66758 3.66699 4.73816V16.7254L4.95033 15.4561Z" fill="currentColor"></path></svg></div></div><div style="position: relative;"><svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="manhua"><path id="Vector" d="M14.8853 4.92364C14.8853 4.92364 16.3905 10.4362 22.6668 4C22.6668 4 20.3381 10.8907 25.3364 10.0843C25.3364 10.0843 22.0563 15.6994 29 18.0599C29 18.0599 22.9934 19.306 21.1617 28C21.1617 28 17.7679 24.54 14.8853 27.3549C14.8853 27.3549 13.3233 23.5724 7.33097 26.27C7.33097 26.27 10.1141 20.6549 4.83179 21.0507C4.83179 21.0507 7.16057 18.8955 3 15.9047C3 15.9047 7.50137 16.1833 6.33697 11.7117C6.33697 11.7117 10.0005 12.3421 8.66576 6.82957C8.65156 6.81491 12.4855 9.80574 14.8853 4.92364Z" fill="#ED6D8F"></path><path id="Vector_2" d="M20.8599 13.7022C20.885 13.1361 20.9543 12.5713 20.9959 12.0052C21.0337 11.568 20.8107 11.2794 20.3876 11.18C20.0759 11.1013 19.7508 11.0867 19.433 11.137C19.1951 11.1945 18.9542 11.2396 18.7113 11.2721C18.2403 11.3028 17.9973 11.5275 17.9796 11.988C17.977 12.0833 17.9596 12.1777 17.928 12.268C17.3034 13.9102 16.6774 15.5499 16.0503 17.1873C16.0301 17.2401 16.0062 17.2904 15.9671 17.3776C15.7291 16.8975 15.4281 16.4898 15.2745 15.9986C14.8073 14.5152 14.3186 13.033 13.8312 11.5594C13.6826 11.1112 13.3489 10.9344 12.8754 11.0216C12.7889 11.0365 12.7008 11.0398 12.6134 11.0314C12.2241 10.9938 11.8311 11.0404 11.4623 11.1677C11.0946 11.2991 10.9498 11.557 11.0152 11.9254C11.0428 12.0371 11.0643 12.1503 11.0795 12.2643C11.1223 13.1902 11.1777 14.1087 11.2054 15.0321C11.257 16.7992 11.2117 18.5651 11.0858 20.3284C11.0644 20.6354 11.0304 20.9424 11.0228 21.2494C11.0115 21.6092 11.1613 21.7811 11.5266 21.8143C11.9976 21.8573 12.4711 21.8708 12.9421 21.9088C13.0309 21.9201 13.121 21.9003 13.1962 21.8528C13.2714 21.8053 13.3268 21.7334 13.3527 21.6497C13.3996 21.5394 13.4252 21.4216 13.4282 21.3022C13.4295 20.8258 13.4207 20.3493 13.4081 19.8741C13.393 19.3264 13.3917 18.7763 13.3438 18.231C13.2857 17.5839 13.266 16.934 13.2847 16.2847C13.2847 16.2466 13.291 16.2073 13.2985 16.1312C13.3338 16.2024 13.3514 16.2356 13.3665 16.2712C13.9017 17.5228 14.3617 18.8037 14.7443 20.1074C14.7928 20.2421 14.7928 20.3889 14.7443 20.5237C14.6322 20.8196 14.7141 21.037 14.9659 21.1377C15.4445 21.3268 15.9331 21.4926 16.4155 21.6731C16.4865 21.7033 16.566 21.7091 16.6408 21.6895C16.7157 21.6698 16.7815 21.6259 16.8273 21.565C16.9085 21.4643 16.9743 21.3526 17.0225 21.2335C17.0537 21.1374 17.0798 21.0399 17.1006 20.9412C17.3185 20.2425 17.5653 19.5499 17.7517 18.8438C17.9785 17.9723 18.2624 17.1158 18.6018 16.2798C18.6201 16.2439 18.6411 16.2094 18.6647 16.1766C18.6761 16.2319 18.6761 16.254 18.6761 16.2761C18.6345 17.59 18.5955 18.8978 18.5501 20.2056C18.5363 20.5949 18.491 20.9829 18.4809 21.3722C18.4721 21.705 18.6207 21.8708 18.9557 21.9002C19.4355 21.9432 19.9191 21.9592 20.4002 21.9973C20.4888 22.0079 20.5784 21.9875 20.653 21.9399C20.7277 21.8922 20.7827 21.8203 20.8082 21.7369C20.8531 21.6305 20.8766 21.5167 20.8775 21.4017C20.88 20.7668 20.8674 20.132 20.8674 19.4971C20.8662 19.2846 20.8687 19.0722 20.8523 18.8622C20.8158 18.3968 20.7264 17.9314 20.7339 17.4685C20.7515 16.2122 20.8044 14.9572 20.8599 13.7022Z" fill="white"></path></g></svg><svg hidden="true" class="imt-manga-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#60BB4C"></circle><path d="M1.40869 5.87858L2.24161 5.18962L4.15357 6.64214C4.15357 6.64214 6.33559 4.15566 9.0067 2.48145L9.32553 2.87514C9.32553 2.87514 6.28678 5.55844 4.71748 9.07881L1.40869 5.87858Z" fill="#EFF8ED"></path></svg></div><svg class="imt-float-ball-loading" hidden="true" width="19" height="19" viewBox="0 0 19 19" fill="none" xmlns="http://www.w3.org/2000/svg" style="margin: 9px;"><path d="M9.42859 0C9.84288 0 10.1929 0.387143 10.1929 0.847143V3.99429C10.1929 4.45429 9.84431 4.84143 9.42859 4.84143C9.01431 4.84143 8.66431 4.45571 8.66431 3.99429V0.847143C8.66431 0.387143 9.01288 0 9.42859 0Z" fill="#E9E9E9"></path><path d="M14.1301 1.38877C14.5158 1.62591 14.6301 2.12163 14.4258 2.52305L12.9515 5.19448C12.901 5.28714 12.8325 5.36876 12.75 5.43455C12.6675 5.50035 12.5727 5.54898 12.4712 5.5776C12.3696 5.60621 12.2634 5.61424 12.1586 5.60119C12.0539 5.58814 11.9529 5.55429 11.8615 5.50163C11.6787 5.38432 11.5468 5.20237 11.4923 4.9921C11.4377 4.78184 11.4645 4.55874 11.5672 4.36734L13.0415 1.69591C13.2686 1.29448 13.7443 1.15305 14.1301 1.38877Z" fill="#989697"></path><path d="M17.4685 4.75707C17.5813 4.95451 17.6123 5.18824 17.5549 5.40825C17.4975 5.62826 17.3563 5.81705 17.1614 5.93422L14.4971 7.52564C14.0971 7.76993 13.6014 7.62422 13.3657 7.20707C13.2532 7.00994 13.2222 6.77667 13.2793 6.55702C13.3365 6.33737 13.4771 6.14874 13.6714 6.03136L16.3357 4.43993C16.7371 4.21993 17.2557 4.34136 17.4685 4.7585V4.75707Z" fill="#9B999A"></path><path d="M18.8572 9.42835C18.8572 9.84263 18.47 10.1926 18.01 10.1926H14.8629C14.4029 10.1926 14.0157 9.84406 14.0157 9.42835C14.0157 9.01406 14.4029 8.66406 14.8629 8.66406H18.01C18.47 8.66406 18.8572 9.01263 18.8572 9.42835Z" fill="#A3A1A2"></path><path d="M17.4686 14.1303C17.3515 14.3134 17.1697 14.4455 16.9594 14.5003C16.7491 14.5552 16.5259 14.5286 16.3343 14.426L13.6629 12.9517C13.5702 12.9012 13.4886 12.8327 13.4228 12.7503C13.357 12.6678 13.3084 12.573 13.2798 12.4714C13.2512 12.3698 13.2431 12.2636 13.2562 12.1589C13.2692 12.0542 13.3031 11.9532 13.3558 11.8617C13.4731 11.6789 13.655 11.547 13.8653 11.4925C14.0755 11.4379 14.2986 11.4647 14.49 11.5674L17.1615 13.0417C17.5629 13.2689 17.7043 13.7446 17.4686 14.1303Z" fill="#ABA9AA"></path><path opacity="0.7" d="M14.1 17.4686C13.9026 17.5814 13.6689 17.6124 13.4489 17.555C13.2288 17.4976 13.04 17.3564 12.9229 17.1615L11.3315 14.4972C11.0872 14.0972 11.2329 13.6015 11.65 13.3658C11.8472 13.2533 12.0804 13.2224 12.3001 13.2795C12.5197 13.3366 12.7084 13.4773 12.8257 13.6715L14.4172 16.3358C14.6372 16.7372 14.5157 17.2558 14.0986 17.4686H14.1Z" fill="#B2B2B2"></path><path opacity="0.6" d="M9.42859 18.8571C9.01431 18.8571 8.66431 18.4699 8.66431 18.0099V14.8628C8.66431 14.4028 9.01288 14.0156 9.42859 14.0156C9.84288 14.0156 10.1929 14.4028 10.1929 14.8628V18.0099C10.1929 18.4699 9.84431 18.8571 9.42859 18.8571Z" fill="#BAB8B9"></path><path opacity="0.5" d="M4.72717 17.4685C4.5441 17.3514 4.41195 17.1696 4.35713 16.9593C4.30231 16.749 4.32885 16.5258 4.43145 16.3342L5.90574 13.6628C5.95622 13.5701 6.02472 13.4885 6.1072 13.4227C6.18969 13.3569 6.2845 13.3083 6.38606 13.2797C6.48762 13.251 6.59387 13.243 6.69857 13.2561C6.80327 13.2691 6.90431 13.303 6.99574 13.3556C7.38145 13.5914 7.49431 14.0885 7.29002 14.4899L5.81574 17.1614C5.5886 17.5628 5.11288 17.7042 4.72717 17.4685Z" fill="#C2C0C1"></path><path opacity="0.4" d="M1.38862 14.1002C1.27584 13.9027 1.24483 13.669 1.30223 13.449C1.35964 13.229 1.50089 13.0402 1.69576 12.923L4.36004 11.3316C4.76004 11.0873 5.25576 11.233 5.49147 11.6502C5.60393 11.8473 5.63491 12.0806 5.5778 12.3002C5.52069 12.5199 5.38 12.7085 5.18576 12.8259L2.52004 14.4173C2.12004 14.6373 1.60004 14.5159 1.38862 14.0987V14.1002Z" fill="#CBCBCB"></path><path d="M0 9.42835C0 9.01406 0.387143 8.66406 0.847143 8.66406H3.99429C4.45429 8.66406 4.84143 9.01263 4.84143 9.42835C4.84143 9.84263 4.45571 10.1926 3.99429 10.1926H0.847143C0.387143 10.1926 0 9.84406 0 9.42835Z" fill="#D2D2D2"></path><path opacity="0.2" d="M1.38852 4.72705C1.50561 4.54398 1.68746 4.41183 1.89774 4.35701C2.10803 4.30219 2.33125 4.32873 2.52281 4.43133L5.19424 5.90562C5.28689 5.9561 5.36851 6.0246 5.43431 6.10708C5.5001 6.18957 5.54874 6.28438 5.57735 6.38594C5.60597 6.48749 5.61399 6.59375 5.60094 6.69845C5.5879 6.80315 5.55405 6.90419 5.50138 6.99562C5.38407 7.17844 5.20212 7.31029 4.99186 7.36484C4.78159 7.4194 4.55849 7.39263 4.3671 7.2899L1.69567 5.81562C1.29424 5.58847 1.15281 5.11276 1.38852 4.72705Z" fill="#DADADA"></path><path d="M4.75719 1.38849C4.95463 1.27571 5.18837 1.24471 5.40838 1.30211C5.62838 1.35952 5.81718 1.50077 5.93434 1.69564L7.52577 4.35992C7.77005 4.75992 7.62434 5.25564 7.20719 5.49135C7.01006 5.60381 6.77679 5.63479 6.55714 5.57768C6.33749 5.52056 6.14886 5.37988 6.03148 5.18564L4.44005 2.51992C4.22005 2.11992 4.34148 1.59992 4.75862 1.38849H4.75719Z" fill="#E2E2E2"></path></svg></div></div><div class=" " style="position: relative; pointer-events: all; display: inline-block;"><div><div style="display: flex; align-items: center; flex-direction: row;"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: block; opacity: 0;"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg><div class="imt-fb-btn  right btn-animate " dir="ltr" style="opacity: 0.7; transform: translateX(15px);"><div><svg class="imt-fb-logo-img imt-fb-logo-img-big-bg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path fill="none" d="M0 0h24v24H0z"></path><path d="M5 15v2a2 2 0 0 0 1.85 1.995L7 19h3v2H7a4 4 0 0 1-4-4v-2h2zm13-5l4.4 11h-2.155l-1.201-3h-4.09l-1.199 3h-2.154L16 10h2zm-1 2.885L15.753 16h2.492L17 12.885zM8 2v2h4v7H8v3H6v-3H2V4h4V2h2zm9 1a4 4 0 0 1 4 4v2h-2V7a2 2 0 0 0-2-2h-3V3h3zM6 6H4v3h2V6zm4 0H8v3h2V6z" fill="rgba(255,255,255,1)"></path></svg><svg class="imt-float-ball-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#60BB4C"></circle><path d="M1.40869 5.87858L2.24161 5.18962L4.15357 6.64214C4.15357 6.64214 6.33559 4.15566 9.0067 2.48145L9.32553 2.87514C9.32553 2.87514 6.28678 5.55844 4.71748 9.07881L1.40869 5.87858Z" fill="#EFF8ED"></path></svg></div></div><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: none; opacity: 0;"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div></div></div><div style="position: relative; width: 100%; opacity: 0;"><div title="关闭悬浮球" class="imt-fb-close-button" style="transform: translateX(100%);"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div></div><div class="imt-fb-more-buttons btn-animate" style="margin-top: 10px; transform: translateX(60px);"><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-more-button"><svg class="imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg" style="width: 22px; height: 22px;"><path d="M16 7.66699H10.375" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><path d="M11.625 14.333L6 14.333" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><path d="M14.125 16C15.1605 16 16 15.1605 16 14.125C16 13.0895 15.1605 12.25 14.125 12.25C13.0895 12.25 12.25 13.0895 12.25 14.125C12.25 15.1605 13.0895 16 14.125 16Z" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><path d="M7.875 9.75C8.91053 9.75 9.75 8.91053 9.75 7.875C9.75 6.83947 8.91053 6 7.875 6C6.83947 6 6 6.83947 6 7.875C6 8.91053 6.83947 9.75 7.875 9.75Z" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><rect x="3" y="3" width="16" height="16" rx="1.66667" stroke="currentColor" stroke-width="1.4"></rect></svg></div></div></div><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-more-button"><svg class="imt-fb-feedback imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.0003 14.2749C11.213 14.2749 11.3895 14.2047 11.5299 14.0643C11.6705 13.9239 11.7408 13.7473 11.7408 13.5345C11.7408 13.3218 11.6705 13.1453 11.5299 13.0049C11.3895 12.8645 11.213 12.7943 11.0003 12.7943C10.7877 12.7943 10.6111 12.8645 10.4707 13.0049C10.3302 13.1453 10.2599 13.3218 10.2599 13.5345C10.2599 13.7473 10.3302 13.9239 10.4707 14.0643C10.6111 14.2047 10.7877 14.2749 11.0003 14.2749ZM11.0003 11.0842C11.1954 11.0842 11.3587 11.0185 11.4903 10.8869C11.622 10.7552 11.6878 10.5918 11.6878 10.3967V6.23645C11.6878 6.04135 11.622 5.87803 11.4903 5.74649C11.3587 5.6148 11.1954 5.54895 11.0003 5.54895C10.8052 5.54895 10.6419 5.6148 10.5104 5.74649C10.3787 5.87803 10.3128 6.04135 10.3128 6.23645V10.3967C10.3128 10.5918 10.3787 10.7552 10.5104 10.8869C10.6419 11.0185 10.8052 11.0842 11.0003 11.0842ZM5.53562 16.8311L3.70045 18.666C3.43966 18.9269 3.13968 18.9861 2.80051 18.8434C2.4615 18.7005 2.29199 18.4434 2.29199 18.072V4.73816C2.29199 4.27509 2.45241 3.88314 2.77324 3.5623C3.09408 3.24147 3.48603 3.08105 3.9491 3.08105H18.0516C18.5146 3.08105 18.9066 3.24147 19.2274 3.5623C19.5482 3.88314 19.7087 4.27509 19.7087 4.73816V15.174C19.7087 15.637 19.5482 16.029 19.2274 16.3498C18.9066 16.6706 18.5146 16.8311 18.0516 16.8311H5.53562ZM4.95033 15.4561H18.0516C18.1221 15.4561 18.1868 15.4266 18.2454 15.3678C18.3042 15.3092 18.3337 15.2445 18.3337 15.174V4.73816C18.3337 4.66758 18.3042 4.60295 18.2454 4.54428C18.1868 4.48546 18.1221 4.45605 18.0516 4.45605H3.9491C3.87851 4.45605 3.81389 4.48546 3.75522 4.54428C3.6964 4.60295 3.66699 4.66758 3.66699 4.73816V16.7254L4.95033 15.4561Z" fill="currentColor"></path></svg></div></div></div></div><div hidden="" id="immersive-translate-popup-overlay" class="immersive-translate-popup-overlay"><div class="immersive-translate-popup-wrapper" style="position: fixed; top: 237px; right: 65px;"></div></div></div></div></template></div><div id="immersive-translate-selection-translation-button" style="all: initial"><template shadowrootmode="open"><div class="imt-selection-translation-button mini-btn" style="top: 521.562px; left: 1215.24px; display: block;"></div><style>

  .imt-selection-translation-button {
    cursor: pointer;
    position: absolute;
    height: 100%;
    z-index: 2147483647;
    transform: translate(-50%, -50%);
  }

  .imt-selection-translation-button.mini-btn {
    width: 10px;
    height: 10px;
    border-radius: 50%;
    border: 1px solid #FFF;
    background: #EA4C89;
  }

  .imt-selection-translation-button.mini-btn:hover {
    scale: 1.1;
  }

  .imt-selection-translation-button.icon-btn {
    width: 24px;
    height: 24px;
  }

  .imt-selection-translation-button .logo {
    width: 24px;
    height: 24px;
  }
</style></template></div></html>