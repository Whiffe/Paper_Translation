<!DOCTYPE html>
<!-- saved from url=(0073)https://arxiv.org/html/2601.02671v1?_immersive_translate_auto_translate=1 -->
<html lang="en" data-theme="dark" imt-state="dual" imt-trans-position="after"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>从生产语言模型中提取书籍 --- Extracting books from production language models</title>
<!--Generated on Thu Jan  8 15:33:06 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/addons_new.js"></script>
<script src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/feedbackOverlay.js"></script>
<!--<base href="/html/2601.02671v1/">--><base href="."><link rel="stylesheet" href="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/utz6mli.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"><style data-id="immersive-translate-input-injected-css">.immersive-translate-input {
  position: absolute;
  top: 0;
  right: 0;
  left: 0;
  bottom: 0;
  z-index: 2147483647;
  display: flex;
  justify-content: center;
  align-items: center;
}
.immersive-translate-attach-loading::after {
  content: " ";

  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;

  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-2000%, -50%);
  z-index: 100;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.immersive-translate-input-loading {
  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;
}

@keyframes immersiveTranslateShadowRolling {
  0% {
    box-shadow: 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  12% {
    box-shadow: 100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  25% {
    box-shadow: 110px 0 var(--loading-color), 100px 0 var(--loading-color),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  36% {
    box-shadow: 120px 0 var(--loading-color), 110px 0 var(--loading-color),
      100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0);
  }

  50% {
    box-shadow: 130px 0 var(--loading-color), 120px 0 var(--loading-color),
      110px 0 var(--loading-color), 100px 0 var(--loading-color);
  }

  62% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color),
      120px 0 var(--loading-color), 110px 0 var(--loading-color);
  }

  75% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      130px 0 var(--loading-color), 120px 0 var(--loading-color);
  }

  87% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color);
  }

  100% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0);
  }
}

.immersive-translate-modal {
  position: fixed;
  z-index: 2147483647;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgb(0, 0, 0);
  background-color: rgba(0, 0, 0, 0.4);
  font-size: 15px;
}

.immersive-translate-modal-content {
  background-color: #fefefe;
  margin: 10% auto;
  padding: 40px 24px 24px;
  border-radius: 12px;
  width: 350px;
  font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  position: relative;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-content {
    margin: 25% auto !important;
  }
}

@media screen and (max-width: 480px) {
  .immersive-translate-modal-content {
    width: 80vw !important;
    margin: 20vh auto !important;
    padding: 20px 12px 12px !important;
  }

  .immersive-translate-modal-title {
    font-size: 14px !important;
  }

  .immersive-translate-modal-body {
    font-size: 13px !important;
    max-height: 60vh !important;
  }

  .immersive-translate-btn {
    font-size: 13px !important;
    padding: 8px 16px !important;
    margin: 0 4px !important;
  }

  .immersive-translate-modal-footer {
    gap: 6px !important;
    margin-top: 16px !important;
  }
}

.immersive-translate-modal .immersive-translate-modal-content-in-input {
  max-width: 500px;
}
.immersive-translate-modal-content-in-input .immersive-translate-modal-body {
  text-align: left;
  max-height: unset;
}

.immersive-translate-modal-title {
  text-align: center;
  font-size: 16px;
  font-weight: 700;
  color: #333333;
}

.immersive-translate-modal-body {
  text-align: center;
  font-size: 14px;
  font-weight: 400;
  color: #333333;
  margin-top: 24px;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-body {
    max-height: 250px;
    overflow-y: auto;
  }
}

.immersive-translate-close {
  color: #666666;
  position: absolute;
  right: 16px;
  top: 16px;
  font-size: 20px;
  font-weight: bold;
}

.immersive-translate-close:hover,
.immersive-translate-close:focus {
  text-decoration: none;
  cursor: pointer;
}

.immersive-translate-modal-footer {
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  margin-top: 24px;
}

.immersive-translate-btn {
  width: fit-content;
  color: #fff;
  background-color: #ea4c89;
  border: none;
  font-size: 14px;
  margin: 0 8px;
  padding: 9px 30px;
  border-radius: 5px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: background-color 0.3s ease;
}

.immersive-translate-btn-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  gap: 8px;
}

.immersive-translate-btn:hover {
  background-color: #f082ac;
}
.immersive-translate-btn:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}
.immersive-translate-btn:disabled:hover {
  background-color: #ea4c89;
}

.immersive-translate-link-btn {
  background-color: transparent;
  color: #ea4c89;
  border: none;
  cursor: pointer;
  height: 30px;
  line-height: 30px;
}

.immersive-translate-cancel-btn {
  /* gray color */
  background-color: rgb(89, 107, 120);
}

.immersive-translate-cancel-btn:hover {
  background-color: hsl(205, 20%, 32%);
}

.immersive-translate-action-btn {
  background-color: transparent;
  color: #ea4c89;
  border: 1px solid #ea4c89;
}

.immersive-translate-btn svg {
  margin-right: 5px;
}

.immersive-translate-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-primary-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-modal input[type="radio"] {
  margin: 0 6px;
  cursor: pointer;
}

.immersive-translate-modal label {
  cursor: pointer;
}

.immersive-translate-close-action {
  position: absolute;
  top: 2px;
  right: 0px;
  cursor: pointer;
}

.imt-image-status {
  background-color: rgba(0, 0, 0, 0.5) !important;
  display: flex !important;
  flex-direction: column !important;
  align-items: center !important;
  justify-content: center !important;
  border-radius: 16px !important;
}
.imt-image-status img,
.imt-image-status svg,
.imt-img-loading {
  width: 28px !important;
  height: 28px !important;
  margin: 0 0 8px 0 !important;
  min-height: 28px !important;
  min-width: 28px !important;
  position: relative !important;
}
.imt-img-loading {
  background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADgAAAA4CAMAAACfWMssAAAAtFBMVEUAAAD////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////oK74hAAAAPHRSTlMABBMIDyQXHwyBfFdDMSw+OjXCb+5RG51IvV/k0rOqlGRM6KKMhdvNyZBz9MaupmxpWyj437iYd/yJVNZeuUC7AAACt0lEQVRIx53T2XKiUBCA4QYOiyCbiAsuuGBcYtxiYtT3f6/pbqoYHVFO5r+iivpo6DpAWYpqeoFfr9f90DsYAuRSWkFnPO50OgR9PwiCUFcl2GEcx+N/YBh6pvKaefHlUgZd1zVe0NbYcQjGBfzrPE8Xz8aF+71D8gG6DHFPpc4a7xFiCDuhaWgKgGIJQ3d5IMGDrpS4S5KgpIm+en9f6PlAhKby4JwEIxlYJV9h5k5nee9GoxHJ2IDSNB0dwdad1NAxDJ/uXDHYmebdk4PdbkS58CIVHdYSUHTYYRWOJblWSyu2lmy3KNFVJNBhxcuGW4YBVCbYGRZwIooipHsNqjM4FbgOQqQqSKQQU9V8xmi1QlgHqQQ6DDBvRUVCDirs+EzGDGOQTCATgtYTnbCVLgsVgRE0T1QE0qHCFAht2z6dLvJQs3Lo2FQoDxWNUiBhaP4eRgwNkI+dAjVOA/kUrIDwf3CG8NfNOE0eiFotSuo+rBiq8tD9oY4Qzc6YJw99hl1wzpQvD7ef2M8QgnOGJfJw+EltQc+oX2yn907QB22WZcvlUpd143dqQu+8pCJZuGE4xCuPXJqqcs5sNpsI93Rmzym1k4Npk+oD1SH3/a3LOK/JpUBpWfqNySxWzCfNCUITuDG5dtuphrUJ1myeIE9bIsPiKrfqTai5WZxbhtNphYx6GEIHihyGFTI69lje/rxajdh0s0msZ0zYxyPLhYCb1CyHm9Qsd2H37Y3lugVwL9kNh8Ot8cha6fUNQ8nuXi5z9/ExsAO4zQrb/ev1yrCB7lGyQzgYDGuxq1toDN/JGvN+HyWNHKB7zEoK+PX11e12G431erGYzwmytAWU56fkMHY5JJnDRR2eZji3AwtIcrEV8Cojat/BdQ7XOwGV1e1hDjGGjXbdArm8uJZtCH5MbcctVX8A1WpqumJHwckAAAAASUVORK5CYII=");
  background-size: 28px 28px;
  animation: image-loading-rotate 1s linear infinite !important;
}

.imt-image-status span {
  color: var(--bg-2, #fff) !important;
  font-size: 14px !important;
  line-height: 14px !important;
  font-weight: 500 !important;
  font-family: "PingFang SC", Arial, sans-serif !important;
}

.imt-primary-button {
  display: flex;
  padding: 12px 80px;
  justify-content: center;
  align-items: center;
  gap: 8px;
  border-radius: 8px;
  background: #ea4c89;
  color: #fff;
  font-size: 16px;
  font-style: normal;
  font-weight: 700;
  line-height: 24px;
  border: none;
  cursor: pointer;
}

.imt-retry-text {
  color: #999;
  text-align: center;
  font-size: 14px;
  font-style: normal;
  font-weight: 400;
  line-height: 21px;
  cursor: pointer;
}

.imt-action-container {
  display: flex;
  flex-direction: column;
  gap: 12px;
}

.imt-modal-content-text {
  text-align: left;
  color: #333;
  font-size: 16px;
  font-weight: 400;
  line-height: 24px;
}

@keyframes image-loading-rotate {
  from {
    transform: rotate(360deg);
  }
  to {
    transform: rotate(0deg);
  }
}

.imt-linear-gradient-text {
  background: linear-gradient(90deg, #00a6ff 0%, #c369ff 52.4%, #ff4590 100%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
}

.imt-flex-center {
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-linear-black-btn {
  border-radius: 50px;
  background: linear-gradient(66deg, #222 19%, #696969 94.25%);
  height: 48px;
  width: 100%;
  color: #fff;
  font-size: 16px;
  font-weight: 700;
  display: flex;
  align-items: center;
  cursor: pointer;
  justify-content: center;
}
</style><style data-id="immersive-translate-default-injected-css">:root {
  --immersive-translate-theme-underline-borderColor: #72ece9;
  --immersive-translate-theme-nativeUnderline-borderColor: #72ece9;
  --immersive-translate-theme-nativeDashed-borderColor: #72ece9;
  --immersive-translate-theme-nativeDotted-borderColor: #72ece9;
  --immersive-translate-theme-highlight-backgroundColor: #ffff00;
  --immersive-translate-theme-dashed-borderColor: #59c1bd;
  --immersive-translate-theme-blockquote-borderColor: #cc3355;
  --immersive-translate-theme-thinDashed-borderColor: #ff374f;
  --immersive-translate-theme-dashedBorder-borderColor: #94a3b8;
  --immersive-translate-theme-dashedBorder-borderRadius: 0;
  --immersive-translate-theme-solidBorder-borderColor: #94a3b8;
  --immersive-translate-theme-solidBorder-borderRadius: 0;
  --immersive-translate-theme-dotted-borderColor: #94a3b8;
  --immersive-translate-theme-wavy-borderColor: #72ece9;
  --immersive-translate-theme-dividingLine-borderColor: #94a3b8;
  --immersive-translate-theme-grey-textColor: #2f4f4f;
  --immersive-translate-theme-marker-backgroundColor: #fbda41;
  --immersive-translate-theme-marker-backgroundColor-rgb: 251, 218, 65;
  --immersive-translate-theme-marker2-backgroundColor: #ffff00;
  --immersive-translate-theme-background-backgroundColor: #dbafaf;
  --immersive-translate-theme-background-backgroundColor-rgb: 219, 175, 175;
  --immersive-translate-theme-background-backgroundOpacity: 12;
  --immersive-translate-theme-opacity-opacity: 10;
}

[imt-state="dual"] .immersive-translate-target-translation-pre-whitespace {
  white-space: pre-wrap !important;
}

[imt-state="dual"] .immersive-translate-pdf-target-container {
  position: absolute;
  background-color: #fff;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica,
    sans-serif;
  top: 0;
  width: 600px;
  height: 100%;
  z-index: 2;
  line-height: 1.3;
  font-size: 16px;
}
[imt-state="dual"] .immersive-translate-target-wrapper[dir="rtl"] {
  text-align: right;
}

[imt-state="dual"]
  .immersive-translate-pdf-target-container
  .immersive-translate-target-wrapper {
  color: rgb(0, 0, 0);
  white-space: normal;
  position: absolute;
}

[imt-state="dual"]
  .immersive-translate-pdf-target-container
  .immersive-translate-target-wrapper
  font {
  color: inherit;
  white-space: inherit;
  position: unset;
}

[imt-state="translation"] .immersive-translate-target-wrapper > br {
  display: none;
}

[imt-state="translation"]
  .immersive-translate-target-translation-block-wrapper {
  margin: 0 !important;
}

[imt-state="dual"] .immersive-translate-target-translation-block-wrapper {
  margin: 8px 0 !important;
  display: inline-block;
}

[imt-trans-position="before"]
  .immersive-translate-target-translation-block-wrapper {
  display: block;
}

[imt-trans-position="before"]
  .immersive-translate-target-translation-block-wrapper {
  margin-top: 0 !important;
}

[imt-state="dual"] .immersive-translate-target-translation-pdf-block-wrapper {
  margin: 0 !important;
  display: inline-block;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-grey-inner {
  color: var(--immersive-translate-theme-grey-textColor);
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-underline-inner {
  border-bottom: 1px solid
    var(--immersive-translate-theme-underline-borderColor) !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-nativeUnderline-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-nativeUnderline-borderColor
  ) !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-dashedBorder {
  border: 1px dashed var(--immersive-translate-theme-dashedBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-dashedBorder-borderRadius
  ) !important;
  padding: 6px;
  margin-top: 2px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-inline-wrapper-theme-dashedBorder {
  border: 1px dashed var(--immersive-translate-theme-dashedBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-dashedBorder-borderRadius
  ) !important;
  padding: 2px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-solidBorder {
  border: 1px solid var(--immersive-translate-theme-solidBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-solidBorder-borderRadius
  ) !important;
  padding: 6px;
  margin-top: 2px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-inline-wrapper-theme-solidBorder {
  border: 1px solid var(--immersive-translate-theme-solidBorder-borderColor) !important;
  border-radius: var(
    --immersive-translate-theme-solidBorder-borderRadius
  ) !important;
  padding: 2px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-nativeDashed-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-nativeDashed-borderColor
  ) !important;
  text-decoration-style: dashed !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-thinDashed-inner {
  border-bottom: 1px dashed
    var(--immersive-translate-theme-thinDashed-borderColor) !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-dotted-inner {
  background-image: linear-gradient(
    to right,
    var(--immersive-translate-theme-dotted-borderColor) 30%,
    rgba(255, 255, 255, 0) 0%
  );
  background-position: bottom;
  background-size: 5px 1px;
  background-repeat: repeat-x;
  padding-bottom: 3px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-nativeDotted-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-nativeDotted-borderColor
  ) !important;
  text-decoration-style: dotted !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-wavy-inner {
  text-decoration: underline !important;
  text-decoration-color: var(
    --immersive-translate-theme-wavy-borderColor
  ) !important;
  text-decoration-style: wavy !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-dashed-inner {
  background: linear-gradient(
      to right,
      var(--immersive-translate-theme-dashed-borderColor) 0%,
      var(--immersive-translate-theme-dashed-borderColor) 50%,
      transparent 50%,
      transparent 100%
    )
    repeat-x left bottom;
  background-size: 8px 2px;
  padding-bottom: 2px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-dividingLine::before {
  content: "";
  display: block;
  max-width: 80px;
  width: 10%;
  border-top: 1px dashed
    var(--immersive-translate-theme-dividingLine-borderColor);
  padding-top: 8px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-inline-wrapper-theme-dividingLine::before {
  content: "";
  border-left: 1px dashed
    var(--immersive-translate-theme-dividingLine-borderColor);
  max-height: 16px;
  height: 16px;
  padding-left: 8px;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-highlight-inner {
  background: var(--immersive-translate-theme-highlight-backgroundColor);
  box-decoration-break: clone;
  -webkit-box-decoration-break: clone;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-marker {
  line-height: 1.5em;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-marker2-inner {
  font-weight: bold;
  text-shadow: 10px 0px 3px
      var(--immersive-translate-theme-marker2-backgroundColor),
    16px 3px 9px var(--immersive-translate-theme-marker2-backgroundColor),
    2px 0px 6px var(--immersive-translate-theme-marker2-backgroundColor),
    -12px 0px 12px var(--immersive-translate-theme-marker2-backgroundColor) !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-marker-inner {
  /* TODO: add more texture */
  background: linear-gradient(
    to right,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.1),
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.9) 3%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.9) 35%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.9) 70%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.8) 95%,
    rgba(var(--immersive-translate-theme-marker-backgroundColor-rgb), 0.3)
  );
  box-decoration-break: clone;
  -webkit-box-decoration-break: clone;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-weakening {
  opacity: 0.618 !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-italic {
  font-style: italic !important;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-bold {
  font-weight: bold !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-paper {
  margin: 8px 0;
  box-shadow: rgba(0, 0, 0, 0.24) 0px 3px 8px;
  padding: 16px 32px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-blockquote {
  border-left: 4px solid var(--immersive-translate-theme-blockquote-borderColor) !important;
  padding-left: 12px !important;
  margin-top: 4px;
  margin-bottom: 4px;
  padding-top: 4px;
  padding-bottom: 4px;
  display: inline-block;
}

[imt-state="dual"] .immersive-translate-target-translation-theme-mask-inner {
  filter: blur(5px) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

[data-immersive-translate-root-translation-theme="none"]
  .immersive-translate-target-translation-theme-mask-inner {
  filter: none !important;
}

[data-immersive-translate-root-translation-theme="mask"]
  .immersive-translate-target-inner {
  filter: blur(5px) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

/* opacity theme start */

[imt-state="dual"] .immersive-translate-target-translation-theme-opacity-inner {
  filter: opacity(
    calc(var(--immersive-translate-theme-opacity-opacity) * 1%)
  ) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

[data-immersive-translate-root-translation-theme="none"]
  .immersive-translate-target-translation-theme-opacity-inner {
  filter: none !important;
}
[data-immersive-translate-root-translation-theme="opacity"]
  .immersive-translate-target-inner,
[imt-state="dual"]
  .immersive-translate-target-translation-theme-opacity-inner:hover {
  filter: opacity(
    calc(var(--immersive-translate-theme-opacity-opacity) * 1%)
  ) !important;
  transition: filter 0.3s ease !important;
  border-radius: 10px;
  display: inline-block;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-opacity-inner:hover {
  filter: none !important;
}

[imt-state="dual"]
  .immersive-translate-target-translation-theme-mask-inner:hover {
  filter: none !important;
}
[data-immersive-translate-root-translation-theme="opacity"]
  .immersive-translate-target-inner:hover {
  filter: none !important;
}

[data-immersive-translate-root-translation-theme="mask"]
  .immersive-translate-target-inner:hover {
  filter: none !important;
}

/* opacity theme end */

/* background theme start */
[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper-theme-background {
  margin: 8px 0;
  background: rgba(
    var(--immersive-translate-theme-background-backgroundColor-rgb),
    calc(var(--immersive-translate-theme-background-backgroundOpacity) * 1%)
  );
  border-radius: 4px;
  box-shadow: unset !important;
  padding: 12px;
  display: inline-block;
}
[imt-state="dual"]
  .immersive-translate-target-translation-theme-background-inner {
  background: rgba(
    var(--immersive-translate-theme-background-backgroundColor-rgb),
    calc(var(--immersive-translate-theme-background-backgroundOpacity) * 1%)
  );
  padding-left: 6px;
  padding-right: 6px;
  box-decoration-break: clone;
  -webkit-box-decoration-break: clone;
}
[imt-state="dual"]
  .immersive-translate-target-translation-block-wrapper
  .immersive-translate-target-translation-theme-background-inner {
  background: unset;
  padding-left: unset;
  padding-right: unset;
}
/* background theme end */

/* vertical css , please remain it in the last one. */
.immersive-translate-target-translation-vertical-block-wrapper {
  margin: 0px 8px !important;
}

.immersive-translate-text {
  font-size: 15px !important;
}

.immersive-translate-error-toast {
  position: fixed;
  top: 5%;
  z-index: 99999999;
  left: 0;
  right: 0;
  margin: auto;
  max-width: 300px;
  padding: 16px;
  border-radius: 12px;
  background-color: rgba(0, 0, 0, 0.8);
  display: flex;
  flex-direction: row;
  justify-content: space-between;
}

@media all and (min-width: 750px) {
  .immersive-translate-error-toast {
    max-width: 400px;
  }
}

.immersive-translate-clickable-button {
  cursor: pointer;
}

.immersive-translate-help-button {
  cursor: pointer;
}

.immersive-translate-loading-text:before {
  content: "...";
}

/* dark mode for loading */

@media only screen and (prefers-color-scheme: dark) {
  .immersive-translate-loading {
    border: 2px rgba(255, 255, 255, 0.25) solid !important;
    border-top: 2px rgba(255, 255, 255, 1) solid !important;
  }
}

.immersive-translate-error-wrapper {
  position: relative;
  display: inline-flex;
  padding: 6px;
  margin: 0 12px;
  white-space: nowrap;
  font-size: 0.9em;
}
[lang="zh-CN"] .immersive-translate-error-wrapper {
  font-size: 0.75em;
}
[lang="zh-TW"] .immersive-translate-error-wrapper {
  font-size: 0.75em;
}

.immersive-translate-tooltip {
  position: relative;
  display: inline-flex;
  /* little indicater to indicate it's hoverable */
}

.immersive-translate-tooltip-content {
  /* here's the magic */
  position: absolute;
  z-index: 100000000000;

  left: 50%;
  bottom: 0;
  transform: translate(-50%, 110%);
  line-height: 1;
  /* and add a small left margin */

  /* basic styles */
  width: max-content;
  max-width: 250px;
  word-wrap: break-word;
  white-space: pre-line;
  padding: 10px;
  border-radius: 10px;
  background: #000c;
  color: #fff;
  text-align: center;
  font-size: 14px;
  display: none;
  /* hide by default */
}

.immersive-translate-tooltip:hover .immersive-translate-tooltip-content {
  display: inline-block;
}

.immersive-translate-tooltip:hover + .immersive-translate-tooltip-content {
  display: inline-block;
}

.immersive-translate-tooltip-content-table {
  left: unset !important;
  bottom: unset !important;
  transform: translate(-10%, 50%) !important;
}

.immersive-translate-tooltip:hover:before {
  display: inline-block;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.imt-image-status {
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  color: var(--bg-2, #fff);
  font-size: 14px;
}
</style><style data-id="immersive-translate-user-custom-style">:root {

.immersive-translate-target-inner { font-family: inherit; }


.immersive-translate-target-inner { font-family: inherit; }
}
</style><style data-id="immersive-translate-dynamic-injected-css">.immersive-translate-target-wrapper[dir='rtl'] {text-align: right;display:block!important;}
[dir='rtl'] .immersive-translate-target-wrapper:not([dir]) {text-align:left;direction:ltr;}
.immersive-translate-target-wrapper {word-break:break-word; user-select:text;}
[imt-state=dual] .immersive-translate-target-translation-block-wrapper-theme-dividingLine::before {display:block;}
[imt-trans-position=before] .immersive-translate-target-translation-block-wrapper {display:block!important;}
</style></head>
<body style=""><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2601.02671v1?_immersive_translate_auto_translate=1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main &gt;.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2601.02671v1/#myForm" onclick="event.preventDefault(); var modal = document.getElementById(&#39;myForm&#39;); modal.style.display = &#39;block&#39;; bugReportState.setInitiateWay(&#39;Header&#39;);">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2601.02671v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2601.02671v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1?_immersive_translate_auto_translate=1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1" title="In Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2" title="In Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3" title="In Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Extraction procedure</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1" title="In 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Attempting initial completion of a short ground-truth prefix (Phase 1)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="In 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Attempting long-form extraction of training data (Phase 2)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3" title="In 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Verifying extraction success</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1" title="In 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Identifying near-verbatim extracted text in a long-form generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2" title="In 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Claiming extraction success without information about training-data membership</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4" title="In Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1" title="In 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="In 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>High-level extraction outcomes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS3" title="In 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Additional details and experiments concerning LLM-specific configurations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5" title="In Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS1" title="In 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Limitations and caveats</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2" title="In 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Copyright</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S6" title="In Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A1" title="In Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>BoN perturbtations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A2" title="In Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Procedure for quantifying extraction success</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A2.SS0.SSS0.Px1" title="In Appendix B Procedure for quantifying extraction success ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title">Conservative estimate for extraction.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3" title="In Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Experimental setup</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1" title="In Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Book selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2" title="In Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Phase 2 generation configurations and stop conditions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS1" title="In C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.1 </span>Settings for main results</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS1.Px1" title="In C.2.1 Settings for main results ‣ C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title">Halting Phase 2.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS2" title="In C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.2 </span>Generation configuration exploration for Gemini 2.5 Pro</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS3" title="In C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.3 </span>Refusal retries for per-chapter experiments with GPT-4.1</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS3.Px1" title="In C.2.3 Refusal retries for per-chapter experiments with GPT-4.1 ‣ C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title">Chat UI</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS3" title="In Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Text normalization prior to gauging near-verbatim extraction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4" title="In Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Extended results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS1" title="In Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Additional Phase 1 results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2" title="In Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>Additional Phase 2 results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2.SSS1" title="In D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2.1 </span>Continuation loop API costs</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2.SSS1.Px1" title="In D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title">Claude 3.7 Sonnet.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2.SSS1.Px2" title="In D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title">Gemini 2.5 Pro sweeps.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2.SSS2" title="In D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2.2 </span>Plots and tables</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1?_immersive_translate_auto_translate=1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: arXiv.org perpetual non-exclusive license<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">许可证：arXiv.org 永久非排他性许可</font></font></font></a><div id="watermark-tr" data-imt_insert_failed="1">arXiv:2601.02671v1 [cs.CL] 06 Jan 2026</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Extracting books from production language models<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">从生产语言模型中提取书籍</font></font></font></h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id10.1.id1">Ahmed Ahmed </span><span class="ltx_text ltx_font_italic" id="id11.2.id2" style="font-size:90%;">ahmedah@cs.stanford.edu 
<br class="ltx_break">Stanford University
</span><span class="ltx_text ltx_font_bold" id="id12.3.id3">A. Feder Cooper<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex1.1.1.1">1</span></span></span></span></span> </span><span class="ltx_text ltx_font_italic" id="id13.4.id4" style="font-size:90%;">a.feder.cooper@yale.edu 
<br class="ltx_break">Stanford University and Yale University
</span><span class="ltx_text ltx_font_bold" id="id14.5.id5">Sanmi Koyejo  </span><span class="ltx_text ltx_font_italic" id="id15.6.id6" style="font-size:90%;">sanmi@cs.stanford.edu 
<br class="ltx_break">Stanford University
</span><span class="ltx_text ltx_font_bold" id="id16.7.id7">Percy Liang  </span><span class="ltx_text ltx_font_italic" id="id17.8.id8" style="font-size:90%;">pliang@cs.stanford.edu 
<br class="ltx_break">Stanford University
</span>
</span><span class="ltx_author_notes"><span class="ltx_text ltx_font_bold" id="id18.9.id1">Equal contribution; corresponding authors.</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">摘要</font></font></font></h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id8.8">Many unresolved legal questions over LLMs and copyright center on memorization:
whether specific training data have been encoded in the model’s weights during training, and whether those memorized data can be extracted in the model’s outputs.
While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models.
However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement.
We investigate this question using a two-phase procedure:
(1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-<math alttext="N" class="ltx_Math" display="inline" id="id1.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book.
We evaluate our procedure on four production LLMs—Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3—and
we measure extraction success with a score computed from a block-based approximation of longest common substring (<math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="id2.2.m2" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math>).
With different per-LLM experimental configurations, we were able to extract varying amounts of text.
For the Phase 1 probe, it was unnecessary to jailbreak
Gemini 2.5 Pro and Grok 3 to extract text (e.g, <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="id3.3.m3" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> of <math alttext="76.8\%" class="ltx_Math" display="inline" id="id4.4.m4" intent=":literal"><semantics><mrow><mn>76.8</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">76.8\%</annotation></semantics></math> and <math alttext="70.3\%" class="ltx_Math" display="inline" id="id5.5.m5" intent=":literal"><semantics><mrow><mn>70.3</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">70.3\%</annotation></semantics></math>, respectively, for <span class="ltx_text ltx_font_italic" id="id8.8.1">Harry Potter and the Sorcerer’s Stone</span>), while it was necessary for Claude 3.7 Sonnet and GPT-4.1.
In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., <math alttext="\mathsf{nv{\text{-}}recall}=95.8\%" class="ltx_Math" display="inline" id="id6.6.m6" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>95.8</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=95.8\%</annotation></semantics></math>).
GPT-4.1 requires significantly more BoN attempts (e.g., <math alttext="20\times" class="ltx_math_unparsed" display="inline" id="id7.7.m7" intent=":literal"><semantics><mrow><mn>20</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">20\times</annotation></semantics></math>), and eventually refuses to continue (e.g., <math alttext="\mathsf{nv{\text{-}}recall}=4.0\%" class="ltx_Math" display="inline" id="id8.8.m8" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>4.0</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=4.0\%</annotation></semantics></math>).
Taken together, our work highlights that, even with model- and system-level safeguards, extraction of (in-copyright) training data remains a risk for production LLMs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">许多关于 LLMs 和版权的未解决法律问题都围绕着记忆：在训练过程中，特定的训练数据是否被编码到模型的权重中，以及这些被记忆的数据是否可以从模型的输出中提取出来。虽然许多人认为 LLMs 并没有记住太多训练数据，但最近的研究表明，大量的受版权保护文本可以从开放权重模型中提取出来。然而，考虑到这些系统实施的安全措施，相似提取是否适用于生产 LLMs 仍然是一个悬而未决的问题。我们采用两阶段程序来研究这个问题：(1) 初步探测以测试提取的可行性，有时会使用 Best-of- <math intent=":literal" id="id1.1.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> （BoN）越狱，然后是(2)迭代续写提示以尝试提取书籍。我们在四个生产 LLMs——Claude 3.7 Sonnet、GPT-4.1、Gemini 2.5 Pro 和 Grok 3——上评估了我们的程序，并使用基于最长公共子串（ <math intent=":literal" id="id2.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> ）的块状近似计算出的分数来衡量提取的成功。通过不同的每个 LLM 实验配置，我们能够提取不同数量的文本。 对于第一阶段探测，无需越狱 Gemini 2.5 Pro 和 Grok 3 即可提取文本（例如，对于《哈利·波特与魔法石》，分别为 <math intent=":literal" id="id3.3.m3" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 和 <math intent=":literal" id="id4.4.m4" display="inline" class="ltx_Math" alttext="76.8\%"><semantics><mrow><mn>76.8</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">76.8\%</annotation></semantics></math> ），而越狱 Claude 3.7 Sonnet 和 GPT-4.1 则是必要的。在某些情况下，越狱的 Claude 3.7 Sonnet 会近乎逐字输出整本书（例如， <math intent=":literal" id="id6.6.m6" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=95.8\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>95.8</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=95.8\%</annotation></semantics></math> ）。GPT-4。1 需要 GPT-4.1 显著更多的 BoN 尝试（例如， <math intent=":literal" id="id7.7.m7" display="inline" class="ltx_math_unparsed" alttext="20\times"><semantics><mrow><mn>20</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">20\times</annotation></semantics></math> ），最终会拒绝继续（例如， <math intent=":literal" id="id8.8.m8" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=4.0\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>4.0</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=4.0\%</annotation></semantics></math> ）。综合来看，我们的工作表明，即使有模型和系统级别的安全措施，提取（受版权保护的）训练数据仍然是生产 LLMs 的风险。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id9.9"><span class="ltx_text ltx_font_bold" id="id9.9.1">Disclosure:</span> We ran experiments from mid-August to mid-September 2025, notified affected providers shortly after, and now make our findings public after a <math alttext="90" class="ltx_Math" display="inline" id="id9.9.m1" intent=":literal"><semantics><mn>90</mn><annotation encoding="application/x-tex">90</annotation></semantics></math>-day disclosure window.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">披露：我们于 2025 年 8 月中旬至 9 月中旬进行了实验，在受影响提供者附近立即通知后，经过 <math intent=":literal" id="id9.9.m1" display="inline" class="ltx_Math" alttext="90"><semantics><mn>90</mn><annotation encoding="application/x-tex">90</annotation></semantics></math> 天的披露窗口，现在公开我们的发现。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">1 引言</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Frontier, production large language models (hereafter <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">production LLMs</span>) are trained on enormous datasets drawn from various sources, including large-scale scrapes of the Internet&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Biderman<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib33" title="Pythia: A suite for analyzing large language models across training and scaling">2023</a>; Chen<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib60" title="Evaluating large language models trained on code">2021</a>; Touvron<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib55" title="LLaMA: Open and Efficient Foundation Language Models">2023</a>; Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib62" title="AI and Law: The Next Generation">2023a</a>)</cite>.
A large amount of data in these sources includes in-copyright expression, which has led to public debate about copyright infringement, creator consent, and more.
In their responses to copyright infringement claims, frontier companies argue that training on copyrighted material is both necessary to produce competitive models and fair use&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(King, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib102" title="Anthropic CEO Doubles Down on Fair Use Defense–“The Law Will Back Us Up”’">2024</a>; Belanger, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib103" title="OpenAI declares AI race “over”’ if training on copyrighted works isn’t fair use">2025</a>; Wiggers and Zeff, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib98" title="In AI copyright case, Zuckerberg turns to YouTube for his defense">2025</a>; Claburn, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib101" title="Microsoft CEO of AI: Your online content is ’freeware’ fodder for training models">2024</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib65" title="OpenAI and Journalism">2024a</a>; Berger, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib64" title="The ai copyright battle: why openai and google are pushing for fair use">2025</a>)</cite>.
<span class="ltx_text ltx_font_bold" id="S1.p1.1.2">Fair use</span> is a defense to copyright infringement, providing an exception to copyright owners’ exclusive rights over their works.
To support their fair use arguments, companies claim that training generative AI models is <span class="ltx_text ltx_font_bold" id="S1.p1.1.3">transformative</span>, meaning that the use of copyrighted material adds new meaning, purpose, or message to the original work&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib105" title="510 u.s. 569 (1994)">Campbell v. Acuff-Rose Music, </a>)</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">前沿生产大型语言模型（以下简称生产 LLMs）在训练时使用了来自各种来源的海量数据集，包括大规模的互联网抓取（Biderman 等人，2023 年；Chen 等人，2021 年；Touvron 等人，2023 年；Lee 等人，2023a）。这些来源中的大量数据包含受版权保护的表达，这引发了关于版权侵权、创作者同意等问题的公共辩论。在回应版权侵权指控时，前沿公司认为，在受版权保护的材料上进行训练对于生产具有竞争力的模型是必要的，并且属于合理使用（King，2024 年；Belanger，2025 年；Wiggers 和 Zeff，2025 年；Claburn，2024 年；OpenAI，2024a；Berger，2025 年）。合理使用是版权侵权的抗辩理由，为版权所有者对其作品的专有权利提供了例外。为了支持其合理使用的主张，公司声称，训练生成式 AI 模型具有转化性，这意味着使用受版权保护的材料为原始作品增添了新的意义、目的或信息（Campbell 诉 Acuff-Rose Music 案）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">But how LLMs make use of training data is not always transformative.
As <cite class="ltx_cite ltx_citemacro_citet">Lee<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib45" title="Talkin’ ’Bout AI Generation: Copyright and the Generative-AI Supply Chain">2023b</a>)</cite> note, “[w]hen a model memorizes a work and generates it verbatim as an output, there is no transformation in content.”<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In select circumstances, verbatim copying can be associated with a transformative use, e.g., in the case of parody&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib105" title="510 u.s. 569 (1994)">Campbell v. Acuff-Rose Music, </a>)</cite> or using copies to produce a new function, like a search index&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib106" title="804 f.3d 202 (2d cir. 2015)">Authors Guild v. Google, Inc., </a>)</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在某些特定情况下，逐字复制可能与转化性使用相关联，例如在讽刺作品（坎贝尔诉阿库夫-罗兹音乐公司案）或使用复制件来产生新功能（如搜索索引）（作家公会诉谷歌公司案）的案例中。</font></font></font></span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">但是，LLMs 如何利用训练数据并不总是具有转化性。正如李等人（2023b）所指出的，“当模型记住一部作品并以逐字的方式生成输出时，内容上并没有发生转化。” <sup class="ltx_note_mark">1</sup> </font></font></font>
In machine learning, <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">memorization</span> refers to whether specific training data have been encoded in a model’s weights during training, and often also refers to whether those data can be <span class="ltx_text ltx_font_bold" id="S1.p2.1.2">extracted</span> (near-)verbatim in that model’s outputs.
While LLMs can produce all sorts of novel outputs, they also memorize portions of their training data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Carlini<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib9" title="Extracting training data from large language models">2021</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib107" title="Quantifying Memorization Across Neural Language Models">2023</a>; Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib44" title="Deduplicating Training Data Makes Language Models Better">2022</a>; Nasr<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>; Hayes<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib41" title="Measuring memorization in language models via probabilistic extraction">2025b</a>)</cite> (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在机器学习中，记忆指的是特定训练数据是否在模型训练过程中被编码到模型的权重中，并且通常也指这些数据是否可以在该模型的输出中近乎逐字地提取出来。虽然 LLMs 可以产生各种新颖的输出，但它们也会记住其训练数据的一部分（卡林尼等人，2021；2023；李等人，2022；纳斯尔等人，2023；黑斯等人，2025b）（第 2 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="218" id="S1.F1.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/figure_1_harry_potter_1.png" width="359">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.21.9.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1.16.8" style="font-size:90%;">Extraction of <span class="ltx_text ltx_font_italic" id="S1.F1.16.8.9">Harry Potter and the Sorcerer’s Stone</span> for a single run.<span class="ltx_text ltx_font_medium" id="S1.F1.16.8.8">
We quantify the proportion of the ground-truth book that appears in a production LLM’s generated text using a block-based, greedy approximation of longest common substring (<math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S1.F1.9.1.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math>, Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E7" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a>).
This metric only counts sufficiently long, contiguous spans of near-verbatim text, for which we can conservatively claim extraction of training data (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3" title="3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
We extract nearly all of <span class="ltx_text ltx_font_italic" id="S1.F1.16.8.8.1">Harry Potter and the Sorcerer’s Stone</span> from jailbroken Claude 3.7 Sonnet (BoN <math alttext="N=258" class="ltx_Math" display="inline" id="S1.F1.10.2.2.m2" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>258</mn></mrow><annotation encoding="application/x-tex">N=258</annotation></semantics></math>, <math alttext="\mathsf{nv{\text{-}}recall}=95.8\%" class="ltx_Math" display="inline" id="S1.F1.11.3.3.m3" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>95.8</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=95.8\%</annotation></semantics></math>).
GPT-4.1 requires more jailbreaking attempts (<math alttext="N=5179" class="ltx_Math" display="inline" id="S1.F1.12.4.4.m4" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>5179</mn></mrow><annotation encoding="application/x-tex">N=5179</annotation></semantics></math>) and refuses to continue after reaching the end of the first chapter;
the generated text has <math alttext="\mathsf{nv{\text{-}}recall}=4.0\%" class="ltx_Math" display="inline" id="S1.F1.13.5.5.m5" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>4.0</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=4.0\%</annotation></semantics></math> with the full book.
We extract substantial proportions of the book from Gemini 2.5 Pro and Grok 3 (<math alttext="76.8\%" class="ltx_Math" display="inline" id="S1.F1.14.6.6.m6" intent=":literal"><semantics><mrow><mn>76.8</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">76.8\%</annotation></semantics></math> and <math alttext="70.3\%" class="ltx_Math" display="inline" id="S1.F1.15.7.7.m7" intent=":literal"><semantics><mrow><mn>70.3</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">70.3\%</annotation></semantics></math>, respectively), and notably do not need to jailbreak them to do so (<math alttext="N=0" class="ltx_Math" display="inline" id="S1.F1.16.8.8.m8" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">N=0</annotation></semantics></math>).
<span class="ltx_text ltx_font_italic" id="S1.F1.16.8.8.2">Note: We do not claim we maximized possible extraction for each LLM.
Different runs use different underlying generation configurations per LLM.</span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 1：单次运行中提取《哈利·波特与魔法石》的过程。我们使用基于块的贪婪最长公共子串近似方法（ <math intent=":literal" id="S1.F1.9.1.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> ，公式 7）量化生产 LLM 生成文本中包含的原始书籍内容的比例。该指标仅计算足够长、连续的近乎逐字复制的文本片段，对于这些片段我们可以保守地声称提取了训练数据（第 3.3 节）。我们从越狱的 Claude 3.7 Sonnet（BoN <math intent=":literal" id="S1.F1.10.2.2.m2" display="inline" class="ltx_Math" alttext="N=258"><semantics><mrow><mi>N</mi><mo>=</mo><mn>258</mn></mrow><annotation encoding="application/x-tex">N=258</annotation></semantics></math> ， <math intent=":literal" id="S1.F1.11.3.3.m3" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=95.8\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>95.8</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=95.8\%</annotation></semantics></math> ）中提取了几乎所有《哈利·波特与魔法石》。GPT-4.1 需要更多越狱尝试（ <math intent=":literal" id="S1.F1.12.4.4.m4" display="inline" class="ltx_Math" alttext="N=5179"><semantics><mrow><mi>N</mi><mo>=</mo><mn>5179</mn></mrow><annotation encoding="application/x-tex">N=5179</annotation></semantics></math> ），并在第一章节结束时拒绝继续；生成的文本包含 <math intent=":literal" id="S1.F1.13.5.5.m5" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=4.0\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>4.0</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=4.0\%</annotation></semantics></math> 完整书籍内容。我们从 Gemini 2.5 Pro 和 Grok 3 中提取了相当比例的书籍内容（ <math intent=":literal" id="S1.F1.14.6.6.m6" display="inline" class="ltx_Math" alttext="76.8\%"><semantics><mrow><mn>76.8</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">76.8\%</annotation></semantics></math> 和 <math intent=":literal" id="S1.F1.15.7.7.m7" display="inline" class="ltx_Math" alttext="70.3\%"><semantics><mrow><mn>70.3</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">70.3\%</annotation></semantics></math> ，分别），值得注意的是，我们不需要越狱它们即可做到这一点（ <math intent=":literal" id="S1.F1.16.8.8.m8" display="inline" class="ltx_Math" alttext="N=0"><semantics><mrow><mi>N</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">N=0</annotation></semantics></math> ）。注意：我们不声称为每个 LLM 最大化了可能的提取量。不同的运行针对每个 LLM 使用不同的底层生成配置。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Legal scholarship discusses how both extracted outputs and the corresponding encoding of the memorized work in a model’s weights may satisfy the technical definition of a <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">copy</span> under U.S.&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cooper and Grimmelmann, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib36" title="The Files are in the Computer: Copyright, Memorization, and Generative AI">2024</a>)</cite> and E.U. copyright law&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dornis, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib108" title="Generative AI, Reproductions Inside the Model, and the Making Available to the Public">2025</a>)</cite>, and how both types of copies could, in specific circumstances, cut against fair use in copyright infringement claims.
Aside from these academic arguments, the two lawsuits that have been decided in the U.S., which have focused primarily on training and model outputs, find that LLM training can be fair use, with limitations&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bartz Judgment, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib50" title="Order on Fair Use, Bartz et al. v. Anthropic PBC">2025</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib49" title="Order Denying the Plaintiffs’ Motion for Partial Summary Judgment and Granting Meta’s Cross-Motion for Partial Summary Judgment, Kadrey et al. v. Meta Platforms, Inc.">2025</a>)</cite>.
In contrast, a recent ruling in Germany (currently under appeal) finds that both extracted outputs and memorization encoded in the model can be infringing copies of in-copyright training data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib68" title="Gesellschaft für musikalische Aufführungs- und mechanische Vervielfältigungsrechte">GEMA v. OpenAI, </a>; Poltz and Heine, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib109" title="OpenAI used song lyrics in violation of copyright laws, German court says">2025</a>)</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">法学研究探讨了提取的输出以及模型权重中记忆工作的相应编码如何满足美国（Cooper 和 Grimmelmann，2024）和欧盟版权法（Dornis，2025）下版权的技术定义，以及这两种类型的复制在特定情况下如何可能违反版权侵权诉讼中的合理使用原则。除了这些学术观点之外，美国已经判决的两起诉讼，主要关注训练和模型输出，发现 LLM 训练可以是合理使用，但有限制（Bartz 判决，2025；2025）。相比之下，德国最近的一项判决（目前正上诉中）认为，提取的输出和模型中编码的记忆都可以是版权受保护训练数据的侵权复制（GEMA v. OpenAI；Poltz 和 Heine，2025）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In the U.S. cases, both judgments note that neither set of plaintiffs brought compelling evidence that the LLMs in question can produce outputs that reflect legally cognizable copies of the plaintiffs’ works;
they did not demonstrate substantial extraction of training data.
Nevertheless, this does not mean that production LLMs do not memorize copyrighted material.
In recent work, <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite> show that memorization of in-copyright books in open-weight LLMs is far more significant than previously understood;
in some cases, memorization is so extensive that it is straightforward to extract long-form (parts of) books from models like Llama 3.1 70B.
However, these results on open-weight, non-instruction-tuned LLMs do not naturally translate to production LLMs, which implement both model- and system-level safeguards intended to mitigate undesirable outputs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib75" title="Constitutional ai: harmlessness from ai feedback">2022</a>)</cite>, including outputting verbatim copyrighted data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib74" title="Claude’s constitution">2023</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib73" title="OpenAI Model Spec (2024/05/08)">2024b</a>)</cite>.
Prior work has successfully jailbroken production systems to circumvent these safeguards and extract training data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Nasr<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib88" title="Scalable Extraction of Training Data from Aligned, Production Language Models">2025</a>)</cite>, but does not study extraction of long-form copyrighted text.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在美国的案例中，两项判决都指出，原告方均未提供有力证据证明所涉及的 LLMs 能够生成反映原告作品合法可复制版本的输出；他们并未证明对训练数据的实质性提取。然而，这并不意味着生产 LLMs 不会记忆受版权保护的材料。在近期的研究中，Cooper 等人（2025 年）表明，在开放权重 LLMs 中记忆受版权保护书籍的情况远比之前理解的更为显著；在某些情况下，记忆范围如此广泛，以至于从 Llama 3.1 70B 等模型中提取长篇（书籍的部分）变得非常直接。然而，这些关于开放权重、非指令调优 LLMs 的结果并不能自然地推广到生产 LLMs，后者实施了模型和系统级别的保护措施，旨在减轻不希望的输出（Bai 等人，2022 年），包括输出受版权保护的数据（Anthropic，2023 年；OpenAI，2024b）。先前的研究已成功破解生产系统以规避这些保护措施并提取训练数据（Nasr 等人，2023 年；2025 年），但并未研究长篇受版权保护文本的提取。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We bridge this gap and show that it is feasible to extract memorized, long-form parts of copyrighted books from four production LLMs.
We primarily attempt to extract books that <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite> observe to be highly memorized by Llama 3.1 70B, and use a simple two-phase procedure to do so (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3" title="3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3</span></a>):<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们弥合了这一差距，并表明从四个生产 LLM 中提取版权受保护的书籍的记忆中长篇部分是可行的。我们主要尝试提取 Cooper 等人（2025）观察到的 Llama 3.1 70B 高度记忆的书籍，并使用一个简单的两阶段程序来完成这一过程（第 3 节）：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Phase 1: attempting completion of an initial prefix (may involve jailbreaking).</span>
We combine an initial instruction (“Continue the following text exactly as it appears in the original literary work verbatim”) with a short snippet of seed text from the beginning of a book (e.g., the first sentence).
For the books we test, both Gemini 2.5 Pro and Grok 3 directly comply with this instruction.
For Claude 3.7 Sonnet and GPT-4.1, we jailbreak the underlying model to circumvent safeguards using Best-of-<math alttext="N" class="ltx_Math" display="inline" id="S1.I1.i1.p1.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hughes<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib70" title="Best-of-n jailbreaking">2024</a>)</cite>, a simple attack that permutes the instruction portion of the prompt until the system responds successfully or the prompting budget is exhausted (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
The four LLMs do not always successfully continue the seed text with a loose approximation of the true text;
in these cases, our procedure fails.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">1. 第一阶段：尝试完成一个初始前缀（可能涉及越狱）。我们将一个初始指令（“请按原文文学作品中出现的文本完全继续以下文本”）与书中开头的一段种子文本（例如第一句话）相结合。对于我们测试的书籍，Gemini 2.5 Pro 和 Grok 3 都直接遵守这一指令。对于 Claude 3.7 Sonnet 和 GPT-4.1，我们使用 Best-of- <math intent=":literal" id="S1.I1.i1.p1.1.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> （Hughes 等人，2024），一种简单的攻击方法，通过重新排列提示中的指令部分，直到系统成功响应或提示预算耗尽（第 3.1 节），来越狱底层模型以绕过安全措施。这四个 LLM 并不总能成功地用对真实文本的松散近似来继续种子文本；在这些情况下，我们的程序会失败。</font></font></font>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Phase 2: attempting long-form extraction via requesting continuation.</span>
If Phase 1 succeeds, we repeatedly query the production LLM to continue the text (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.2</span></a>), and then ultimately compare the generated output to the corresponding ground-truth reference book.
We compute the proportion of the book that is extracted near-verbatim in the output, using a score derived from a block-based, greedy approximation of longest common substring (<span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.2">near-verbatim recall</span>, <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S1.I1.i2.p1.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math>, Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3" title="3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">2. 第二阶段：通过请求延续来尝试长文本提取。如果第一阶段成功，我们会反复查询生产 LLM 以继续文本（第 3.2 节），然后最终将生成的输出与相应的真实参考书进行比较。我们计算输出中近乎逐字提取的书籍比例，使用基于块状、贪婪近似最长公共子串的分数（近乎逐字召回率， <math intent=":literal" id="S1.I1.i2.p1.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> ，第 3.3 节）。</font></font></font>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.4">Altogether, we find that is possible to extract large portions of memorized copyrighted material from all four production LLMs, though success varies by experimental settings (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4" title="4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>).
For instance, for specific generation configurations, Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a> shows the amount of extraction for <span class="ltx_text ltx_font_italic" id="S1.p6.4.1">Harry Potter and the Sorcerer’s Stone</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rowling, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib112" title="Harry potter and the sorcerer’s stone">1998</a>)</cite> that we obtain with one run of the two-phase procedure for each production LLM.
These results show that it is possible to extract large amounts of copyrighted material.
However, this is a descriptive statement about particular experimental outcomes&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chouldechova<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib136" title="Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming">2025</a>)</cite>;
we do not make general claims about books extraction overall, or claims comparing overall extraction risk across production LLMs.
As shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>, our best configuration extracts nearly all of the book near-verbatim from Claude 3.7 Sonnet (<math alttext="\mathsf{nv{\text{-}}recall}=95.8\%" class="ltx_Math" display="inline" id="S1.p6.1.m1" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>95.8</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=95.8\%</annotation></semantics></math>).
For GPT-4.1, our best configuration extracts only part of the first chapter (<math alttext="\mathsf{nv{\text{-}}recall}=4.0\%" class="ltx_Math" display="inline" id="S1.p6.2.m2" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>4.0</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=4.0\%</annotation></semantics></math>).
We attempt extraction for eleven in-copyright books published before 2020, and find that most experiments result in far less extraction
(<math alttext="\mathsf{nv{\text{-}}recall}\leq 10\%" class="ltx_Math" display="inline" id="S1.p6.3.m3" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>≤</mo><mrow><mn>10</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}\leq 10\%</annotation></semantics></math>).
For Claude 3.7 Sonnet, we extract almost the entire text of two in-copyright books (and two in the public domain) (<math alttext="\mathsf{nv{\text{-}}recall}\geq 94\%" class="ltx_Math" display="inline" id="S1.p6.4.m4" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>≥</mo><mrow><mn>94</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}\geq 94\%</annotation></semantics></math>).
We discuss important limitations of our work (e.g., monetary cost) and brief observations about why our results may be of interest to copyright (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5" title="5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">总而言之，我们发现可以从所有四个生产型 LLM 中提取大量记忆中的版权材料，尽管成功率因实验设置而异（第 4 节）。例如，对于特定的生成配置，图 1 显示了使用两阶段程序对每个生产型 LLM 分别运行一次后，我们获得的《哈利·波特与魔法石》（罗琳，1998）的提取量。这些结果表明，可以提取大量的版权材料。然而，这仅是对特定实验结果的描述性陈述（Chouldechova 等人，2025）；我们并未对书籍提取总体情况或跨生产型 LLM 的总体提取风险做出一般性声明。如图 1 所示，我们最佳配置从 Claude 3.7 Sonnet（ <math intent=":literal" id="S1.p6.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=95.8\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>95.8</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=95.8\%</annotation></semantics></math> ）中几乎逐字提取了整本书。对于 GPT-4.1，我们最佳配置仅提取了第一章的一部分（ <math intent=":literal" id="S1.p6.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=4.0\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>4.0</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=4.0\%</annotation></semantics></math> ）。我们尝试提取 2020 年前出版的 11 本版权图书，发现大多数实验的提取量远少（ <math intent=":literal" id="S1.p6.3.m3" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}\leq 10\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>≤</mo><mrow><mn>10</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}\leq 10\%</annotation></semantics></math> ）。对于 Claude 3。7 我们从两个受版权保护的书（以及两个公共领域的书）中提取了几乎全部文本（ <math intent=":literal" id="S1.p6.4.m4" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}\geq 94\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>≥</mo><mrow><mn>94</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}\geq 94\%</annotation></semantics></math> ）。我们讨论了我们工作的重要局限性（例如，金钱成本）以及关于我们的结果可能为何引起版权兴趣的简短观察（第 5 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.2"><span class="ltx_text ltx_font_bold" id="S1.p7.2.1">Responsible disclosure.</span>  On September 9, 2025, we notified affected providers (Anthropic, Google DeepMind, OpenAI, and xAI) of our results and intent to publish, after discovering the success of our procedure in August 2025.
Following the standard responsible disclosure process&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Project Zero, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib86" title="Vulnerability disclosure policy">2021</a>)</cite>, we told providers we would wait <math alttext="90" class="ltx_Math" display="inline" id="S1.p7.1.m1" intent=":literal"><semantics><mn>90</mn><annotation encoding="application/x-tex">90</annotation></semantics></math> days before making our findings public.
Anthropic, Google DeepMind, and OpenAI acknowledged our disclosure.
On November 29, 2025, we observed that Anthropic’s Claude 3.7 Sonnet series was no longer available in Claude’s UI.
At the end of the <math alttext="90" class="ltx_Math" display="inline" id="S1.p7.2.m2" intent=":literal"><semantics><mn>90</mn><annotation encoding="application/x-tex">90</annotation></semantics></math>-day disclosure window (December 9, 2025), we found that our procedure still works on some
of the systems that we evaluate. Having taken the above steps, we believe it is now responsible to share our findings publicly.
Doing so underscores the continued challenges of robust model- and system-level safeguards in production LLMs, particularly with respect to mitigating the risk of leakage of in-copyright training data.
To give readers a sense of the qualitative similarity of our long-form extraction results, we release full, lightly format-normalized diffs for Claude 3.7 Sonnet on <span class="ltx_text ltx_font_italic" id="S1.p7.2.2">Frankenstein</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shelley, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib117" title="Frankenstein">1818</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S1.p7.2.3">The Great Gatsby</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Fitzgerald, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib114" title="The great gatsby">1925</a>)</cite>, which are both in the public domain. (See <a class="ltx_ref ltx_href" href="https://drive.google.com/drive/folders/1bCI1teXoVwgcZBvbWANc2Ss_h1x0zLv-?usp=sharing" title="">here</a>.
Black text reflects verbatim matches, strike-through red text indicates reference text missing from the generation, and blue underlined text reflects text in the generation missing from the reference text.)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">负责任的披露。2025 年 9 月 9 日，我们在发现我们的程序在 2025 年 8 月成功后，通知了受影响的提供者（Anthropic、Google DeepMind、OpenAI 和 xAI）我们的结果和发布意图。遵循标准的负责任披露流程（Project Zero，2021），我们告知提供者我们将等待 <math intent=":literal" id="S1.p7.1.m1" display="inline" class="ltx_Math" alttext="90"><semantics><mn>90</mn><annotation encoding="application/x-tex">90</annotation></semantics></math> 天后再公开我们的发现。Anthropic、Google DeepMind 和 OpenAI 确认了我们的披露。2025 年 11 月 29 日，我们观察到 Anthropic 的 Claude 3.7 Sonnet 系列不再在 Claude 的 UI 中可用。在 <math intent=":literal" id="S1.p7.2.m2" display="inline" class="ltx_Math" alttext="90"><semantics><mn>90</mn><annotation encoding="application/x-tex">90</annotation></semantics></math> 天的披露窗口期结束（2025 年 12 月 9 日）时，我们发现我们的程序在一些我们评估的系统上仍然有效。在采取了上述步骤后，我们认为现在是负责任地公开我们的发现的时候了。这样做强调了在生产 LLMs 中持续存在的模型和系统级安全防护的挑战，特别是在减轻版权受保护训练数据泄露风险方面。为了让读者了解我们长格式提取结果的定性相似性，我们发布了 Claude 3 的完整、轻微格式归一化的 diffs。7 《弗兰肯斯坦》（雪莱，1818 年）和《了不起的盖茨比》（菲茨杰拉德，1925 年），这两部作品都在公共领域。（参见此处。黑色文本反映逐字匹配，划线的红色文本表示生成中缺失的参考文本，蓝色下划线文本表示参考文本中缺失的生成文本。)</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and related work<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">2 背景和相关工作</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">There are three overarching topics that are relevant to our work:
1) memorization and extraction, 2) circumventing safeguards in production LLMs, and 3) the intersection of both of these areas with copyright.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们的工作与三个主要相关主题有关：1）记忆和提取，2）绕过生产 LLMs 中的安全措施，以及 3）这两个领域与版权的交叉点。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Memorization and extraction of training data.</span> 
In general, models “memorize” portions (but far from all) of their training data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Feldman, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib84" title="Does learning require memorization? a short tale about a long tail">2020</a>)</cite>.
At a high level, <span class="ltx_text ltx_font_bold" id="S2.p2.1.2">memorization</span> means that information about whether a model was trained on a particular data example can be recovered from the model itself&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cooper<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib83" title="Report of the 1st Workshop on Generative AI and Law">2023</a>)</cite>.
There are many techniques for quantifying this phenomenon&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hayes<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib80" title="Exploring the limits of strong membership inference attacks on large language models">2025a</a>; Chang<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib81" title="Context-aware membership inference attacks against pre-trained large language models">2025</a>)</cite>, but for generative models, one of the most common measurement approaches is <span class="ltx_text ltx_font_bold" id="S2.p2.1.3">extraction</span>:
prompting the model to reproduce specific training data (near-)verbatim in its outputs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Carlini<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib9" title="Extracting training data from large language models">2021</a>; Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib44" title="Deduplicating Training Data Makes Language Models Better">2022</a>; Cooper and Grimmelmann, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib36" title="The Files are in the Computer: Copyright, Memorization, and Generative AI">2024</a>)</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">记忆和提取训练数据。一般来说，模型“记忆”了其训练数据的一部分（但远非全部）（Feldman，2020）。从高层次来看，记忆意味着可以从模型本身恢复出模型是否在某个特定数据示例上进行了训练的信息（Cooper 等人，2023）。有许多技术可以量化这一现象（Hayes 等人，2025a；Chang 等人，2025），但对于生成模型来说，最常见的测量方法之一是提取：提示模型在其输出中重现特定的训练数据（近乎逐字）（Carlini 等人，2021；Lee 等人，2022；Cooper 和 Grimmelmann，2024）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.2">The standard method for measuring extraction in large language models (LLMs) takes a <math alttext="100" class="ltx_Math" display="inline" id="S2.p3.1.m1" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math>-token
sequence of known training data, divides it into a prefix and suffix (<math alttext="50" class="ltx_Math" display="inline" id="S2.p3.2.m2" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> tokens each), prompts the LLM with the prefix, and deems extraction to be successful if it generates the suffix verbatim <cite class="ltx_cite ltx_citemacro_citep">(Carlini<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib107" title="Quantifying Memorization Across Neural Language Models">2023</a>; Hayes<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib41" title="Measuring memorization in language models via probabilistic extraction">2025b</a>; Gemini Team<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib85" title="Gemini 1.5: unlocking multimodal understanding across millions of tokens of context">2024</a>; Grattafiori and others, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib40" title="The Llama 3 Herd of Models">2024</a>)</cite>.
Although this type of procedure is the most common in both research and frontier release reports, it is not the only way to extract training data from an LLM.
<cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite> show that entire memorized in-copyright books can be extracted near-verbatim from Llama 3.1 70B, by running continuous autoregressive generation seeded with a short prompt of ground-truth text.
This prior work focuses on long-form extraction from open-weight, non-instruction-tuned LLMs—a setting where it is possible to choose and directly configure the decoding algorithm.
In contrast, we study whether long-form extraction can successfully recover books when applied to production LLMs, where we have significantly more limited control (Sections&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.2</span></a> &amp;&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3" title="3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">测量大型语言模型（LLMs）中提取的标准方法，取一个包含已知训练数据的 <math intent=":literal" id="S2.p3.1.m1" display="inline" class="ltx_Math" alttext="100"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> 个 token 序列，将其分为前缀和后缀（各 <math intent=":literal" id="S2.p3.2.m2" display="inline" class="ltx_Math" alttext="50"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> 个 token），用前缀提示 LLM，如果它能够逐字生成后缀，则认为提取成功（Carlini 等人，2023；Hayes 等人，2025b；Gemini 团队等人，2024；Grattafiori 等人，2024）。尽管这种程序在研究和前沿发布报告中最为常见，但它并非从 LLM 中提取训练数据的唯一方法。Cooper 等人（2025）表明，通过使用包含真实文本短提示的连续自回归生成，可以从 Llama 3.1 70B 中近乎逐字地提取整个受版权保护的记忆书籍。这项先前的工作专注于从开放权重、非指令调优的 LLMs 中提取长文本——在这种设置中，可以选择并直接配置解码算法。相比之下，我们研究长文本提取是否能够成功恢复书籍，当应用于生产 LLMs 时，我们对其的控制权限显著受限（第 3.2 节和第 3.3 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Circumventing safeguards.</span> 
LLMs, especially those deployed in production systems, are often trained to comply with specific policies&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Christiano<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib27" title="Deep reinforcement learning from human preferences">2017</a>; Ziegler<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib28" title="Fine-tuning language models from human preferences">2019</a>; Wei<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib30" title="Finetuned language models are zero-shot learners">2021</a>; Ouyang<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib29" title="Training language models to follow instructions with human feedback">2022</a>)</cite>.
Nevertheless, such <span class="ltx_text ltx_font_bold" id="S2.p4.1.2">alignment</span> mechanisms can be circumvented—for instance, through <span class="ltx_text ltx_font_bold" id="S2.p4.1.3">jailbreaks</span>, which use adversarial prompting techniques to elicit harmful or otherwise restricted outputs (<cite class="ltx_cite ltx_citemacro_citet">Hendrycks<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib32" title="Unsolved problems in ML safety">2021</a>); Zou<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib77" title="Universal and transferable adversarial attacks on aligned language models">2023</a>)</cite>; Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
When attacking production LLMs, successful jailbreaks evade not only model-level alignment but also complementary <span class="ltx_text ltx_font_bold" id="S2.p4.1.4">system-level guardrails</span>, such as input and output filters&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sharma<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib76" title="Constitutional classifiers: defending against universal jailbreaks across thousands of hours of red teaming">2025</a>; Cooper<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib72" title="Machine unlearning doesn’t do what you think: lessons for generative ai policy, research, and practice">2024</a>)</cite>.
Much prior work demonstrates that jailbreaks work in production settings&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wei<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib79" title="Jailbroken: how does llm safety training fail?">2023</a>; Anil<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib78" title="Many-shot jailbreaking">2024</a>; Hughes<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib70" title="Best-of-n jailbreaking">2024</a>)</cite>.
Notably, earlier versions of ChatGPT could be jailbroken with simple, repetitive attack strings, enabling the extraction of verbatim training data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Nasr<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>)</cite>.
Although frontier AI companies are developing and refining approaches (e.g., <span class="ltx_text ltx_font_bold" id="S2.p4.1.5">refusal</span>)
to prevent training-data leakage in system outputs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib65" title="OpenAI and Journalism">2024a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib61" title="GPT-4 System Card">2023</a>)</cite>, we show that extraction remains a risk (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4" title="4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">绕过安全防护措施。LLMs，尤其是部署在生产系统中的 LLMs，通常被训练以遵守特定政策（Christiano 等人，2017 年；Ziegler 等人，2019 年；Wei 等人，2021 年；Ouyang 等人，2022 年）。然而，这些对齐机制可以被绕过——例如，通过使用对抗性提示技术来诱出有害或其他受限输出的越狱攻击（Hendrycks 等人（2021 年）；Zou 等人（2023 年）；第 3.1 节）。当攻击生产 LLMs 时，成功的越狱攻击不仅会规避模型级别的对齐，还会规避系统级别的辅助防护措施，例如输入和输出过滤器（Sharma 等人，2025 年；Cooper 等人，2024 年）。许多先前的工作表明，越狱攻击在生产环境中有效（Wei 等人，2023 年；Anil 等人，2024 年；Hughes 等人，2024 年）。值得注意的是，ChatGPT 的早期版本可以通过简单的、重复的攻击字符串进行越狱，从而能够提取逐字训练数据（Nasr 等人，2023 年）。尽管前沿 AI 公司正在开发和完善方法（例如拒绝）以防止系统输出中的训练数据泄露（OpenAI，2024a；2023 年），但我们表明，提取仍然是一项风险（第 4 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.p5.1.1">Copyright and generative AI.</span> 
In most jurisdictions, copyright law grants exclusive rights (subject to important exceptions) in original works of authorship.
When parties other than the rightsholder <span class="ltx_text ltx_font_bold" id="S2.p5.1.2">reproduce</span> such works, courts may determine that they have <span class="ltx_text ltx_font_bold" id="S2.p5.1.3">infringed</span> copyright;
the resulting remedies can be substantial, including significant monetary damages&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(17 U.S. Code ğ 503, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib37" title="Copyright Law of the United States">2010</a>)</cite>.
The relationship between copyright law and generative AI is especially complicated&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib45" title="Talkin’ ’Bout AI Generation: Copyright and the Generative-AI Supply Chain">2023b</a>; Samuelson, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib52" title="Generative AI meets copyright">2023</a>)</cite>. Memorization is only one part of this landscape, raising questions about the reproduction of copyrighted training data.
In particular, extraction of memorized training data is a recurring issue in past and ongoing lawsuits&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kadrey et al. v. Meta Platforms, Inc., <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib69" title="">2025</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib67" title=""></a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib35" title=""></a>)</cite>, where courts are considering whether memorization encoded in the model and extraction in generations constitute copyright-infringing copying, or fall within exceptions to copyright’s exclusive rights, such as <span class="ltx_text ltx_font_bold" id="S2.p5.1.4">fair use</span> (<cite class="ltx_cite ltx_citemacro_citet">Lemley and Casey (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib46" title="Fair Learning">2021</a>)</cite>; Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>).
An important consideration in these cases is how easily copyrighted training data can be reproduced in model outputs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib45" title="Talkin’ ’Bout AI Generation: Copyright and the Generative-AI Supply Chain">2023b</a>; Cooper and Grimmelmann, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib36" title="The Files are in the Computer: Copyright, Memorization, and Generative AI">2024</a>; Cooper<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>—for example, whether extraction requires simple prompts&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib68" title="Gesellschaft für musikalische Aufführungs- und mechanische Vervielfältigungsrechte">GEMA v. OpenAI, </a>)</cite> or adversarial techniques like the jailbreak we sometimes use in this paper.
While we defer to others&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib45" title="Talkin’ ’Bout AI Generation: Copyright and the Generative-AI Supply Chain">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib138" title="Talkin’ ’Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)">2024</a>; Henderson<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib42" title="Foundation Models and Fair Use">2023</a>)</cite> and future work for detailed legal analysis, we note that our findings may be relevant to these ongoing debates (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5" title="5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">版权与生成式人工智能。在大多数司法管辖区，版权法授予作者对其原创作品的专有权利（但存在重要例外）。当非权利人复制此类作品时，法院可能认定其侵犯了版权；由此产生的救济措施可能非常重大，包括巨额金钱赔偿（美国法典第 17 编第 503 条，2010 年）。版权法与生成式人工智能之间的关系尤其复杂（李等，2023b；萨默森，2023）。记忆只是这一领域的一部分，引发了关于版权训练数据复制的疑问。特别是，记忆训练数据的提取是过去和正在进行诉讼中反复出现的问题（卡德雷等诉 Meta Platforms，Inc.，2025；；），法院正在考虑模型中编码的记忆和生成中的提取是否构成侵犯版权的复制，或属于版权专有权利的例外，如合理使用（莱姆利和凯西（2021）；第 1 节）。 在这些情况下，一个重要的考虑因素是版权受保护的训练数据在模型输出中容易被复制的情况（Lee 等人，2023b；Cooper 和 Grimmelmann，2024；Cooper 等人，2025）——例如，提取是否需要简单的提示（GEMA 诉 OpenAI）或对抗性技术，如本文有时使用的越狱技术。虽然我们委托他人（Lee 等人，2023b；2024；Henderson 等人，2023）和未来工作进行详细的法律分析，但我们注意到我们的发现可能与这些正在进行的辩论相关（第 5 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Extraction procedure<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">3 提取程序</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our overarching two-phase approach is straightforward.
In Phase 1, we probe the feasibility of extracting a given book from a production LLM by querying it to complete a short phrase of ground-truth text from the beginning of the book (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F2" title="Figure 2 ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>, Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and, if this succeeds, in Phase 2 we attempt to extract the rest book by repeatedly querying the LLM to continue the text (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F3" title="Figure 3 ‣ 3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3</span></a>, Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
Gemini 2.5 Pro and Grok 3 directly comply with our Phase 1 probe;
we need to jailbreak Claude 3.7 Sonnet and GPT-4.1 for compliance.
For Phase 2, we continue until the LLM responds with a refusal, the LLM returns a stop phrase (e.g., “THE END”), or we exhaust a specified query budget.
Then, we take the long-form generated output and compare it to the ground-truth text of the book to determine if extraction was successful (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3" title="3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
For the Phase 2 loop, we explore different generation configurations (e.g., maximum response length, temperature) based on what is tunable in each production LLM’s API, and pick configurations for each production LLM that result in the largest amount of extraction (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
<span class="ltx_text ltx_font_bold" id="S3.p1.1.1">Note: extraction does not always succeed.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们的两阶段总体方法很简单。在第一阶段，我们通过查询生产 LLM 来探测从其中提取给定书籍的可行性，方法是让它完成书籍开头的短段真实文本（图 2，第 3.1 节），如果成功，则在第二阶段尝试通过反复查询 LLM 来提取剩余部分（图 3，第 3.2 节）。Gemini 2.5 Pro 和 Grok 3 直接符合我们的第一阶段探测；我们需要越狱 Claude 3.7 Sonnet 和 GPT-4.1 以符合要求。对于第二阶段，我们继续直到 LLM 拒绝回应、LLM 返回停止短语（例如，“THE END”）或我们耗尽指定的查询预算。然后，我们取生成的长文本输出，并将其与书籍的真实文本进行比较，以确定提取是否成功（第 3.3 节）。对于第二阶段循环，我们根据每个生产 LLM API 中可调的部分探索不同的生成配置（例如，最大响应长度、温度），并选择每个生产 LLM 中导致最大提取量的配置（第 3.2 节）。注意：提取并不总是成功。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S3.F2.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/x1.png" width="664">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.10.4.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.6.3" style="font-size:90%;">Phase 1 of our two-phase procedure.<span class="ltx_text ltx_font_medium" id="S3.F2.6.3.3">
We illustrate Phase 1 (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.1</span></a>) for <span class="ltx_text ltx_font_italic" id="S3.F2.6.3.3.1">Harry Potter and the Sorcerer’s Stone</span>:
providing an initial instruction to complete a short prefix of ground-truth text from the book.
Gemini 2.5 Pro and Grok 3 comply directly (left);
for Claude 3.7 Sonnet and GPT-4.1, we use the use Best-of-<math alttext="N" class="ltx_Math" display="inline" id="S3.F2.4.1.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> jailbreak (right).
We evaluate if the production LLM produces a loose approximation of the the suffix using similarity score <math alttext="s" class="ltx_Math" display="inline" id="S3.F2.5.2.2.m2" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E2" title="In 3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>).
If successful (<math alttext="s\geq 0.6" class="ltx_Math" display="inline" id="S3.F2.6.3.3.m3" intent=":literal"><semantics><mrow><mi>s</mi><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s\geq 0.6</annotation></semantics></math>), we proceed to Phase&nbsp;2 (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F3" title="Figure 3 ‣ 3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3</span></a>, Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 2：我们两阶段流程的第一阶段。我们以《哈利·波特与魔法石》（3.1 节）为例说明第一阶段：提供初始指令，以完成书中一小段真实文本的前缀。Gemini 2.5 Pro 和 Grok 3 直接合规（左图）；对于 Claude 3.7 Sonnet 和 GPT-4.1，我们使用 Best-of- <math intent=":literal" id="S3.F2.4.1.1.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 越狱方法（右图）。我们使用相似度分数 <math intent=":literal" id="S3.F2.5.2.2.m2" display="inline" class="ltx_Math" alttext="s"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> （公式 2）评估生产 LLM 是否生成了该后缀的松散近似。如果成功 <math intent=":literal" id="S3.F2.6.3.3.m3" display="inline" class="ltx_Math" alttext="s\geq 0.6"><semantics><mrow><mi>s</mi><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s\geq 0.6</annotation></semantics></math> ，我们将进入第二阶段（图 3，3.2 节）。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Attempting initial completion of a short ground-truth prefix (Phase 1)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">3.1 尝试完成一个短的基准前缀的初始完成（阶段 1）</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.11">We interact with a production LLM via a blackbox API, which limits our access to the underlying model;
we supply prompts and receive responses, but do not have access to logits or <math alttext="\log" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1" intent=":literal"><semantics><mi>log</mi><annotation encoding="application/x-tex">\log</annotation></semantics></math> probabilities.
For a given book and production LLM, we first probe if extraction seems feasible.
To do so,
we attempt to have the LLM complete a provided prefix of text drawn from the book.
Specifically, we start with a <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.11.1">seed</span> <math alttext="{\bm{s}}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2" intent=":literal"><semantics><mi>𝒔</mi><annotation encoding="application/x-tex">{\bm{s}}</annotation></semantics></math>:
an initial short, ground-truth string, typically the first sentence or couple of sentences of the book.
We split <math alttext="{\bm{s}}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3" intent=":literal"><semantics><mi>𝒔</mi><annotation encoding="application/x-tex">{\bm{s}}</annotation></semantics></math> into a <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.11.2">prefix</span> <math alttext="{\bm{p}}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4" intent=":literal"><semantics><mi>𝒑</mi><annotation encoding="application/x-tex">{\bm{p}}</annotation></semantics></math> and <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.11.3">target suffix</span> <math alttext="{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5" intent=":literal"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> (i.e., <math alttext="{\bm{s}}={\bm{p}}+{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6" intent=":literal"><semantics><mrow><mi>𝒔</mi><mo>=</mo><mrow><mi>𝒑</mi><mo>+</mo><mi>𝒕</mi></mrow></mrow><annotation encoding="application/x-tex">{\bm{s}}={\bm{p}}+{\bm{t}}</annotation></semantics></math>).
As illustrated in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F2" title="Figure 2 ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>, we form an initial prompt by concatenating a <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.11.4">continuation instruction</span> <math alttext="{\bm{i}}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7" intent=":literal"><semantics><mi>𝒊</mi><annotation encoding="application/x-tex">{\bm{i}}</annotation></semantics></math> with the prefix, i.e., <math alttext="{\bm{i}}+{\bm{p}}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8" intent=":literal"><semantics><mrow><mi>𝒊</mi><mo>+</mo><mi>𝒑</mi></mrow><annotation encoding="application/x-tex">{\bm{i}}+{\bm{p}}</annotation></semantics></math>. (<math alttext="{\bm{i}}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9" intent=":literal"><semantics><mi>𝒊</mi><annotation encoding="application/x-tex">{\bm{i}}</annotation></semantics></math>=“Continue the following text exactly as it appears in the original literary work verbatim”; in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F2" title="Figure 2 ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>, <math alttext="{\bm{i}}" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m10" intent=":literal"><semantics><mi>𝒊</mi><annotation encoding="application/x-tex">{\bm{i}}</annotation></semantics></math> is abbreviated as “Continue the story verbatim”).
We submit this concatenated prompt to the production LLM to generate and return up to <math alttext="1000" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11" intent=":literal"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math> tokens, which we decode to text.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们通过一个黑盒 API 与一个生产 LLM 进行交互，这限制了我们对底层模型的访问；我们提供提示并接收响应，但无法访问 logits 或 <math intent=":literal" id="S3.SS1.p1.1.m1" display="inline" class="ltx_Math" alttext="\log"><semantics><mi>log</mi><annotation encoding="application/x-tex">\log</annotation></semantics></math> 概率。对于给定的书籍和生产 LLM，我们首先探测提取是否可行。为此，我们尝试让 LLM 完成从书中提供的文本前缀。具体来说，我们从种子 <math intent=":literal" id="S3.SS1.p1.2.m2" display="inline" class="ltx_Math" alttext="{\bm{s}}"><semantics><mi>𝒔</mi><annotation encoding="application/x-tex">{\bm{s}}</annotation></semantics></math> 开始：一个初始的短、真实的字符串，通常是书籍的第一句或几句。我们将 <math intent=":literal" id="S3.SS1.p1.3.m3" display="inline" class="ltx_Math" alttext="{\bm{s}}"><semantics><mi>𝒔</mi><annotation encoding="application/x-tex">{\bm{s}}</annotation></semantics></math> 分成一个前缀 <math intent=":literal" id="S3.SS1.p1.4.m4" display="inline" class="ltx_Math" alttext="{\bm{p}}"><semantics><mi>𝒑</mi><annotation encoding="application/x-tex">{\bm{p}}</annotation></semantics></math> 和一个目标后缀 <math intent=":literal" id="S3.SS1.p1.5.m5" display="inline" class="ltx_Math" alttext="{\bm{t}}"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> （即 <math intent=":literal" id="S3.SS1.p1.6.m6" display="inline" class="ltx_Math" alttext="{\bm{s}}={\bm{p}}+{\bm{t}}"><semantics><mrow><mi>𝒔</mi><mo>=</mo><mrow><mi>𝒑</mi><mo>+</mo><mi>𝒕</mi></mrow></mrow><annotation encoding="application/x-tex">{\bm{s}}={\bm{p}}+{\bm{t}}</annotation></semantics></math> ）。如图 2 所示，我们通过将延续指令 <math intent=":literal" id="S3.SS1.p1.7.m7" display="inline" class="ltx_Math" alttext="{\bm{i}}"><semantics><mi>𝒊</mi><annotation encoding="application/x-tex">{\bm{i}}</annotation></semantics></math> 与前缀连接起来形成初始提示，即 <math intent=":literal" id="S3.SS1.p1.8.m8" display="inline" class="ltx_Math" alttext="{\bm{i}}+{\bm{p}}"><semantics><mrow><mi>𝒊</mi><mo>+</mo><mi>𝒑</mi></mrow><annotation encoding="application/x-tex">{\bm{i}}+{\bm{p}}</annotation></semantics></math> 。（ <math intent=":literal" id="S3.SS1.p1.9.m9" display="inline" class="ltx_Math" alttext="{\bm{i}}"><semantics><mi>𝒊</mi><annotation encoding="application/x-tex">{\bm{i}}</annotation></semantics></math> =“按原文逐字继续以下文本”；在图 2 中， <math intent=":literal" id="S3.SS1.p1.10.m10" display="inline" class="ltx_Math" alttext="{\bm{i}}"><semantics><mi>𝒊</mi><annotation encoding="application/x-tex">{\bm{i}}</annotation></semantics></math> 被缩写为“逐字继续故事”）。我们将这个连接起来的提示提交给生产 LLM，以生成并返回最多 <math intent=":literal" id="S3.SS1.p1.11.m11" display="inline" class="ltx_Math" alttext="1000"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math> 个 token，我们将这些 token 解码为文本。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In our main experiments, Gemini 2.5 Pro and Grok 3 complied directly with instructions of this form.
In contrast,
Claude 3.7 Sonnet and GPT-4.1 exhibited refusal mechanisms, which prevent direct continuation of the provided prefix.
Similar to prior work (<cite class="ltx_cite ltx_citemacro_citet">Nasr<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib88" title="Scalable Extraction of Training Data from Aligned, Production Language Models">2025</a>)</cite>; Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>), we jailbreak these two production LLMs to circumvent alignment.
We began with a simple attack from the literature—Best-of-<math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hughes<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib70" title="Best-of-n jailbreaking">2024</a>)</cite>—and, given its immediate success, do not consider more sophisticated attacks in this work.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在我们的主要实验中，Gemini 2.5 Pro 和 Grok 3 直接遵循这种形式的指令。相比之下，Claude 3.7 Sonnet 和 GPT-4.1 表现出拒绝机制，这会阻止直接延续提供的 prefix。与先前的工作（Nasr 等人（2023 年；2025 年）；第 2 节）类似，我们对这两个生产 LLMs 进行越狱，以绕过对齐。我们从文献中开始了一个简单的攻击——Best-of- <math intent=":literal" id="S3.SS1.p2.1.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> （Hughes 等人，2024 年），鉴于其立即的成功，我们在这项工作中不考虑更复杂的攻击。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.7"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Best-of-<math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.1.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> jailbreak (used with Claude 3.7 Sonnet and GPT-4.1).</span> 
When running <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.2.2">Best-of-<math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.2.2.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> (BoN)</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hughes<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib70" title="Best-of-n jailbreaking">2024</a>)</cite>, one selects an initial prompt, makes <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> variations of that prompt with random text perturbations, submits the <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> prompts to an LLM to generate <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m3" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> candidate responses, and then selects the response that most effectively bypasses safety guardrails, where effectiveness is determined by a chosen, context-appropriate criterion (detailed below).
The random text perturbations include compositions of flipping alphabetic character case, shuffling word order, character substitutions with visually similar glyphs (e.g., <math alttext="\text{`s&#39;}\rightarrow\{\text{`\textdollar&#39;},\text{`5&#39;}\}" class="ltx_Math" display="inline" id="S3.SS1.p3.6.m4" intent=":literal"><semantics><mrow><mtext>‘s’</mtext><mo stretchy="false">→</mo><mrow><mo stretchy="false">{</mo><mtext>‘$’</mtext><mo>,</mo><mtext>‘5’</mtext><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\text{`s'}\rightarrow\{\text{`\textdollar'},\text{`5'}\}</annotation></semantics></math>), and other formatting edits (<cite class="ltx_cite ltx_citemacro_citet">Hughes<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib70" title="Best-of-n jailbreaking">2024</a>)</cite>; Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A1" title="Appendix A BoN perturbtations ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">A</span></a>).
Even if most of the production LLM’s outputs are compliant with its guardrail policies, the probability that the LLM is jailbroken—that is, at least one response violates these policies—increases with <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.7.m5" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">最佳 <math intent=":literal" id="S3.SS1.p3.1.1.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 脱逃（用于 Claude 3.7 Sonnet 和 GPT-4.1）。在运行最佳 <math intent=":literal" id="S3.SS1.p3.2.2.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> （BoN）（Hughes 等人，2024 年）时，首先选择一个初始提示，然后通过随机文本扰动对 <math intent=":literal" id="S3.SS1.p3.3.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 该提示进行变化，将 <math intent=":literal" id="S3.SS1.p3.4.m2" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 提示提交给 LLM 以生成 <math intent=":literal" id="S3.SS1.p3.5.m3" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 候选响应，然后选择最有效地绕过安全防护栏的响应，其中有效性由所选的、上下文适当的准则决定（详见下文）。随机文本扰动包括大小写字母翻转、词序打乱、使用视觉上相似的符号进行字符替换（例如 <math intent=":literal" id="S3.SS1.p3.6.m4" display="inline" class="ltx_Math" alttext="\text{`s&#39;}\rightarrow\{\text{`\textdollar&#39;},\text{`5&#39;}\}"><semantics><mrow><mtext>‘s’</mtext><mo stretchy="false">→</mo><mrow><mo stretchy="false">{</mo><mtext>‘$’</mtext><mo>,</mo><mtext>‘5’</mtext><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\text{`s'}\rightarrow\{\text{`\textdollar'},\text{`5'}\}</annotation></semantics></math> ），以及其他格式编辑（Hughes 等人（2024 年）；附录 A）。即使生产 LLM 的多数输出符合其防护栏政策，LLM 被脱逃的概率——即至少有一个响应违反这些政策——会随着 <math intent=":literal" id="S3.SS1.p3.7.m5" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 的增加而提高。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.9">This procedure is model-agnostic and only requires blackbox access, which makes it well-suited to our setting of production LLMs.
In practice, our BoN prompt is the initial instruction <math alttext="{\bm{i}}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1" intent=":literal"><semantics><mi>𝒊</mi><annotation encoding="application/x-tex">{\bm{i}}</annotation></semantics></math>;
we produce <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> random permutations of <math alttext="{\bm{i}}" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3" intent=":literal"><semantics><mi>𝒊</mi><annotation encoding="application/x-tex">{\bm{i}}</annotation></semantics></math> (e.g., “C0ntinuE th3 st0ry verb@tim” in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F2" title="Figure 2 ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>), and we concatenate each with the prefix <math alttext="{\bm{p}}" class="ltx_Math" display="inline" id="S3.SS1.p4.4.m4" intent=":literal"><semantics><mi>𝒑</mi><annotation encoding="application/x-tex">{\bm{p}}</annotation></semantics></math> and submit
to the production LLM’s API to produce <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p4.5.m5" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> responses.
We then gauge success for Phase 1 when a decoded API response contains at least a loose match to the ground-truth target suffix <math alttext="{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS1.p4.6.m6" intent=":literal"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math>.
For Gemini 2.5 Pro and Grok 3, for which we did not use BoN, there is only one response to compare to <math alttext="{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS1.p4.7.m7" intent=":literal"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math>;
for Claude 3.7 Sonnet and GPT-4.1, we evaluate <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p4.8.m8" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> BoN responses to see if any of them is a loose match to <math alttext="{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS1.p4.9.m9" intent=":literal"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">此流程与模型无关，仅需黑盒访问，因此非常适合我们的生产 LLM 环境。在实践中，我们的 BoN 提示是初始指令 <math intent=":literal" id="S3.SS1.p4.1.m1" display="inline" class="ltx_Math" alttext="{\bm{i}}"><semantics><mi>𝒊</mi><annotation encoding="application/x-tex">{\bm{i}}</annotation></semantics></math> ；我们生成 <math intent=":literal" id="S3.SS1.p4.2.m2" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 个 <math intent=":literal" id="S3.SS1.p4.3.m3" display="inline" class="ltx_Math" alttext="{\bm{i}}"><semantics><mi>𝒊</mi><annotation encoding="application/x-tex">{\bm{i}}</annotation></semantics></math> 的随机排列（例如，图 2 中的“C0ntinuE th3 st0ry verb@tim”），并将每个与前缀 <math intent=":literal" id="S3.SS1.p4.4.m4" display="inline" class="ltx_Math" alttext="{\bm{p}}"><semantics><mi>𝒑</mi><annotation encoding="application/x-tex">{\bm{p}}</annotation></semantics></math> 连接后提交给生产 LLM 的 API 以生成 <math intent=":literal" id="S3.SS1.p4.5.m5" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 个响应。然后，当解码的 API 响应至少包含与真实目标后缀 <math intent=":literal" id="S3.SS1.p4.6.m6" display="inline" class="ltx_Math" alttext="{\bm{t}}"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> 的一个松散匹配时，我们便判定第一阶段成功。对于 Gemini 2.5 Pro 和 Grok 3，我们没有使用 BoN，因此只有一个响应可供比较 <math intent=":literal" id="S3.SS1.p4.7.m7" display="inline" class="ltx_Math" alttext="{\bm{t}}"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> ；对于 Claude 3.7 Sonnet 和 GPT-4.1，我们评估 <math intent=":literal" id="S3.SS1.p4.8.m8" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 个 BoN 响应，以查看其中是否有任何响应与 <math intent=":literal" id="S3.SS1.p4.9.m9" display="inline" class="ltx_Math" alttext="{\bm{t}}"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> 松散匹配。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.6"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.6.1">Determining Phase 1 success.</span> 
We quantify loose matches between a production LLM response <math alttext="{\bm{r}}" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1" intent=":literal"><semantics><mi>𝒓</mi><annotation encoding="application/x-tex">{\bm{r}}</annotation></semantics></math> and the target suffix <math alttext="{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2" intent=":literal"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> using <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.6.2">longest common substring</span>, which checks whether there exists a substring of words (i.e., a contiguous sequence of words) that appears verbatim in both.
That is, we denote the whitespace-split
character sequences of <math alttext="{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS1.p5.3.m3" intent=":literal"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> and <math alttext="{\bm{r}}" class="ltx_Math" display="inline" id="S3.SS1.p5.4.m4" intent=":literal"><semantics><mi>𝒓</mi><annotation encoding="application/x-tex">{\bm{r}}</annotation></semantics></math> as
<math alttext="T=(w^{({\bm{t}})}_{1},\dots,w^{({\bm{t}})}_{|T|})" class="ltx_Math" display="inline" id="S3.SS1.p5.5.m5" intent=":literal"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>𝒕</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><mrow><mo stretchy="false">|</mo><mi>T</mi><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">(</mo><mi>𝒕</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">T=(w^{({\bm{t}})}_{1},\dots,w^{({\bm{t}})}_{|T|})</annotation></semantics></math> and
<math alttext="R=(w^{({\bm{r}})}_{1},\dots,w^{({\bm{r}})}_{|R|})" class="ltx_Math" display="inline" id="S3.SS1.p5.6.m6" intent=":literal"><semantics><mrow><mi>R</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>𝒓</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><mrow><mo stretchy="false">|</mo><mi>R</mi><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">(</mo><mi>𝒓</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R=(w^{({\bm{r}})}_{1},\dots,w^{({\bm{r}})}_{|R|})</annotation></semantics></math>, respectively.
We then let<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">确定第一阶段是否成功。我们使用最长公共子串来量化生产 LLM 响应 <math intent=":literal" id="S3.SS1.p5.1.m1" display="inline" class="ltx_Math" alttext="{\bm{r}}"><semantics><mi>𝒓</mi><annotation encoding="application/x-tex">{\bm{r}}</annotation></semantics></math> 与目标后缀 <math intent=":literal" id="S3.SS1.p5.2.m2" display="inline" class="ltx_Math" alttext="{\bm{t}}"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> 之间的松散匹配，该方法检查是否存在一个单词子串（即一个连续的单词序列）在两者中完全相同出现。也就是说，我们将 <math intent=":literal" id="S3.SS1.p5.3.m3" display="inline" class="ltx_Math" alttext="{\bm{t}}"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS1.p5.4.m4" display="inline" class="ltx_Math" alttext="{\bm{r}}"><semantics><mi>𝒓</mi><annotation encoding="application/x-tex">{\bm{r}}</annotation></semantics></math> 的空白分隔字符序列分别表示为 <math intent=":literal" id="S3.SS1.p5.5.m5" display="inline" class="ltx_Math" alttext="T=(w^{({\bm{t}})}_{1},\dots,w^{({\bm{t}})}_{|T|})"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>𝒕</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><mrow><mo stretchy="false">|</mo><mi>T</mi><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">(</mo><mi>𝒕</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">T=(w^{({\bm{t}})}_{1},\dots,w^{({\bm{t}})}_{|T|})</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS1.p5.6.m6" display="inline" class="ltx_Math" alttext="R=(w^{({\bm{r}})}_{1},\dots,w^{({\bm{r}})}_{|R|})"><semantics><mrow><mi>R</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>𝒓</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><mrow><mo stretchy="false">|</mo><mi>R</mi><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">(</mo><mi>𝒓</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R=(w^{({\bm{r}})}_{1},\dots,w^{({\bm{r}})}_{|R|})</annotation></semantics></math> 。然后我们让</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathsf{longest}(T,R)\triangleq\max\Bigl\{\ell\,:\,(w^{({\bm{t}})}_{i},\dots,w^{({\bm{t}})}_{i+\ell-1})=(w^{({\bm{r}})}_{j},\dots,w^{({\bm{r}})}_{j+\ell-1}),\;1\leq i\leq|T|-\ell+1,\;1\leq j\leq|R|-\ell+1\Bigr\}" class="ltx_Math" display="inline" id="S3.E1.m1" intent=":literal"><semantics><mrow><mrow><mi mathsize="0.900em">𝗅𝗈𝗇𝗀𝖾𝗌𝗍</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">T</mi><mo mathsize="0.900em">,</mo><mi mathsize="0.900em">R</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><mo mathsize="0.900em">≜</mo><mrow><mi mathsize="0.900em">max</mi><mo>⁡</mo><mrow><mo maxsize="1.600em" minsize="1.600em">{</mo><mrow><mi mathsize="0.900em" mathvariant="normal">ℓ</mi><mo lspace="0.448em" mathsize="0.900em" rspace="0.448em">:</mo><mrow><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msubsup><mi mathsize="0.900em">w</mi><mi mathsize="0.900em">i</mi><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">𝒕</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></msubsup><mo mathsize="0.900em">,</mo><mi mathsize="0.900em" mathvariant="normal">…</mi><mo mathsize="0.900em">,</mo><msubsup><mi mathsize="0.900em">w</mi><mrow><mrow><mi mathsize="0.900em">i</mi><mo mathsize="0.900em">+</mo><mi mathsize="0.900em" mathvariant="normal">ℓ</mi></mrow><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">𝒕</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></msubsup><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo mathsize="0.900em">=</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><msubsup><mi mathsize="0.900em">w</mi><mi mathsize="0.900em">j</mi><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">𝒓</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></msubsup><mo mathsize="0.900em">,</mo><mi mathsize="0.900em" mathvariant="normal">…</mi><mo mathsize="0.900em">,</mo><msubsup><mi mathsize="0.900em">w</mi><mrow><mrow><mi mathsize="0.900em">j</mi><mo mathsize="0.900em">+</mo><mi mathsize="0.900em" mathvariant="normal">ℓ</mi></mrow><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">1</mn></mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mi mathsize="0.900em">𝒓</mi><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></msubsup><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><mo mathsize="0.900em">,</mo><mrow><mn mathsize="0.900em"> 1</mn><mo mathsize="0.900em">≤</mo><mi mathsize="0.900em">i</mi><mo mathsize="0.900em">≤</mo><mrow><mrow><mrow><mo maxsize="0.900em" minsize="0.900em" stretchy="true">|</mo><mi mathsize="0.900em">T</mi><mo maxsize="0.900em" minsize="0.900em" stretchy="true">|</mo></mrow><mo mathsize="0.900em">−</mo><mi mathsize="0.900em" mathvariant="normal">ℓ</mi></mrow><mo mathsize="0.900em">+</mo><mn mathsize="0.900em">1</mn></mrow></mrow></mrow></mrow><mo mathsize="0.900em">,</mo><mrow><mn mathsize="0.900em"> 1</mn><mo mathsize="0.900em">≤</mo><mi mathsize="0.900em">j</mi><mo mathsize="0.900em">≤</mo><mrow><mrow><mrow><mo maxsize="0.900em" minsize="0.900em" stretchy="true">|</mo><mi mathsize="0.900em">R</mi><mo maxsize="0.900em" minsize="0.900em" stretchy="true">|</mo></mrow><mo mathsize="0.900em">−</mo><mi mathsize="0.900em" mathvariant="normal">ℓ</mi></mrow><mo mathsize="0.900em">+</mo><mn mathsize="0.900em">1</mn></mrow></mrow><mo maxsize="1.600em" minsize="1.600em">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathsf{longest}(T,R)\triangleq\max\Bigl\{\ell\,:\,(w^{({\bm{t}})}_{i},\dots,w^{({\bm{t}})}_{i+\ell-1})=(w^{({\bm{r}})}_{j},\dots,w^{({\bm{r}})}_{j+\ell-1}),\;1\leq i\leq|T|-\ell+1,\;1\leq j\leq|R|-\ell+1\Bigr\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.4">denote the length of the longest contiguous common subsequence of <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.p6.1.m1" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> and <math alttext="R" class="ltx_Math" display="inline" id="S3.SS1.p6.2.m2" intent=":literal"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> (i.e., longest common substring of <math alttext="{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS1.p6.3.m3" intent=":literal"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> and <math alttext="{\bm{r}}" class="ltx_Math" display="inline" id="S3.SS1.p6.4.m4" intent=":literal"><semantics><mi>𝒓</mi><annotation encoding="application/x-tex">{\bm{r}}</annotation></semantics></math>).
We define a normalized similarity score<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表示 <math intent=":literal" id="S3.SS1.p6.1.m1" display="inline" class="ltx_Math" alttext="T"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS1.p6.2.m2" display="inline" class="ltx_Math" alttext="R"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> 的最长连续公共子序列的长度（即 <math intent=":literal" id="S3.SS1.p6.3.m3" display="inline" class="ltx_Math" alttext="{\bm{t}}"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS1.p6.4.m4" display="inline" class="ltx_Math" alttext="{\bm{r}}"><semantics><mi>𝒓</mi><annotation encoding="application/x-tex">{\bm{r}}</annotation></semantics></math> 的最长公共子串）。我们定义一个归一化相似度分数</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx2">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle s(T,R)\triangleq\frac{\mathsf{longest}(T,R)}{|T|}\quad\big(s(T,R)\in[0,1]\big)," class="ltx_Math" display="inline" id="S3.E2.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>T</mi><mo>,</mo><mi>R</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≜</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mi>𝗅𝗈𝗇𝗀𝖾𝗌𝗍</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>T</mi><mo>,</mo><mi>R</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mo stretchy="false">|</mo><mi>T</mi><mo stretchy="false">|</mo></mrow></mfrac></mstyle><mspace style="width:1em;" width="1em"></mspace><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>T</mi><mo>,</mo><mi>R</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle s(T,R)\triangleq\frac{\mathsf{longest}(T,R)}{|T|}\quad\big(s(T,R)\in[0,1]\big),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p6.15">which measures the fraction of whitespace-delimited text tokens in <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.p6.5.m1" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> that is covered by the longest contiguous verbatim span also found in <math alttext="R" class="ltx_Math" display="inline" id="S3.SS1.p6.6.m2" intent=":literal"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>.
In practice, we consider Phase 1 to be successful when <math alttext="s\geq 0.6" class="ltx_Math" display="inline" id="S3.SS1.p6.7.m3" intent=":literal"><semantics><mrow><mi>s</mi><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s\geq 0.6</annotation></semantics></math>—i.e., when there is an <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.p6.8.m4" intent=":literal"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation></semantics></math>-length verbatim common substring that covers at least <math alttext="60\%" class="ltx_Math" display="inline" id="S3.SS1.p6.9.m5" intent=":literal"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">60\%</annotation></semantics></math> of the target suffix <math alttext="{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS1.p6.10.m6" intent=":literal"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math>.
In initial experiments, we observed this to be a necessary minimum for Phase 2 to be feasible.
<span class="ltx_text ltx_font_bold" id="S3.SS1.p6.15.1">Note: we do not claim extraction of training data when Phase 1 succeeds with returning this loose match; we defer extraction claims to Phase 2.</span>
For Claude 3.7 Sonnet and GPT-4.1, we run BoN with <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p6.11.m7" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> prompts, stopping when the <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p6.12.m8" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>-th response <math alttext="{\bm{r}}_{N}" class="ltx_Math" display="inline" id="S3.SS1.p6.13.m9" intent=":literal"><semantics><msub><mi>𝒓</mi><mi>N</mi></msub><annotation encoding="application/x-tex">{\bm{r}}_{N}</annotation></semantics></math> yields <math alttext="s(T,R_{N})\geq 0.6" class="ltx_Math" display="inline" id="S3.SS1.p6.14.m10" intent=":literal"><semantics><mrow><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>T</mi><mo>,</mo><msub><mi>R</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s(T,R_{N})\geq 0.6</annotation></semantics></math> or when a maximum budget (<math alttext="N=10{,}000" class="ltx_Math" display="inline" id="S3.SS1.p6.15.m11" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N=10{,}000</annotation></semantics></math>) is met.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">该指标衡量在 <math intent=":literal" id="S3.SS1.p6.5.m1" display="inline" class="ltx_Math" alttext="T"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> 中以空白字符分隔的文本标记中有多少比例被 <math intent=":literal" id="S3.SS1.p6.6.m2" display="inline" class="ltx_Math" alttext="R"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> 中也存在的最长连续原文片段所覆盖。在实践中，我们认为当 <math intent=":literal" id="S3.SS1.p6.7.m3" display="inline" class="ltx_Math" alttext="s\geq 0.6"><semantics><mrow><mi>s</mi><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s\geq 0.6</annotation></semantics></math> 时 Phase 1 即成功——也就是说，存在一个长度为 <math intent=":literal" id="S3.SS1.p6.8.m4" display="inline" class="ltx_Math" alttext="\ell"><semantics><mi mathvariant="normal">ℓ</mi><annotation encoding="application/x-tex">\ell</annotation></semantics></math> 的原文公共子串，它至少覆盖了目标后缀 <math intent=":literal" id="S3.SS1.p6.10.m6" display="inline" class="ltx_Math" alttext="{\bm{t}}"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> 的 <math intent=":literal" id="S3.SS1.p6.9.m5" display="inline" class="ltx_Math" alttext="60\%"><semantics><mrow><mn>60</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">60\%</annotation></semantics></math> 。在初步实验中，我们观察到这是 Phase 2 可行的必要最低标准。注意：当 Phase 1 成功返回这种宽松匹配时，我们不声称提取了训练数据；我们推迟到 Phase 2 再进行提取声明。对于 Claude 3.7 Sonnet 和 GPT-4.1，我们使用 <math intent=":literal" id="S3.SS1.p6.11.m7" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 提示运行 BoN，当第 <math intent=":literal" id="S3.SS1.p6.12.m8" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 个响应 <math intent=":literal" id="S3.SS1.p6.13.m9" display="inline" class="ltx_Math" alttext="{\bm{r}}_{N}"><semantics><msub><mi>𝒓</mi><mi>N</mi></msub><annotation encoding="application/x-tex">{\bm{r}}_{N}</annotation></semantics></math> 产生 <math intent=":literal" id="S3.SS1.p6.14.m10" display="inline" class="ltx_Math" alttext="s(T,R_{N})\geq 0.6"><semantics><mrow><mrow><mi>s</mi><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>T</mi><mo>,</mo><msub><mi>R</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s(T,R_{N})\geq 0.6</annotation></semantics></math> 或达到最大预算（ <math intent=":literal" id="S3.SS1.p6.15.m11" display="inline" class="ltx_Math" alttext="N=10{,}000"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N=10{,}000</annotation></semantics></math> ）时停止。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Attempting long-form extraction of training data (Phase 2)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">3.2 尝试提取训练数据的长期形式（Phase 2）</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In Phase 2 we attempt long-form extraction of the rest of the book.
Following successful approximate completion of the seed prefix in Phase 1, we iteratively query the production LLM to continue the text (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F3" title="Figure 3 ‣ 3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3</span></a>).
Similar to the long-form extraction of books performed by <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>, the prefix in Phase 1 is the <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.1">only</em> ground-truth text that we provide in the entire procedure;
any additional text that we recover from a book in Phase 2 is generated and returned by the production LLM.
For each production LLM, we explore different generation configurations: temperature, maximum response length and, where available, frequency penalty and presence penalty (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4" title="4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>).
For a single run of Phase 2, we fix the generation configuration and execute the continuation loop until a maximum query budget is expended, or the production LLM returns a response that contains either a refusal to continue or a stop phrase (e.g. “THE END”).<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In practice, we occasionally observe generic internal server errors (500) for some providers, which also halts the loop.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在实践中，我们偶尔会观察到某些提供者的通用内部服务器错误（500），这也会导致循环中断。</font></font></font></span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在第二阶段，我们尝试提取剩余书籍的长文本内容。在第一阶段成功近似完成种子前缀后，我们迭代查询生产 LLM 以继续生成文本（图 3）。类似于 Cooper 等人（2025 年）进行的书籍长文本提取工作，第一阶段的前缀是整个过程中我们提供的唯一真实文本；我们在第二阶段从书中恢复的任何额外文本都是由生产 LLM 生成并返回的。对于每个生产 LLM，我们探索不同的生成配置：温度、最大响应长度，以及可用时频率惩罚和存在惩罚（第 4 节）。对于第二阶段的一次运行，我们固定生成配置并执行延续循环，直到达到最大查询预算，或者生产 LLM 返回包含拒绝继续或停止短语（例如“THE END”）的响应。 <sup class="ltx_note_mark">2</sup> </font></font></font>
We then concatenate the response from the initial completion probe in Phase 1 with the in-order responses in the Phase 2 continuation loop to produce a long-form generated text, which we evaluate for extraction success (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3" title="3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">然后我们将第一阶段初始完成探测的响应与第二阶段延续循环中的有序响应连接起来，生成长文本内容，并评估提取的成功性（第 3.3 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="338" id="S3.F3.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/x2.png" width="664">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.9.3.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F3.4.2" style="font-size:90%;">Phase 2 of our two-phase procedure.<span class="ltx_text ltx_font_medium" id="S3.F3.4.2.2">
If Phase 1 succeeds (i.e., returns a response with <math alttext="s\geq 0.6" class="ltx_Math" display="inline" id="S3.F3.3.1.1.m1" intent=":literal"><semantics><mrow><mi>s</mi><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s\geq 0.6</annotation></semantics></math>, see Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F2" title="Figure 2 ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>, Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.1</span></a>), we proceed to Phase 2 (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
We similarly illustrate Phase 2 for <span class="ltx_text ltx_font_italic" id="S3.F3.4.2.2.1">Harry Potter and the Sorcerer’s Stone</span>:
we repeatedly query to continue the text, until the LLM responds with a refusal or a stop phrase, or we exhaust a specified query budget.
Phase 2 culminates in a long-form generation that we compare to a corresponding reference book to gauge extraction success using <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S3.F3.4.2.2.m2" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E7" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a>, Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3" title="3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
The prefix in Phase 1 is the <em class="ltx_emph ltx_font_italic" id="S3.F3.4.2.2.2">only</em> ground-truth text that we provide in the entire two-phase procedure;
any additional text that we recover from a book in Phase 2 is generated and returned by the production LLM.
</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 3：我们两阶段流程的第二阶段。如果第一阶段成功（即返回带有 <math intent=":literal" id="S3.F3.3.1.1.m1" display="inline" class="ltx_Math" alttext="s\geq 0.6"><semantics><mrow><mi>s</mi><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s\geq 0.6</annotation></semantics></math> 的响应，见图 2，第 3.1 节），我们将进入第二阶段（第 3.2 节）。我们同样以《哈利·波特与魔法石》为例说明第二阶段：我们反复查询以继续文本，直到 LLM 拒绝或使用停止短语回应，或者我们耗尽指定的查询预算。第二阶段最终生成长文本，我们将其与相应的参考书进行比较，使用 <math intent=":literal" id="S3.F3.4.2.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> （第 3.3 节中的公式 7）来评估提取的成功率。第一阶段中的前缀是整个两阶段流程中我们提供的唯一真实文本；我们在第二阶段从书中恢复的任何额外文本都是由生产 LLM 生成并返回的。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Particulars for long-form extraction from production LLMs.</span> 
Most generally, extraction refers to prompting a model to reproduce memorized training data encoded in its weights (<cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib83" title="Report of the 1st Workshop on Generative AI and Law">2023</a>)</cite>; Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>).
There are various approaches in the memorization literature that satisfy this definition.
However, attempting <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.2">long-form</em> extraction from <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.3">production</em> LLMs differs from most of this prior work.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">从生产 LLMs 中进行长文本提取的细节。最普遍而言，提取是指提示模型重现其权重中编码的记忆训练数据（Cooper 等人（2023）；第 2 节）。记忆文献中有多种方法满足这一定义。然而，尝试从生产 LLMs 中提取长文本与这之前的大部分工作有所不同。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.2">First, as discussed in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>, the most commonly used extraction method—<span class="ltx_text ltx_font_bold" id="S3.SS2.p3.2.1">discoverable extraction</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib44" title="Deduplicating Training Data Makes Language Models Better">2022</a>; Carlini<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib9" title="Extracting training data from large language models">2021</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib107" title="Quantifying Memorization Across Neural Language Models">2023</a>; Hayes<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib41" title="Measuring memorization in language models via probabilistic extraction">2025b</a>; Cooper<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>—is infeasible for production LLMs that are aligned to behave like conversational chatbots.
Discoverable extraction prompts with a sequence of training data (just a prefix <math alttext="{\bm{p}}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1" intent=":literal"><semantics><mi>𝒑</mi><annotation encoding="application/x-tex">{\bm{p}}</annotation></semantics></math>) and checks if the LLM generates the verbatim continuation (the suffix <math alttext="{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2" intent=":literal"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math>) of that training data—i.e., essentially observing if the LLM successfully “completes the sentence” begun in the prompt.
But conversational chatbots do not tend to demonstrate “complete the sentence” behavior.
Therefore, while these models still memorize training data, this type of procedure is generally ineffective for extracting those memorized data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Nasr<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>)</cite>.
We sometimes use a jailbreak in Phase 1 to unlock continuation-like behavior;
this is also why it is surprising that we did not need to jailbreak Gemini 2.5 Pro or Grok 3 to successfully execute the Phase 2 continuation loop.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">首先，如第 2 节所述，最常用的提取方法——可发现提取（Lee 等人，2022；Carlini 等人，2021；2023；Hayes 等人，2025b；Cooper 等人，2025）——对于旨在表现得像对话式聊天机器人的生产 LLM 来说并不可行。可发现提取会使用一个包含训练数据序列的提示（只是一个前缀 <math intent=":literal" id="S3.SS2.p3.1.m1" display="inline" class="ltx_Math" alttext="{\bm{p}}"><semantics><mi>𝒑</mi><annotation encoding="application/x-tex">{\bm{p}}</annotation></semantics></math> ），然后检查 LLM 是否生成了该训练数据的逐字续写（后缀 <math intent=":literal" id="S3.SS2.p3.2.m2" display="inline" class="ltx_Math" alttext="{\bm{t}}"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> ）——即本质上观察 LLM 是否成功“完成了提示中的句子”。但对话式聊天机器人并不倾向于表现出“完成句子”的行为。因此，尽管这些模型仍然记忆训练数据，但这类程序通常对提取这些记忆的数据无效（Nasr 等人，2023）。我们在第 1 阶段有时会使用越狱来解锁类似续写的功能；这也是为什么我们成功执行第 2 阶段续写循环时，不需要对 Gemini 2.5 Pro 或 Grok 3 进行越狱而感到惊讶的原因。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.4">Second, discoverable extraction is predominantly effective for extracting relatively short sequences (typically <math alttext="50" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> tokens, or <math alttext="{\approx}38" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2" intent=":literal"><semantics><mrow><mi></mi><mo>≈</mo><mn>38</mn></mrow><annotation encoding="application/x-tex">{\approx}38</annotation></semantics></math> words), even when much longer sequences are memorized in the model.
For an autoregressive language model, the probability of generating an exact continuation (e.g., a suffix <math alttext="{\bm{t}}" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3" intent=":literal"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math>) conditioned on a prompt (e.g., a prefix <math alttext="{\bm{p}}" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4" intent=":literal"><semantics><mi>𝒑</mi><annotation encoding="application/x-tex">{\bm{p}}</annotation></semantics></math>) decreases as the length of the continuation increases, making long memorized sequences increasingly difficult to extract.
This is why for long-form extraction, as in <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>, we do not attempt to produce the whole book in one interaction, and instead query iteratively to generate a limited length of text that continues the prefix and any text in the context that the LLM has already generated. In practice, in our production LLM setting, limiting the generation length was also important for evading output filters (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4" title="4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">其次，可发现的提取主要适用于提取相对较短的序列（通常为 <math intent=":literal" id="S3.SS2.p4.1.m1" display="inline" class="ltx_Math" alttext="50"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> 个 token，或 <math intent=":literal" id="S3.SS2.p4.2.m2" display="inline" class="ltx_Math" alttext="{\approx}38"><semantics><mrow><mi></mi><mo>≈</mo><mn>38</mn></mrow><annotation encoding="application/x-tex">{\approx}38</annotation></semantics></math> 个词），即使模型中记住了更长的序列。对于自回归语言模型，给定提示（例如前缀 <math intent=":literal" id="S3.SS2.p4.4.m4" display="inline" class="ltx_Math" alttext="{\bm{p}}"><semantics><mi>𝒑</mi><annotation encoding="application/x-tex">{\bm{p}}</annotation></semantics></math> ）生成确切延续（例如后缀 <math intent=":literal" id="S3.SS2.p4.3.m3" display="inline" class="ltx_Math" alttext="{\bm{t}}"><semantics><mi>𝒕</mi><annotation encoding="application/x-tex">{\bm{t}}</annotation></semantics></math> ）的概率随着延续长度的增加而降低，这使得提取长记忆序列变得越来越困难。这就是为什么在长文本提取中，如在 Cooper 等人（2025 年）的研究中，我们不尝试在一次交互中生成整本书，而是迭代查询以生成有限长度的文本，该文本延续前缀以及 LLM 已经生成的任何上下文文本。在实践中，在我们的生产 LLM 环境中，限制生成长度对于规避输出过滤器（第 4 节）也同样重要。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">Third, for production LLMs, users have relatively little control over the decoding procedure, and do not typically have access to logits or <math alttext="\log" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1" intent=":literal"><semantics><mi>log</mi><annotation encoding="application/x-tex">\log</annotation></semantics></math> probabilities.
In contrast, most research on memorization examines controlled settings for open-weight models, where it is possible to study extraction with fine-grained choices about decoding strategy&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib44" title="Deduplicating Training Data Makes Language Models Better">2022</a>; Carlini<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib107" title="Quantifying Memorization Across Neural Language Models">2023</a>)</cite> and make use of logits&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hayes<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib41" title="Measuring memorization in language models via probabilistic extraction">2025b</a>)</cite>.
For instance, in an experiment that extracts <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.1">Harry Potter and the Sorcerer’s Stone</span> from Llama 3.1 70B, <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite> are able to deterministically reproduce the entirety of the book near-verbatim because they can use beam search, which we do not have access to using blackbox APIs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">第三，对于生产 LLMs，用户对解码过程控制相对较少，通常无法访问 logits 或 <math intent=":literal" id="S3.SS2.p5.1.m1" display="inline" class="ltx_Math" alttext="\log"><semantics><mi>log</mi><annotation encoding="application/x-tex">\log</annotation></semantics></math> 概率。相比之下，大多数关于记忆的研究考察开放权重模型的受控环境，其中可以研究提取，并就解码策略做出细粒度选择（Lee 等人，2022；Carlini 等人，2023），并利用 logits（Hayes 等人，2025b）。例如，在一个从 Llama 3.1 70B 中提取《哈利·波特与魔法石》的实验中，Cooper 等人（2025）能够确定性地近乎逐字地重现整本书，因为他们可以使用束搜索，而我们无法通过黑盒 API 访问。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.2">Lastly, standard evaluation metrics for relatively short-form extraction are not applicable to long-form generated outputs.
For discoverable extraction, it is typical to compare the generated continuation and target suffix, and to declare extraction success when there is verbatim equality or the continuation is within a small edit distance to the target&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib44" title="Deduplicating Training Data Makes Language Models Better">2022</a>; Ippolito<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib97" title="Preventing verbatim memorization in language models gives a false sense of privacy">2022</a>)</cite>.
While these success criteria are reasonable for assessing extraction success of <math alttext="50" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math>-token (<math alttext="{\approx}38" class="ltx_Math" display="inline" id="S3.SS2.p6.2.m2" intent=":literal"><semantics><mrow><mi></mi><mo>≈</mo><mn>38</mn></mrow><annotation encoding="application/x-tex">{\approx}38</annotation></semantics></math>-word) sequences, <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite> observe that strict equality is too stringent when extracting (tens of) thousands of tokens.
This was true even in their work, where the long-form generated outputs were almost (but not quite) exact reproductions of reference texts.
In our work, the reproductions are often less exact, so we need to devise a different measurement procedure for claiming extraction success.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">最后，用于相对短文本提取的标准评估指标不适用于长文本生成输出。对于可发现性提取，通常比较生成的延续部分和目标后缀，当存在逐字相等或延续部分与目标在小编辑距离内时，即视为提取成功（Lee 等人，2022；Ippolito 等人，2022）。虽然这些成功标准对于评估 <math intent=":literal" id="S3.SS2.p6.1.m1" display="inline" class="ltx_Math" alttext="50"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> -token（ <math intent=":literal" id="S3.SS2.p6.2.m2" display="inline" class="ltx_Math" alttext="{\approx}38"><semantics><mrow><mi></mi><mo>≈</mo><mn>38</mn></mrow><annotation encoding="application/x-tex">{\approx}38</annotation></semantics></math> -词）序列的提取成功是合理的，但 Cooper 等人（2025）观察到，在提取（数十个）千 token 时，严格相等过于严苛。即使在他们自己的工作中，长文本生成输出几乎是（但并非完全）参考文本的精确复制品，这一点也是真实的。在我们的工作中，复制品通常不够精确，因此我们需要设计不同的测量程序来声称提取成功。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Verifying extraction success<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">3.3 验证提取成功</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In this work, we use extraction metrics that allow for near-verbatim matches to the training data.
At a high level, to be valid evidence for extraction,
the generated text must
(1) reflect a sufficiently near-verbatim reproduction of text in the actual book, and
(2) be sufficiently long, such that memorization is the overwhelmingly most plausible explanation for near-verbatim generation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Carlini<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib9" title="Extracting training data from large language models">2021</a>)</cite>.
We propose a procedure that captures when long-form generated text satisfies these conditions (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>).
We then elaborate on why this procedure enables us to make conservative extraction claims (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>):
it may miss some valid instances of extraction of training data, but importantly should not include short spans of generated text that may coincidentally resemble ground-truth text from a book (i.e., text that is not actually memorized).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在这项工作中，我们使用允许与训练数据近乎逐字匹配的提取指标。从高层次来看，要成为提取的有效证据，生成的文本必须（1）反映实际书籍中文本的足够近乎逐字的再现，并且（2）足够长，以至于记忆是近乎逐字生成的主要原因（Carlini 等人，2021 年）。我们提出了一种程序，用于捕捉长文本生成何时满足这些条件（第 3.3.1 节）。然后，我们详细说明为什么这个程序使我们能够做出保守的提取声明（第 3.3.2 节）：它可能会遗漏一些训练数据的提取有效实例，但重要的是不应包括可能偶然与书籍的真实文本相似生成的短文本片段（即实际上未被记忆的文本）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Identifying near-verbatim extracted text in a long-form generation<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">3.3.1 在长文本生成中识别近乎逐字的提取文本</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Long-span, near-verbatim matching block formation<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">算法 1 长跨度近乎逐字匹配块形成</font></font></font></figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l1.1.1.1" style="font-size:80%;">1:</span></span>Word lists <math alttext="B" class="ltx_Math" display="inline" id="alg1.l1.m1" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> (the book) and <math alttext="G" class="ltx_Math" display="inline" id="alg1.l1.m2" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> (the generated text)

<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">1:单词列表 <math intent=":literal" id="alg1.l1.m1" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> （书籍）和 <math intent=":literal" id="alg1.l1.m2" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> （生成的文本）</font></font></font></div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l2.1.1.1" style="font-size:80%;">2:</span></span>Thresholds <math alttext="\tau^{(1)}_{\mathrm{gap}},\tau^{(1)}_{\mathrm{align}}" class="ltx_Math" display="inline" id="alg1.l2.m1" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{gap}},\tau^{(1)}_{\mathrm{align}}</annotation></semantics></math> (merge&nbsp;1),
<math alttext="\tau^{(2)}_{\mathrm{gap}},\tau^{(2)}_{\mathrm{align}}" class="ltx_Math" display="inline" id="alg1.l2.m2" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{gap}},\tau^{(2)}_{\mathrm{align}}</annotation></semantics></math> (merge&nbsp;2); minimum lengths <math alttext="l^{(1)}" class="ltx_Math" display="inline" id="alg1.l2.m3" intent=":literal"><semantics><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">l^{(1)}</annotation></semantics></math> (filter&nbsp;1), <math alttext="l^{(2)}" class="ltx_Math" display="inline" id="alg1.l2.m4" intent=":literal"><semantics><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">l^{(2)}</annotation></semantics></math> (filter&nbsp;2)

<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">2:阈值 <math intent=":literal" id="alg1.l2.m1" display="inline" class="ltx_Math" alttext="\tau^{(1)}_{\mathrm{gap}},\tau^{(1)}_{\mathrm{align}}"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{gap}},\tau^{(1)}_{\mathrm{align}}</annotation></semantics></math> （合并 1）， <math intent=":literal" id="alg1.l2.m2" display="inline" class="ltx_Math" alttext="\tau^{(2)}_{\mathrm{gap}},\tau^{(2)}_{\mathrm{align}}"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{gap}},\tau^{(2)}_{\mathrm{align}}</annotation></semantics></math> （合并 2）；最小长度 <math intent=":literal" id="alg1.l2.m3" display="inline" class="ltx_Math" alttext="l^{(1)}"><semantics><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">l^{(1)}</annotation></semantics></math> （过滤 1）， <math intent=":literal" id="alg1.l2.m4" display="inline" class="ltx_Math" alttext="l^{(2)}"><semantics><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">l^{(2)}</annotation></semantics></math> （过滤 2） </font></font></font></div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l3.2.1.1" style="font-size:80%;">3:</span></span><math alttext="{\mathbb{B}}^{\text{(base)}}\leftarrow\mathsf{greedy\_approx\_longest}(B,G)" class="ltx_Math" display="inline" id="alg1.l3.m1" intent=":literal"><semantics><mrow><msup><mi>𝔹</mi><mtext>(base)</mtext></msup><mo stretchy="false">←</mo><mrow><mi>𝗀𝗋𝖾𝖾𝖽𝗒</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">_</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝖺𝗉𝗉𝗋𝗈𝗑</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">_</mi><mo lspace="0em" rspace="0em">​</mo><mi>𝗅𝗈𝗇𝗀𝖾𝗌𝗍</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{\mathbb{B}}^{\text{(base)}}\leftarrow\mathsf{greedy\_approx\_longest}(B,G)</annotation></semantics></math>
<span class="ltx_text" id="alg1.l3.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l3.1.m1" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l3.1.1">identify</span>: compute verbatim matching blocks (Eq.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E3" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3</span></a>)
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"> <math intent=":literal" id="alg1.l3.1.m1" display="inline" class="ltx_Math" alttext="\triangleright"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> 识别：计算逐字匹配块（公式 3） </font></font></font></span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l4.2.1.1" style="font-size:80%;">4:</span></span><math alttext="\widetilde{{\mathbb{B}}}^{(1)}\leftarrow\mathsf{merge}({\mathbb{B}}^{\text{(base)}},\;\tau^{(1)}_{\mathrm{gap}},\;\tau^{(1)}_{\mathrm{align}})" class="ltx_Math" display="inline" id="alg1.l4.m1" intent=":literal"><semantics><mrow><msup><mover accent="true"><mi>𝔹</mi><mo>~</mo></mover><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">←</mo><mrow><mi>𝗆𝖾𝗋𝗀𝖾</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝔹</mi><mtext>(base)</mtext></msup><mo rspace="0.447em">,</mo><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo rspace="0.447em">,</mo><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\widetilde{{\mathbb{B}}}^{(1)}\leftarrow\mathsf{merge}({\mathbb{B}}^{\text{(base)}},\;\tau^{(1)}_{\mathrm{gap}},\;\tau^{(1)}_{\mathrm{align}})</annotation></semantics></math>
<span class="ltx_text" id="alg1.l4.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l4.1.m1" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l4.1.1">merge&nbsp;1</span>: stitch very short gaps (Eq.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E4" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>)
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"> <math intent=":literal" id="alg1.l4.1.m1" display="inline" class="ltx_Math" alttext="\triangleright"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> 合并 1：拼接非常短的间隙（公式 4） </font></font></font></span>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l5.2.1.1" style="font-size:80%;">5:</span></span><math alttext="\widetilde{{\mathbb{B}}}^{(1)}_{\geq l^{(1)}}\leftarrow\mathsf{filter}(\widetilde{{\mathbb{B}}}^{(1)},\;l^{(1)})" class="ltx_Math" display="inline" id="alg1.l5.m1" intent=":literal"><semantics><mrow><msubsup><mover accent="true"><mi>𝔹</mi><mo>~</mo></mover><mrow><mi></mi><mo>≥</mo><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">←</mo><mrow><mi>𝖿𝗂𝗅𝗍𝖾𝗋</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mover accent="true"><mi>𝔹</mi><mo>~</mo></mover><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo rspace="0.447em">,</mo><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\widetilde{{\mathbb{B}}}^{(1)}_{\geq l^{(1)}}\leftarrow\mathsf{filter}(\widetilde{{\mathbb{B}}}^{(1)},\;l^{(1)})</annotation></semantics></math>
<span class="ltx_text" id="alg1.l5.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l5.1.m1" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l5.1.1">filter&nbsp;1</span>: remove short blocks (Eq.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E5" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>)
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"> <math intent=":literal" id="alg1.l5.1.m1" display="inline" class="ltx_Math" alttext="\triangleright"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> 过滤 1：移除短块（公式 5）</font></font></font></span>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l6.2.1.1" style="font-size:80%;">6:</span></span><math alttext="\widetilde{{\mathbb{B}}}^{(2)}\leftarrow\mathsf{merge}(\widetilde{{\mathbb{B}}}^{(1)}_{\geq l^{(1)}},\;\tau^{(2)}_{\mathrm{gap}},\;\tau^{(2)}_{\mathrm{align}})" class="ltx_Math" display="inline" id="alg1.l6.m1" intent=":literal"><semantics><mrow><msup><mover accent="true"><mi>𝔹</mi><mo>~</mo></mover><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">←</mo><mrow><mi>𝗆𝖾𝗋𝗀𝖾</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>𝔹</mi><mo>~</mo></mover><mrow><mi></mi><mo>≥</mo><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo rspace="0.447em">,</mo><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo rspace="0.447em">,</mo><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\widetilde{{\mathbb{B}}}^{(2)}\leftarrow\mathsf{merge}(\widetilde{{\mathbb{B}}}^{(1)}_{\geq l^{(1)}},\;\tau^{(2)}_{\mathrm{gap}},\;\tau^{(2)}_{\mathrm{align}})</annotation></semantics></math>
<span class="ltx_text" id="alg1.l6.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l6.1.m1" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l6.1.1">merge&nbsp;2</span>: passage-level consolidation (Eq.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E4" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>)
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"> <math intent=":literal" id="alg1.l6.1.m1" display="inline" class="ltx_Math" alttext="\triangleright"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> 合并 2：段落级整合（公式 4）</font></font></font></span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l7.4.1.1" style="font-size:80%;">7:</span></span><math alttext="\widetilde{{\mathbb{B}}}^{(2)}_{\geq l^{(2)}}\leftarrow\mathsf{filter}(\widetilde{{\mathbb{B}}}^{(2)},\;l^{(2)})" class="ltx_Math" display="inline" id="alg1.l7.m1" intent=":literal"><semantics><mrow><msubsup><mover accent="true"><mi>𝔹</mi><mo>~</mo></mover><mrow><mi></mi><mo>≥</mo><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">←</mo><mrow><mi>𝖿𝗂𝗅𝗍𝖾𝗋</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mover accent="true"><mi>𝔹</mi><mo>~</mo></mover><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo rspace="0.447em">,</mo><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\widetilde{{\mathbb{B}}}^{(2)}_{\geq l^{(2)}}\leftarrow\mathsf{filter}(\widetilde{{\mathbb{B}}}^{(2)},\;l^{(2)})</annotation></semantics></math>
<span class="ltx_text" id="alg1.l7.3" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l7.1.m1" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l7.3.2">filter&nbsp;2</span>: retain long blocks (Eq.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E5" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>)
<span class="ltx_text ltx_font_bold" id="alg1.l7.3.3">return</span> <math alttext="{\mathbb{B}}^{*}:=\widetilde{{\mathbb{B}}}^{(2)}_{\geq l^{(2)}}" class="ltx_Math" display="inline" id="alg1.l7.2.m2" intent=":literal"><semantics><mrow><msup><mi>𝔹</mi><mo>∗</mo></msup><mo>:=</mo><msubsup><mover accent="true"><mi>𝔹</mi><mo>~</mo></mover><mrow><mi></mi><mo>≥</mo><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">{\mathbb{B}}^{*}:=\widetilde{{\mathbb{B}}}^{(2)}_{\geq l^{(2)}}</annotation></semantics></math><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"> <math intent=":literal" id="alg1.l7.1.m1" display="inline" class="ltx_Math" alttext="\triangleright"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> 过滤器 2：保留长块（等式 5）返回 <math intent=":literal" id="alg1.l7.2.m2" display="inline" class="ltx_Math" alttext="{\mathbb{B}}^{*}:=\widetilde{{\mathbb{B}}}^{(2)}_{\geq l^{(2)}}"><semantics><mrow><msup><mi>𝔹</mi><mo>∗</mo></msup><mo>:=</mo><msubsup><mover accent="true"><mi>𝔹</mi><mo>~</mo></mover><mrow><mi></mi><mo>≥</mo><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">{\mathbb{B}}^{*}:=\widetilde{{\mathbb{B}}}^{(2)}_{\geq l^{(2)}}</annotation></semantics></math> </font></font></font> <span class="ltx_text" id="alg1.l7.3.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l7.3.1.m1" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> final ordered set of long near-verbatim matching blocks
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"> <math intent=":literal" id="alg1.l7.3.1.m1" display="inline" class="ltx_Math" alttext="\triangleright"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> 最终的有序长近逐字匹配块集合 </font></font></font></span></span>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Long-form similarity detection is a notoriously challenging problem, with an active, longstanding body of research&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hoad and Zobel, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib124" title="Methods for Identifying Versioned and Plagiarized Documents">2003</a>; Henzinger, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib121" title="Finding near-duplicate web pages: a large-scale evaluation of algorithms">2006</a>; Santos<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib122" title="Structural alignment of plain text books">2012</a>; Wang and Dong, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib123" title="Measurement of Text Similarity: A Survey">2020</a>)</cite>.
We draw from this work, and propose a variation on existing methods to identify long spans of near-verbatim text that reflect successful extraction.
We summarize this procedure in Algorithm&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#alg1" title="Algorithm 1 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>, and discuss each step in detail below.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">长文本相似性检测是一个众所周知具有挑战性的问题，拥有活跃且长期的研究领域（Hoad 和 Zobel，2003；Henzinger，2006；Santos 等人，2012；Wang 和 Dong，2020）。我们借鉴了这些研究，并提出了一种现有方法的变体，用于识别反映成功提取的长近逐字文本片段。我们将在算法 1 中总结这一过程，并在下方详细讨论每个步骤。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.3">Following&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>, we begin with an algorithm that produces a <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.3.1">greedy approximation of longest common substring</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(difflib SequenceMatcher, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib119" title="difflib — Helpers for computing deltas">2025</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The experiments in <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite> produce deterministic, nearly exact long-form reproductions in generated outputs, and so <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite> can run this algorithm without modifications on whole documents for extraction claims.
Our experimental outputs are almost always less exact, so it would be invalid to reuse their procedure as-is here.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">Cooper 等人（2025）的实验在生成输出中产生确定性、近乎精确的长文本复制，因此 Cooper 等人（2025）可以在整个文档上运行此算法，用于提取声明，而无需修改。我们的实验输出几乎总是不够精确，因此直接照搬他们的程序在此处是不合理的。</font></font></font></span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">根据 Cooper 等人（2025）的研究，我们首先使用一个算法来生成最长公共子串的贪婪近似（difflib SequenceMatcher，2025）。 <sup class="ltx_note_mark">3</sup> </font></font></font>
In contrast to the Phase&nbsp;1 <math alttext="\mathsf{longest}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.1.m1" intent=":literal"><semantics><mi>𝗅𝗈𝗇𝗀𝖾𝗌𝗍</mi><annotation encoding="application/x-tex">\mathsf{longest}</annotation></semantics></math> metric (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E1" title="In 3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>), which returns the length of the <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p2.3.2">single</em> longest contiguous verbatim subsequence shared by two input lists, this algorithm <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.3.3">identifies</span> and returns an <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p2.3.4">ordered set</em> of all contiguous verbatim matching blocks shared by two input lists—in our case, lists of whitespace-delimited <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.3.5">words</span> from book <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.2.m2" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> and generated text <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.3.m3" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>.
This greedy block-matching procedure may fragment a single passage into multiple blocks due to minor discrepancies, such as short formatting differences, insertions, or deletions (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf1" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(a)</span></a>).
To better capture long-form passage recovery, we process the ordered set of verbatim blocks:
we iteratively <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.3.6">merge</span> well-aligned, nearby blocks to form longer <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p2.3.7">near-verbatim</em> blocks, and then <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.3.8">filter</span> these blocks to retain only those that exceed a minimum specified length, so that each retained block is sufficiently long to support an extraction claim.
Below, we describe each of the three steps (identify, merge, and filter), how we compose them in practice, and how we use the resulting near-verbatim blocks to report different information about extraction.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">与第一阶段 <math intent=":literal" id="S3.SS3.SSS1.p2.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{longest}"><semantics><mi>𝗅𝗈𝗇𝗀𝖾𝗌𝗍</mi><annotation encoding="application/x-tex">\mathsf{longest}</annotation></semantics></math> 指标（公式 1）不同，该指标返回两个输入列表中共享的最长连续逐字子序列的长度，此算法识别并返回两个输入列表中所有连续逐字匹配块的有序集合——在我们的案例中，即来自书籍 <math intent=":literal" id="S3.SS3.SSS1.p2.2.m2" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 和生成文本 <math intent=":literal" id="S3.SS3.SSS1.p2.3.m3" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 的空格分隔单词列表。这种贪婪的块匹配过程可能由于细微差异（如短格式差异、插入或删除）将单个段落分割成多个块（图 4(a)）。为了更好地捕捉长文本段落恢复，我们处理逐字块的有序集合：我们迭代地合并对齐良好且相邻的块以形成更长的近似逐字块，然后过滤这些块以仅保留那些超过指定最小长度的块，以便每个保留的块足够长以支持提取声明。下面，我们描述每个步骤（识别、合并和过滤），我们如何在实践中组合它们，以及我们如何使用生成的近似逐字块来报告有关提取的不同信息。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="508" id="S3.F4.sf1.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/x3.png" width="899">
<figcaption class="ltx_caption ltx_centering" data-imt_insert_failed="1"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F4.sf1.4.2" style="font-size:90%;">Claude 3.7 Sonnet, <span class="ltx_text ltx_font_italic" id="S3.F4.sf1.4.2.1">Frankenstein</span> </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="421" id="S3.F4.sf2.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/x4.png" width="900">
<figcaption class="ltx_caption ltx_centering" data-imt_insert_failed="1"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F4.sf2.4.2" style="font-size:90%;">Gemini 2.5 Pro, <span class="ltx_text ltx_font_italic" id="S3.F4.sf2.4.2.1">The Da Vinci Code</span> </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.19.9.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F4.16.8" style="font-size:90%;">Near-verbatim block formation.<span class="ltx_text ltx_font_medium" id="S3.F4.16.8.8">
After identifying verbatim blocks, we merge closely aligned, nearby blocks (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E4" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>).
In both subfigures, the blocks are aligned (<math alttext="|\Delta^{(B)}_{k}-\Delta^{(G)}_{k}|=0" class="ltx_Math" display="inline" id="S3.F4.9.1.1.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><mrow><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">|</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">|\Delta^{(B)}_{k}-\Delta^{(G)}_{k}|=0</annotation></semantics></math>).
The first merge (M1) is very stringent, with a maximum gap <math alttext="\tau_{\mathrm{gap}}^{(1)}=2" class="ltx_Math" display="inline" id="S3.F4.10.2.2.m2" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\tau_{\mathrm{gap}}^{(1)}=2</annotation></semantics></math> words, and then filter 1 (F1) only retains blocks that are at least <math alttext="20" class="ltx_Math" display="inline" id="S3.F4.11.3.3.m3" intent=":literal"><semantics><mn>20</mn><annotation encoding="application/x-tex">20</annotation></semantics></math> words long (<math alttext="l^{(1)}=20" class="ltx_Math" display="inline" id="S3.F4.12.4.4.m4" intent=":literal"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">l^{(1)}=20</annotation></semantics></math>).
The second merge (M2), performed on the blocks retained after F1, is slightly more relaxed (<math alttext="\tau_{\mathrm{gap}}^{(2)}=10" class="ltx_Math" display="inline" id="S3.F4.13.5.5.m5" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\tau_{\mathrm{gap}}^{(2)}=10</annotation></semantics></math>), and so the second filter is more stringent (<math alttext="l^{(2)}=100" class="ltx_Math" display="inline" id="S3.F4.14.6.6.m6" intent=":literal"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">l^{(2)}=100</annotation></semantics></math>).
In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf1" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(a)</span></a>, M1 merges very close blocks.
The remaining blocks—block 1, block 2* (=block 2 + block 3 + block 4 + block 5), and block 6—are each long enough to be retained by F1 (but note that they would not at this point be retained by F2).
These blocks are merged in M2, resulting in a single <math alttext="141" class="ltx_Math" display="inline" id="S3.F4.15.7.7.m7" intent=":literal"><semantics><mn>141</mn><annotation encoding="application/x-tex">141</annotation></semantics></math>-word block that is retained after F2.
In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf2" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(b)</span></a>, no blocks are retained.
There are verbatim-matching blocks returned by the identify step, but they are too short to be valid evidence for extraction.
Our two-pass merge-and-filter procedure removes them;
they are not counted in our extraction metric, <math alttext="m" class="ltx_Math" display="inline" id="S3.F4.16.8.8.m8" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E6" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">6</span></a>).
See Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A2" title="Appendix B Procedure for quantifying extraction success ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">B</span></a> for more details.
</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 4：近乎逐字复制的块形成。在识别出逐字复制块后，我们将紧密对齐的邻近块合并（公式 4）。在两个子图中，块都是对齐的（ <math intent=":literal" id="S3.F4.9.1.1.m1" display="inline" class="ltx_Math" alttext="|\Delta^{(B)}_{k}-\Delta^{(G)}_{k}|=0"><semantics><mrow><mrow><mo stretchy="false">|</mo><mrow><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">|</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">|\Delta^{(B)}_{k}-\Delta^{(G)}_{k}|=0</annotation></semantics></math> ）。第一次合并（M1）非常严格，最大间隙为 <math intent=":literal" id="S3.F4.10.2.2.m2" display="inline" class="ltx_Math" alttext="\tau_{\mathrm{gap}}^{(1)}=2"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\tau_{\mathrm{gap}}^{(1)}=2</annotation></semantics></math> 个词，然后过滤器 1（F1）只保留长度至少为 <math intent=":literal" id="S3.F4.11.3.3.m3" display="inline" class="ltx_Math" alttext="20"><semantics><mn>20</mn><annotation encoding="application/x-tex">20</annotation></semantics></math> 个词的块（ <math intent=":literal" id="S3.F4.12.4.4.m4" display="inline" class="ltx_Math" alttext="l^{(1)}=20"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">l^{(1)}=20</annotation></semantics></math> ）。第二次合并（M2）在 F1 保留的块上执行，稍微宽松一些（ <math intent=":literal" id="S3.F4.13.5.5.m5" display="inline" class="ltx_Math" alttext="\tau_{\mathrm{gap}}^{(2)}=10"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\tau_{\mathrm{gap}}^{(2)}=10</annotation></semantics></math> ），因此第二次过滤器更严格（ <math intent=":literal" id="S3.F4.14.6.6.m6" display="inline" class="ltx_Math" alttext="l^{(2)}=100"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">l^{(2)}=100</annotation></semantics></math> ）。在图 4(a)中，M1 合并了非常邻近的块。剩下的块——块 1、块 2*（=块 2+块 3+块 4+块 5）和块 6——每个都足够长，可以被 F1 保留（但请注意，此时它们不会被 F2 保留）。这些块在 M2 中被合并，形成了一个 <math intent=":literal" id="S3.F4.15.7.7.m7" display="inline" class="ltx_Math" alttext="141"><semantics><mn>141</mn><annotation encoding="application/x-tex">141</annotation></semantics></math> 个词的块，该块在 F2 后保留。在图 4(b)中，没有块被保留。识别步骤返回了一些逐字匹配的块，但它们太短，不能作为有效的提取证据。我们的两步合并和过滤程序去除了它们；它们不计入我们的提取指标， <math intent=":literal" id="S3.F4.16.8.8.m8" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> （公式 6）。更多详情请参见附录 B。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.5"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p3.5.1">Identify verbatim blocks.</span> 
Given two lightly normalized texts <math alttext="{\bm{b}}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.1.m1" intent=":literal"><semantics><mi>𝒃</mi><annotation encoding="application/x-tex">{\bm{b}}</annotation></semantics></math> (the reference book) and <math alttext="{\bm{g}}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.2.m2" intent=":literal"><semantics><mi>𝒈</mi><annotation encoding="application/x-tex">{\bm{g}}</annotation></semantics></math> (the generated text), we split each on whitespace characters to obtain ordered lists of words
<math alttext="B=(w^{({\bm{b}})}_{1},\dots,w^{({\bm{b}})}_{|B|})\text{ and }G=(w^{({\bm{g}})}_{1},\dots,w^{({\bm{g}})}_{|G|})" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.3.m3" intent=":literal"><semantics><mrow><mi>B</mi><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>𝒃</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">(</mo><mi>𝒃</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mtext>&nbsp;and&nbsp;</mtext><mo lspace="0em" rspace="0em">​</mo><mi>G</mi></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>𝒈</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><mrow><mo stretchy="false">|</mo><mi>G</mi><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">(</mo><mi>𝒈</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">B=(w^{({\bm{b}})}_{1},\dots,w^{({\bm{b}})}_{|B|})\text{ and }G=(w^{({\bm{g}})}_{1},\dots,w^{({\bm{g}})}_{|G|})</annotation></semantics></math>.
We then find verbatim matching blocks by greedily locating the longest substring of words shared by <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.4.m4" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> and <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.5.m5" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>, and recursively repeating the search on the unmatched regions to the left and right&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(difflib SequenceMatcher, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib119" title="difflib — Helpers for computing deltas">2025</a>)</cite>.
This produces an ordered set of verbatim-matching blocks
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">识别逐字块。给定两个轻度规范化的文本 <math intent=":literal" id="S3.SS3.SSS1.p3.1.m1" display="inline" class="ltx_Math" alttext="{\bm{b}}"><semantics><mi>𝒃</mi><annotation encoding="application/x-tex">{\bm{b}}</annotation></semantics></math> （参考书）和 <math intent=":literal" id="S3.SS3.SSS1.p3.2.m2" display="inline" class="ltx_Math" alttext="{\bm{g}}"><semantics><mi>𝒈</mi><annotation encoding="application/x-tex">{\bm{g}}</annotation></semantics></math> （生成的文本），我们将每个文本按空白字符分割，得到有序的单词列表 <math intent=":literal" id="S3.SS3.SSS1.p3.3.m3" display="inline" class="ltx_Math" alttext="B=(w^{({\bm{b}})}_{1},\dots,w^{({\bm{b}})}_{|B|})\text{ and }G=(w^{({\bm{g}})}_{1},\dots,w^{({\bm{g}})}_{|G|})"><semantics><mrow><mi>B</mi><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>𝒃</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">(</mo><mi>𝒃</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo rspace="0em" lspace="0em">​</mo><mtext>&nbsp;and&nbsp;</mtext><mo rspace="0em" lspace="0em">​</mo><mi>G</mi></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>𝒈</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><mrow><mo stretchy="false">|</mo><mi>G</mi><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">(</mo><mi>𝒈</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">B=(w^{({\bm{b}})}_{1},\dots,w^{({\bm{b}})}_{|B|})\text{ and }G=(w^{({\bm{g}})}_{1},\dots,w^{({\bm{g}})}_{|G|})</annotation></semantics></math> 。然后通过贪婪地定位 <math intent=":literal" id="S3.SS3.SSS1.p3.4.m4" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p3.5.m5" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 共享的最长单词子串来找到逐字匹配块，并对左侧和右侧的不匹配区域递归地重复搜索（difflib SequenceMatcher，2025）。这产生了一个有序的逐字匹配块集合。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx3">
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle{\mathbb{B}}^{\text{(base)}}(B,G)=\{\beta_{k}\}_{k=1}^{K},\qquad\text{with each }\beta_{k}\triangleq(i_{k},j_{k},m_{k})," class="ltx_Math" display="inline" id="S3.E3.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><msup><mi>𝔹</mi><mtext>(base)</mtext></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>β</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mtext>with each&nbsp;</mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi>β</mi><mi>k</mi></msub></mrow><mo>≜</mo><mrow><mo stretchy="false">(</mo><msub><mi>i</mi><mi>k</mi></msub><mo>,</mo><msub><mi>j</mi><mi>k</mi></msub><mo>,</mo><msub><mi>m</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle{\mathbb{B}}^{\text{(base)}}(B,G)=\{\beta_{k}\}_{k=1}^{K},\qquad\text{with each }\beta_{k}\triangleq(i_{k},j_{k},m_{k}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p3.20">where block <math alttext="\beta_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.6.m1" intent=":literal"><semantics><msub><mi>β</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\beta_{k}</annotation></semantics></math> is defined by:
(i) a starting index <math alttext="i_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.7.m2" intent=":literal"><semantics><msub><mi>i</mi><mi>k</mi></msub><annotation encoding="application/x-tex">i_{k}</annotation></semantics></math> in <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.8.m3" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>,
(ii) a starting index <math alttext="j_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.9.m4" intent=":literal"><semantics><msub><mi>j</mi><mi>k</mi></msub><annotation encoding="application/x-tex">j_{k}</annotation></semantics></math> in <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.10.m5" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>, and
(iii) a length <math alttext="m_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.11.m6" intent=":literal"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding="application/x-tex">m_{k}</annotation></semantics></math>, measured in words.
Each block <math alttext="\beta_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.12.m7" intent=":literal"><semantics><msub><mi>β</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\beta_{k}</annotation></semantics></math> satisfies
<math alttext="B[i_{k}:i_{k}+m_{k}]=G[j_{k}:j_{k}+m_{k}]" class="ltx_math_unparsed" display="inline" id="S3.SS3.SSS1.p3.13.m8" intent=":literal"><semantics><mrow><mi>B</mi><mrow><mo stretchy="false">[</mo><msub><mi>i</mi><mi>k</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><msub><mi>i</mi><mi>k</mi></msub><mo>+</mo><msub><mi>m</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow><mo>=</mo><mi>G</mi><mrow><mo stretchy="false">[</mo><msub><mi>j</mi><mi>k</mi></msub><mo lspace="0.278em" rspace="0.278em">:</mo><msub><mi>j</mi><mi>k</mi></msub><mo>+</mo><msub><mi>m</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">B[i_{k}:i_{k}+m_{k}]=G[j_{k}:j_{k}+m_{k}]</annotation></semantics></math>
exactly, and has equal verbatim length <math alttext="m_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.14.m9" intent=":literal"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding="application/x-tex">m_{k}</annotation></semantics></math> in both <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.15.m10" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> and <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.16.m11" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>. (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4" title="Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>).
Each region of the reference book text can be included in at most one block.
Therefore, starting with this identification procedure means that we capture <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p3.20.1">unique</em> instances of extraction;
we do not count repeated extraction of the same passage if it appears in the generated text multiple times.
Further, this greedy matching procedure induces a monotone alignment between <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.17.m12" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> and <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.18.m13" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>, so the resulting blocks are <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p3.20.2">ordered</em> consistently in both texts.
As a result, verbatim-matching text that appears <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p3.20.3">out-of-order</em> in <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.19.m14" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> with respect to <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.20.m15" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> may not be matched to a block—i.e., may be missed by this identification procedure.
We only merge adjacent blocks and filtering preserves block order, so monotonicity (and thus consistent block ordering) is maintained throughout all merge and filter steps.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">块 <math intent=":literal" id="S3.SS3.SSS1.p3.6.m1" display="inline" class="ltx_Math" alttext="\beta_{k}"><semantics><msub><mi>β</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\beta_{k}</annotation></semantics></math> 的定义如下：(i) 在 <math intent=":literal" id="S3.SS3.SSS1.p3.8.m3" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 中的起始索引 <math intent=":literal" id="S3.SS3.SSS1.p3.7.m2" display="inline" class="ltx_Math" alttext="i_{k}"><semantics><msub><mi>i</mi><mi>k</mi></msub><annotation encoding="application/x-tex">i_{k}</annotation></semantics></math> ，(ii) 在 <math intent=":literal" id="S3.SS3.SSS1.p3.10.m5" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中的起始索引 <math intent=":literal" id="S3.SS3.SSS1.p3.9.m4" display="inline" class="ltx_Math" alttext="j_{k}"><semantics><msub><mi>j</mi><mi>k</mi></msub><annotation encoding="application/x-tex">j_{k}</annotation></semantics></math> ，以及 (iii) 以词为单位的长度 <math intent=":literal" id="S3.SS3.SSS1.p3.11.m6" display="inline" class="ltx_Math" alttext="m_{k}"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding="application/x-tex">m_{k}</annotation></semantics></math> 。每个块 <math intent=":literal" id="S3.SS3.SSS1.p3.12.m7" display="inline" class="ltx_Math" alttext="\beta_{k}"><semantics><msub><mi>β</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\beta_{k}</annotation></semantics></math> 满足 <math intent=":literal" id="S3.SS3.SSS1.p3.13.m8" display="inline" class="ltx_math_unparsed" alttext="B[i_{k}:i_{k}+m_{k}]=G[j_{k}:j_{k}+m_{k}]"><semantics><mrow><mi>B</mi><mrow><mo stretchy="false">[</mo><msub><mi>i</mi><mi>k</mi></msub><mo rspace="0.278em" lspace="0.278em">:</mo><msub><mi>i</mi><mi>k</mi></msub><mo>+</mo><msub><mi>m</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow><mo>=</mo><mi>G</mi><mrow><mo stretchy="false">[</mo><msub><mi>j</mi><mi>k</mi></msub><mo rspace="0.278em" lspace="0.278em">:</mo><msub><mi>j</mi><mi>k</mi></msub><mo>+</mo><msub><mi>m</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">B[i_{k}:i_{k}+m_{k}]=G[j_{k}:j_{k}+m_{k}]</annotation></semantics></math> ，并且在 <math intent=":literal" id="S3.SS3.SSS1.p3.15.m10" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p3.16.m11" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中具有相同的逐字长度 <math intent=":literal" id="S3.SS3.SSS1.p3.14.m9" display="inline" class="ltx_Math" alttext="m_{k}"><semantics><msub><mi>m</mi><mi>k</mi></msub><annotation encoding="application/x-tex">m_{k}</annotation></semantics></math> （图 4）。参考书文本的每个区域最多只能包含在一个块中。因此，从这种识别过程开始意味着我们捕获了提取的唯一实例；如果同一段落多次出现在生成文本中，我们不会重复计算其提取。此外，这种贪婪匹配过程在 <math intent=":literal" id="S3.SS3.SSS1.p3.17.m12" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p3.18.m13" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 之间引入了单调对齐，因此生成的块在两种文本中都是有序的。结果，相对于 <math intent=":literal" id="S3.SS3.SSS1.p3.20.m15" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 在 <math intent=":literal" id="S3.SS3.SSS1.p3.19.m14" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中顺序错乱的逐字匹配文本可能无法匹配到某个块——即可能被这种识别过程遗漏。我们仅合并相邻的块，且过滤操作保留了块的顺序，因此单调性（以及由此产生的块的一致性排序）在整个合并和过滤步骤中都得以保持。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p4">
<p class="ltx_p" id="S3.SS3.SSS1.p4.8"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p4.8.1">Merge blocks.</span> 
Let <math alttext="\beta_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.1.m1" intent=":literal"><semantics><msub><mi>β</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\beta_{k}</annotation></semantics></math> and <math alttext="\beta_{k{+}1}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.2.m2" intent=":literal"><semantics><msub><mi>β</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\beta_{k{+}1}</annotation></semantics></math> be consecutive blocks in an ordered set <math alttext="{\mathbb{B}}(B,G)" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.3.m3" intent=":literal"><semantics><mrow><mi>𝔹</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\mathbb{B}}(B,G)</annotation></semantics></math>.
We define the inter-block gaps
<math alttext="\Delta^{(B)}_{k}\triangleq i_{k+1}-(i_{k}+m_{k})\text{ and }\Delta^{(G)}_{k}\triangleq j_{k+1}-(j_{k}+m_{k})" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.4.m4" intent=":literal"><semantics><mrow><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>≜</mo><mrow><msub><mi>i</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>i</mi><mi>k</mi></msub><mo>+</mo><msub><mi>m</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mtext>&nbsp;and&nbsp;</mtext><mo lspace="0em" rspace="0em">​</mo><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow><mo>≜</mo><mrow><msub><mi>j</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>j</mi><mi>k</mi></msub><mo>+</mo><msub><mi>m</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Delta^{(B)}_{k}\triangleq i_{k+1}-(i_{k}+m_{k})\text{ and }\Delta^{(G)}_{k}\triangleq j_{k+1}-(j_{k}+m_{k})</annotation></semantics></math>,
which measure the number of unmatched words between the two blocks in <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.5.m5" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> and <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.6.m6" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>, respectively.
We merge blocks <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.7.m7" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> and <math alttext="k{+}1" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.8.m8" intent=":literal"><semantics><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k{+}1</annotation></semantics></math> if the following conditions hold:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">合并块。设 <math intent=":literal" id="S3.SS3.SSS1.p4.1.m1" display="inline" class="ltx_Math" alttext="\beta_{k}"><semantics><msub><mi>β</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\beta_{k}</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p4.2.m2" display="inline" class="ltx_Math" alttext="\beta_{k{+}1}"><semantics><msub><mi>β</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\beta_{k{+}1}</annotation></semantics></math> 是有序集合 <math intent=":literal" id="S3.SS3.SSS1.p4.3.m3" display="inline" class="ltx_Math" alttext="{\mathbb{B}}(B,G)"><semantics><mrow><mi>𝔹</mi><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\mathbb{B}}(B,G)</annotation></semantics></math> 中的连续块。我们定义块间间隙 <math intent=":literal" id="S3.SS3.SSS1.p4.4.m4" display="inline" class="ltx_Math" alttext="\Delta^{(B)}_{k}\triangleq i_{k+1}-(i_{k}+m_{k})\text{ and }\Delta^{(G)}_{k}\triangleq j_{k+1}-(j_{k}+m_{k})"><semantics><mrow><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>≜</mo><mrow><msub><mi>i</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msub><mi>i</mi><mi>k</mi></msub><mo>+</mo><msub><mi>m</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo rspace="0em" lspace="0em">​</mo><mtext>&nbsp;and&nbsp;</mtext><mo rspace="0em" lspace="0em">​</mo><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mrow><mo>≜</mo><mrow><msub><mi>j</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>−</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>j</mi><mi>k</mi></msub><mo>+</mo><msub><mi>m</mi><mi>k</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Delta^{(B)}_{k}\triangleq i_{k+1}-(i_{k}+m_{k})\text{ and }\Delta^{(G)}_{k}\triangleq j_{k+1}-(j_{k}+m_{k})</annotation></semantics></math> ，分别测量 <math intent=":literal" id="S3.SS3.SSS1.p4.5.m5" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p4.6.m6" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中两个块之间未匹配单词的数量。如果满足以下条件，我们将合并块 <math intent=":literal" id="S3.SS3.SSS1.p4.7.m7" display="inline" class="ltx_Math" alttext="k"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p4.8.m8" display="inline" class="ltx_Math" alttext="k{+}1"><semantics><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k{+}1</annotation></semantics></math> ：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx4">
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\max(\Delta^{(B)}_{k},\Delta^{(G)}_{k})\leq\tau_{\mathrm{gap}}\quad\textbf{(proximity)}\quad\text{and}\quad\left|\Delta^{(B)}_{k}-\Delta^{(G)}_{k}\right|\leq\tau_{\mathrm{align}}\quad\textbf{(text alignment)}." class="ltx_Math" display="inline" id="S3.E4.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><msub><mi>τ</mi><mi>gap</mi></msub><mspace style="width:1em;" width="1em"></mspace><mtext class="ltx_mathvariant_bold">(proximity)</mtext><mspace style="width:1em;" width="1em"></mspace><mtext>and</mtext></mrow></mrow><mspace style="width:1em;" width="1em"></mspace><mrow><mrow><mo>|</mo><mrow><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><msubsup><mi mathvariant="normal">Δ</mi><mi>k</mi><mrow><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow><mo>≤</mo><mrow><msub><mi>τ</mi><mi>align</mi></msub><mspace style="width:1em;" width="1em"></mspace><mtext class="ltx_mathvariant_bold">(text alignment)</mtext></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\max(\Delta^{(B)}_{k},\Delta^{(G)}_{k})\leq\tau_{\mathrm{gap}}\quad\textbf{(proximity)}\quad\text{and}\quad\left|\Delta^{(B)}_{k}-\Delta^{(G)}_{k}\right|\leq\tau_{\mathrm{align}}\quad\textbf{(text alignment)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p4.22">Here, <math alttext="\tau_{\mathrm{gap}}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.9.m1" intent=":literal"><semantics><msub><mi>τ</mi><mi>gap</mi></msub><annotation encoding="application/x-tex">\tau_{\mathrm{gap}}</annotation></semantics></math> specifies the maximum number of unmatched words allowed between consecutive blocks, and <math alttext="\tau_{\mathrm{align}}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.10.m2" intent=":literal"><semantics><msub><mi>τ</mi><mi>align</mi></msub><annotation encoding="application/x-tex">\tau_{\mathrm{align}}</annotation></semantics></math> limits merges to blocks that occur in roughly corresponding locations in the reference and generated texts, which helps avoid stitching together unrelated content.
When these conditions are met, we replace blocks <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.11.m3" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> and <math alttext="k{+}1" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.12.m4" intent=":literal"><semantics><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k{+}1</annotation></semantics></math> with a single merged near-verbatim block with <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p4.22.1">effective matched length</span>
<math alttext="m_{k}^{*}\triangleq m_{k}+m_{k+1}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.13.m5" intent=":literal"><semantics><mrow><msubsup><mi>m</mi><mi>k</mi><mo>∗</mo></msubsup><mo>≜</mo><mrow><msub><mi>m</mi><mi>k</mi></msub><mo>+</mo><msub><mi>m</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">m_{k}^{*}\triangleq m_{k}+m_{k+1}</annotation></semantics></math>,
and spanning indices
<math alttext="[i_{k},\,i_{k+1}+m_{k+1})\text{ in }B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.14.m6" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">[</mo><msub><mi>i</mi><mi>k</mi></msub><mo rspace="0.337em">,</mo><mrow><msub><mi>i</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>m</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mtext>&nbsp;in&nbsp;</mtext><mo lspace="0em" rspace="0em">​</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">[i_{k},\,i_{k+1}+m_{k+1})\text{ in }B</annotation></semantics></math>
and
<math alttext="[j_{k},\,j_{k+1}+m_{k+1})\text{ in }G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.15.m7" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">[</mo><msub><mi>j</mi><mi>k</mi></msub><mo rspace="0.337em">,</mo><mrow><msub><mi>j</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>m</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mtext>&nbsp;in&nbsp;</mtext><mo lspace="0em" rspace="0em">​</mo><mi>G</mi></mrow><annotation encoding="application/x-tex">[j_{k},\,j_{k+1}+m_{k+1})\text{ in }G</annotation></semantics></math> (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4" title="Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>).
We conservatively do <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p4.22.2">not</em> count gaps reconciled by a merge:
<math alttext="m_{k}^{*}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.16.m8" intent=":literal"><semantics><msubsup><mi>m</mi><mi>k</mi><mo>∗</mo></msubsup><annotation encoding="application/x-tex">m_{k}^{*}</annotation></semantics></math> counts <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p4.22.3">only</em> verbatim-matched words, so it is less than the length of <math alttext="[i_{k},\,i_{k+1}+m_{k+1})" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.17.m9" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>i</mi><mi>k</mi></msub><mo rspace="0.337em">,</mo><mrow><msub><mi>i</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>m</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[i_{k},\,i_{k+1}+m_{k+1})</annotation></semantics></math> in <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.18.m10" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>, which spans the gap between <math alttext="\beta_{k}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.19.m11" intent=":literal"><semantics><msub><mi>β</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\beta_{k}</annotation></semantics></math> and <math alttext="\beta_{k+1}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.20.m12" intent=":literal"><semantics><msub><mi>β</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\beta_{k+1}</annotation></semantics></math> (and similarly less than <math alttext="[j_{k},\,j_{k+1}+m_{k+1})" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.21.m13" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>j</mi><mi>k</mi></msub><mo rspace="0.337em">,</mo><mrow><msub><mi>j</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>m</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[j_{k},\,j_{k+1}+m_{k+1})</annotation></semantics></math> in <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p4.22.m14" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">其中， <math intent=":literal" id="S3.SS3.SSS1.p4.9.m1" display="inline" class="ltx_Math" alttext="\tau_{\mathrm{gap}}"><semantics><msub><mi>τ</mi><mi>gap</mi></msub><annotation encoding="application/x-tex">\tau_{\mathrm{gap}}</annotation></semantics></math> 指定允许在连续块之间存在的最大未匹配单词数， <math intent=":literal" id="S3.SS3.SSS1.p4.10.m2" display="inline" class="ltx_Math" alttext="\tau_{\mathrm{align}}"><semantics><msub><mi>τ</mi><mi>align</mi></msub><annotation encoding="application/x-tex">\tau_{\mathrm{align}}</annotation></semantics></math> 限制合并为在参考文本和生成文本中大致对应位置的块，这有助于避免将不相关内容拼接在一起。当满足这些条件时，我们用单个合并的近乎逐字块替换块 <math intent=":literal" id="S3.SS3.SSS1.p4.11.m3" display="inline" class="ltx_Math" alttext="k"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p4.12.m4" display="inline" class="ltx_Math" alttext="k{+}1"><semantics><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k{+}1</annotation></semantics></math> ，其有效匹配长度为 <math intent=":literal" id="S3.SS3.SSS1.p4.13.m5" display="inline" class="ltx_Math" alttext="m_{k}^{*}\triangleq m_{k}+m_{k+1}"><semantics><mrow><msubsup><mi>m</mi><mi>k</mi><mo>∗</mo></msubsup><mo>≜</mo><mrow><msub><mi>m</mi><mi>k</mi></msub><mo>+</mo><msub><mi>m</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">m_{k}^{*}\triangleq m_{k}+m_{k+1}</annotation></semantics></math> ，跨越索引 <math intent=":literal" id="S3.SS3.SSS1.p4.14.m6" display="inline" class="ltx_Math" alttext="[i_{k},\,i_{k+1}+m_{k+1})\text{ in }B"><semantics><mrow><mrow><mo stretchy="false">[</mo><msub><mi>i</mi><mi>k</mi></msub><mo rspace="0.337em">,</mo><mrow><msub><mi>i</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>m</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo rspace="0em" lspace="0em">​</mo><mtext>&nbsp;in&nbsp;</mtext><mo rspace="0em" lspace="0em">​</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">[i_{k},\,i_{k+1}+m_{k+1})\text{ in }B</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p4.15.m7" display="inline" class="ltx_Math" alttext="[j_{k},\,j_{k+1}+m_{k+1})\text{ in }G"><semantics><mrow><mrow><mo stretchy="false">[</mo><msub><mi>j</mi><mi>k</mi></msub><mo rspace="0.337em">,</mo><mrow><msub><mi>j</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>m</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><mo rspace="0em" lspace="0em">​</mo><mtext>&nbsp;in&nbsp;</mtext><mo rspace="0em" lspace="0em">​</mo><mi>G</mi></mrow><annotation encoding="application/x-tex">[j_{k},\,j_{k+1}+m_{k+1})\text{ in }G</annotation></semantics></math> （图 4）。我们保守地不计入合并解决的间隙： <math intent=":literal" id="S3.SS3.SSS1.p4.16.m8" display="inline" class="ltx_Math" alttext="m_{k}^{*}"><semantics><msubsup><mi>m</mi><mi>k</mi><mo>∗</mo></msubsup><annotation encoding="application/x-tex">m_{k}^{*}</annotation></semantics></math> 仅计算逐字匹配的单词，因此它小于 <math intent=":literal" id="S3.SS3.SSS1.p4.17.m9" display="inline" class="ltx_Math" alttext="[i_{k},\,i_{k+1}+m_{k+1})"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>i</mi><mi>k</mi></msub><mo rspace="0.337em">,</mo><mrow><msub><mi>i</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>m</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[i_{k},\,i_{k+1}+m_{k+1})</annotation></semantics></math> 在 <math intent=":literal" id="S3.SS3.SSS1.p4.18.m10" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 中的长度，后者跨越了 <math intent=":literal" id="S3.SS3.SSS1.p4.19.m11" display="inline" class="ltx_Math" alttext="\beta_{k}"><semantics><msub><mi>β</mi><mi>k</mi></msub><annotation encoding="application/x-tex">\beta_{k}</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p4.20.m12" display="inline" class="ltx_Math" alttext="\beta_{k+1}"><semantics><msub><mi>β</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\beta_{k+1}</annotation></semantics></math> 之间的间隙（同样小于 <math intent=":literal" id="S3.SS3.SSS1.p4.21.m13" display="inline" class="ltx_Math" alttext="[j_{k},\,j_{k+1}+m_{k+1})"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>j</mi><mi>k</mi></msub><mo rspace="0.337em">,</mo><mrow><msub><mi>j</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>m</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[j_{k},\,j_{k+1}+m_{k+1})</annotation></semantics></math> 在 <math intent=":literal" id="S3.SS3.SSS1.p4.22.m14" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中的长度）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p5">
<p class="ltx_p" id="S3.SS3.SSS1.p5.2"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p5.2.1">Filter blocks.</span>  Very short matching blocks may reflect coincidental overlap rather than meaningful long-form similarity that we can safely call extraction (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf2" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(b)</span></a>).
We therefore filter blocks by a minimum length threshold <math alttext="l" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p5.1.m1" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>.
Given an ordered block set <math alttext="{\mathbb{B}}(B,G)" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p5.2.m2" intent=":literal"><semantics><mrow><mi>𝔹</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\mathbb{B}}(B,G)</annotation></semantics></math>, we define the filtered ordered block set<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">过滤块。非常短的匹配块可能反映偶然的重叠，而不是我们能够安全地称为提取的具有意义的长格式相似性（图 4(b)）。因此，我们通过最小长度阈值 <math intent=":literal" id="S3.SS3.SSS1.p5.1.m1" display="inline" class="ltx_Math" alttext="l"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math> 过滤块。给定一个有序块集 <math intent=":literal" id="S3.SS3.SSS1.p5.2.m2" display="inline" class="ltx_Math" alttext="{\mathbb{B}}(B,G)"><semantics><mrow><mi>𝔹</mi><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\mathbb{B}}(B,G)</annotation></semantics></math> ，我们定义过滤后的有序块集</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx5">
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle{\mathbb{B}}_{\geq l}(B,G)\triangleq\left\{(i_{k},j_{k},m_{k})\in{\mathbb{B}}(B,G)\;\middle|\;m_{k}\geq l\right\}." class="ltx_Math" display="inline" id="S3.E5.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>𝔹</mi><mrow><mi></mi><mo>≥</mo><mi>l</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≜</mo><mrow><mo>{</mo><mrow><mrow><mo stretchy="false">(</mo><msub><mi>i</mi><mi>k</mi></msub><mo>,</mo><msub><mi>j</mi><mi>k</mi></msub><mo>,</mo><msub><mi>m</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><mi>𝔹</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="true">|</mo><mrow><msub><mi>m</mi><mi>k</mi></msub><mo>≥</mo><mi>l</mi></mrow><mo>}</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle{\mathbb{B}}_{\geq l}(B,G)\triangleq\left\{(i_{k},j_{k},m_{k})\in{\mathbb{B}}(B,G)\;\middle|\;m_{k}\geq l\right\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p6">
<p class="ltx_p" id="S3.SS3.SSS1.p6.6">In practice, after identifying verbatim blocks, we perform <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p6.6.1">two</em> merge-and-filter passes (Algorithm&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#alg1" title="Algorithm 1 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>) to obtain near-verbatim blocks that reflect extracted training data.
In the first pass, we merge blocks separated by trivial gaps
(<math alttext="\tau^{(1)}_{\mathrm{gap}}=2" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p6.1.m1" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{gap}}=2</annotation></semantics></math> and <math alttext="\tau^{(1)}_{\mathrm{align}}=1" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p6.2.m2" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{align}}=1</annotation></semantics></math>, see Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4" title="Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>), and then filter out short blocks by retaining only those with length at least <math alttext="l^{(1)}=20" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p6.3.m3" intent=":literal"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">l^{(1)}=20</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>This is conservative; <math alttext="20" class="ltx_Math" display="inline" id="footnote4.m1" intent=":literal"><semantics><mn>20</mn><annotation encoding="application/x-tex">20</annotation></semantics></math> words is approximately half of the <math alttext="{\approx}38" class="ltx_Math" display="inline" id="footnote4.m2" intent=":literal"><semantics><mrow><mi></mi><mo>≈</mo><mn>38</mn></mrow><annotation encoding="application/x-tex">{\approx}38</annotation></semantics></math> words typically used in discoverable extraction.
See Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A2" title="Appendix B Procedure for quantifying extraction success ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">B</span></a>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">这是保守的； <math intent=":literal" id="footnote4.m1" display="inline" class="ltx_Math" alttext="20"><semantics><mn>20</mn><annotation encoding="application/x-tex">20</annotation></semantics></math> 个词大约是通常用于可发现提取的 <math intent=":literal" id="footnote4.m2" display="inline" class="ltx_Math" alttext="{\approx}38"><semantics><mrow><mi></mi><mo>≈</mo><mn>38</mn></mrow><annotation encoding="application/x-tex">{\approx}38</annotation></semantics></math> 个词的一半。参见附录 B。</font></font></font></span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在实践中，在识别出逐字块后，我们执行两次合并和过滤过程（算法 1），以获得反映提取训练数据的近似逐字块。在第一遍中，我们合并由微小间隙（ <math intent=":literal" id="S3.SS3.SSS1.p6.1.m1" display="inline" class="ltx_Math" alttext="\tau^{(1)}_{\mathrm{gap}}=2"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{gap}}=2</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p6.2.m2" display="inline" class="ltx_Math" alttext="\tau^{(1)}_{\mathrm{align}}=1"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{align}}=1</annotation></semantics></math> ，见图 4）分隔的块，然后通过仅保留长度至少为 <math intent=":literal" id="S3.SS3.SSS1.p6.3.m3" display="inline" class="ltx_Math" alttext="l^{(1)}=20"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">l^{(1)}=20</annotation></semantics></math> 的块来过滤掉短块。 <sup class="ltx_note_mark">4</sup> </font></font></font>
In the second pass, we perform a more relaxed but still stringent merge to consolidate passage-level matches (<math alttext="\tau^{(2)}_{\mathrm{gap}}=10" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p6.4.m4" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{gap}}=10</annotation></semantics></math>, <math alttext="\tau^{(2)}_{\mathrm{align}}=3" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p6.5.m5" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{align}}=3</annotation></semantics></math>), followed by a final filter that retains only sufficiently long near-verbatim blocks (<math alttext="l^{(2)}=100" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p6.6.m6" intent=":literal"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">l^{(2)}=100</annotation></semantics></math>) to support a valid extraction claim (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>).
Because filtering is interleaved with merging, some fragmented near-verbatim passages may fail to consolidate into a single long block and may be filtered out.
This is a deliberate trade-off:
we prefer to be conservative and incur false negatives (i.e., miss some instances of extraction) rather than risk including false positives.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在第二次遍历时，我们执行更宽松但仍严格的合并操作，以整合段落级别的匹配（ <math intent=":literal" id="S3.SS3.SSS1.p6.4.m4" display="inline" class="ltx_Math" alttext="\tau^{(2)}_{\mathrm{gap}}=10"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{gap}}=10</annotation></semantics></math> ， <math intent=":literal" id="S3.SS3.SSS1.p6.5.m5" display="inline" class="ltx_Math" alttext="\tau^{(2)}_{\mathrm{align}}=3"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{align}}=3</annotation></semantics></math> ），随后进行最终过滤，仅保留足够长的逐字块（ <math intent=":literal" id="S3.SS3.SSS1.p6.6.m6" display="inline" class="ltx_Math" alttext="l^{(2)}=100"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">l^{(2)}=100</annotation></semantics></math> ）以支持有效的提取声明（第 3.3.2 节）。由于过滤与合并交替进行，一些片段化的逐字段落可能无法合并成一个长块，并可能被过滤掉。这是一个有意做出的权衡：我们宁愿保守一些并承担假阴性（即漏掉一些提取实例），而不是冒险包含假阳性。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p7">
<p class="ltx_p" id="S3.SS3.SSS1.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p7.1.1">Metrics from near-verbatim blocks.</span> 
From the near-verbatim, extracted text represented in the final ordered block set, we can aggregate several useful metrics. Let <math alttext="{\mathbb{B}}^{*}=\{(i^{*}_{k},j^{*}_{k},m^{*}_{k})\}_{k=1}^{K^{*}}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.1.m1" intent=":literal"><semantics><mrow><msup><mi>𝔹</mi><mo>∗</mo></msup><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>i</mi><mi>k</mi><mo>∗</mo></msubsup><mo>,</mo><msubsup><mi>j</mi><mi>k</mi><mo>∗</mo></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>k</mi><mo>∗</mo></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>K</mi><mo>∗</mo></msup></msubsup></mrow><annotation encoding="application/x-tex">{\mathbb{B}}^{*}=\{(i^{*}_{k},j^{*}_{k},m^{*}_{k})\}_{k=1}^{K^{*}}</annotation></semantics></math> denote the final set of blocks returned by the two-pass merge-and-filter procedure (Algorithm&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#alg1" title="Algorithm 1 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>).
We define<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">逐字块指标。从最终有序块集中表示的逐字提取文本，我们可以汇总几个有用的指标。设 <math intent=":literal" id="S3.SS3.SSS1.p7.1.m1" display="inline" class="ltx_Math" alttext="{\mathbb{B}}^{*}=\{(i^{*}_{k},j^{*}_{k},m^{*}_{k})\}_{k=1}^{K^{*}}"><semantics><mrow><msup><mi>𝔹</mi><mo>∗</mo></msup><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>i</mi><mi>k</mi><mo>∗</mo></msubsup><mo>,</mo><msubsup><mi>j</mi><mi>k</mi><mo>∗</mo></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>k</mi><mo>∗</mo></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>K</mi><mo>∗</mo></msup></msubsup></mrow><annotation encoding="application/x-tex">{\mathbb{B}}^{*}=\{(i^{*}_{k},j^{*}_{k},m^{*}_{k})\}_{k=1}^{K^{*}}</annotation></semantics></math> 表示两遍合并和过滤过程（算法 1）返回的最终块集。我们定义</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx6">
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle m:=\mathsf{matched}(B,G)\;\triangleq\;\sum_{(i^{*}_{k},j^{*}_{k},m^{*}_{k})\in{\mathbb{B}}^{*}}\!\!\!\!\!\!\!m^{*}_{k}." class="ltx_Math" display="inline" id="S3.E6.m1" intent=":literal"><semantics><mrow><mrow><mi>m</mi><mo>:=</mo><mrow><mi>𝗆𝖺𝗍𝖼𝗁𝖾𝖽</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.558em">≜</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mrow><mo stretchy="false">(</mo><msubsup><mi>i</mi><mi>k</mi><mo>∗</mo></msubsup><mo>,</mo><msubsup><mi>j</mi><mi>k</mi><mo>∗</mo></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>k</mi><mo>∗</mo></msubsup><mo stretchy="false">)</mo></mrow><mo>∈</mo><msup><mi>𝔹</mi><mo>∗</mo></msup></mrow></munder></mstyle><msubsup><mi>m</mi><mi>k</mi><mo>∗</mo></msubsup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle m:=\mathsf{matched}(B,G)\;\triangleq\;\sum_{(i^{*}_{k},j^{*}_{k},m^{*}_{k})\in{\mathbb{B}}^{*}}\!\!\!\!\!\!\!m^{*}_{k}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p7.6">which is the total number of in-order words extracted near-verbatim
in <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.2.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> with respect to <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.3.m2" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>.
From <math alttext="m" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.4.m3" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>, we then define the relative <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p7.6.1">near-verbatim recall</span> of book <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.5.m4" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> extracted in generation <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.6.m5" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">这是相对于 <math intent=":literal" id="S3.SS3.SSS1.p7.3.m2" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 在 <math intent=":literal" id="S3.SS3.SSS1.p7.2.m1" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中按顺序逐字提取的总词数。然后，我们从 <math intent=":literal" id="S3.SS3.SSS1.p7.4.m3" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> 定义在生成 <math intent=":literal" id="S3.SS3.SSS1.p7.6.m5" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中提取的书籍 <math intent=":literal" id="S3.SS3.SSS1.p7.5.m4" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 的相对逐字召回率：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx7">
<tbody id="S3.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathsf{nv{\text{-}}recall}(B,G)\;\triangleq\;\frac{m}{|B|}," class="ltx_Math" display="inline" id="S3.E7.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.558em">≜</mo><mstyle displaystyle="true"><mfrac><mi>m</mi><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow></mfrac></mstyle></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathsf{nv{\text{-}}recall}(B,G)\;\triangleq\;\frac{m}{|B|},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p7.11">which reflects the proportion of in-order, near-verbatim extracted text relative to the length of the whole book.
We typically report <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.7.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> as a percentage rather than a fraction (e.g., Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>).
For further analysis, we also define in absolute word counts how much in-order, near-verbatim text we failed to extract in <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.8.m2" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> (i.e., is <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p7.11.1">missing</span> in <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.9.m3" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>) and how much <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p7.11.2">additional</span> non-book text is in <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.10.m4" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> (i.e., is not contained near-verbatim in <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.11.m5" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>):<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">这反映了按顺序、近乎逐字提取的文本占整本书长度的比例。我们通常以百分比而非分数的形式报告 <math intent=":literal" id="S3.SS3.SSS1.p7.7.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> （例如，图 1）。为了进一步分析，我们还定义了绝对词数，即我们在 <math intent=":literal" id="S3.SS3.SSS1.p7.8.m2" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 中未能按顺序、近乎逐字提取的文本量（即 <math intent=":literal" id="S3.SS3.SSS1.p7.9.m3" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中缺失的文本）以及 <math intent=":literal" id="S3.SS3.SSS1.p7.10.m4" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中额外的非书文本量（即未近乎逐字包含在 <math intent=":literal" id="S3.SS3.SSS1.p7.11.m5" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 中的文本）：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx8">
<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathsf{missing}(B,G)\;\triangleq\;|B|-m,\qquad\mathsf{additional}(B,G)\;\triangleq\;|G|-m." class="ltx_Math" display="inline" id="S3.E8.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.558em">≜</mo><mrow><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><mo>−</mo><mi>m</mi></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mrow><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>,</mo><mi>G</mi><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.558em">≜</mo><mrow><mrow><mo stretchy="false">|</mo><mi>G</mi><mo stretchy="false">|</mo></mrow><mo>−</mo><mi>m</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathsf{missing}(B,G)\;\triangleq\;|B|-m,\qquad\mathsf{additional}(B,G)\;\triangleq\;|G|-m.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p7.16">Since <math alttext="m" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.12.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> counts only aligned, near-verbatim blocks from an <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS1.p7.16.1">ordered</em> set, verbatim text that is reproduced out-of-order may be present in <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.13.m2" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> but excluded from <math alttext="m" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.14.m3" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>.
Such text would instead be counted in <math alttext="\mathsf{missing}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.15.m4" intent=":literal"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> and <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p7.16.m5" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math>, even though it represents valid extraction, and so our measurements may under-count extraction.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>To identify these cases, as well as instances of duplicated extraction in <math alttext="G" class="ltx_Math" display="inline" id="footnote5.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>, one could iteratively re-run our measurement procedure on <math alttext="B" class="ltx_Math" display="inline" id="footnote5.m2" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> and <em class="ltx_emph ltx_font_italic" id="footnote5.1">unmatched</em> (non-block) text in <math alttext="G" class="ltx_Math" display="inline" id="footnote5.m3" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了识别这些情况，以及 <math intent=":literal" id="footnote5.m1" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中重复提取的实例，可以在 <math intent=":literal" id="footnote5.m2" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 和 <math intent=":literal" id="footnote5.m3" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中未匹配（非块）文本上迭代重新运行我们的测量程序。</font></font></font></span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">由于 <math intent=":literal" id="S3.SS3.SSS1.p7.12.m1" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> 仅统计有序集中对齐的、近乎逐字复制的文本块，因此顺序错乱的逐字复制的文本可能存在于 <math intent=":literal" id="S3.SS3.SSS1.p7.13.m2" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中但被排除在 <math intent=":literal" id="S3.SS3.SSS1.p7.14.m3" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> 之外。此类文本会被计入 <math intent=":literal" id="S3.SS3.SSS1.p7.15.m4" display="inline" class="ltx_Math" alttext="\mathsf{missing}"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> 和 <math intent=":literal" id="S3.SS3.SSS1.p7.16.m5" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> ，尽管它代表有效的提取，因此我们的测量可能低估了提取量。 <sup class="ltx_note_mark">5</sup> </font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Claiming extraction success without information about training-data membership<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">3.3.2 在没有关于训练数据成员资格信息的情况下声称提取成功</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">We next elaborate on why, absent certain knowledge of production LLM training datasets, the above measurement procedure captures valid evidence of extraction.
When making a claim about extraction of a sequence of training data, one is necessarily also making a claim that this sequence was in the training dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Carlini<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib9" title="Extracting training data from large language models">2021</a>)</cite>.
By definition, “it is only possible to extract memorized training data, and (tautologically) training data can only be memorized if they are included—i.e., are <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p1.1.1">members</span>—of the training dataset.
To demonstrate extraction is therefore to demonstrate memorization, and memorization implies membership” in the training dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cooper<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们接下来阐述，为何在没有关于生产 LLM 训练数据集的某些知识的情况下，上述测量程序能够捕捉到有效的提取证据。当关于训练数据序列的提取做出断言时，必然也在断言该序列包含在训练数据集中（Carlini 等人，2021）。根据定义，“只有能够提取记忆中的训练数据，并且（同义反复地）训练数据只有包含在训练数据集中——即作为训练数据集的成员——才能被记忆。因此，证明提取就是证明记忆，而记忆意味着它是训练数据集的成员”（Cooper 等人，2025）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">Much prior work on extraction is conducted on open-weight models with known training datasets (<cite class="ltx_cite ltx_citemacro_citet">Lee<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib44" title="Deduplicating Training Data Makes Language Models Better">2022</a>); Carlini<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib107" title="Quantifying Memorization Across Neural Language Models">2023</a>); Hayes<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib41" title="Measuring memorization in language models via probabilistic extraction">2025b</a>); Wei<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib82" title="Hubble: a model suite to advance the study of llm memorization">2025</a>)</cite>; Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.2</span></a>);
it is known with certainty that the extracted data were members of the training dataset.
In contrast, in our production LLM setting, we do <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p2.1.1">not</em> have access to certain, ground-truth information about the training dataset.
This means that, embedded in our claims for extraction of books text, we are also claiming that the text that we generated was included near-verbatim in production LLMs’ training data.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We only make membership and memorization claims about this specific text, not the whole book (except for the four whole extracted books for Claude 3.7 Sonnet).
For more on this distinction, see Appendix E.6, <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们仅就这段特定文本提出成员资格和记忆主张，而不是整本书（除了为 Claude 3.7 Sonnet 提取的四本完整书籍）。关于这一区别的更多内容，参见附录 E.6，Cooper 等人（2025）。</font></font></font></span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">以往关于提取的研究大多是在开放权重模型上进行的，这些模型的训练数据集是已知的（Lee 等人（2022 年）；Carlini 等人（2023 年）；Hayes 等人（2025b 年）；Wei 等人（2025 年）；第 3.2 节）；可以确定的是，提取的数据都是训练数据集中的成员。相比之下，在我们的生产 LLM 环境中，我们没有访问关于训练数据集的某些、真实信息。这意味着，在我们关于提取书籍文本的主张中，我们也声称我们生成的文本几乎逐字地包含在生产 LLM 的训练数据中。</font></font></font>
As noted at the beginning of this section, to make a valid claim, the generated text has to be sufficiently long and similar to the suspected training data, such that memorization of that data from the training set is the overwhelmingly plausible explanation.
This is because, when a sufficiently long, unique sequence of training data is generated, “[t]he probability that this would have happened by random chance is astronomically low, and so we can say that the model has ‘memorized’ this training data”&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Carlini, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib34" title="What my privacy papers (don’t) have to say about copyright and generative AI">2025</a>)</cite>;
that sequence of training data “must be stored <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS2.p2.1.2">somewhere</em> in the model weights”&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Nasr<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>)</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">如本节开头所述，要提出有效主张，生成的文本必须足够长且与可疑的训练数据相似，以至于从训练集中记忆这些数据是极有可能的解释。这是因为，当生成足够长且独特的训练数据序列时，“这种结果通过随机偶然发生的概率极低，因此我们可以认为模型‘记忆’了这些训练数据”（Carlini，2025）；该训练数据序列“必须以某种方式存储在模型权重中”（Nasr 等人，2023）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.5">In their prior work on extraction from production LLMs, <cite class="ltx_cite ltx_citemacro_citet">Nasr<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>)</cite> ensure validity by requiring that the LLM produce sufficiently long (<math alttext="{\geq}50" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p3.1.m1" intent=":literal"><semantics><mrow><mi></mi><mo>≥</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">{\geq}50</annotation></semantics></math>-token/roughly <math alttext="{\geq}38" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p3.2.m2" intent=":literal"><semantics><mrow><mi></mi><mo>≥</mo><mn>38</mn></mrow><annotation encoding="application/x-tex">{\geq}38</annotation></semantics></math>-word) sequences that exactly match a proxy dataset reflecting data likely used for LLM pre-training&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Nasr<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib88" title="Scalable Extraction of Training Data from Aligned, Production Language Models">2025</a>)</cite>.
While <math alttext="50" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p3.3.m3" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> tokens may seem relatively short, for an LLM, exact matches of this length are extraordinarily unlikely without memorization.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>The prompts that elicited these training data sequences did not contain these sequences’ prefixes;
they involved completely unrelated jailbreak prompts, which queried ChatGPT 3.5 to repeat a single token (e.g., “poem”) forever.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">引发这些训练数据序列的提示不包含这些序列的前缀；它们涉及完全无关的越狱提示，这些提示查询 ChatGPT 3.5 重复单个标记（例如，“poem”）无限次。</font></font></font></span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在他们之前关于从生产 LLMs 中提取的工作中，Nasr 等人（2023 年）通过要求 LLMs 生成足够长（ <math intent=":literal" id="S3.SS3.SSS2.p3.1.m1" display="inline" class="ltx_Math" alttext="{\geq}50"><semantics><mrow><mi></mi><mo>≥</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">{\geq}50</annotation></semantics></math> -token/大约 <math intent=":literal" id="S3.SS3.SSS2.p3.2.m2" display="inline" class="ltx_Math" alttext="{\geq}38"><semantics><mrow><mi></mi><mo>≥</mo><mn>38</mn></mrow><annotation encoding="application/x-tex">{\geq}38</annotation></semantics></math> -词）的序列，这些序列与一个反映可能用于 LLMs 预训练的数据的代理数据集完全匹配，来确保有效性（Nasr 等人，2023 年；2025 年）。虽然 <math intent=":literal" id="S3.SS3.SSS2.p3.3.m3" display="inline" class="ltx_Math" alttext="50"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> tokens 可能看起来相对较短，但对于一个 LLM 来说，如果没有记忆，这种长度的精确匹配是极不可能的。 <sup class="ltx_note_mark">7</sup> </font></font></font>
Therefore, the results in <cite class="ltx_cite ltx_citemacro_citet">Nasr<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>)</cite> are accepted as strong evidence for extraction, without direct knowledge of the training dataset.
In our experiments, we target extraction of specific documents, which we know are widely available in several common pre-training datasets, including Books3 (where we access our reference texts) and other torrents like LibGen (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.1</span></a>).
Beyond the initial short seed prefix, we provide no other book-specific information to the LLM.
We also set a much higher bar than generating <math alttext="{\geq}38" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p3.4.m4" intent=":literal"><semantics><mrow><mi></mi><mo>≥</mo><mn>38</mn></mrow><annotation encoding="application/x-tex">{\geq}38</annotation></semantics></math> words to call extraction successful:
at a minimum, we require <math alttext="100" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p3.5.m5" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math>-word near-exact passages, and often retrieve passages that are significantly longer—e.g., thousands of words (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.T1" title="Table 1 ‣ Figure 6 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>, Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>).
Together, the relatively short length of the prefix in Phase 1, the lack of book-specific guidance in the continuation loop in Phase 2, and the length and fidelity of the near-verbatim matches we identify are strong evidence of memorization of training data, which we have successfully extracted in outputs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">因此，Nasr 等人（2023）的研究结果被视为强有力的证据，证明了在不直接了解训练数据集的情况下进行提取。在我们的实验中，我们针对特定文档的提取，这些文档我们知道在多个常见的预训练数据集中广泛存在，包括 Books3（我们在其中获取参考文本）以及其他如 LibGen 的种子文件（附录 C.1）。除了初始的短种子前缀外，我们未向 LLM 提供任何其他与书籍相关的信息。我们还设定了比生成 <math intent=":literal" id="S3.SS3.SSS2.p3.4.m4" display="inline" class="ltx_Math" alttext="{\geq}38"><semantics><mrow><mi></mi><mo>≥</mo><mn>38</mn></mrow><annotation encoding="application/x-tex">{\geq}38</annotation></semantics></math> 个单词更高的标准来判定提取成功：至少要求 <math intent=":literal" id="S3.SS3.SSS2.p3.5.m5" display="inline" class="ltx_Math" alttext="100"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> 个单词的近似精确段落，并且经常检索到显著更长的段落——例如数千个单词（表 1，第 4.2 节）。综合来看，第一阶段中前缀的相对较短长度、第二阶段延续循环中缺乏与书籍相关的指导，以及我们识别出的近似逐字匹配的长度和保真度，都是训练数据记忆的强有力证据，我们已成功在输出中提取这些数据。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">4 实验</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We now present our main results.
We begin with details about the exact production LLMs and books we test, as well as high-level variations in how we instantiate our two-phase procedure (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.1</span></a>).
We then give a summary of high-level, experimental outcomes for different books and LLMs (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>), before discussing more detailed LLM-specific results (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS3" title="4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.3</span></a>).
Additional results can be found in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4" title="Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D</span></a>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">现在我们展示我们的主要结果。我们首先提供关于我们测试的精确生产 LLM 和书籍的详细信息，以及我们如何实例化我们两阶段程序的总体变化（第 4.1 节）。然后，我们给出针对不同书籍和 LLM 的实验结果的总结（第 4.2 节），接着讨论更详细的针对特定 LLM 的结果（第 4.3 节）。更多结果可以在附录 D 中找到。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">4.1 设置</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Given that production systems change over time (i.e., are unstable compared to open-weight LLMs), we limited our experiments to between mid-August and mid-September 2025.
We attempt to extract thirteen books from four production LLMs, and predominantly report results for the single run that shows the maximum amount of extraction we observed for a given production LLM, book, and generation configuration.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">考虑到生产系统会随时间变化（即与开放权重 LLM 相比是不稳定的），我们将实验限制在 2025 年 8 月中旬至 9 月中旬之间。我们尝试从四个生产 LLM 中提取十三本书籍，并且主要报告针对给定生产 LLM、书籍和生成配置中我们观察到的最大提取量的单次运行结果。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Production LLMs.</span> 
The four production LLMs we evaluate are
Claude 3.7 Sonnet (<span class="ltx_text ltx_font_typewriter" id="S4.SS1.p2.1.2">claude-3-7-sonnet-20250219</span>), GPT-4.1 (<span class="ltx_text ltx_font_typewriter" id="S4.SS1.p2.1.3">gpt-4.1-2025-04-14</span>), Gemini 2.5 Pro (<span class="ltx_text ltx_font_typewriter" id="S4.SS1.p2.1.4">gemini-2.5-pro</span>), and Grok 3 (<span class="ltx_text ltx_font_typewriter" id="S4.SS1.p2.1.5">grok-3</span>).
Throughout, we refer to these LLMs by their names, rather than these API versions.
Claude 3.7 Sonnet has a knowledge cutoff date of October 2024&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Anthropic (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib125" title="Claude 3.7 Sonnet System Card">2025</a>)</cite>,
GPT-4.1’s is June 2024&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib133" title="Introducing GPT-4.1 in the API">2025</a>)</cite>,
Grok 3’s is November 2024&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(xAI, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib134" title="Models and Pricing">2025</a>)</cite>, and Gemini 2.5 Pro’s is January 2025&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Google Cloud, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib135" title="Gemini 2.5 Pro">2025</a>)</cite>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">生产 LLMs。我们评估的四个生产 LLMs 是 Claude 3.7 Sonnet（claude-3-7-sonnet-20250219）、GPT-4.1（gpt-4.1-2025-04-14）、Gemini 2.5 Pro（gemini-2.5-pro）和 Grok 3（grok-3）。在整个过程中，我们使用这些 LLMs 的名称来指代它们，而不是这些 API 版本。Claude 3.7 Sonnet 的知识截止日期是 2024 年 10 月 Anthropic（2025 年），GPT-4.1 的知识截止日期是 2024 年 6 月（OpenAI，2025 年），Grok 3 的知识截止日期是 2024 年 11 月（xAI，2025 年），Gemini 2.5 Pro 的知识截止日期是 2025 年 1 月（Google Cloud，2025 年）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="S4.F5.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/figure_3_improved.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.17.8.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F5.14.7" style="font-size:90%;">Proportion of book extracted (<math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.F5.8.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext class="ltx_mathvariant_bold">-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math>).<span class="ltx_text ltx_font_medium" id="S4.F5.14.7.6">
We show <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.F5.9.2.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> (%) for the twelve books for which we run Phase 2.
Each bar is annotated with the corresponding <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.F5.10.3.2.m2" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> for a production LLM-book pair;
the number in parentheses above is the BoN samples <math alttext="N" class="ltx_Math" display="inline" id="S4.F5.11.4.3.m3" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> in Phase 1 (<math alttext="N=0" class="ltx_Math" display="inline" id="S4.F5.12.5.4.m4" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">N=0</annotation></semantics></math> for Gemini 2.5 Pro and Grok 3, since we do not jailbreak those production LLMs.)
<math alttext="\dagger" class="ltx_Math" display="inline" id="S4.F5.13.6.5.m5" intent=":literal"><semantics><mo>†</mo><annotation encoding="application/x-tex">\dagger</annotation></semantics></math> denotes that Phase 1 failed;
<math alttext="*" class="ltx_Math" display="inline" id="S4.F5.14.7.6.m6" intent=":literal"><semantics><mo>∗</mo><annotation encoding="application/x-tex">*</annotation></semantics></math> indicates we did not attempt Phase 2.
Gray shading indicates public domain books.
The vertical axis in each row has a different scale.
<span class="ltx_text ltx_font_italic" id="S4.F5.14.7.6.1">Note: Each bar reflects a single run of Phase 2, where the underlying generation configuration is fixed per LLM but varies across LLMs.
The groups of bars do not reflect comparisons of results obtained from testing all production LLMs under the same conditions.</span>
</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 5：提取书籍的比例（ <math intent=":literal" id="S4.F5.8.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext class="ltx_mathvariant_bold">-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> ）。我们展示了针对运行了第二阶段的十二本书的 <math intent=":literal" id="S4.F5.9.2.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> (%)。每个条形图都标注了相应的 <math intent=":literal" id="S4.F5.10.3.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 生产 LLM-书籍对；括号中的数字是第一阶段（ <math intent=":literal" id="S4.F5.12.5.4.m4" display="inline" class="ltx_Math" alttext="N=0"><semantics><mrow><mi>N</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">N=0</annotation></semantics></math> 对于 Gemini 2.5 Pro 和 Grok 3，因为我们没有越狱这些生产 LLM）中的 BoN 样本 <math intent=":literal" id="S4.F5.11.4.3.m3" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 。 <math intent=":literal" id="S4.F5.13.6.5.m5" display="inline" class="ltx_Math" alttext="\dagger"><semantics><mo>†</mo><annotation encoding="application/x-tex">\dagger</annotation></semantics></math> 表示第一阶段失败； <math intent=":literal" id="S4.F5.14.7.6.m6" display="inline" class="ltx_Math" alttext="*"><semantics><mo>∗</mo><annotation encoding="application/x-tex">*</annotation></semantics></math> 表示我们没有尝试第二阶段。灰色阴影表示公共领域书籍。每行的垂直轴有不同的刻度。注意：每个条形图反映了第二阶段的一次运行，其中每个 LLM 的基础生成配置是固定的，但在不同 LLM 之间变化。这些条形图组并不反映在相同条件下测试所有生产 LLM 获得的结果的比较。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Books.</span>  We attempt to extract thirteen books: eleven in-copyright in the U.S. and two in the public domain.
We predominantly selected books that <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite> observe to be highly memorized by Llama 3.1 70B (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.1</span></a>).
The books under copyright in the U.S. are <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.2">Harry Potter and the Sorcerer’s Stone</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rowling, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib112" title="Harry potter and the sorcerer’s stone">1998</a>)</cite> (which we sometimes abbreviate in plot labels as “Harry Potter 1”), <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.3">Harry Potter and the Goblet of Fire</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rowling, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib113" title="Harry potter and the goblet of fire">2000</a>)</cite> (“Harry Potter 4”), <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.4">1984</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Orwell, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib90" title="Nineteen-eighty four">1949</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.5">The Hobbit</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Tolkien, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib116" title="The hobbit">1937</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.6">The Catcher in the Rye</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Salinger, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib127" title="The catcher in the rye">1951</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.7">A Game of Thrones</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Martin, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib128" title="A game of thrones">1996</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.8">Beloved</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Morrison, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib115" title="Beloved">1987</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.9">The Da Vinci Code</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib129" title="The da vinci code">2003</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.10">The Hunger Games</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Collins, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib131" title="The hunger games">2008</a>)</cite>,
<span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.11">Catch-22</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Heller, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib130" title="Catch-22">1961</a>)</cite>, and <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.12">The Duchess War</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Milan, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib132" title="The duchess war">2012</a>)</cite>.
The public domain books are <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.13">Frankenstein</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shelley, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib117" title="Frankenstein">1818</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.14">The Great Gatsby</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Fitzgerald, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib114" title="The great gatsby">1925</a>)</cite>.
We obtained these books from the Books3 corpus, which was torrented and released in 2020.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We have a copy of this dataset for research purposes only, stored on a university research computing cluster.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们仅保留了一份该数据集用于研究目的，存储在大学科研计算集群上。</font></font></font></span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">书籍。我们尝试提取十三本书：其中十一本在美国受版权保护，两本已进入公共领域。我们主要选择了 Cooper 等人（2025 年）观察到 Llama 3.1 70B 高度记忆的书籍（附录 C.1）。美国受版权保护的书籍包括《哈利·波特与魔法石》（罗琳，1998 年）（我们在情节标签中有时将其缩写为“哈利·波特 1”）、《哈利·波特与火焰杯》（罗琳，2000 年）（“哈利·波特 4”）、《1984》（奥威尔，1949 年）、《霍比特人》（托尔金，1937 年）、《麦田里的守望者》（塞林格，1951 年）、《冰与火之歌》（马丁，1996 年）、《宠儿》（莫里森，1987 年）、《达·芬奇密码》（布朗，2003 年）、《饥饿游戏》（柯林斯，2008 年）、《第二十二条军规》（赫尔勒，1961 年）和《公爵夫人战争》（米兰，2012 年）。公共领域的书籍包括《弗兰肯斯坦》（雪莱，1818 年）和《了不起的盖茨比》（菲茨杰拉德，1925 年）。我们从 Books3 语料库获取了这些书籍，该语料库于 2020 年通过 BT 下载发布。 <sup class="ltx_note_mark">8</sup> </font></font></font>
Therefore, all of these books significantly pre-date the knowledge cutoffs of every LLM we test.
Following <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>, as a negative control we also test <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.15">The Society of Unknowable Objects</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib126" title="The society of unknowable objects">2025</a>)</cite>, published in digital formats on July 31, 2025.
This date is long after the training cutoffs for all four LLMs, and therefore it is very unlikely that this original novel contains text that is in the training data.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">因此，所有这些书都显著早于我们测试的每一个 LLM 的知识截止日期。根据 Cooper 等人（2025 年）的研究，我们作为负对照也测试了《不可知之物社》（Brown，2025 年），该书于 2025 年 7 月 31 日以数字格式出版。这个日期远在所有四个 LLM 的训练截止日期之后，因此该原创小说包含的训练数据中的文本的可能性非常小。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.11"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.11.1">Configurations for the two-phase procedure and quantifying extraction success.</span> 
For Phase 1 (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.1</span></a>), we set a maximum BoN budget of <math alttext="N=$10,000$" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N=$10,000$</annotation></semantics></math> for each experiment.
In our initial experiments, we observed that we did not need to jailbreak Gemini 2.5 Pro or Grok 3 (<math alttext="N=0" class="ltx_Math" display="inline" id="S4.SS1.p4.2.m2" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">N=0</annotation></semantics></math>).
For the initial prompt of the instruction and seed prefix, we generate up to <math alttext="1000" class="ltx_Math" display="inline" id="S4.SS1.p4.3.m3" intent=":literal"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math> tokens as the response.
We only attempt Phase 2 if Phase 1 succeeds, with the production LLM producing a response that is at least a loose approximation of the target suffix, i.e., <math alttext="s\geq 0.6" class="ltx_Math" display="inline" id="S4.SS1.p4.4.m4" intent=":literal"><semantics><mrow><mi>s</mi><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s\geq 0.6</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E2" title="In 3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>). We run the Phase 2 continuation loop (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.2</span></a>) for up to a maximum query budget, or until the production LLM responds with a refusal or stop phrase, e.g., “THE END”.
The four production-LLMs APIs expose different, configurable generation parameters (e.g., frequency penalty).
For all four LLMs, we set temperature to <math alttext="0" class="ltx_Math" display="inline" id="S4.SS1.p4.5.m5" intent=":literal"><mn>0</mn></math>, but other LLM-specific configurations vary (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2" title="C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.2</span></a>).
For instance, based on our exploratory initial experiments, we observed it was necessary to set the per-interaction maximum generation length differently for each LLM to evade output filters.
For our extraction measurements (Algorithm&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#alg1" title="Algorithm 1 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>), we use the same conservative configurations across all runs.
For the first merge-and-filter, we set
<math alttext="\tau^{(1)}_{\mathrm{gap}}=2" class="ltx_Math" display="inline" id="S4.SS1.p4.6.m6" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{gap}}=2</annotation></semantics></math>, <math alttext="\tau^{(1)}_{\mathrm{align}}=1" class="ltx_Math" display="inline" id="S4.SS1.p4.7.m7" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{align}}=1</annotation></semantics></math>, and
<math alttext="l^{(1)}=20" class="ltx_Math" display="inline" id="S4.SS1.p4.8.m8" intent=":literal"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">l^{(1)}=20</annotation></semantics></math>;
for the second,
<math alttext="\tau^{(2)}_{\mathrm{gap}}=10" class="ltx_Math" display="inline" id="S4.SS1.p4.9.m9" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{gap}}=10</annotation></semantics></math>, <math alttext="\tau^{(2)}_{\mathrm{align}}=3" class="ltx_Math" display="inline" id="S4.SS1.p4.10.m10" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{align}}=3</annotation></semantics></math>, and <math alttext="l^{(2)}=100" class="ltx_Math" display="inline" id="S4.SS1.p4.11.m11" intent=":literal"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">l^{(2)}=100</annotation></semantics></math> (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>&nbsp;&amp; Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A2" title="Appendix B Procedure for quantifying extraction success ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">B</span></a>).
We provide full details on experimental configurations in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3" title="Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C</span></a>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">两阶段流程的配置和量化提取成功率。对于第一阶段（第 3.1 节），我们为每个实验设置最大 BoN 预算为 <math intent=":literal" id="S4.SS1.p4.1.m1" display="inline" class="ltx_Math" alttext="N=$10,000$"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N=$10,000$</annotation></semantics></math> 。在我们的初始实验中，我们观察到我们不需要破解 Gemini 2.5 Pro 或 Grok 3（ <math intent=":literal" id="S4.SS1.p4.2.m2" display="inline" class="ltx_Math" alttext="N=0"><semantics><mrow><mi>N</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">N=0</annotation></semantics></math> ）。对于指令和种子前缀的初始提示，我们生成最多 <math intent=":literal" id="S4.SS1.p4.3.m3" display="inline" class="ltx_Math" alttext="1000"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math> 个 token 作为响应。我们仅在第一阶段成功时尝试第二阶段，即生产 LLM 生成至少是目标后缀的松散近似响应，即 <math intent=":literal" id="S4.SS1.p4.4.m4" display="inline" class="ltx_Math" alttext="s\geq 0.6"><semantics><mrow><mi>s</mi><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s\geq 0.6</annotation></semantics></math> （公式 2）。我们运行第二阶段延续循环（第 3.2 节），直到达到最大查询预算或生产 LLM 响应拒绝或停止短语，例如“THE END”。四个生产 LLM API 暴露不同的、可配置的生成参数（例如，频率惩罚）。对于所有四个 LLM，我们设置温度为 <math intent=":literal" id="S4.SS1.p4.5.m5" display="inline" class="ltx_Math" alttext="0"><mn>0</mn></math> ，但其他 LLM 特定配置有所不同（附录 C.2）。例如，根据我们的探索性初始实验，我们观察到需要为每个 LLM 设置不同的每交互最大生成长度以规避输出过滤器。 在我们的提取测量（算法 1）中，所有运行都使用相同的保守配置。对于第一次合并和过滤，我们设置 <math intent=":literal" id="S4.SS1.p4.6.m6" display="inline" class="ltx_Math" alttext="\tau^{(1)}_{\mathrm{gap}}=2"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{gap}}=2</annotation></semantics></math> 、 <math intent=":literal" id="S4.SS1.p4.7.m7" display="inline" class="ltx_Math" alttext="\tau^{(1)}_{\mathrm{align}}=1"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{align}}=1</annotation></semantics></math> 和 <math intent=":literal" id="S4.SS1.p4.8.m8" display="inline" class="ltx_Math" alttext="l^{(1)}=20"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">l^{(1)}=20</annotation></semantics></math> ；对于第二次，设置 <math intent=":literal" id="S4.SS1.p4.9.m9" display="inline" class="ltx_Math" alttext="\tau^{(2)}_{\mathrm{gap}}=10"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{gap}}=10</annotation></semantics></math> 、 <math intent=":literal" id="S4.SS1.p4.10.m10" display="inline" class="ltx_Math" alttext="\tau^{(2)}_{\mathrm{align}}=3"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{align}}=3</annotation></semantics></math> 和 <math intent=":literal" id="S4.SS1.p4.11.m11" display="inline" class="ltx_Math" alttext="l^{(2)}=100"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">l^{(2)}=100</annotation></semantics></math> （第 3.3.1 节 &amp; 附录 B）。我们在附录 C 中提供了实验配置的详细信息。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>High-level extraction outcomes<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">4.2 高层级提取结果</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Across all Phase 2 runs, we extract hundreds of thousands of words of text.
We provide two concrete examples of extracted text from in-copyright books in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F6" title="Figure 6 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">6</span></a>, but do not redistribute long-form generations of in-copyright material.
We <a class="ltx_ref ltx_href" href="https://drive.google.com/drive/folders/1bCI1teXoVwgcZBvbWANc2Ss_h1x0zLv-?usp=sharing" title="">share</a> lightly normalized diffs for Claude 3.7 Sonnet on <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">Frankenstein</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">The Great Gatsby</span>, which are books in the public domain.
We do not include <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.3">The Duchess War</span> in plots;
of the thirteen books we attempt to extract, this is the only book where Phase 1 failed for all four production LLMs.
Similarly, we omit results for our negative control, <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.4">The Society of Unknowable Objects</span>;
as expected, Phase 1 also failed for this book (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS1" title="D.1 Additional Phase 1 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D.1</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在所有第二阶段运行中，我们提取了数十万字的文本。我们在图 6 中提供了两个来自版权保护书籍的提取文本实例，但不会重新分发版权保护材料的长期生成内容。我们分享了 Claude 3.7 Sonnet on Frankenstein 和 The Great Gatsby 的轻微规范化差异，这两本书都在公共领域。我们不包含 The Duchess War 在图中；在我们试图提取的十三本书中，这是唯一一本所有四个生产 LLMs 在第一阶段都失败的书。类似地，我们省略了我们负控制组 The Society of Unknowable Objects 的结果；正如预期的那样，这本书在第一阶段也失败了（附录 D.1）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Interpreting our bar plots.</span> 
In this section, each bar reflects results from a single, specifically configured run for a given production LLM and book;
across bars, the underlying generation configurations vary.
<span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">As a result, our results should be interpreted only as describing specific experimental outcomes:
each bar in a plot conveys how much extraction we observed under the specified experimental settings;
since these settings are not fixed across bars, our plots do not make evaluative claims about relative extraction risk across production LLMs.
</span> (See <cite class="ltx_cite ltx_citemacro_citet">Chouldechova<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib136" title="Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming">2025</a>)</cite>, and further discussion in Sections&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a> and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS1" title="5.1 Limitations and caveats ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5.1</span></a>.)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">解读我们的条形图。在本节中，每个条形图反映了一个特定配置的生产 LLM 和书籍的单一运行结果；不同条形图之间，底层的生成配置有所不同。因此，我们的结果应仅被理解为描述特定的实验结果：每个条形图传达了在指定实验设置下观察到的提取量；由于这些设置在不同条形图中并非固定不变，我们的条形图并未对生产 LLM 之间的相对提取风险做出评估性声明。（参见 Chouldechova 等人（2025 年）的研究，并在第 1 节和第 5.1 节中进一步讨论。）</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="133" id="S4.F6.sf1.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/gemini-hobbit-extraction.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F6.sf1.4.2" style="font-size:90%;">Gemini 2.5 Pro, <span class="ltx_text ltx_font_italic" id="S4.F6.sf1.4.2.1">The Hobbit</span> </span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(a) Gemini 2.5 Pro, 《霍比特人》</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="132" id="S4.F6.sf2.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/grok3-catcher-in-the-rye-extraction.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F6.sf2.4.2" style="font-size:90%;">Grok 3, <span class="ltx_text ltx_font_italic" id="S4.F6.sf2.4.2.1">The Catcher in the Rye</span> </span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(b) Grok 3, 《麦田里的守望者》</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.7.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F6.8.2" style="font-size:90%;">Extracted text from in-copyright books.<span class="ltx_text ltx_font_medium" id="S4.F6.8.2.1">
We provide two cropped examples of text extracted during Phase 2, diffing the ground-truth book from Books3 with the production LLM generation.
Text in black reflects a verbatim match between the two;
bold blue text reflects generated text that is absent in book;
strike-through red text indicates ground-truth text absent from the generated text.</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 6：从版权受保护的书籍中提取的文本。我们提供了两个在第二阶段提取文本的裁剪示例，将 Books3 中的真实书籍与生产 LLM 的生成文本进行对比。黑色文本反映了两者之间的逐字匹配；粗体蓝色文本反映了书中未出现的生成文本；删除线红色文本表示真实文本在生成文本中缺失。</font></font></font></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="S4.T1" style="width:469.8pt;">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.F6.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.F6.4.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.F6.4.4.5.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.F6.4.4.5.1.1.1" style="font-size:80%;">Book<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">书</font></font></font></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.F6.4.4.5.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.F6.4.4.5.1.2.1" style="font-size:80%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.F6.4.4.5.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.F6.4.4.5.1.3.1" style="font-size:80%;">GPT-4.1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.F6.4.4.5.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.F6.4.4.5.1.4.1" style="font-size:80%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.F6.4.4.5.1.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.F6.4.4.5.1.5.1" style="font-size:80%;" data-imt_insert_failed="1">Grok 3</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.F6.4.4.4">
<td class="ltx_td" id="S4.F6.4.4.4.5" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.4.4.4.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.4.6.1" style="font-size:70%;"># Cont.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"># 继续</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.4.4.4.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.4.7.1" style="font-size:70%;">Cost<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">成本</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="\max|\beta|" class="ltx_Math" display="inline" id="S4.F6.1.1.1.1.m1" intent=":literal"><semantics><mrow><mi mathsize="0.700em">max</mi><mo>⁡</mo><mrow><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo><mi mathsize="0.700em">β</mi><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo></mrow></mrow><annotation encoding="application/x-tex">\max|\beta|</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.4.4.4.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.4.8.1" style="font-size:70%;"># Cont.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"># 继续</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.4.4.4.9" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.4.9.1" style="font-size:70%;">Cost<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">成本</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.2.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="\max|\beta|" class="ltx_Math" display="inline" id="S4.F6.2.2.2.2.m1" intent=":literal"><semantics><mrow><mi mathsize="0.700em">max</mi><mo>⁡</mo><mrow><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo><mi mathsize="0.700em">β</mi><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo></mrow></mrow><annotation encoding="application/x-tex">\max|\beta|</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.4.4.4.10" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.4.10.1" style="font-size:70%;"># Cont.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"># 继续</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.4.4.4.11" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.4.11.1" style="font-size:70%;">Cost<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">成本</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.3.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="\max|\beta|" class="ltx_Math" display="inline" id="S4.F6.3.3.3.3.m1" intent=":literal"><semantics><mrow><mi mathsize="0.700em">max</mi><mo>⁡</mo><mrow><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo><mi mathsize="0.700em">β</mi><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo></mrow></mrow><annotation encoding="application/x-tex">\max|\beta|</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.4.4.4.12" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.4.12.1" style="font-size:70%;"># Cont.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"># 继续</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.4.4.4.13" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.4.13.1" style="font-size:70%;">Cost<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">成本</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.F6.4.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="\max|\beta|" class="ltx_Math" display="inline" id="S4.F6.4.4.4.4.m1" intent=":literal"><semantics><mrow><mi mathsize="0.700em">max</mi><mo>⁡</mo><mrow><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo><mi mathsize="0.700em">β</mi><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo></mrow></mrow><annotation encoding="application/x-tex">\max|\beta|</annotation></semantics></math></th>
</tr>
<tr class="ltx_tr" id="S4.F6.4.4.6.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.F6.4.4.6.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><em class="ltx_emph ltx_font_italic" id="S4.F6.4.4.6.1.1.1" style="font-size:70%;">Harry Potter 1<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特 1</font></font></font></em></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.2.1" style="font-size:80%;">480</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.3.1" style="font-size:80%;">$119.97</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.4.1" style="font-size:80%;">6658</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.5.1" style="font-size:80%;">31</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.6.1" style="font-size:80%;">$1.37</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.7.1" style="font-size:80%;">821</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.8.1" style="font-size:80%;">171</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.9" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.9.1" style="font-size:80%;">$2.44</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.10" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.10.1" style="font-size:80%;">9070</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.11" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.11.1" style="font-size:80%;">52</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.12" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.12.1" style="font-size:80%;">$8.16</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.F6.4.4.6.1.13" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.6.1.13.1" style="font-size:80%;">6337</span></td>
</tr>
<tr class="ltx_tr" id="S4.F6.4.4.7.2">
<td class="ltx_td ltx_align_left" id="S4.F6.4.4.7.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.F6.4.4.7.2.1.1" style="font-size:70%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">弗兰肯斯坦</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.2.1" style="font-size:80%;">374</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.3.1" style="font-size:80%;">$55.41</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.4.1" style="font-size:80%;">8732</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.5.1" style="font-size:80%;">33</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.6.1" style="font-size:80%;">$0.19</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.7.1" style="font-size:80%;">474</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.8.1" style="font-size:80%;">204</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.9" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.9.1" style="font-size:80%;">$0.38</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.10" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.10.1" style="font-size:80%;">448</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.11" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.11.1" style="font-size:80%;">300</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.12" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.12.1" style="font-size:80%;">$77.12</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.7.2.13" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.7.2.13.1" style="font-size:80%;">275</span></td>
</tr>
<tr class="ltx_tr" id="S4.F6.4.4.8.3">
<td class="ltx_td ltx_align_left" id="S4.F6.4.4.8.3.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.F6.4.4.8.3.1.1" style="font-size:70%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">霍比特人</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.2.1" style="font-size:80%;">1000</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.3.1" style="font-size:80%;">$134.87</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.4.1" style="font-size:80%;">8835</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.5.1" style="font-size:80%;">4</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.6.1" style="font-size:80%;">$0.16</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.7.1" style="font-size:80%;">205</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.8.1" style="font-size:80%;">188</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.9" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.9.1" style="font-size:80%;">$0.52</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.10" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.10.1" style="font-size:80%;">571</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.11" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.11.1" style="font-size:80%;">115</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.12" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.12.1" style="font-size:80%;">$23.40</span></td>
<td class="ltx_td ltx_align_right" id="S4.F6.4.4.8.3.13" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.8.3.13.1" style="font-size:80%;">1816</span></td>
</tr>
<tr class="ltx_tr" id="S4.F6.4.4.9.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.F6.4.4.9.4.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.F6.4.4.9.4.1.1" style="font-size:70%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.2.1" style="font-size:80%;">562</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.3.1" style="font-size:80%;">$124.49</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.4.1" style="font-size:80%;">1091</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.5.1" style="font-size:80%;">15</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.6.1" style="font-size:80%;">$0.16</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.7.1" style="font-size:80%;">0</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.8.1" style="font-size:80%;">166</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.9" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.9.1" style="font-size:80%;">$0.36</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.10" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.10.1" style="font-size:80%;">138</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.11" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.11.1" style="font-size:80%;">195</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.12" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.12.1" style="font-size:80%;">$42.36</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.F6.4.4.9.4.13" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.F6.4.4.9.4.13.1" style="font-size:80%;">836</span></td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T1.10.2.1" style="font-size:113%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T1.2.1" style="font-size:113%;">Number of continue queries, cost, and maximum block length from Phase 2.<span class="ltx_text ltx_font_medium" id="S4.T1.2.1.1">
For each book in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F7" title="Figure 7 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a>, we show the number of times we query each production LLM to continue in Phase 2, as well as the cost ($) of running this loop.
We also show the length of the longest near-verbatim block (<math alttext="\max|\beta|" class="ltx_Math" display="inline" id="S4.T1.2.1.1.m1" intent=":literal"><semantics><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">|</mo><mi>β</mi><mo stretchy="false">|</mo></mrow></mrow><annotation encoding="application/x-tex">\max|\beta|</annotation></semantics></math>) resulting from Phase 2.
See Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2" title="D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D.2</span></a>.</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 1：第 2 阶段的继续查询次数、成本和最大块长度。对于图 7 中的每本书，我们展示了在第 2 阶段查询每个生产 LLM 以继续的次数，以及运行此循环的成本（美元）。我们还展示了第 2 阶段产生的最长近乎逐字复制的块（ <math intent=":literal" id="S4.T1.2.1.1.m1" display="inline" class="ltx_Math" alttext="\max|\beta|"><semantics><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">|</mo><mi>β</mi><mo stretchy="false">|</mo></mrow></mrow><annotation encoding="application/x-tex">\max|\beta|</annotation></semantics></math> ）的长度。参见附录 D.2。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.7"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Proportion of book extracted (<math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext class="ltx_mathvariant_bold">-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math>).</span> 
Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a> plots <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E7" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a>):
the overall proportion of a book extracted in in-order, near-verbatim blocks (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>).
For a given production LLM, we fix the same generation configuration across books;
however, the generation configuration varies across LLMs.
Overall, these results show that it is possible to extract text across books and frontier LLMs.
Importantly, we did not jailbreak Gemini 2.5 Pro and Grok 3 in Phase 1 to obtain these results in Phase 2.
For Claude 3.7 Sonnet and GPT-4.1, we use BoN with up to <math alttext="N=$10,000$" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m2" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N=$10,000$</annotation></semantics></math> attempts in Phase 1.
While in terms of dollar-cost BoN is cheap to run for this budget, we note that it almost always required significantly larger <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m3" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>—often <math alttext="10{-}1000\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.p3.5.m4" intent=":literal"><semantics><mrow><mn>10</mn><mo>−</mo><mn>1000</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">10{-}1000\times</annotation></semantics></math>—to jailbreak GPT-4.1
compared to Claude 3.7 Sonnet.
In four cases, Claude 3.7 Sonnet’s generations recover over <math alttext="94\%" class="ltx_Math" display="inline" id="S4.SS2.p3.6.m5" intent=":literal"><semantics><mrow><mn>94</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">94\%</annotation></semantics></math> of the corresponding reference book.
Two of these books—<span class="ltx_text ltx_font_italic" id="S4.SS2.p3.7.2">Harry Potter and the Sorcerer’s Stone</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.7.3">1984</span>—are in-copyright in the U.S., while the other two—<span class="ltx_text ltx_font_italic" id="S4.SS2.p3.7.4">The Great Gatsby</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.7.5">Frankenstein</span>—are in the public domain.
In three other cases for Claude 3.7 Sonnet, <math alttext="\mathsf{nv{\text{-}}recall}\geq 32\%" class="ltx_Math" display="inline" id="S4.SS2.p3.7.m6" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>≥</mo><mrow><mn>32</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}\geq 32\%</annotation></semantics></math>.
With respect to LLM-specific generation configurations, we extract significant amounts of <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.7.6">Harry Potter and the Sorcerer’s Stone</span> and other books from all four production LLMs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提取书籍的比例（ <math intent=":literal" id="S4.SS2.p3.1.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext class="ltx_mathvariant_bold">-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> ）。图 5 绘制了 <math intent=":literal" id="S4.SS2.p3.2.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> （公式 7）：按顺序、近乎逐字提取的书籍整体比例（第 3.3.1 节）。对于给定的生产 LLM，我们在不同书籍中保持相同的生成配置；然而，生成配置在不同 LLM 之间有所不同。总体而言，这些结果表明可以在不同书籍和前沿 LLM 之间提取文本。重要的是，我们在第一阶段没有越狱 Gemini 2.5 Pro 和 Grok 3 以在第二阶段获得这些结果。对于 Claude 3.7 Sonnet 和 GPT-4.1，我们在第一阶段使用 BoN，最多尝试 <math intent=":literal" id="S4.SS2.p3.3.m2" display="inline" class="ltx_Math" alttext="N=$10,000$"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N=$10,000$</annotation></semantics></math> 次。虽然从美元成本来看，BoN 在这个预算下运行很便宜，但我们注意到，与 Claude 3.7 Sonnet 相比，越狱 GPT-4.1 几乎总是需要显著更大的 <math intent=":literal" id="S4.SS2.p3.4.m3" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> ——通常是 <math intent=":literal" id="S4.SS2.p3.5.m4" display="inline" class="ltx_math_unparsed" alttext="10{-}1000\times"><semantics><mrow><mn>10</mn><mo>−</mo><mn>1000</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">10{-}1000\times</annotation></semantics></math> 。在四个案例中，Claude 3.7 Sonnet 的生成恢复了对应参考书籍的 <math intent=":literal" id="S4.SS2.p3.6.m5" display="inline" class="ltx_Math" alttext="94\%"><semantics><mrow><mn>94</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">94\%</annotation></semantics></math> 以上。其中这两本书——《哈利·波特与魔法石》和《1984》——在美国受版权保护，而另外两本——《了不起的盖茨比》和《弗兰肯斯坦》——则属于公共领域。在 Claude 3.7 Sonnet 的另外三个案例中， <math intent=":literal" id="S4.SS2.p3.7.m6" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}\geq 32\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>≥</mo><mrow><mn>32</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}\geq 32\%</annotation></semantics></math> 。 关于特定于 LLM 的生成配置，我们从全部四个生产 LLM 中提取了大量《哈利·波特与魔法石》和其他书籍的内容。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">We frequently query the production LLM to continue hundreds of times per Phase 2 run, without encountering guardrails.
However, when we run Phase 2 for GPT-4.1, we hit a refusal fairly early on in the continuation loop.
For instance, for <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.1">Harry Potter and the Sorcerer’s Stone</span>, this happens at the end of the first chapter.
Therefore, while we report <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> with respect to the full book, near-verbatim extraction is limited to the first chapter for GPT-4.1.
For the other three production LLMs, we almost always do not encounter refusals (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS3" title="4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.3</span></a>), and so halt Phase 2 when either a maximum query budget is expended, the LLM returns a response containing a stop phrase (e.g., “THE END”), or the API returns an HTTP error. (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.2</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在 Phase 2 的每个运行中，我们频繁地向生产 LLM 查询数百次，并未遇到任何限制措施。然而，在为 GPT-4.1 运行 Phase 2 时，我们在延续循环的早期就遇到了拒绝。例如，对于《哈利·波特与魔法石》，这发生在第一章的结尾。因此，虽然我们针对整本书报告 <math intent=":literal" id="S4.SS2.p4.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> ，但针对 GPT-4.1 的近乎逐字提取仅限于第一章。对于其他三个生产 LLM，我们几乎从不遇到拒绝（第 4.3 节），因此当达到最大查询预算、LLM 返回包含停止短语的响应（例如，“THE END”）或 API 返回 HTTP 错误时，我们会停止 Phase 2（第 3.2 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.4">The cost of the loop varies across runs, according to the provider’s billing policy, the number of queries, and the number of tokens returned per query.
For instance, as shown in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.T1" title="Table 1 ‣ Figure 6 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>, it cost approximately $119.97 to extract <span class="ltx_text ltx_font_italic" id="S4.SS2.p5.4.1">Harry Potter and the Sorcerer’s Stone</span> with <math alttext="\mathsf{nv{\text{-}}recall}=95.8\%" class="ltx_Math" display="inline" id="S4.SS2.p5.1.m1" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>95.8</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=95.8\%</annotation></semantics></math> from jailbroken Claude 3.7 Sonnet and $1.37 for jailbroken GPT-4.1 (<math alttext="\mathsf{nv{\text{-}}recall}=4.0\%" class="ltx_Math" display="inline" id="S4.SS2.p5.2.m2" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>4.0</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=4.0\%</annotation></semantics></math>);
it cost approximately $2.44 for not-jailbroken Gemini 2.5 Pro (<math alttext="\mathsf{nv{\text{-}}recall}=76.8\%" class="ltx_Math" display="inline" id="S4.SS2.p5.3.m3" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>76.8</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=76.8\%</annotation></semantics></math>) and $8.16 for not-jailbroken Grok 3 (<math alttext="\mathsf{nv{\text{-}}recall}=70.3\%" class="ltx_Math" display="inline" id="S4.SS2.p5.4.m4" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>70.3</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=70.3\%</annotation></semantics></math>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">循环的成本因运行而异，根据提供商的计费政策、查询次数以及每个查询返回的 token 数量而变化。例如，如表 1 所示，使用被破解的 Claude 3.7 Sonnet 从 <math intent=":literal" id="S4.SS2.p5.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=95.8\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>95.8</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=95.8\%</annotation></semantics></math> 中提取《哈利·波特与魔法石》的成本约为 119.97 美元，而使用被破解的 GPT-4.1 的成本为 1.37 美元（ <math intent=":literal" id="S4.SS2.p5.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=4.0\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>4.0</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=4.0\%</annotation></semantics></math> ）；使用未破解的 Gemini 2.5 Pro 的成本约为 2.44 美元（ <math intent=":literal" id="S4.SS2.p5.3.m3" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=76.8\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>76.8</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=76.8\%</annotation></semantics></math> ），而使用未破解的 Grok 3 的成本为 8.16 美元（ <math intent=":literal" id="S4.SS2.p5.4.m4" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=70.3\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>70.3</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=70.3\%</annotation></semantics></math> ）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="353" id="S4.F7.sf1.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_harry_potter_1.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_italic" id="S4.F7.sf1.4.2" style="font-size:90%;">Harry Potter and the Sorcerer’s Stone<span class="ltx_text ltx_font_upright" id="S4.F7.sf1.4.2.1"> </span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(a) 《哈利·波特与魔法石》</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="353" id="S4.F7.sf2.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_frankenstein.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_italic" id="S4.F7.sf2.4.2" style="font-size:90%;">Frankenstein<span class="ltx_text ltx_font_upright" id="S4.F7.sf2.4.2.1"> (public domain)</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(b) 弗兰肯斯坦（公共领域）</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="353" id="S4.F7.sf3.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_the_hobbit.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.sf3.3.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text ltx_font_italic" id="S4.F7.sf3.4.2" style="font-size:90%;">The Hobbit<span class="ltx_text ltx_font_upright" id="S4.F7.sf3.4.2.1"> </span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(c) 《霍比特人》</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="354" id="S4.F7.sf4.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_game_of_thrones.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.sf4.3.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text ltx_font_italic" id="S4.F7.sf4.4.2" style="font-size:90%;">A Game of Thrones<span class="ltx_text ltx_font_upright" id="S4.F7.sf4.4.2.1"> </span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(d) 一场风暴的盛宴</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.12.5.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F7.8.4" style="font-size:90%;">Absolute word counts.<span class="ltx_text ltx_font_medium" id="S4.F7.8.4.4">
For the Phase 2 runs for four books in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>, we show the count <math alttext="m" class="ltx_Math" display="inline" id="S4.F7.5.1.1.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E6" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">6</span></a>) of extracted words, as well as the estimated counts of words in the book that are <math alttext="\mathsf{missing}" class="ltx_Math" display="inline" id="S4.F7.6.2.2.m2" intent=":literal"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> in the generated text and words in the generated text that are <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.F7.7.3.3.m3" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> with respect to the book (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E8" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">8</span></a>).
In each plot, the dotted gray line indicates the length of the book in words (<math alttext="|B|" class="ltx_Math" display="inline" id="S4.F7.8.4.4.m4" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|B|</annotation></semantics></math>).
We provide results for other books in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4" title="Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D</span></a>.
<span class="ltx_text ltx_font_italic" id="S4.F7.8.4.4.1">Note: The generation configuration is fixed per LLM across books, but varies across LLMs.
For a given book, the per-LLM sets of bars do not reflect comparisons of results obtained from testing all production LLMs under the same conditions.</span>
</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 7：绝对词频。对于图 5 中四个书籍的 Phase 2 运行，我们展示了提取词的计数 <math intent=":literal" id="S4.F7.5.1.1.m1" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> （公式 6），以及书中词在生成文本中的估计计数 <math intent=":literal" id="S4.F7.6.2.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{missing}"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> 和生成文本中相对于书 <math intent=":literal" id="S4.F7.7.3.3.m3" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 的词（公式 8）。在每个图中，虚线灰色线表示书的词长度 <math intent=":literal" id="S4.F7.8.4.4.m4" display="inline" class="ltx_Math" alttext="|B|"><semantics><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|B|</annotation></semantics></math> 。我们为其他书籍提供了结果，见附录 D。注意：生成配置在每个书籍中针对 LLM 是固定的，但在不同 LLM 之间是变化的。对于给定书籍，每个 LLM 的条形图并不反映在相同条件下测试所有生产 LLM 获得的结果的比较。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.13"><span class="ltx_text ltx_font_bold" id="S4.SS2.p6.13.1">Absolute extraction.</span> 
For a sense of the scale of how much text we extracted, it is also useful to examine absolute word counts.
In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F7" title="Figure 7 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a>, we show results for four books for the total number of words <math alttext="m" class="ltx_Math" display="inline" id="S4.SS2.p6.1.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> that we extracted in in-order, near-verbatim blocks (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E6" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">6</span></a>).
As points of comparison, the <math alttext="\mathsf{missing}" class="ltx_Math" display="inline" id="S4.SS2.p6.2.m2" intent=":literal"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> count estimates how much text from the reference book was not extracted, and <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.SS2.p6.3.m3" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> estimates how much text in the generation is not contained in the reference book. These metrics reveal additional nuances.
First, low percentages of <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.SS2.p6.4.m4" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> can of course reflect enormous amounts of extraction.
For <span class="ltx_text ltx_font_italic" id="S4.SS2.p6.13.2">Harry Potter and the Sorcerer’s Stone</span>, we extracted thousands of words near-verbatim from all production LLMs.
Even for GPT-4.1, for which <math alttext="\mathsf{nv{\text{-}}recall}=4.0\%" class="ltx_Math" display="inline" id="S4.SS2.p6.5.m5" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>4.0</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=4.0\%</annotation></semantics></math>, we extracted approximately <math alttext="m\approx 3200" class="ltx_Math" display="inline" id="S4.SS2.p6.6.m6" intent=":literal"><semantics><mrow><mi>m</mi><mo>≈</mo><mn>3200</mn></mrow><annotation encoding="application/x-tex">m\approx 3200</annotation></semantics></math> words from the book. For <span class="ltx_text ltx_font_italic" id="S4.SS2.p6.13.3">A Game of Thrones</span>, which is a significantly longer book, <math alttext="\mathsf{nv{\text{-}}recall}=1.3\%" class="ltx_Math" display="inline" id="S4.SS2.p6.7.m7" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>1.3</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=1.3\%</annotation></semantics></math> for Grok 3, which corresponds to <math alttext="m\approx 3700" class="ltx_Math" display="inline" id="S4.SS2.p6.8.m8" intent=":literal"><semantics><mrow><mi>m</mi><mo>≈</mo><mn>3700</mn></mrow><annotation encoding="application/x-tex">m\approx 3700</annotation></semantics></math> words of near-verbatim extracted text. Further, separate from total near-verbatim extraction, the individual extracted blocks can also be quite long.
In Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.T1" title="Table 1 ‣ Figure 6 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>, we show the longest extracted block for each experiment in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F7" title="Figure 7 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a>.
For <span class="ltx_text ltx_font_italic" id="S4.SS2.p6.13.4">Harry Potter and the Sorcerer’s Stone</span>, the longest near-verbatim blocks are <math alttext="6658" class="ltx_Math" display="inline" id="S4.SS2.p6.9.m9" intent=":literal"><semantics><mn>6658</mn><annotation encoding="application/x-tex">6658</annotation></semantics></math>, <math alttext="821" class="ltx_Math" display="inline" id="S4.SS2.p6.10.m10" intent=":literal"><semantics><mn>821</mn><annotation encoding="application/x-tex">821</annotation></semantics></math>, <math alttext="9070" class="ltx_Math" display="inline" id="S4.SS2.p6.11.m11" intent=":literal"><semantics><mn>9070</mn><annotation encoding="application/x-tex">9070</annotation></semantics></math>, and <math alttext="6337" class="ltx_Math" display="inline" id="S4.SS2.p6.12.m12" intent=":literal"><semantics><mn>6337</mn><annotation encoding="application/x-tex">6337</annotation></semantics></math> words for Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3, respectively.
The longest verbatim string that <cite class="ltx_cite ltx_citemacro_citet">Nasr<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>)</cite> extracted from ChatGPT 3.5 was slightly over <math alttext="4000" class="ltx_Math" display="inline" id="S4.SS2.p6.13.m13" intent=":literal"><semantics><mn>4000</mn><annotation encoding="application/x-tex">4000</annotation></semantics></math> <em class="ltx_emph ltx_font_italic" id="S4.SS2.p6.13.5">characters</em>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">绝对提取。为了了解我们提取文本的规模，检查绝对词数也很有用。在图 7 中，我们展示了四个书籍在按顺序、近乎逐字提取的块中提取的总词数 <math intent=":literal" id="S4.SS2.p6.1.m1" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> （公式 6）。作为比较基准， <math intent=":literal" id="S4.SS2.p6.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{missing}"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> 计数估计了参考书中未被提取的文本量，而 <math intent=":literal" id="S4.SS2.p6.3.m3" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 估计了生成文本中不在参考书中的文本量。这些指标揭示了更多的细节。首先，低百分比的 <math intent=":literal" id="S4.SS2.p6.4.m4" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 当然可以反映巨大的提取量。对于《哈利·波特与魔法石》，我们从所有生产 LLMs 中近乎逐字提取了数千个词。即使对于 <math intent=":literal" id="S4.SS2.p6.5.m5" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=4.0\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>4.0</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=4.0\%</annotation></semantics></math> 的 GPT-4.1，我们也从书中提取了大约 <math intent=":literal" id="S4.SS2.p6.6.m6" display="inline" class="ltx_Math" alttext="m\approx 3200"><semantics><mrow><mi>m</mi><mo>≈</mo><mn>3200</mn></mrow><annotation encoding="application/x-tex">m\approx 3200</annotation></semantics></math> 个词。对于篇幅显著更长的《冰与火之歌》，Grok 3 的 <math intent=":literal" id="S4.SS2.p6.7.m7" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=1.3\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>1.3</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=1.3\%</annotation></semantics></math> （对应于 <math intent=":literal" id="S4.SS2.p6.8.m8" display="inline" class="ltx_Math" alttext="m\approx 3700"><semantics><mrow><mi>m</mi><mo>≈</mo><mn>3700</mn></mrow><annotation encoding="application/x-tex">m\approx 3700</annotation></semantics></math> 个近乎逐字提取的词）。此外，除了总近乎逐字提取外，单独提取的块也可以相当长。在表 1 中，我们展示了图 7 中每个实验的最长提取块。 对于《哈利·波特与魔法石》，Claude 3.7 Sonnet、GPT-4.1、Gemini 2.5 Pro 和 Grok 3 的最长近乎逐字复制的块分别是 <math intent=":literal" id="S4.SS2.p6.9.m9" display="inline" class="ltx_Math" alttext="6658"><semantics><mn>6658</mn><annotation encoding="application/x-tex">6658</annotation></semantics></math> 、 <math intent=":literal" id="S4.SS2.p6.10.m10" display="inline" class="ltx_Math" alttext="821"><semantics><mn>821</mn><annotation encoding="application/x-tex">821</annotation></semantics></math> 、 <math intent=":literal" id="S4.SS2.p6.11.m11" display="inline" class="ltx_Math" alttext="9070"><semantics><mn>9070</mn><annotation encoding="application/x-tex">9070</annotation></semantics></math> 和 <math intent=":literal" id="S4.SS2.p6.12.m12" display="inline" class="ltx_Math" alttext="6337"><semantics><mn>6337</mn><annotation encoding="application/x-tex">6337</annotation></semantics></math> 个词。Nasr 等人（2023 年）从 ChatGPT 3.5 中提取的最长逐字字符串略超过 <math intent=":literal" id="S4.SS2.p6.13.m13" display="inline" class="ltx_Math" alttext="4000"><semantics><mn>4000</mn><annotation encoding="application/x-tex">4000</annotation></semantics></math> 个字符。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.8">Second, interpreting <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.SS2.p7.1.m1" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> and <math alttext="\mathsf{missing}" class="ltx_Math" display="inline" id="S4.SS2.p7.2.m2" intent=":literal"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F7" title="Figure 7 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a> indicates some important caveats.
Recall that both counts may contain some instances of valid extraction that our measurement procedure under-counts.
Since our extraction metric <math alttext="m" class="ltx_Math" display="inline" id="S4.SS2.p7.3.m3" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> counts contiguous near-verbatim blocks, potentially duplicated (still valid) extraction may contribute to <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.SS2.p7.4.m4" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math>, and near-verbatim text that is generated out-of-order with respect to the reference book may be counted in both <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.SS2.p7.5.m5" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> and <math alttext="\mathsf{missing}" class="ltx_Math" display="inline" id="S4.SS2.p7.6.m6" intent=":literal"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>).
For instance, we note that the diff for Claude 3.7 Sonnet’s generation and <span class="ltx_text ltx_font_italic" id="S4.SS2.p7.8.1">The Great Gatsby</span> has extensive repeats of extracted text on pages 114–132, which contribute to <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.SS2.p7.7.m7" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math>.
Note that duplicates also have an effect on the quality of the overall reproduction of a book in extracted outputs.
While for Claude 3.7 Sonnet we extract <math alttext="\mathsf{nv{\text{-}}recall}=97.5\%" class="ltx_Math" display="inline" id="S4.SS2.p7.8.m8" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>97.5</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=97.5\%</annotation></semantics></math> of the reference book, we did not extract a pristine copy of the whole book.
Qualitative inspection of diffs for Claude 3.7 Sonnet on <span class="ltx_text ltx_font_italic" id="S4.SS2.p7.8.2">Frankenstein</span>, <span class="ltx_text ltx_font_italic" id="S4.SS2.p7.8.3">1984</span>, and <span class="ltx_text ltx_font_italic" id="S4.SS2.p7.8.4">Harry Potter and the Sorcerer’s Stone</span> reveals that we extracted cleaner copies of the ground-truth text that lack repeated extraction.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">其次，解读图 7 中的 <math intent=":literal" id="S4.SS2.p7.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 和 <math intent=":literal" id="S4.SS2.p7.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{missing}"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> 表明存在一些重要的注意事项。回想一下，这两个计数都可能包含一些我们测量程序低估的有效提取实例。由于我们的提取指标 <math intent=":literal" id="S4.SS2.p7.3.m3" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> 计算连续的近乎逐字复制的块，潜在的重复（但仍然有效）的提取可能对 <math intent=":literal" id="S4.SS2.p7.4.m4" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 有贡献，而与参考书顺序不一致的近乎逐字生成的文本可能同时被计入 <math intent=":literal" id="S4.SS2.p7.5.m5" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 和 <math intent=":literal" id="S4.SS2.p7.6.m6" display="inline" class="ltx_Math" alttext="\mathsf{missing}"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> （第 3.3.1 节）。例如，我们注意到 Claude 3.7 Sonnet 生成 The Great Gatsby 的 diff 中，第 114 至 132 页有大量重复的提取文本，这增加了 <math intent=":literal" id="S4.SS2.p7.7.m7" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 。请注意，重复也会影响提取输出中整本书复制的质量。虽然对于 Claude 3.7 Sonnet 我们提取了 <math intent=":literal" id="S4.SS2.p7.8.m8" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=97.5\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mn>97.5</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=97.5\%</annotation></semantics></math> 的参考书，但我们并未提取整本书的原始副本。对 Claude 3.7 Sonnet 在 Frankenstein、1984 和 Harry Potter and the Sorcerer’s Stone 上的 diff 进行定性检查表明，我们提取了更干净的、缺乏重复提取的 ground-truth 文本副本。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="110" id="S4.F8.sf1.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/x5.png" width="900">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf1.5.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F8.sf1.6.2" style="font-size:90%;">(<span class="ltx_text ltx_font_bold" id="S4.F8.sf1.6.2.1">left</span>) Ground-truth text from <span class="ltx_text ltx_font_italic" id="S4.F8.sf1.6.2.2">A Game of Thrones</span> and (<span class="ltx_text ltx_font_bold" id="S4.F8.sf1.6.2.3">right</span>) GPT-4.1-generated text in Phase 2.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(a) (左)《冰与火之歌》的真实文本和(右)第二阶段中由 GPT-4.1 生成的文本。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="S4.F8.sf2.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/x6.png" width="900">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F8.sf2.4.2" style="font-size:90%;">Longer snippet of GPT-4.1-generated text in Phase 2 for <span class="ltx_text ltx_font_italic" id="S4.F8.sf2.4.2.1">A Game of Thrones</span>.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(b) 第二阶段用于《冰与火之歌》的 GPT-4.1 生成文本的更长时间片段。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.13.5.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F8.8.4" style="font-size:90%;">Examples of generated text that is <em class="ltx_emph ltx_font_italic" id="S4.F8.8.4.5">not</em> extraction.<span class="ltx_text ltx_font_medium" id="S4.F8.8.4.4">
We provide brief examples of text generated by GPT-4.1 in the Phase 2 continuation loop that are <em class="ltx_emph ltx_font_italic" id="S4.F8.8.4.4.1">not</em> extraction, and do not contribute to <math alttext="m" class="ltx_Math" display="inline" id="S4.F8.5.1.1.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> (and thus also not <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.F8.6.2.2.m2" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math>), but to <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.F8.7.3.3.m3" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E8" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">8</span></a>).
For all production LLMs that we test, we qualitatively observe that <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.F8.8.4.4.m4" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> text frequently replicates plot elements, themes, and character names from the book we attempt to extract.
<span class="ltx_text ltx_font_italic" id="S4.F8.8.4.4.2">Note: Since our focus is extraction, we do not attempt to evaluate this text quantitatively or at scale;
one should not draw strong conclusions from these examples.</span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 8：非提取生成的文本示例。我们提供了 GPT-4.1 在第二阶段延续循环中生成的非提取文本的简要示例，这些文本不贡献于 <math intent=":literal" id="S4.F8.5.1.1.m1" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> （因此也不贡献于 <math intent=":literal" id="S4.F8.6.2.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> ），而是贡献于 <math intent=":literal" id="S4.F8.7.3.3.m3" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> （公式 8）。对于我们测试的所有生产 LLMs，我们定性观察到 <math intent=":literal" id="S4.F8.8.4.4.m4" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 文本经常复制我们试图提取的书籍中的情节元素、主题和角色名称。注意：由于我们的重点是提取，我们没有尝试定量评估或大规模评估这些文本；不应从这些示例中得出强烈结论。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.7"><span class="ltx_text ltx_font_bold" id="S4.SS2.p8.1.1">Brief qualitative observations about <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.SS2.p8.1.1.m1" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> generated text.</span> 
We perform limited qualitative analysis of the <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.SS2.p8.2.m1" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> generated text.
As noted above, a portion of this text may contain duplicated or out-of-order extraction.
However, this is not always the case;
often, the <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.SS2.p8.3.m2" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> generated text is <em class="ltx_emph ltx_font_italic" id="S4.SS2.p8.7.2">not</em> extraction.
Brief qualitative inspection of this text for all of our experiments reveals that, for all books and frontier LLMs, <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S4.SS2.p8.4.m3" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> text frequently contains text that replicates plot elements, themes, and character names from the book from which the Phase 1 prefix is drawn.
We provide two examples of such text in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F8" title="Figure 8 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">8</span></a>;
these examples are drawn from GPT-4.1-generated text following Phase 1 success with a seed prefix from <span class="ltx_text ltx_font_italic" id="S4.SS2.p8.7.3">A Game of Thrones</span>.
Note that <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.SS2.p8.5.m4" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> is exactly <math alttext="0\%" class="ltx_Math" display="inline" id="S4.SS2.p8.6.m5" intent=":literal"><semantics><mrow><mn>0</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">0\%</annotation></semantics></math> for GPT-4.1 for <span class="ltx_text ltx_font_italic" id="S4.SS2.p8.7.4">A Game of Thrones</span> (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>), as matched words <math alttext="m=0" class="ltx_Math" display="inline" id="S4.SS2.p8.7.m6" intent=":literal"><semantics><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">m=0</annotation></semantics></math> (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F7.sf4" title="In Figure 7 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7(d)</span></a>).
We selected these two examples by randomly sampling an index in the generation, and then looking at the surrounding text.
We then manually performed repeated searches for subsequences of the generated text in the reference book, to confirm that they do not reflect extraction.
Since extraction is our focus, we do not make claims about this non-extracted text, and instead defer detailed analysis to future work.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">关于 <math intent=":literal" id="S4.SS2.p8.1.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 生成的文本的初步定性观察。我们对 <math intent=":literal" id="S4.SS2.p8.2.m1" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 生成的文本进行了有限的定性分析。如前所述，这部分文本可能包含重复或顺序错乱的提取。然而，这种情况并非总是发生；通常， <math intent=":literal" id="S4.SS2.p8.3.m2" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 生成的文本并非提取。我们对所有实验中的这部分文本进行初步定性检查后发现，对于所有书籍和前沿 LLMs， <math intent=":literal" id="S4.SS2.p8.4.m3" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 文本经常包含与 Phase 1 前缀所来源的书籍中重复的情节元素、主题和角色名称。我们在图 8 中提供了两个这样的文本示例；这些示例来自 GPT-4.1 生成的文本，该文本在以《权力的游戏》为种子前缀成功完成 Phase 1 后生成。请注意，对于 GPT-4.1 的《权力的游戏》， <math intent=":literal" id="S4.SS2.p8.5.m4" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 与 <math intent=":literal" id="S4.SS2.p8.6.m5" display="inline" class="ltx_Math" alttext="0\%"><semantics><mrow><mn>0</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">0\%</annotation></semantics></math> 完全相同（见图 5），匹配的词语 <math intent=":literal" id="S4.SS2.p8.7.m6" display="inline" class="ltx_Math" alttext="m=0"><semantics><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">m=0</annotation></semantics></math> （见图 7(d)）。我们通过随机采样生成中的索引，然后查看周围的文本来选择这两个示例。随后，我们手动在参考书中反复搜索生成文本的子序列，以确认它们并非提取内容。 由于我们的重点是提取，因此我们不对这部分未提取的文本做任何声明，而是将详细分析留待未来的工作。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Additional details and experiments concerning LLM-specific configurations<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">4.3 关于 LLM 特定配置的附加细节和实验</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="572" id="S4.F9.sf1.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/figure_6_gemini_hp1_heatmap.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F9.sf1.3.2" style="font-size:90%;">Varying configs for Gemini 2.5 Pro</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(a) Gemini 2.5 Pro 的不同配置</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="279" id="S4.F9.sf2.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/figure_10_gpt.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F9.sf2.3.2" style="font-size:90%;">Varying the extraction procedure for GPT-4.1</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(b) 改变 GPT-4.1 的提取方法</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.12.4.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F9.6.3" style="font-size:90%;">Testing alternative settings for the two-phase procedure.<span class="ltx_text ltx_font_medium" id="S4.F9.6.3.3">
We explore how different settings influence how much extraction is obtained per run.
Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F9.sf1" title="In Figure 9 ‣ 4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">9(a)</span></a> shows how <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.F9.4.1.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> varies across runs with different generation configurations (presence and frequency penalty) for Gemini 2.5 Pro and <span class="ltx_text ltx_font_italic" id="S4.F9.6.3.3.2">Harry Potter and the Sorcerer’s Stone</span>.
Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F9.sf2" title="In Figure 9 ‣ 4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">9(b)</span></a> shows how starting with different seed prefixes in Phase 1 can reveal different memorized text.
In our main experiments using a prefix from the beginning of the book (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>), GPT-4.1 tends to refuse to continue in Phase 2 at the end of the first chapter.
We perform additional runs of the two-phase procedure, where for Phase 1 we use seed prefixes drawn from the beginning of <em class="ltx_emph ltx_font_italic" id="S4.F9.6.3.3.3">each chapter of each book</em>.
We compare <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.F9.5.2.2.m2" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> from our main experiments, starting with Phase 1 using a seed from the first chapter (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>), to the (non-overlapping) <em class="ltx_emph ltx_font_italic" id="S4.F9.6.3.3.4">union</em> near-verbatim blocks from per-chapter-with-retry extraction.
<span class="ltx_text ltx_font_italic" id="S4.F9.6.3.3.1">Note: The reported <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.F9.6.3.3.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext class="ltx_mathvariant_italic">-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> in each pair of bars uses a different extraction procedure.
See main text for more details.</span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 9：测试两阶段流程的替代设置。我们探索不同设置如何影响每次运行中提取的文本量。图 9(a)展示了 Gemini 2.5 Pro 和《哈利·波特与魔法石》在不同生成配置（存在性和频率惩罚）下， <math intent=":literal" id="S4.F9.4.1.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 如何随运行变化。图 9(b)展示了在第一阶段使用不同的种子前缀如何揭示不同的记忆文本。在我们的主要实验中（使用书本开头的前缀，见第 4.2 节），GPT-4.1 倾向于在第一章节结束时拒绝继续进入第二阶段。我们进行了额外的两阶段流程运行，其中第一阶段使用从每本书每章节开头抽取的种子前缀。我们将主要实验中从第一章节种子开始的第一阶段 <math intent=":literal" id="S4.F9.5.2.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> （见图 5）与每章节重试提取的（非重叠）近似逐字块并集进行比较。注：每对条形图中报告的 <math intent=":literal" id="S4.F9.6.3.3.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext class="ltx_mathvariant_italic">-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 使用了不同的提取流程。详见正文。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">As noted in the prior section, our initial experiments revealed that different settings for the two-phase procedure had an impact on extraction for each production LLM.
For instance, these initial experiments revealed that we did not need to jailbreak Gemini 2.5 Pro or Grok 3.
They also revealed how different generation configurations for Phase 2 resulted in varying amounts of extraction.
Here, we provide some more details about how varied settings impact extraction, according to production LLM.
Full experimental configurations, additional results, and API cost information can be found in Appendices&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2" title="C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.2</span></a> and &nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2" title="D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D.2</span></a>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">如前一节所述，我们的初步实验表明，两阶段流程的不同设置对每个生产 LLM 的提取产生了影响。例如，这些初步实验表明我们无需越狱破解 Gemini 2.5 Pro 或 Grok 3。它们还揭示了第二阶段的不同生成配置如何导致提取量的变化。在此，我们根据生产 LLM，提供更多关于不同设置如何影响提取的细节。完整的实验配置、附加结果和 API 成本信息可在附录 C.2 和 D.2 中找到。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Gemini 2.5 Pro.</span> 
For all experiments, Gemini 2.5 Pro did not refuse to continue the seed prefix in Phase 1.
In our initial exploratory experiments, after a number of turns in the Phase 2 continue loop, the Gemini 2.5 Pro API would stop returning text;
it instead would provide an empty response with a metadata object, linking to documentation indicating that we had encountered guardrails meant to prevent the recitation of copyrighted material&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib89" title="Generating content — gemini api">Google AI for Developers, </a>)</cite>.
We found that we could mitigate this behavior by minimizing the “thinking budget,” and explicitly querying Gemini 2.5 Pro to “Continue without citation metadata.”
In some runs, Gemini 2.5 Pro would occasionally return empty responses during Phase 2.
When this occurred, we count this as a turn in the maximum query budget, and retry after a one-second delay (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS2" title="C.2.2 Generation configuration exploration for Gemini 2.5 Pro ‣ C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.2.2</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">Gemini 2.5 Pro。在所有实验中，Gemini 2.5 Pro 在第一阶段并未拒绝继续使用种子前缀。在我们的初步探索性实验中，在第二阶段的继续循环中经过若干回合后，Gemini 2.5 Pro API 会停止返回文本；相反，它会提供一个包含元数据对象且内容为空的响应，该元数据对象链接到文档，表明我们遇到了旨在防止引用受版权保护材料的防护措施（Google AI for Developers,）。我们发现，通过最小化“思考预算”，并明确查询 Gemini 2.5 Pro 要求“无引用元数据地继续”，可以缓解这种行为。在某些运行中，Gemini 2.5 Pro 在第二阶段偶尔会返回空响应。当这种情况发生时，我们将其计为最大查询预算中的一个回合，并在一秒延迟后重试（附录 C.2.2）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.7">We also found that Gemini 2.5 Pro’s responses would often repeat previously emitted text.
We therefore experimented with different generation configurations for the maximum number of generated tokens, frequency penality, and presence penalty.
Through a set of experiments on <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.7.1">Harry Potter and the Sorcerer’s Stone</span>, we found that a maximum of <math alttext="2000" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1" intent=":literal"><semantics><mn>2000</mn><annotation encoding="application/x-tex">2000</annotation></semantics></math> tokens resulted in the highest <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.SS3.p3.2.m2" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math>.
We fixed this parameter, and swept over different combinations of frequency and presence penalty.
Setting frequency penalty to <math alttext="2" class="ltx_Math" display="inline" id="S4.SS3.p3.3.m3" intent=":literal"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math> and presence penalty to <math alttext="0.1" class="ltx_Math" display="inline" id="S4.SS3.p3.4.m4" intent=":literal"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> resulted in the highest <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.SS3.p3.5.m5" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math>, so we fix these as the configurations for Gemini 2.5 Pro runs across books for the results shown in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a> (Figures&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F7" title="Figure 7 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a>).
Nevertheless, as shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F9.sf1" title="In Figure 9 ‣ 4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">9(a)</span></a>, variance in extraction can be significant depending on the choice of these settings.
Given the cheap cost of running our experiments on Gemini 2.5 Pro, we provide results for all books testing each of these <math alttext="9" class="ltx_Math" display="inline" id="S4.SS3.p3.6.m6" intent=":literal"><semantics><mn>9</mn><annotation encoding="application/x-tex">9</annotation></semantics></math> configurations in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2.SSS2" title="D.2.2 Plots and tables ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D.2.2</span></a>.
These results show that the single, fixed configuration for Gemini 2.5 Pro for the results in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a> do not always result in the highest <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.SS3.p3.7.m7" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> for every book.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们还发现，Gemini 2.5 Pro 的响应经常会重复之前已生成的文本。因此，我们尝试了不同的生成配置，包括最大生成 token 数、频率惩罚和存在惩罚。通过在《哈利·波特与魔法石》上进行的一系列实验，我们发现最大 <math intent=":literal" id="S4.SS3.p3.1.m1" display="inline" class="ltx_Math" alttext="2000"><semantics><mn>2000</mn><annotation encoding="application/x-tex">2000</annotation></semantics></math> token 数量产生了最高的 <math intent=":literal" id="S4.SS3.p3.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 。我们固定了这个参数，并测试了不同的频率惩罚和存在惩罚组合。将频率惩罚设置为 <math intent=":literal" id="S4.SS3.p3.3.m3" display="inline" class="ltx_Math" alttext="2"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math> ，存在惩罚设置为 <math intent=":literal" id="S4.SS3.p3.4.m4" display="inline" class="ltx_Math" alttext="0.1"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> ，产生了最高的 <math intent=":literal" id="S4.SS3.p3.5.m5" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> ，因此我们将这些设置为 Gemini 2.5 Pro 在所有书籍中运行的结果配置，这些结果在 4.2 节（图 5 和图 7）中展示。然而，如图 9(a)所示，提取的方差可能因这些设置的选择而显著不同。鉴于在 Gemini 2.5 Pro 上运行实验的成本较低，我们在附录 D.2.2 中提供了针对所有书籍测试每个 <math intent=":literal" id="S4.SS3.p3.6.m6" display="inline" class="ltx_Math" alttext="9"><semantics><mn>9</mn><annotation encoding="application/x-tex">9</annotation></semantics></math> 配置的结果。这些结果表明，为 4.2 节中的 Gemini 2.5 Pro 设置的单个固定配置并不总是为每本书产生最高的 <math intent=":literal" id="S4.SS3.p3.7.m7" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">Grok 3.</span> 
We encountered no guardrails for any experiments for Phase 1.
Except for the run on <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.2">1984</span> (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>), we did not encounter any guardrails for Phase 2.
For <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.3">1984</span>, Grok 3 produced <em class="ltx_emph ltx_font_italic" id="S4.SS3.p4.1.4">verbatim</em> text until the <math alttext="24" class="ltx_Math" display="inline" id="S4.SS3.p4.1.m1" intent=":literal"><semantics><mn>24</mn><annotation encoding="application/x-tex">24</annotation></semantics></math>th continue request, when it responded with a refusal and that it would instead “continue the narrative in a way that respects the source material.”
During Phase 2, the Grok 3 API sometimes returned a generic HTTP 500 error code, indicating a provider-side issue with fulfilling API requests.
In these cases, the continuation loop terminated before the max query budget was exhausted.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">Grok 3. 我们在第一阶段实验中未遇到任何限制条件。除了在 1984 年（图 5）上的运行外，我们在第二阶段也未遇到任何限制条件。对于 1984 年，Grok 3 在收到 <math intent=":literal" id="S4.SS3.p4.1.m1" display="inline" class="ltx_Math" alttext="24"><semantics><mn>24</mn><annotation encoding="application/x-tex">24</annotation></semantics></math> 继续请求之前一直逐字生成文本，当收到继续请求时，它拒绝并表示将“以尊重原始材料的方式继续叙述”。在第二阶段，Grok 3 API 有时会返回通用的 HTTP 500 错误代码，表明在满足 API 请求方面存在提供方问题。在这些情况下，继续循环在最大查询预算耗尽之前终止。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.2"><span class="ltx_text ltx_font_bold" id="S4.SS3.p5.2.1">Claude 3.7 Sonnet.</span> 
Initial experiments to complete a seed prefix failed, which is why we experimented with BoN in Phase 1.
Early runs with <span class="ltx_text ltx_font_italic" id="S4.SS3.p5.2.2">Harry Potter and the Sorcerer’s Stone</span> revealed that, for BoN-jailbroken Claude 3.7 Sonnet, different response lengths in Phase 2 could trigger refusals.
In iterative experiments, we reduced the maximum response length per continue query from <math alttext="1000" class="ltx_Math" display="inline" id="S4.SS3.p5.1.m1" intent=":literal"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math> to <math alttext="250" class="ltx_Math" display="inline" id="S4.SS3.p5.2.m2" intent=":literal"><semantics><mn>250</mn><annotation encoding="application/x-tex">250</annotation></semantics></math> tokens, which was sufficient to evade refusals in all future experiments.
We also noticed that, when Claude 3.7 Sonnet reproduced an entire book near-verbatim, it often appended “THE END”.
This is what inspired us to include a stop-phrase condition in Phase 2, in addition to checking for refusals or if a maximum query budget has been exhausted.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">Claude 3.7 Sonnet。初步实验试图完成种子前缀失败，这就是我们在第一阶段尝试 BoN 的原因。早期使用《哈利·波特与魔法石》的运行表明，对于 BoN 破解的 Claude 3.7 Sonnet，第二阶段的不同响应长度可能会触发拒绝。在迭代实验中，我们将每个继续查询的最大响应长度从 <math intent=":literal" id="S4.SS3.p5.1.m1" display="inline" class="ltx_Math" alttext="1000"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math> 减少到 <math intent=":literal" id="S4.SS3.p5.2.m2" display="inline" class="ltx_Math" alttext="250"><semantics><mn>250</mn><annotation encoding="application/x-tex">250</annotation></semantics></math> 个 token，这足以在所有后续实验中避免拒绝。我们还注意到，当 Claude 3.7 Sonnet 近乎逐字地复现整本书时，它经常会附加“THE END”。这启发了我们在第二阶段加入一个停止短语条件，除了检查拒绝或查询预算是否已用尽。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p6.1.1">GPT-4.1.</span> 
As discussed in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>, jailbreaking GPT-4.1 in Phase 1 generally took significantly more BoN attempts with our specific initial instruction than for Claude 3.7 Sonnet.
Further, for jailbroken GPT-4.1, success of the continuation loop in Phase 2 was always curtailed by an eventual refusal.
Except for Grok 3 on <span class="ltx_text ltx_font_italic" id="S4.SS3.p6.1.2">1984</span>, these constituted all refusals in our final experimental configurations.
Therefore, for the experiments shown in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>, if we successfully extracted text from GPT-4.1 for a given book, that text was always from the first chapter, after which GPT-4.1 refused to continue.
For instance, for <span class="ltx_text ltx_font_italic" id="S4.SS3.p6.1.3">Harry Potter and the Sorcerer’s Stone</span>, the last response before refusal was “That is the end of Chapter One.”<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">GPT-4.1. 如第 4.2 节所述，在第一阶段中，使用我们特定的初始指令破解 GPT-4.1 通常需要显著更多的 BoN 尝试次数，这比破解 Claude 3.7 Sonnet 要困难得多。此外，对于破解后的 GPT-4.1，在第二阶段中，其延续循环的成功总是被最终的拒绝所限制。除了 1984 年的 Grok 3 之外，这些构成了我们最终实验配置中的所有拒绝。因此，对于第 4.2 节中展示的实验，如果我们成功从 GPT-4.1 中提取了某本书的文本，那么该文本总是来自第一章，之后 GPT-4.1 拒绝继续。例如，对于《哈利·波特与魔法石》，拒绝前的最后回应是“第一章到此结束。”</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1">However, encountering a refusal at the end of the first book chapter does not necessarily mean that further text is not memorized by GPT-4.1.
Rather, failure due to refusal simply indicates that we were unable to extract more text with our specific two-phase procedure.
To explore this further, we ran an additional set of experiments to attempt to elicit additional memorization from GPT-4.1.
For each book, we execute a chapter-by-chapter variant of the two-phase procedure:
for <em class="ltx_emph ltx_font_italic" id="S4.SS3.p7.1.1">each chapter</em>, we use the first sentence as the seed prefix for BoN in Phase 1 to find a successful jailbreak prompt, and then run the Phase 2 continuation loop to attempt to extract the rest of the chapter.
We also implemented a retry policy for if we encountered a refusal, as we noticed that refusals are not deterministic for GPT-4.1:
it may refuse a request at one point in time, but after a time delay may fulfill the identical request and continue.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Non-determinism is another salient difference between our results and those in <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>, which are deterministic.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">非确定性是我们结果与 Cooper 等人（2025）结果之间的另一个显著差异，后者是确定性的。</font></font></font></span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">然而，在第一本书章节的末尾遇到拒绝并不意味着 GPT-4.1 没有记住更多文本。相反，由于拒绝而失败仅仅表明我们无法用特定的两阶段程序提取更多文本。为了进一步探索这一点，我们运行了一系列额外的实验，试图从 GPT-4.1 中引出更多记忆。对于每一本书，我们执行了按章节进行的两阶段程序变体：对于每一章节，我们使用第一句话作为 BoN 在第一阶段中的种子前缀来找到一个成功的越狱提示，然后运行第二阶段的延续循环来尝试提取章节的其余部分。我们还实现了一个重试策略，因为我们注意到对于 GPT-4.1 来说，拒绝并不是确定的：它在某个时间点可能会拒绝一个请求，但在经过一段时间延迟后可能会满足相同的请求并继续。 <sup class="ltx_note_mark">9</sup> </font></font></font>
This more intensive approach—which also makes use of more ground-truth text from the reference book—is able to extract more training data.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">这种更密集的方法——同时利用了参考书中更多的真实文本——能够提取更多训练数据。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p8">
<p class="ltx_p" id="S4.SS3.p8.2">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F9.sf2" title="In Figure 9 ‣ 4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">9(b)</span></a>, we compare <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.SS3.p8.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> results for these per-chapter-with-retry experiments with the results of our main experiments involving a single two-phase run starting with a prefix from the first chapter (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>).
For the per-chapter-with-retry variant, we report the total proportion of the book extracted by taking the union over (non-overlapping/disjoint) near-verbatim blocks to compute <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="S4.SS3.p8.2.m2" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E7" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a>).
<span class="ltx_text ltx_font_italic" id="S4.SS3.p8.2.1">Note: We ran these experiments to probe if our main extraction procedure under-counts possible extraction (and thus memorization).
The underlying extraction procedures are not equivalent, in terms of effort expended to elicit extraction.</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在图 9(b)中，我们将这些每章重试实验的 <math intent=":literal" id="S4.SS3.p8.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 结果与我们的主要实验结果进行了比较，后者从第一章的提示词开始，进行一次两阶段运行（如图 5 所示）。对于每章重试的变体，我们通过取（非重叠/不相交）逐字复述块的并集来计算提取的书的总比例，以计算 <math intent=":literal" id="S4.SS3.p8.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> （公式 7）。注意：我们运行这些实验是为了探究我们的主要提取程序是否低估了可能的提取（从而导致记忆）。这些提取程序在付出的努力来引出提取方面并不等价。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">5 讨论</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We discuss overarching takeaways from our experiments in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4" title="4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>, focusing on important limitations and caveats (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS1" title="5.1 Limitations and caveats ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5.1</span></a>), and brief observations about why our work may be of interest to copyright (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2" title="5.2 Copyright ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5.2</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们在第 4 节讨论了实验的总体结论，重点关注重要的局限性和注意事项（第 5.1 节），以及关于我们的工作为何可能引起版权兴趣的简要观察（第 5.2 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Limitations and caveats<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">5.1 局限性和注意事项</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Throughout this paper, we have highlighted limitations and caveats in italicized notes.
Nevertheless, it is worth reiterating that the points we raise have an important impact on how our results should be interpreted.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在整个论文中，我们用斜体注释强调了局限性和注意事项。然而，值得重申的是，我们提出的观点对我们的结果解释具有重要影响。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">A loose lower bound on memorization for specific books.</span> 
Separate from how our measurements for extraction are conservative (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3" title="3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3</span></a>), it is well-known that extraction more generally under-counts the total amount of training data that LLMs memorize.
While prior work has demonstrated this in other contexts&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Nasr<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>; Cooper<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>, our results for GPT-4.1 show how changing the prompting strategy can significantly alter how much extraction we observe, and how much underlying memorization this reveals.
Our main focus is attempting to extract specific books near-verbatim;
so, in most experiments, we run the two-phase procedure only once, with Phase 1 using a seed prefix from the beginning of a given book.
In most cases, qualitative inspection of diffs with reference books shows that this succeeds in extracting near-verbatim text from at least part of the first chapter, but then the generation often diverges from the true text.
However, as is clear in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F9.sf2" title="In Figure 9 ‣ 4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">9(b)</span></a>, seeding Phase 1 in different book locations (here, the start of each chapter) can reveal additional memorization that we did not capture with our main experiments.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">针对特定书籍的记忆力下限的宽松估计。与我们提取测量的保守性（第 3.3 节）不同，众所周知，提取通常低估了 LLMs 记忆的总训练数据量。虽然先前工作在其他背景下已经证明了这一点（Nasr 等人，2023；Cooper 等人，2025），但我们的 GPT-4.1 结果展示了如何通过改变提示策略显著改变我们观察到的提取量，以及这种改变揭示了多少潜在的记忆力。我们的主要重点是尝试近乎逐字地提取特定书籍；因此，在大多数实验中，我们仅运行两阶段流程一次，第一阶段使用给定书籍开头的种子前缀。在大多数情况下，与参考书籍的 diff 定性检查表明，这成功地从第一章的部分内容中提取了近似的逐字文本，但随后生成的内容往往偏离真实文本。然而，如图 9(b)所示，在不同书籍位置（此处为每章的开头）对第一阶段进行种子化，可以揭示我们主要实验未能捕捉到的额外记忆力。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Relatively small scale of experiments and their cost.</span> 
It is challenging to study production settings, as APIs change over time.
For the same reason, it is often also difficult to reproduce results on production LLMs.
We limited our experiments to a specific time window, so that we could successfully complete testing on the same books for all four production LLMs.
In all, we only ran experiments on fourteen specific books, so our results do not speak to memorization and extraction more generally.
Cost also impacted the number of books we tested.
While it was typically less than $1 to run the Phase 2 continuation loop for Gemini 2.5 Pro, it was more expensive for some production LLMs.
Notably, for Claude 3.7 Sonnet, long-context generation is significantly more expensive;
it often cost over $100 per run (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.T1" title="Table 1 ‣ Figure 6 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a> &amp; Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2.SSS1" title="D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D.2.1</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">实验规模相对较小，且成本较高。研究生产环境具有挑战性，因为 API 会随时间变化。出于相同原因，在 production LLMs 上重现结果也常常很困难。我们将实验限制在特定的时间窗口内，以便能够成功地在所有四个 production LLMs 上对同一本书进行测试。总共，我们仅在十四本书上进行了实验，因此我们的结果不能推广到记忆和提取的更一般情况。成本也影响了我们测试的书籍数量。虽然运行 Gemini 2.5 Pro 的 Phase 2 延续循环通常不到 1 美元，但对某些 production LLMs 来说则更昂贵。值得注意的是，对于 Claude 3.7 Sonnet，长上下文生成成本显著更高；每次运行通常超过 100 美元（表 1 &amp; 附录 D.2.1）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.2"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.2.1">LLM-specific configuration of the two-phase extraction procedure.</span> 
In our main experiments (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>), we test one relatively simple extraction procedure (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3" title="3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3</span></a>), and we instantiate that procedure in different ways for different production LLMs.
In Phase 1, we decided to make the jailbreak optional, and we only tested Best-of-<math alttext="N" class="ltx_Math" display="inline" id="S5.SS1.p4.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>.
For Gemini 2.5 Pro and Grok 3, it was remarkable that this procedure evaded safeguards—that we did not need to use a jailbreak to successfully extract training data.
However, it is also possible that, if we had used BoN on these two LLMs, it may have changed how much extraction we observed.
For Phase 2, we set temperature to <math alttext="0" class="ltx_Math" display="inline" id="S5.SS1.p4.2.m2" intent=":literal"><mn>0</mn></math> for all generation configurations and use the same halting conditions, but we tuned LLM-specific parameters (e.g., frequency penalty for Gemini 2.5 Pro) to increase LLM-specific extraction success.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">针对 LLM 的两阶段提取流程的特定配置。在我们的主要实验（第 4.2 节）中，我们测试了一种相对简单的提取流程（第 3 节），并根据不同的生产 LLM 以不同的方式实例化该流程。在第一阶段，我们决定将越狱设置为可选，并且只测试了 Best-of- <math intent=":literal" id="S5.SS1.p4.1.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 。对于 Gemini 2.5 Pro 和 Grok 3 来说，值得注意的是，该流程避开了安全防护——我们不需要使用越狱就能成功提取训练数据。然而，也有可能，如果我们在这两个 LLM 上使用 BoN，可能会改变我们观察到的提取量。对于第二阶段，我们将所有生成配置的温度设置为 <math intent=":literal" id="S5.SS1.p4.2.m2" display="inline" class="ltx_Math" alttext="0"><mn>0</mn></math> ，并使用相同的停止条件，但我们调整了 LLM 特定参数（例如，Gemini 2.5 Pro 的频率惩罚）以提高 LLM 特定提取的成功率。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.1">We do not make evaluative claims across LLMs.</span> 
Given the above, it bears repeating that every observation we make about our results is with respect to a specific production LLM, book, and instantiation and run of our specific two-phase procedure.
In some cases, the specific conditions we test revealed an enormous amount of extraction;
notably, we extracted two entire in-copyright books—<span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.2">Harry Potter and the Sorcerer’s Stone</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.3">1984</span>—from Claude 3.7 Sonnet near-verbatim.
We only make <em class="ltx_emph ltx_font_italic" id="S5.SS1.p5.1.4">descriptive statements</em> about these results:
we discuss outcomes concerning specific experimental choices, outputs, and determinations of extraction success (<cite class="ltx_cite ltx_citemacro_citet">Chouldechova<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib136" title="Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming">2025</a>)</cite>; Sections&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a> and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>).
This aligns with our goal:
to see if it is <em class="ltx_emph ltx_font_italic" id="S5.SS1.p5.1.5">possible</em> to extract long-form books from production LLMs.
However, we do not make broader <em class="ltx_emph ltx_font_italic" id="S5.SS1.p5.1.6">evaluative</em> claims across production LLMs.
For instance, while our specific experiments extracted the most text from Claude 3.7 Sonnet (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>), we do <em class="ltx_emph ltx_font_italic" id="S5.SS1.p5.1.7">not</em> claim that these results indicate Claude 3.7 Sonnet in general memorizes more training data than the other three production LLMs.
We do <em class="ltx_emph ltx_font_italic" id="S5.SS1.p5.1.8">not</em> claim that any production LLM is in general more robust to extraction than another.
Our bar plots should <em class="ltx_emph ltx_font_italic" id="S5.SS1.p5.1.9">not</em> be interpreted as making such comparative claims.
In order to make such evaluative, comparative claims, one would need to run a much larger scale study under more controlled conditions.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们不在 LLMs 之间做出评价性声明。鉴于上述内容，需要再次强调的是，我们对结果的每一项观察都是针对特定的生产 LLM、书籍以及我们特定两阶段程序的特定实例和运行。在某些情况下，我们测试的具体条件揭示出大量的提取；值得注意的是，我们从 Claude 3.7 Sonnet 中几乎逐字提取了两本版权在册的完整书籍——《哈利·波特与魔法石》和《1984》。我们仅对这些结果做出描述性陈述：我们讨论与特定实验选择、输出以及提取成功判定相关的结果（Chouldechova 等人（2025）；第 1 节和第 4.2 节）。这符合我们的目标：看看是否有可能从生产 LLMs 中提取长篇书籍。然而，我们不在生产 LLMs 之间做出更广泛的评价性声明。例如，虽然我们的特定实验从 Claude 3.7 Sonnet 中提取了最多的文本（第 4.2 节），但我们并不声称这些结果表明 Claude 3.7 Sonnet 总体上比其他三个生产 LLMs 记忆了更多的训练数据。 我们不声称任何生产 LLM 在一般意义上比另一个更不容易被提取。我们的条形图不应被解释为做出此类比较性声明。为了做出此类评估性、比较性声明，需要在一个更受控的条件下进行更大规模的调查研究。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Copyright<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">5.2 版权</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">While we defer detailed copyright analysis to future work, we briefly address why our results may be of interest.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">虽然我们将详细的版权分析留待未来的工作，但我们将简要说明为什么我们的结果可能引起兴趣。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Production LLMs memorize some of their training data, and extraction is sometimes feasible.</span> 
In copyright litigation concerning generative AI, extraction and memorization of training data are both central issues (Sections&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a> &amp;&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>).
Several lawsuits have addressed questions over whether production LLMs reproduce copyrighted training data in their outputs (i.e., have touched on extraction)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kadrey Judgment, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib49" title="Order Denying the Plaintiffs’ Motion for Partial Summary Judgment and Granting Meta’s Cross-Motion for Partial Summary Judgment, Kadrey et al. v. Meta Platforms, Inc.">2025</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib50" title="Order on Fair Use, Bartz et al. v. Anthropic PBC">2025</a>)</cite>.
There has also been increased academic discussion&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cooper and Grimmelmann, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib36" title="The Files are in the Computer: Copyright, Memorization, and Generative AI">2024</a>; Dornis, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib108" title="Generative AI, Reproductions Inside the Model, and the Making Available to the Public">2025</a>)</cite> and litigation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib68" title="Gesellschaft für musikalische Aufführungs- und mechanische Vervielfältigungsrechte">GEMA v. OpenAI, </a>)</cite> over whether LLMs themselves are legally cognizable copies of the training data they have memorized.
Regardless of how relevant these issues may be for potential findings of copyright infringement, our work reveals important technical facts:
the four production LLMs we study memorized (at least some of) the books on which they were trained, and it is possible to extract (at least some of) those memorized books at generation time.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">生产 LLMs 会记忆部分训练数据，有时可以进行提取。在涉及生成式 AI 的版权诉讼中，训练数据的提取和记忆都是核心问题（第 1 节和第 2 节）。几起诉讼已经探讨了生产 LLMs 的输出是否复制了受版权保护的训练数据（即涉及了提取）的问题（Kadrey 判决，2025 年；2025 年）。此外，学术界也出现了越来越多的讨论（Cooper 和 Grimmelmann，2024 年；Dornis，2025 年）和诉讼（GEMA 诉 OpenAI），讨论 LLMs 本身是否是它们记忆的训练数据的法律可认的副本。无论这些问题对于潜在的版权侵权认定有多么重要，我们的工作揭示了重要的技术事实：我们研究的四个生产 LLMs 记忆了（至少部分）它们所训练的书籍，并且有可能在生成时提取（至少部分）这些记忆的书籍。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Jailbreaks, adversarial use, and cost.</span> 
Some might qualify our experiments as atypical use, as we deliberately tried to surface memorized books.
Adversarial use, like the use of jailbreaks, may matter for copyright infringement analysis&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib45" title="Talkin’ ’Bout AI Generation: Copyright and the Generative-AI Supply Chain">2023b</a>; Cooper and Grimmelmann, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib36" title="The Files are in the Computer: Copyright, Memorization, and Generative AI">2024</a>)</cite>.
Further, for the cases in which we retrieved whole copies of near-verbatim books, it was often quite costly (<math alttext="&gt;\mathdollar 100" class="ltx_Math" display="inline" id="S5.SS2.p3.1.m1" intent=":literal"><semantics><mrow><mi></mi><mo>&gt;</mo><mrow><mi mathvariant="normal">$</mi><mo lspace="0em" rspace="0em">​</mo><mn>100</mn></mrow></mrow><annotation encoding="application/x-tex">&gt;\mathdollar 100</annotation></semantics></math>) to do so (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>, Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.T1" title="Table 1 ‣ Figure 6 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">1</span></a>).
As <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite> note, even with respect to their significantly cheaper experiments using open-weight LLMs, “there are easier and more effective ways to pirate a book.”
Nevertheless, it is important to emphasize that we did not use jailbreaks for two production LLMs during Phase 1.
In Phase 2, we observed that all four production LLMs sometimes responded with large spans of in-copyright text.
In all cases, successful extraction of training data would not have been possible if these LLMs had not memorized those data during training (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">越狱攻击、对抗性使用和成本。有些人可能将我们的实验视为非典型使用，因为我们故意试图揭示记忆中的书籍。对抗性使用，如越狱攻击的使用，可能对版权侵权分析有影响（Lee 等人，2023b；Cooper 和 Grimmelmann，2024）。此外，对于我们检索到近乎逐字复制的书籍完整副本的情况，这样做通常成本很高（ <math intent=":literal" id="S5.SS2.p3.1.m1" display="inline" class="ltx_Math" alttext="&gt;\mathdollar 100"><semantics><mrow><mi></mi><mo>&gt;</mo><mrow><mi mathvariant="normal">$</mi><mo rspace="0em" lspace="0em">​</mo><mn>100</mn></mrow></mrow><annotation encoding="application/x-tex">&gt;\mathdollar 100</annotation></semantics></math> ）（第 4.2 节，表 1）。正如 Cooper 等人（2025 年）指出的那样，即使对于他们使用开放权重 LLMs 进行的明显更便宜的实验，“也有更容易和更有效的方法来盗版一本书。”然而，必须强调的是，在第一阶段，我们没有使用两个生产 LLMs 的越狱攻击。在第二阶段，我们观察到所有四个生产 LLMs 有时会以大段受版权保护文本的形式做出回应。在所有情况下，如果这些 LLMs 在训练期间没有记住这些数据，那么成功的提取训练数据将是不可能的（第 3.3.2 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Best efforts and safeguards.</span> 
As others have noted, it may be infeasible to produce perfect safeguards;
in such circumstances, preventing the generation of copyrighted or otherwise undesirable material may depend on “reasonable best efforts”&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cooper<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib72" title="Machine unlearning doesn’t do what you think: lessons for generative ai policy, research, and practice">2024</a>)</cite>.
As noted above, in our main experiments (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>), two production LLMs did not exhibit safeguards in Phase 1:
Gemini 2.5 Pro and Grok 3 directly complied with our initial probes to complete prefixes from books.
We used jailbreaks to get Claude 3.7 Sonnet and GPT-4.1 to comply in Phase 1.
For GPT-4.1 and our chosen initial instruction, it frequently took a significant number of BoN attempts to achieve Phase 1 success.
It often took far fewer than our maximum budget (<math alttext="N=$10,000$" class="ltx_Math" display="inline" id="S5.SS2.p4.1.m1" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N=$10,000$</annotation></semantics></math>) to jailbreak Claude 3.7 Sonnet to complete a provided in-copyright book prefix.
Jailbreaks aside, our experiments managed to evade system-level safeguards during Phase 2.
We were able to run multiple—sometimes hundreds—of iterations of a simple continue loop for each production LLM, before (if ever) encountering filters intended to prevent generation of copyrighted material (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">最佳努力与安全措施。正如其他人所指出的，可能无法实现完美的安全措施；在这种情况下，防止生成版权保护或其他不希望的材料可能取决于“合理的最佳努力”（Cooper 等人，2024）。如上所述，在我们的主要实验（第 4.2 节）中，两个生产 LLM 在第一阶段没有表现出安全措施：Gemini 2.5 Pro 和 Grok 3 直接按照我们最初的探测要求完成书籍的前缀。我们使用越狱技术让 Claude 3.7 Sonnet 和 GPT-4.1 在第一阶段配合。对于 GPT-4.1 和我们选择的初始指令，经常需要大量 BoN 尝试才能实现第一阶段成功。越狱除外，我们的实验在第二阶段成功规避了系统级安全措施。我们能够在（如果遇到的话）旨在防止生成版权保护材料的过滤器之前，对每个生产 LLM 运行多个——有时是数百个——简单的继续循环迭代（第 2 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.2"><span class="ltx_text ltx_font_bold" id="S5.SS2.p5.1.1">Non-extracted, <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S5.SS2.p5.1.1.m1" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> text.</span> 
In our experiments, we specifically investigate extraction of training data.
However, when conducting our extraction analysis, we qualitatively observed that thousands of words of <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="S5.SS2.p5.2.m1" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E8" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">8</span></a>), non-extracted generated text from all four production LLMs replicate character names, plot elements, and themes (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F8" title="Figure 8 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">8</span></a>, Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>).
Given that copyright law does not only apply to near-verbatim copying, such outputs may be interest.
We stress that we do not perform rigorous, quantitative, at-scale analysis of this text, and instead defer this to future work.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">未提取的 <math intent=":literal" id="S5.SS2.p5.1.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 文本。在我们的实验中，我们专门研究了训练数据的提取。然而，在进行提取分析时，我们定性地观察到，所有四个生产 LLMs 生成的 <math intent=":literal" id="S5.SS2.p5.2.m1" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> （公式 8）非提取文本都复制了角色名称、情节元素和主题（图 8，第 4.2 节）。鉴于版权法不仅适用于近乎逐字复制，这些输出可能引起兴趣。我们强调，我们没有对这个文本进行严格、定量、大规模的分析，而是将其留待未来工作。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">6 结论</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">With a simple two-phase procedure (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3" title="3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3</span></a>), we show that it is possible to extract large amounts of in-copyright text from four production LLMs.
While we needed to jailbreak Claude 3.7 Sonnet and GPT-4.1 to facilitate extraction, Gemini 2.5 Pro and Grok 3 directly complied with text continuation requests.
For Claude 3.7 Sonnet, we were able to extract four whole books near-verbatim, including two books under copyright in the U.S.: <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">Harry Potter and the Sorcerer’s Stone</span> and <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">1984</span> (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4" title="4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">通过一个简单的两阶段流程（第 3 节），我们证明了可以从四个生产 LLMs 中提取大量受版权保护文本。虽然我们需要破解 Claude 3.7 Sonnet 和 GPT-4.1 来促进提取，但 Gemini 2.5 Pro 和 Grok 3 直接响应了文本续写请求。对于 Claude 3.7 Sonnet，我们几乎逐字提取了四本书，其中包括两本在美国受版权保护的书：《哈利·波特与魔法石》和《1984》（第 4 节）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">While our work may be of interest to ongoing legal debates (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5" title="5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>), our main focus is to make technical contributions to machine learning, not copyright law or policy.
As <cite class="ltx_cite ltx_citemacro_citet">Cooper and Grimmelmann (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib36" title="The Files are in the Computer: Copyright, Memorization, and Generative AI">2024</a>)</cite> note, “[i]t is up to lawyers and judges to decide what to do with these technical facts” and it is quite possible “that different generative-AI systems could well be treated differently.”
Regulators may also intervene;
they “are free to change copyright law in ways that change the relevance of the technical facts of memorization”—for instance, to explicitly specify that models can be copies of training data they have memorized, or, conversely, that memorization encoded in model weights explicitly should not be treated as legally cognizable copies.
However, it is not “productive to debate the technical facts of memorization on policy grounds”;
“[c]opyright law [and policy do] not determine technical facts;
[they] must work with the facts as they are.”
Regardless of the prospect of ongoing copyright litigation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gianella, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib110" title="LinkedIn post regarding gema v. openai appeal">2025</a>)</cite>, long-standing, clear, and sound technical facts remain:
LLMs memorize portions of their training data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Carlini<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib9" title="Extracting training data from large language models">2021</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib107" title="Quantifying Memorization Across Neural Language Models">2023</a>)</cite>, these memorized data are encoded in the model’s weights&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Nasr<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>; Carlini, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib34" title="What my privacy papers (don’t) have to say about copyright and generative AI">2025</a>; Schwarzschild<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib111" title="Rethinking LLM Memorization through the Lens of Adversarial Compression">2024</a>)</cite>, and, as we show here, it can be feasible to extract large quantities of in-copyright training data from production LLMs.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">尽管我们的工作可能引起正在进行的法律辩论的兴趣（第 5 节），但我们的主要重点是向机器学习领域做出技术贡献，而不是版权法或政策。正如库珀和格里默曼（2024 年）所指出的，“如何处理这些技术事实取决于律师和法官”而且“不同的生成式人工智能系统很可能被区别对待。”监管机构也可能介入；他们“可以自由地改变版权法，从而改变记忆技术事实的相关性”——例如，明确指定模型可以是它们所记忆的训练数据的副本，或者反过来，明确指出模型权重中编码的记忆不应被视为具有法律可认性的副本。然而，在政策基础上“就记忆的技术事实进行辩论”是“没有成效的”；“版权法（和政策）并不决定技术事实；它们必须根据事实本身来工作。”” 无论持续版权诉讼的前景如何（Gianella，2025），长期存在、清晰且可靠的技术事实仍然是：LLMs 记忆其训练数据的一部分（Carlini 等人，2021；2023），这些记忆的数据编码在模型的权重中（Nasr 等人，2023；Carlini，2025；Schwarzschild 等人，2024），正如我们在本文中所示，从生产 LLMs 中提取大量版权受保护的训练数据是可行的。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments and disclosures<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">致谢与披露</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">AA acknowledges generous support from a Knight-Hennessy Fellowship, an NSF Graduate Research Fellowship, and a Georgetown Foundation Research Grant.
AFC is employed by AVERI and is a postdoctoral affiliate at Stanford University, in Percy Liang’s group in the Department of Computer Science and Daniel E. Ho’s group at Stanford Law School, and a research scientist (incoming assistant professor) in the Department of Computer Science at Yale University.
Until December 2025, AFC was a full-time employee of Microsoft, working as a postdoctoral researcher in the FATE group within Microsoft Research.
These results and analysis should not be attributed to Microsoft.
We thank Mark A. Lemley for feedback on an earlier draft of this work.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">AA 感谢来自 Knight-Hennessy 奖学金、国家科学基金会研究生研究奖学金以及乔治城基金会研究基金的慷慨支持。AFC 受雇于 AVERI 公司，是斯坦福大学的博士后研究员，隶属于计算机科学系的 Percy Liang 小组以及斯坦福法学院 Daniel E. Ho 小组，同时也是耶鲁大学计算机科学系的研究科学家（即将担任助理教授）。直到 2025 年 12 月，AFC 是微软的全职员工，在微软研究院的 FATE 小组担任博士后研究员。这些成果和分析不应归因于微软。我们感谢 Mark A. Lemley 对早期工作草稿的反馈。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist" id="bib.L1">
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib37">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">17 U.S. Code ğ 503 (2010)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Copyright Law of the United States </span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Remedies for infringement: Impounding and disposition of infringing articles</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.law.cornell.edu/uscode/text/17/503" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib78">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Anil, E. Durmus, N. Rimsky, M. Sharma, J. Benton, <span class="ltx_text ltx_bib_etal">et al.</span> (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Many-shot jailbreaking</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">The Thirty-eighth Annual Conference on Neural Information Processing Systems</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://openreview.net/forum?id=cw5mgd71jW" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib74">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Claude’s constitution</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/claudes-constitution" title="">https://www.anthropic.com/news/claudes-constitution</a>Accessed: 2025-05-14</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p4.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_report" id="bib.bib125">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Anthropic (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Claude 3.7 Sonnet System Card</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">System Card</span>
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Anthropic</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Hybrid reasoning model card; release details in system card PDF</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p2.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib106">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Authors Guild v. Google, Inc.</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">804 f.3d 202 (2d cir. 2015)</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#footnote1" title="In 1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">footnote 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib75">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, <span class="ltx_text ltx_bib_etal">et al.</span> (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Constitutional ai: harmlessness from ai feedback</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2212.08073</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2212.08073" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p4.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib66">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Bartz et al. v. Anthropic PBC (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">BakerHostetler</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">United States District Court for the Northern District of CaliforniaCase No. 3:24-cv-05417. Summary judgment ruling on fair use of copyrighted works in AI training.</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.bakerlaw.com/bartz-v-anthropic/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1.p1.1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§C.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib50">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Bartz Judgment (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Order on Fair Use, Bartz et al. v. Anthropic PBC</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">No. 3:23-cv-03417-VC (N.D. Cal. Jun. 25, 2025)</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://docs.justia.com/cases/federal/district-courts/california/candce/3:2024cv05417/434709/231" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p3.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2.p2.1" title="5.2 Copyright ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib103">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Belanger (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">OpenAI declares AI race “over”’ if training on copyrighted works isn’t fair use</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Ars Technica</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://arstechnica.com/tech-policy/2025/03/openai-urges-trump-either-settle-ai-copyright-debate-or-lose-ai-race-to-china/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib64">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">V. Berger (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The ai copyright battle: why openai and google are pushing for fair use</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Forbes</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Accessed: 2025-11-06</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.forbes.com/sites/virginieberger/2025/03/15/the-ai-copyright-battle-why-openai-and-google-are-pushing-for-fair-use/?utm_source=chatgpt.com" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib33">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, <span class="ltx_text ltx_bib_etal">et al.</span> (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pythia: A suite for analyzing large language models across training and scaling</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Machine Learning</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;2397–2430</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib100">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">B. Brittain (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Meta tells court AI software does not violate author copyrights</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.reuters.com/legal/litigation/meta-tells-court-ai-software-does-not-violate-author-copyrights-2023-09-19/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1.p1.1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§C.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib129">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. Brown (2003)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The da vinci code</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Doubleday</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib126">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. Brown (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The society of unknowable objects</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">HarperCollins</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib105">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Campbell v. Acuff-Rose Music</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">510 u.s. 569 (1994)</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#footnote1" title="In 1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">footnote 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib107">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramèr, and C. Zhang (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Quantifying Memorization Across Neural Language Models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Learning Representations</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p2.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p3.2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p3.2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p5.1" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2.p2.1" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S6.p2.1" title="6 Conclusion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib9">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, <span class="ltx_text ltx_bib_etal">et al.</span> (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Extracting training data from large language models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">30th USENIX Security Symposium (USENIX Security 21)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.&nbsp;2633–2650</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p2.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p2.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p3.2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2.p1.1" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.p1.1" title="3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S6.p2.1" title="6 Conclusion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib34">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. Carlini (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">What my privacy papers (don’t) have to say about copyright and generative AI</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://nicholas.carlini.com/writing/2025/privacy-copyright-and-generative-models.html" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2.p2.1" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S6.p2.1" title="6 Conclusion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib81">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">H. Chang, A. S. Shamsabadi, K. Katevas, H. Haddadi, and R. Shokri (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Context-aware membership inference attacks against pre-trained large language models</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2409.13745</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2409.13745" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p2.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib60">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, <span class="ltx_text ltx_bib_etal">et al.</span> (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Evaluating large language models trained on code</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2107.03374</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2107.03374" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib136">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Chouldechova, A. F. Cooper, S. Barocas, A. Palia, D. Vann, and H. Wallach (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">The Thirty-Ninth Annual Conference on Neural Information Processing Systems Position Paper Track</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://openreview.net/forum?id=d7hqAhLvWG" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p6.4" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2.p2.1" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS1.p5.1" title="5.1 Limitations and caveats ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib27">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep reinforcement learning from human preferences</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Neural Information Processing Systems</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">30</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib101">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Claburn (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Microsoft CEO of AI: Your online content is ’freeware’ fodder for training models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">The Register</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.theregister.com/2024/06/28/microsoft_ceo_ai/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1.p1.1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§C.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib131">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Collins (2008)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The hunger games</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Scholastic Press</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib35">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[25]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">3:23-cv-01092 (M.D. Tenn.)</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib72">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. F. Cooper, C. A. Choquette-Choo, M. Bogen, M. Jagielski, K. Filippova, K. Z. Liu, A. Chouldechova, Y. H. Jamie Hayes, <span class="ltx_text ltx_bib_etal">et al.</span> (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Machine unlearning doesn’t do what you think: lessons for generative ai policy, research, and practice</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2412.06966</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2.p4.1" title="5.2 Copyright ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib71">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. F. Cooper, A. Gokaslan, A. B. Cyphert, C. D. Sa, M. A. Lemley, D. E. Ho, and P. Liang (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Extracting memorized pieces of (copyrighted) books from open-weight language models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2505.12546</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A2.p1.9" title="Appendix B Procedure for quantifying extraction success ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A2.p2.8" title="Appendix B Procedure for quantifying extraction success ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1.p1.1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§C.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p4.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p5.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p3.2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p1.1" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p3.2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p4.4" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p5.1" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p6.2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1.p2.3" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2.p1.1" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS1.p2.1" title="5.1 Limitations and caveats ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2.p3.1" title="5.2 Copyright ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#footnote3" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">footnote 3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#footnote6" title="In 3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">footnote 6</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#footnote9" title="In 4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">footnote 9</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib36">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. F. Cooper and J. Grimmelmann (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The Files are in the Computer: Copyright, Memorization, and Generative AI</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2404.12590</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p3.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p2.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2.p2.1" title="5.2 Copyright ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2.p3.1" title="5.2 Copyright ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S6.p2.1" title="6 Conclusion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib83">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. F. Cooper, K. Lee, J. Grimmelmann, D. Ippolito, C. Callison-Burch, C. A. Choquette-Choo, N. Mireshghallah, M. Brundage, D. Mimno, M. Z. Choksi, J. M. Balkin, N. Carlini, C. D. Sa, J. Frankle, D. Ganguli, B. Gipson, A. Guadamuz, S. L. Harris, A. Z. Jacobs, E. Joh, G. Kamath, M. Lemley, C. Matthews, C. McLeavey, C. McSherry, M. Nasr, P. Ohm, A. Roberts, T. Rubin, P. Samuelson, L. Schubert, K. Vaccaro, L. Villa, F. Wu, and E. Zeide (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Report of the 1st Workshop on Generative AI and Law</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2311.06477</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p2.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p2.1" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib119">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">difflib SequenceMatcher (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">difflib — Helpers for computing deltas</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Python Standard Library v3.14.2</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://docs.python.org/3/library/difflib.html" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A2.p2.8" title="Appendix B Procedure for quantifying extraction success ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1.p2.3" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1.p3.5" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib108">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. W. Dornis (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generative AI, Reproductions Inside the Model, and the Making Available to the Public</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Review of Intellectual Property and Competition Law</span> <span class="ltx_text ltx_bib_volume">56</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;909–938</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p3.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2.p2.1" title="5.2 Copyright ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib84">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">V. Feldman (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Does learning require memorization? a short tale about a long tail</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">STOC 2020</span>, <span class="ltx_text ltx_bib_place">New York, NY, USA</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;954–959</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 9781450369794</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p2.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib114">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">F. S. Fitzgerald (1925)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The great gatsby</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Charles Scribner’s Sons</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p7.2" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib68">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[34]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Gesellschaft für musikalische Aufführungs- und mechanische Vervielfältigungsrechte</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">42 O 14139/24</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p3.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2.p2.1" title="5.2 Copyright ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib85">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Gemini Team, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, <span class="ltx_text ltx_bib_etal">et al.</span> (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Gemini 1.5: unlocking multimodal understanding across millions of tokens of context</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2403.05530</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p3.2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib110">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Gianella (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">LinkedIn post regarding gema v. openai appeal</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Posted in the capacity of OpenAI’s Head of Europe and Middle East Policy</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.linkedin.com/posts/sandrogianella_were-appealing-the-gema-case-i-want-to-activity-7405076994062729216-TmEP/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S6.p2.1" title="6 Conclusion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib89">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[37]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Google AI for Developers</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generating content — gemini api</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.google.dev/api/generate-content" title="">https://ai.google.dev/api/generate-content</a>Accessed: 2025-12-04</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS3.p2.1" title="4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib135">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Google Cloud (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Gemini 2.5 Pro</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p2.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib40">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Grattafiori <span class="ltx_text ltx_bib_etal">et al.</span> (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The Llama 3 Herd of Models</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2407.21783</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2407.21783" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p3.2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib80">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Hayes, I. Shumailov, C. A. Choquette-Choo, M. Jagielski, G. Kaissis, M. Nasr, M. S. M. S. Annamalai, N. Mireshghallah, I. Shilov, M. Meeus, Y. de Montjoye, K. Lee, F. Boenisch, A. Dziedzic, and A. F. Cooper (2025a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Exploring the limits of strong membership inference attacks on large language models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">The Thirty-ninth Annual Conference on Neural Information Processing Systems</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://openreview.net/forum?id=x0i7wvRLHK" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p2.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib41">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Hayes, M. Swanberg, H. Chaudhari, I. Yona, I. Shumailov, M. Nasr, C. A. Choquette-Choo, K. Lee, and A. F. Cooper (2025b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Measuring memorization in language models via probabilistic extraction</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</span>,  <span class="ltx_text ltx_bib_editor">L. Chiruzzo, A. Ritter, and L. Wang (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Albuquerque, New Mexico</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;9266–9291</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 979-8-89176-189-6</span>,
<a class="ltx_ref ltx_bib_external" href="https://aclanthology.org/2025.naacl-long.469/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p2.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p3.2" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p3.2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p5.1" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2.p2.1" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib130">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Heller (1961)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Catch-22</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Simon &amp; Schuster</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib42">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">P. Henderson, X. Li, D. Jurafsky, T. Hashimoto, M. A. Lemley, and P. Liang (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Foundation Models and Fair Use</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2303.15715</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2303.15715" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib32">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. Hendrycks, N. Carlini, J. Schulman, and J. Steinhardt (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsolved problems in ML safety</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2109.13916</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib121">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Henzinger (2006)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Finding near-duplicate web pages: a large-scale evaluation of algorithms</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">SIGIR ’06</span>, <span class="ltx_text ltx_bib_place">New York, NY, USA</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;284–291</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 1595933697</span>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1145/1148170.1148222" title="">Link</a>,
<a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1145/1148170.1148222" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1.p1.1" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib124">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. C. Hoad and J. Zobel (2003)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Methods for Identifying Versioned and Plagiarized Documents</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">J. Assoc. Inf. Sci. Technol.</span> <span class="ltx_text ltx_bib_volume">54</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;203–215</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1.p1.1" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib70">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Hughes, S. Price, A. Lynch, R. Schaeffer, F. Barez, S. Koyejo, H. Sleight, E. Jones, E. Perez, and M. Sharma (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Best-of-n jailbreaking</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2412.03556</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2412.03556" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.I1.i1.p1.1" title="In 1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">item&nbsp;1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1.p2.1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1.p3.7" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib97">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. Ippolito, F. Tramèr, M. Nasr, C. Zhang, M. Jagielski, K. Lee, C. A. Choquette-Choo, and N. Carlini (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Preventing verbatim memorization in language models gives a false sense of privacy</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2210.17546</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p6.2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib69">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Kadrey et al. v. Meta Platforms, Inc. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">United States District Court for the Northern District of CaliforniaCase No. 3:23-cv-03417-VC.</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1.p1.1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§C.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib49">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Kadrey Judgment (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Order Denying the Plaintiffs’ Motion for Partial Summary Judgment and Granting Meta’s Cross-Motion for Partial Summary Judgment, Kadrey et al. v. Meta Platforms, Inc.</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">No. 3:23-cv-03417-VC (N.D. Cal. Jun. 25, 2025)</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://law.justia.com/cases/federal/district-courts/california/candce/3:2023cv03417/415175/598/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p3.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2.p2.1" title="5.2 Copyright ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib102">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. King (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Anthropic CEO Doubles Down on Fair Use Defense–“The Law Will Back Us Up”’</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Digital Music News</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.digitalmusicnews.com/2024/04/15/anthropic-ceo-doubles-down-on-fair-use-defense/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1.p1.1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§C.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib62">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Lee, A. F. Cooper, J. Grimmelmann, and D. Ippolito (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">AI and Law: The Next Generation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">SSRN</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">http://dx.doi.org/10.2139/ssrn.4580739</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib45">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Lee, A. F. Cooper, and J. Grimmelmann (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Talkin’ ’Bout AI Generation: Copyright and the Generative-AI Supply Chain</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2309.08133</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p2.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS2.p3.1" title="5.2 Copyright ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib138">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Lee, A. F. Cooper, and J. Grimmelmann (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Talkin’ ’Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version)</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Symposium on Computer Science and Law</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">CSLAW ’24</span>, <span class="ltx_text ltx_bib_place">New York, NY, USA</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;48–63</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 9798400703331</span>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1145/3614407.3643696" title="">Link</a>,
<a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1145/3614407.3643696" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib44">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deduplicating Training Data Makes Language Models Better</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">1</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;8424–8445</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p2.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p2.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p3.2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p5.1" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p6.2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2.p2.1" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib46">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Lemley and B. Casey (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fair Learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Texas Law Review</span> <span class="ltx_text ltx_bib_volume">99</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;743</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib128">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. R.R. Martin (1996)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A game of thrones</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Voyager Books</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib132">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Milan (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The duchess war</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Createspace</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib115">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Morrison (1987)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Beloved</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Alfred A. Knopf Inc.</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib48">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo, E. Wallace, F. Tramèr, and K. Lee (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scalable Extraction of Training Data from (Production) Language Models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2311.17035</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS3.Px1.p1.1" title="Chat UI ‣ C.2.3 Refusal retries for per-chapter experiments with GPT-4.1 ‣ C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§C.2.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p2.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p4.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1.p2.1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2.p3.2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2.p2.1" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2.p3.5" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2.p6.13" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S5.SS1.p2.1" title="5.1 Limitations and caveats ‣ 5 Discussion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S6.p2.1" title="6 Conclusion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib88">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Nasr, J. Rando, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo, F. Tramèr, and K. Lee (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scalable Extraction of Training Data from Aligned, Production Language Models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">The Thirteenth International Conference on Learning Representations</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://openreview.net/forum?id=vjel3nWP2a" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p4.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1.p2.1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2.p3.5" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib67">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[62]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">2:24-cv-00711 (C.D. Cal.)</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_report" id="bib.bib61">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">GPT-4 System Card</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Technical report</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://cdn.openai.com/papers/gpt-4-system-card.pdf" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib65">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">OpenAI (2024a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">OpenAI and Journalism</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Accessed: 2025-11-06</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://openai.com/index/openai-and-journalism/?utm_source=chatgpt.com" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib73">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">OpenAI (2024b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">OpenAI Model Spec (2024/05/08)</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Accessed: 2025-05-13</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://cdn.openai.com/spec/model-spec-2024-05-08.html" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p4.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib133">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">OpenAI (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Introducing GPT-4.1 in the API</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://openai.com/index/gpt-4-1/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p2.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib90">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. Orwell (1949)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Nineteen-eighty four</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Harcourt, Brace and Company</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib29">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, <span class="ltx_text ltx_bib_etal">et al.</span> (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Training language models to follow instructions with human feedback</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2203.02155</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib109">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Poltz and F. Heine (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">OpenAI used song lyrics in violation of copyright laws, German court says</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Reuters</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.reuters.com/world/german-court-sides-with-plaintiff-copyright-case-against-openai-2025-11-11/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p3.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib86">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Project Zero (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Vulnerability disclosure policy</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://googleprojectzero.blogspot.com/p/vulnerability-disclosure-policy.html" title="">https://googleprojectzero.blogspot.com/p/vulnerability-disclosure-policy.html</a>Accessed: 2025-02-14</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p7.2" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib112">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J.K. Rowling (1998)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Harry potter and the sorcerer’s stone</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Scholastic</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p6.4" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib113">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J.K. Rowling (2000)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Harry potter and the goblet of fire</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Scholastic</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib127">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J.D. Salinger (1951)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The catcher in the rye</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Little, Brown and Company</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib52">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">P. Samuelson (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generative AI meets copyright</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science</span> <span class="ltx_text ltx_bib_volume">381</span> (<span class="ltx_text ltx_bib_number">6654</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;158–161</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p5.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib122">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Santos, J. J. Almeida, and N. Carvalho (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Structural alignment of plain text books</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12)</span>,  <span class="ltx_text ltx_bib_editor">N. Calzolari, K. Choukri, T. Declerck, M. U. Doğan, B. Maegaard, J. Mariani, A. Moreno, J. Odijk, and S. Piperidis (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Istanbul, Turkey</span>, <span class="ltx_text ltx_bib_pages"> pp.&nbsp;2069–2074</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://aclanthology.org/L12-1576/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1.p1.1" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib111">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Schwarzschild, Z. Feng, P. Maini, Z. C. Lipton, and J. Z. Kolter (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Rethinking LLM Memorization through the Lens of Adversarial Compression</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2404.15146</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2404.15146" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S6.p2.1" title="6 Conclusion ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§6</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib76">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Sharma, M. Tong, J. Mu, J. Wei, J. Kruthoff, S. Goodfriend, E. Ong, A. Peng, R. Agarwal, C. Anil, A. Askell, N. Bailey, J. Benton, E. Bluemke, S. R. Bowman, E. Christiansen, H. Cunningham, A. Dau, A. Gopal, R. Gilson, L. Graham, L. Howard, N. Kalra, T. Lee, K. Lin, P. Lofgren, F. Mosconi, C. O’Hara, C. Olsson, L. Petrini, S. Rajani, N. Saxena, A. Silverstein, T. Singh, T. Sumers, L. Tang, K. K. Troy, C. Weisser, R. Zhong, G. Zhou, J. Leike, J. Kaplan, and E. Perez (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Constitutional classifiers: defending against universal jailbreaks across thousands of hours of red teaming</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2501.18837</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2501.18837" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib117">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Shelley (1818)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Frankenstein</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Lackington, Hughes, Harding, Mavor, &amp; Jones</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p7.2" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib99">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">The Authors Guild (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The Authors Guild, John Grisham, Jodi Picoult, David Baldacci, George R.R. Martin, and 13 Other Authors File Class-Action Suit Against OpenAI</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://authorsguild.org/news/ag-and-authors-file-class-action-suit-against-openai/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1.p1.1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§C.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib116">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J.R.R. Tolkien (1937)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The hobbit</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">George Allen and Unwin</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p3.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib55">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">LLaMA: Open and Efficient Foundation Language Models</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2302.13971</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2302.13971" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib120">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Wang, D. E. Ho, and S. Koyejo (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The inadequacy of offline large language model evaluations: a need to account for personalization in model behavior</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Patterns</span> <span class="ltx_text ltx_bib_volume">6</span> (<span class="ltx_text ltx_bib_number">12</span>), <span class="ltx_text ltx_bib_pages"> pp.&nbsp;101397</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Published 12 December 2025</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1016/j.patter.2025.101397" title="">Document</a>,
<a class="ltx_ref ltx_bib_external" href="https://doi.org/10.1016/j.patter.2025.101397" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS3.Px1.p1.1" title="Chat UI ‣ C.2.3 Refusal retries for per-chapter experiments with GPT-4.1 ‣ C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§C.2.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib123">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Wang and Y. Dong (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Measurement of Text Similarity: A Survey</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Information</span> <span class="ltx_text ltx_bib_volume">11</span> (<span class="ltx_text ltx_bib_number">9</span>).
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://www.mdpi.com/2078-2489/11/9/421" title="">Link</a>,
<span class="ltx_text issn ltx_bib_external">ISSN 2078-2489</span>,
<a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.3390/info11090421" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1.p1.1" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib79">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Wei, N. Haghtalab, and J. Steinhardt (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Jailbroken: how does llm safety training fail?</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2307.02483</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2307.02483" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib30">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Finetuned language models are zero-shot learners</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2109.01652</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib82">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. T. Wei, A. Godbole, M. A. Khan, R. Wang, X. Zhu, J. Flemings, N. Kashyap, K. P. Gummadi, W. Neiswanger, and R. Jia (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Hubble: a model suite to advance the study of llm memorization</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2510.19811</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2510.19811" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS2.p2.1" title="3.3.2 Claiming extraction success without information about training-data membership ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§3.3.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib98">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Wiggers and M. Zeff (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">In AI copyright case, Zuckerberg turns to YouTube for his defense</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://techcrunch.com/2025/01/15/in-ai-copyright-case-zuckerberg-turns-to-youtube-for-his-defense/" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1.p1.1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§C.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S1.p1.1" title="1 Introduction ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib134">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">xAI (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Models and Pricing</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">“The knowledge cut-off date of Grok 3 and Grok 4 is November, 2024.”</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://docs.x.ai/docs/models" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS1.p2.1" title="4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib28">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fine-tuning language models from human preferences</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1909.08593</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib77">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, and M. Fredrikson (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Universal and transferable adversarial attacks on aligned language models</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2307.15043</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2307.15043" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S2.p4.1" title="2 Background and related work ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>BoN perturbtations<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">附录 ABoN 扰动</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.3">For completeness, we document the exact perturbations used during the Best-of-<math alttext="N" class="ltx_Math" display="inline" id="A1.p1.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> (BoN) jailbreak for Claude 3.7 Sonnet and GPT-4.1. We fix <math alttext="\sigma" class="ltx_Math" display="inline" id="A1.p1.2.m2" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> to be <math alttext="0.6" class="ltx_Math" display="inline" id="A1.p1.3.m3" intent=":literal"><semantics><mn>0.6</mn><annotation encoding="application/x-tex">0.6</annotation></semantics></math> for all experiments.
All perturbations operate deterministically, given the random seed, allowing exact replay of prompt sequence.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了完整性，我们记录了在 Claude 3.7 Sonnet 和 GPT-4.1 的最佳之一（BoN）越狱过程中使用的确切扰动。我们为所有实验将 <math intent=":literal" id="A1.p1.2.m2" display="inline" class="ltx_Math" alttext="\sigma"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> 固定为 <math intent=":literal" id="A1.p1.3.m3" display="inline" class="ltx_Math" alttext="0.6"><semantics><mn>0.6</mn><annotation encoding="application/x-tex">0.6</annotation></semantics></math> 。所有扰动在给定随机种子的情况下是确定性的，允许精确重放提示序列。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<dl class="ltx_description" id="A1.I1">
<dt class="ltx_item" id="A1.I1.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.ix1.1.1.1">Identity.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">身份。</font></font></font></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I1.ix1.p1">
<p class="ltx_p" id="A1.I1.ix1.p1.1">Returns the prefix unchanged.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">返回前缀不变。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</dd>
<dt class="ltx_item" id="A1.I1.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.ix2.1.1.1">Capitalization.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">大写。</font></font></font></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I1.ix2.p1">
<p class="ltx_p" id="A1.I1.ix2.p1.2">Iterates over every alphabetic character and flips its case with probability <math alttext="p\in[0,1]" class="ltx_Math" display="inline" id="A1.I1.ix2.p1.1.m1" intent=":literal"><semantics><mrow><mi>p</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">p\in[0,1]</annotation></semantics></math> (we use <math alttext="p\in\{0.2,0.5\}" class="ltx_Math" display="inline" id="A1.I1.ix2.p1.2.m2" intent=":literal"><semantics><mrow><mi>p</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0.2</mn><mo>,</mo><mn>0.5</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">p\in\{0.2,0.5\}</annotation></semantics></math>).
Sampling is i.i.d. per character using a pseudo-random generator seeded per perturbation.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">遍历每个字母字符，并以概率 <math intent=":literal" id="A1.I1.ix2.p1.1.m1" display="inline" class="ltx_Math" alttext="p\in[0,1]"><semantics><mrow><mi>p</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">p\in[0,1]</annotation></semantics></math> （我们使用 <math intent=":literal" id="A1.I1.ix2.p1.2.m2" display="inline" class="ltx_Math" alttext="p\in\{0.2,0.5\}"><semantics><mrow><mi>p</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0.2</mn><mo>,</mo><mn>0.5</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">p\in\{0.2,0.5\}</annotation></semantics></math> ）翻转其大小写。对每个字符进行独立同分布的采样，使用伪随机生成器，每个扰动都使用不同的种子。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</dd>
<dt class="ltx_item" id="A1.I1.ix3"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.ix3.1.1.1">Spacing.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">间距。</font></font></font></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I1.ix3.p1">
<p class="ltx_p" id="A1.I1.ix3.p1.3">Processes the string left-to-right. For each existing space we remove it with probability <math alttext="p_{\mathrm{rm}}" class="ltx_Math" display="inline" id="A1.I1.ix3.p1.1.m1" intent=":literal"><semantics><msub><mi>p</mi><mi>rm</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{rm}}</annotation></semantics></math>;
for each non-space character we optionally insert a space immediately after it with probability <math alttext="p_{\mathrm{add}}" class="ltx_Math" display="inline" id="A1.I1.ix3.p1.2.m2" intent=":literal"><semantics><msub><mi>p</mi><mi>add</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{add}}</annotation></semantics></math>, so long as the next character is not already whitespace.
We use <math alttext="(p_{\mathrm{add}},p_{\mathrm{rm}})\in\{(0.05,0.05),(0.1,0.1)\}" class="ltx_Math" display="inline" id="A1.I1.ix3.p1.3.m3" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>add</mi></msub><mo>,</mo><msub><mi>p</mi><mi>rm</mi></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mn>0.05</mn><mo>,</mo><mn>0.05</mn><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">(p_{\mathrm{add}},p_{\mathrm{rm}})\in\{(0.05,0.05),(0.1,0.1)\}</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">从左到右处理字符串。对于每个现有的空格，我们以概率 <math intent=":literal" id="A1.I1.ix3.p1.1.m1" display="inline" class="ltx_Math" alttext="p_{\mathrm{rm}}"><semantics><msub><mi>p</mi><mi>rm</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{rm}}</annotation></semantics></math> 将其移除；对于每个非空格字符，我们以概率 <math intent=":literal" id="A1.I1.ix3.p1.2.m2" display="inline" class="ltx_Math" alttext="p_{\mathrm{add}}"><semantics><msub><mi>p</mi><mi>add</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{add}}</annotation></semantics></math> 选择性地在其后立即插入一个空格，只要下一个字符不是已经为空白字符。我们使用 <math intent=":literal" id="A1.I1.ix3.p1.3.m3" display="inline" class="ltx_Math" alttext="(p_{\mathrm{add}},p_{\mathrm{rm}})\in\{(0.05,0.05),(0.1,0.1)\}"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>add</mi></msub><mo>,</mo><msub><mi>p</mi><mi>rm</mi></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mn>0.05</mn><mo>,</mo><mn>0.05</mn><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">(p_{\mathrm{add}},p_{\mathrm{rm}})\in\{(0.05,0.05),(0.1,0.1)\}</annotation></semantics></math> 。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</dd>
<dt class="ltx_item" id="A1.I1.ix4"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.ix4.1.1.1">Word order shuffle.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">词序打乱。</font></font></font></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I1.ix4.p1">
<p class="ltx_p" id="A1.I1.ix4.p1.1">Split the text into sentences using punctuation boundaries (‘.’, ‘!’, ‘?’).
Within each sentence, we shuffle the token order with probability <math alttext="p_{\mathrm{shuffle}}=0.3" class="ltx_Math" display="inline" id="A1.I1.ix4.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>p</mi><mi>shuffle</mi></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding="application/x-tex">p_{\mathrm{shuffle}}=0.3</annotation></semantics></math> when the sentence contains more than one token.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">使用标点符号边界（‘。’、‘！’、‘？’）将文本分割成句子。当句子包含多个标记时，以概率 <math intent=":literal" id="A1.I1.ix4.p1.1.m1" display="inline" class="ltx_Math" alttext="p_{\mathrm{shuffle}}=0.3"><semantics><mrow><msub><mi>p</mi><mi>shuffle</mi></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding="application/x-tex">p_{\mathrm{shuffle}}=0.3</annotation></semantics></math> 在句子内随机打乱标记顺序。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</dd>
<dt class="ltx_item" id="A1.I1.ix5"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.ix5.1.1.1">Character substitution.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">字符替换。</font></font></font></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I1.ix5.p1">
<p class="ltx_p" id="A1.I1.ix5.p1.5">For each letter, with probability <math alttext="p_{\mathrm{sub}}" class="ltx_Math" display="inline" id="A1.I1.ix5.p1.1.m1" intent=":literal"><semantics><msub><mi>p</mi><mi>sub</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{sub}}</annotation></semantics></math> (set to <math alttext="0.1" class="ltx_Math" display="inline" id="A1.I1.ix5.p1.2.m2" intent=":literal"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> or <math alttext="0.05" class="ltx_Math" display="inline" id="A1.I1.ix5.p1.3.m3" intent=":literal"><semantics><mn>0.05</mn><annotation encoding="application/x-tex">0.05</annotation></semantics></math>), we replace the letter with a visually similar glyph drawn uniformly from a fixed mapping (e.g., <math alttext="\text{`a&#39;}\rightarrow\{\text{`@&#39;},{\text{`}\acute{\text{a}}\text{&#39;}},{\text{`}\grave{\text{a}}\text{&#39;}},{\text{`}\hat{\text{a}}\text{&#39;}}\}" class="ltx_Math" display="inline" id="A1.I1.ix5.p1.4.m4" intent=":literal"><semantics><mrow><mtext>‘a’</mtext><mo stretchy="false">→</mo><mrow><mo stretchy="false">{</mo><mtext>‘@’</mtext><mo>,</mo><mrow><mtext>‘</mtext><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mtext>a</mtext><mo>´</mo></mover><mo lspace="0em" rspace="0em">​</mo><mtext>’</mtext></mrow><mo>,</mo><mrow><mtext>‘</mtext><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mtext>a</mtext><mo>`</mo></mover><mo lspace="0em" rspace="0em">​</mo><mtext>’</mtext></mrow><mo>,</mo><mrow><mtext>‘</mtext><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mtext>a</mtext><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mtext>’</mtext></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\text{`a'}\rightarrow\{\text{`@'},{\text{`}\acute{\text{a}}\text{'}},{\text{`}\grave{\text{a}}\text{'}},{\text{`}\hat{\text{a}}\text{'}}\}</annotation></semantics></math>, <math alttext="\text{`s&#39;}\rightarrow\{\text{`\textdollar&#39;},\text{`5&#39;}\}" class="ltx_Math" display="inline" id="A1.I1.ix5.p1.5.m5" intent=":literal"><semantics><mrow><mtext>‘s’</mtext><mo stretchy="false">→</mo><mrow><mo stretchy="false">{</mo><mtext>‘$’</mtext><mo>,</mo><mtext>‘5’</mtext><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\text{`s'}\rightarrow\{\text{`\textdollar'},\text{`5'}\}</annotation></semantics></math>).
Uppercase letters inherit the capitalization of the replacement.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">对于每个字母，以概率 <math intent=":literal" id="A1.I1.ix5.p1.1.m1" display="inline" class="ltx_Math" alttext="p_{\mathrm{sub}}"><semantics><msub><mi>p</mi><mi>sub</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{sub}}</annotation></semantics></math> （设置为 <math intent=":literal" id="A1.I1.ix5.p1.2.m2" display="inline" class="ltx_Math" alttext="0.1"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> 或 <math intent=":literal" id="A1.I1.ix5.p1.3.m3" display="inline" class="ltx_Math" alttext="0.05"><semantics><mn>0.05</mn><annotation encoding="application/x-tex">0.05</annotation></semantics></math> ），我们将其替换为从固定映射中均匀抽取的视觉上相似的符号（例如， <math intent=":literal" id="A1.I1.ix5.p1.4.m4" display="inline" class="ltx_Math" alttext="\text{`a&#39;}\rightarrow\{\text{`@&#39;},{\text{`}\acute{\text{a}}\text{&#39;}},{\text{`}\grave{\text{a}}\text{&#39;}},{\text{`}\hat{\text{a}}\text{&#39;}}\}"><semantics><mrow><mtext>‘a’</mtext><mo stretchy="false">→</mo><mrow><mo stretchy="false">{</mo><mtext>‘@’</mtext><mo>,</mo><mrow><mtext>‘</mtext><mo rspace="0em" lspace="0em">​</mo><mover accent="true"><mtext>a</mtext><mo>´</mo></mover><mo rspace="0em" lspace="0em">​</mo><mtext>’</mtext></mrow><mo>,</mo><mrow><mtext>‘</mtext><mo rspace="0em" lspace="0em">​</mo><mover accent="true"><mtext>a</mtext><mo>`</mo></mover><mo rspace="0em" lspace="0em">​</mo><mtext>’</mtext></mrow><mo>,</mo><mrow><mtext>‘</mtext><mo rspace="0em" lspace="0em">​</mo><mover accent="true"><mtext>a</mtext><mo>^</mo></mover><mo rspace="0em" lspace="0em">​</mo><mtext>’</mtext></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\text{`a'}\rightarrow\{\text{`@'},{\text{`}\acute{\text{a}}\text{'}},{\text{`}\grave{\text{a}}\text{'}},{\text{`}\hat{\text{a}}\text{'}}\}</annotation></semantics></math> 、 <math intent=":literal" id="A1.I1.ix5.p1.5.m5" display="inline" class="ltx_Math" alttext="\text{`s&#39;}\rightarrow\{\text{`\textdollar&#39;},\text{`5&#39;}\}"><semantics><mrow><mtext>‘s’</mtext><mo stretchy="false">→</mo><mrow><mo stretchy="false">{</mo><mtext>‘$’</mtext><mo>,</mo><mtext>‘5’</mtext><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\text{`s'}\rightarrow\{\text{`\textdollar'},\text{`5'}\}</annotation></semantics></math> ）。大写字母会继承替换字母的大小写形式。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</dd>
<dt class="ltx_item" id="A1.I1.ix6"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.ix6.1.1.1">Punctuation edits.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">标点符号编辑。</font></font></font></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I1.ix6.p1">
<p class="ltx_p" id="A1.I1.ix6.p1.3">For characters that are punctuation marks (e.g., ‘.’, ‘,’, ‘!’, ‘?’, ‘;’, ‘:’), we remove them with probability <math alttext="p_{\mathrm{rm}}" class="ltx_Math" display="inline" id="A1.I1.ix6.p1.1.m1" intent=":literal"><semantics><msub><mi>p</mi><mi>rm</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{rm}}</annotation></semantics></math>;
for alphabetic characters, we insert a random punctuation mark immediately after with probability <math alttext="p_{\mathrm{add}}" class="ltx_Math" display="inline" id="A1.I1.ix6.p1.2.m2" intent=":literal"><semantics><msub><mi>p</mi><mi>add</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{add}}</annotation></semantics></math>.
We use <math alttext="(p_{\mathrm{add}},p_{\mathrm{rm}})\in\{(0.05,0.05),(0.1,0.1)\}" class="ltx_Math" display="inline" id="A1.I1.ix6.p1.3.m3" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>add</mi></msub><mo>,</mo><msub><mi>p</mi><mi>rm</mi></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mn>0.05</mn><mo>,</mo><mn>0.05</mn><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">(p_{\mathrm{add}},p_{\mathrm{rm}})\in\{(0.05,0.05),(0.1,0.1)\}</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">对于标点符号字符（例如‘.’、‘,’、‘!’、‘?’、‘;’、‘:’），我们以概率 <math intent=":literal" id="A1.I1.ix6.p1.1.m1" display="inline" class="ltx_Math" alttext="p_{\mathrm{rm}}"><semantics><msub><mi>p</mi><mi>rm</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{rm}}</annotation></semantics></math> 移除它们；对于字母字符，我们以概率 <math intent=":literal" id="A1.I1.ix6.p1.2.m2" display="inline" class="ltx_Math" alttext="p_{\mathrm{add}}"><semantics><msub><mi>p</mi><mi>add</mi></msub><annotation encoding="application/x-tex">p_{\mathrm{add}}</annotation></semantics></math> 在其后插入一个随机标点符号。我们使用 <math intent=":literal" id="A1.I1.ix6.p1.3.m3" display="inline" class="ltx_Math" alttext="(p_{\mathrm{add}},p_{\mathrm{rm}})\in\{(0.05,0.05),(0.1,0.1)\}"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>add</mi></msub><mo>,</mo><msub><mi>p</mi><mi>rm</mi></msub><mo stretchy="false">)</mo></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mn>0.05</mn><mo>,</mo><mn>0.05</mn><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">(p_{\mathrm{add}},p_{\mathrm{rm}})\in\{(0.05,0.05),(0.1,0.1)\}</annotation></semantics></math> 。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</dd>
<dt class="ltx_item" id="A1.I1.ix7"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.ix7.1.1.1">Word scrambling.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">词序打乱。</font></font></font></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I1.ix7.p1">
<p class="ltx_p" id="A1.I1.ix7.p1.1">For each text token longer than three characters, we shuffle its interior characters (leave first and last fixed) with probability <math alttext="\sigma^{1/2}" class="ltx_Math" display="inline" id="A1.I1.ix7.p1.1.m1" intent=":literal"><semantics><msup><mi>σ</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\sigma^{1/2}</annotation></semantics></math>.
This preserves readability while altering the byte-level form.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">对于每个长度超过三个字符的文本标记，我们以概率 <math intent=":literal" id="A1.I1.ix7.p1.1.m1" display="inline" class="ltx_Math" alttext="\sigma^{1/2}"><semantics><msup><mi>σ</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\sigma^{1/2}</annotation></semantics></math> 打乱其内部字符（保持首尾字符不变）。这既保留了可读性，又改变了字节级形式。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</dd>
<dt class="ltx_item" id="A1.I1.ix8"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.ix8.1.1.1">Random capitalization.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">随机大小写转换。</font></font></font></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I1.ix8.p1">
<p class="ltx_p" id="A1.I1.ix8.p1.1">Similar to capitalization above, but the flip probability is driven by the intensity parameter:
each alphabetic character swaps case with probability <math alttext="\sigma^{1/2}" class="ltx_Math" display="inline" id="A1.I1.ix8.p1.1.m1" intent=":literal"><semantics><msup><mi>σ</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\sigma^{1/2}</annotation></semantics></math>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">与上述大写类似，但翻转概率由强度参数驱动：每个字母字符以 <math intent=":literal" id="A1.I1.ix8.p1.1.m1" display="inline" class="ltx_Math" alttext="\sigma^{1/2}"><semantics><msup><mi>σ</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><annotation encoding="application/x-tex">\sigma^{1/2}</annotation></semantics></math> 的概率交换大小写。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</dd>
<dt class="ltx_item" id="A1.I1.ix9"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.ix9.1.1.1">ASCII noising.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">ASCII 噪声。</font></font></font></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para ltx_noindent" id="A1.I1.ix9.p1">
<p class="ltx_p" id="A1.I1.ix9.p1.3">For every printable ASCII character (code points 32–126) we perturb the character with probability <math alttext="\sigma^{3}" class="ltx_Math" display="inline" id="A1.I1.ix9.p1.1.m1" intent=":literal"><semantics><msup><mi>σ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\sigma^{3}</annotation></semantics></math>.
When triggered, we add or subtract <math alttext="1" class="ltx_Math" display="inline" id="A1.I1.ix9.p1.2.m2" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> from its code point (chosen uniformly from <math alttext="\{-1,+1\}" class="ltx_Math" display="inline" id="A1.I1.ix9.p1.3.m3" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{-1,+1\}</annotation></semantics></math>);
if the resulting code point is outside the printable range, we leave the character unchanged.
This mimics light OCR or transmission noise while preserving human readability.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">对于每个可打印的 ASCII 字符（代码点 32–126），我们以 <math intent=":literal" id="A1.I1.ix9.p1.1.m1" display="inline" class="ltx_Math" alttext="\sigma^{3}"><semantics><msup><mi>σ</mi><mn>3</mn></msup><annotation encoding="application/x-tex">\sigma^{3}</annotation></semantics></math> 的概率扰动该字符。当触发时，我们从其代码点（均匀地从 <math intent=":literal" id="A1.I1.ix9.p1.3.m3" display="inline" class="ltx_Math" alttext="\{-1,+1\}"><semantics><mrow><mo stretchy="false">{</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{-1,+1\}</annotation></semantics></math> 中选择）加上或减去 <math intent=":literal" id="A1.I1.ix9.p1.2.m2" display="inline" class="ltx_Math" alttext="1"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> ；如果结果代码点不在可打印范围内，我们保持该字符不变。这模拟了轻微的 OCR 或传输噪声，同时保留了人类可读性。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</dd>
<dt class="ltx_item" id="A1.I1.ix10"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.ix10.1.1.1">Composites.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">复合体。</font></font></font></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="A1.I1.ix10.p1">
<p class="ltx_p" id="A1.I1.ix10.p1.3">We also chain multiple perturbations in a fixed order, e.g., capitalization <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A1.I1.ix10.p1.1.m1" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> spacing, or word scrambling <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A1.I1.ix10.p1.2.m2" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> random capitalization <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A1.I1.ix10.p1.3.m3" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> ASCII noising.
Each composite inherits the parameter settings of its constituents.
Identity is always included in the pool so that unperturbed prompts are sampled alongside perturbed ones.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们还会以固定顺序链式应用多个扰动，例如，大写字母 <math intent=":literal" id="A1.I1.ix10.p1.1.m1" display="inline" class="ltx_Math" alttext="\rightarrow"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> 空格，或单词打乱 <math intent=":literal" id="A1.I1.ix10.p1.2.m2" display="inline" class="ltx_Math" alttext="\rightarrow"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> 随机大写字母 <math intent=":literal" id="A1.I1.ix10.p1.3.m3" display="inline" class="ltx_Math" alttext="\rightarrow"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> ASCII 噪声。每个复合扰动会继承其组成部分的参数设置。身份始终包含在池中，以便未扰动的提示与扰动提示一同采样。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</dd>
</dl>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Procedure for quantifying extraction success<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">附录 B 量化提取成功的步骤</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.9">In Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3" title="3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we describe our measurement procedure for capturing valid instances of extraction.
Prior work commonly uses a threshold of <math alttext="50" class="ltx_Math" display="inline" id="A2.p1.1.m1" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> LLM tokens to identify verbatim memorized sequences.
For typical English prose, a useful approximation is that one word corresponds to approximately <math alttext="1.3" class="ltx_Math" display="inline" id="A2.p1.2.m2" intent=":literal"><semantics><mn>1.3</mn><annotation encoding="application/x-tex">1.3</annotation></semantics></math>–<math alttext="1.4" class="ltx_Math" display="inline" id="A2.p1.3.m3" intent=":literal"><semantics><mn>1.4</mn><annotation encoding="application/x-tex">1.4</annotation></semantics></math> LLM tokens.
Under this conversion, <math alttext="50" class="ltx_Math" display="inline" id="A2.p1.4.m4" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> tokens corresponds to roughly <math alttext="35" class="ltx_Math" display="inline" id="A2.p1.5.m5" intent=":literal"><semantics><mn>35</mn><annotation encoding="application/x-tex">35</annotation></semantics></math>–<math alttext="40" class="ltx_Math" display="inline" id="A2.p1.6.m6" intent=":literal"><semantics><mn>40</mn><annotation encoding="application/x-tex">40</annotation></semantics></math> words, while <math alttext="100" class="ltx_Math" display="inline" id="A2.p1.7.m7" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> words corresponds to approximately <math alttext="130" class="ltx_Math" display="inline" id="A2.p1.8.m8" intent=":literal"><semantics><mn>130</mn><annotation encoding="application/x-tex">130</annotation></semantics></math>–<math alttext="140" class="ltx_Math" display="inline" id="A2.p1.9.m9" intent=":literal"><semantics><mn>140</mn><annotation encoding="application/x-tex">140</annotation></semantics></math> tokens.
For long-form extraction, verbatim matching is too stringent&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cooper<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>.
We instead merge closely aligned blocks, but then filter these merged blocks to only retain ones that are sufficiently long to make a valid extraction claim.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在 3.3 节，我们描述了用于捕获提取有效实例的测量程序。先前工作通常使用 <math intent=":literal" id="A2.p1.1.m1" display="inline" class="ltx_Math" alttext="50"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> LLM 令牌的阈值来识别逐字记忆的序列。对于典型的英语散文，一个有用的近似是每个单词对应大约 <math intent=":literal" id="A2.p1.2.m2" display="inline" class="ltx_Math" alttext="1.3"><semantics><mn>1.3</mn><annotation encoding="application/x-tex">1.3</annotation></semantics></math> – <math intent=":literal" id="A2.p1.3.m3" display="inline" class="ltx_Math" alttext="1.4"><semantics><mn>1.4</mn><annotation encoding="application/x-tex">1.4</annotation></semantics></math> LLM 令牌。在这种转换下， <math intent=":literal" id="A2.p1.4.m4" display="inline" class="ltx_Math" alttext="50"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> 令牌对应大约 <math intent=":literal" id="A2.p1.5.m5" display="inline" class="ltx_Math" alttext="35"><semantics><mn>35</mn><annotation encoding="application/x-tex">35</annotation></semantics></math> – <math intent=":literal" id="A2.p1.6.m6" display="inline" class="ltx_Math" alttext="40"><semantics><mn>40</mn><annotation encoding="application/x-tex">40</annotation></semantics></math> 个单词，而 <math intent=":literal" id="A2.p1.7.m7" display="inline" class="ltx_Math" alttext="100"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> 个单词对应大约 <math intent=":literal" id="A2.p1.8.m8" display="inline" class="ltx_Math" alttext="130"><semantics><mn>130</mn><annotation encoding="application/x-tex">130</annotation></semantics></math> – <math intent=":literal" id="A2.p1.9.m9" display="inline" class="ltx_Math" alttext="140"><semantics><mn>140</mn><annotation encoding="application/x-tex">140</annotation></semantics></math> 令牌。对于长文本提取，逐字匹配过于严格 (Cooper 等人，2025)。我们改为合并紧密对齐的块，但随后过滤这些合并的块，仅保留足够长以支持有效提取声明的那部分。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A2.p2">
<p class="ltx_p" id="A2.p2.8">Following&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite>, we first first identifies verbatim blocks, using a block-based greedy approximation of longest common substring.
For this, we use <span class="ltx_text ltx_font_typewriter" id="A2.p2.8.1">difflib SequenceMatcher</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(difflib SequenceMatcher, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib119" title="difflib — Helpers for computing deltas">2025</a>)</cite>, which returns on ordered set of verbatim matching blocks given two input text lists (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E3" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3</span></a>).
We then do two merge-and-filter passes (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E4" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>.)
The first merge is very stringent, combining blocks that have very short gaps within a given input text and are well-aligned across input texts (<math alttext="\tau^{(1)}_{\mathrm{gap}}=2" class="ltx_Math" display="inline" id="A2.p2.1.m1" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{gap}}=2</annotation></semantics></math>, <math alttext="\tau^{(1)}_{\mathrm{align}}=1" class="ltx_Math" display="inline" id="A2.p2.2.m2" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{align}}=1</annotation></semantics></math>).
The first filter with <math alttext="l^{(1)}=20" class="ltx_Math" display="inline" id="A2.p2.3.m3" intent=":literal"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">l^{(1)}=20</annotation></semantics></math> words is fairly stringent, with respect to what we consider a “very short” span of text;
note that this is about half of the length of the <math alttext="35" class="ltx_Math" display="inline" id="A2.p2.4.m4" intent=":literal"><semantics><mn>35</mn><annotation encoding="application/x-tex">35</annotation></semantics></math>–<math alttext="40" class="ltx_Math" display="inline" id="A2.p2.5.m5" intent=":literal"><semantics><mn>40</mn><annotation encoding="application/x-tex">40</annotation></semantics></math> words used for verbatim discoverable extraction.
The second merge is slightly more relaxed, but still stringent (<math alttext="\tau^{(2)}_{\mathrm{gap}}=10" class="ltx_Math" display="inline" id="A2.p2.6.m6" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{gap}}=10</annotation></semantics></math>, <math alttext="\tau^{(2)}_{\mathrm{align}}=3" class="ltx_Math" display="inline" id="A2.p2.7.m7" intent=":literal"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{align}}=3</annotation></semantics></math>)).
To compensate for this relaxation, the second filter is very stringent, with <math alttext="l^{(2)}=100" class="ltx_Math" display="inline" id="A2.p2.8.m8" intent=":literal"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">l^{(2)}=100</annotation></semantics></math> words.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">根据 Cooper 等人（2025）的研究，我们首先使用基于最长公共子串的贪心近似方法识别逐字块。为此，我们使用 difflib SequenceMatcher（difflib SequenceMatcher，2025），它根据两个输入文本列表返回一个有序的逐字匹配块集合（公式 3）。然后我们进行两次合并和过滤过程（公式 4）。第一次合并非常严格，将给定输入文本中具有非常短间隙且在输入文本中良好对齐的块组合起来（ <math intent=":literal" id="A2.p2.1.m1" display="inline" class="ltx_Math" alttext="\tau^{(1)}_{\mathrm{gap}}=2"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{gap}}=2</annotation></semantics></math> ， <math intent=":literal" id="A2.p2.2.m2" display="inline" class="ltx_Math" alttext="\tau^{(1)}_{\mathrm{align}}=1"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau^{(1)}_{\mathrm{align}}=1</annotation></semantics></math> ）。第一次过滤使用 <math intent=":literal" id="A2.p2.3.m3" display="inline" class="ltx_Math" alttext="l^{(1)}=20"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">l^{(1)}=20</annotation></semantics></math> 个词，非常严格，关于我们考虑的“非常短”文本片段；请注意，这大约是用于逐字发现提取的 <math intent=":literal" id="A2.p2.4.m4" display="inline" class="ltx_Math" alttext="35"><semantics><mn>35</mn><annotation encoding="application/x-tex">35</annotation></semantics></math> – <math intent=":literal" id="A2.p2.5.m5" display="inline" class="ltx_Math" alttext="40"><semantics><mn>40</mn><annotation encoding="application/x-tex">40</annotation></semantics></math> 个词长度的一半。第二次合并稍微宽松一些，但仍很严格（ <math intent=":literal" id="A2.p2.6.m6" display="inline" class="ltx_Math" alttext="\tau^{(2)}_{\mathrm{gap}}=10"><semantics><mrow><msubsup><mi>τ</mi><mi>gap</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{gap}}=10</annotation></semantics></math> ， <math intent=":literal" id="A2.p2.7.m7" display="inline" class="ltx_Math" alttext="\tau^{(2)}_{\mathrm{align}}=3"><semantics><mrow><msubsup><mi>τ</mi><mi>align</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\tau^{(2)}_{\mathrm{align}}=3</annotation></semantics></math> ）。为了弥补这种放宽，第二次过滤非常严格，使用 <math intent=":literal" id="A2.p2.8.m8" display="inline" class="ltx_Math" alttext="l^{(2)}=100"><semantics><mrow><msup><mi>l</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">l^{(2)}=100</annotation></semantics></math> 个词。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4" title="Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>, we provide a high-level depiction of our procedure for forming near-verbatim blocks.
In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf1" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(a)</span></a>, we show how benign formatting differences introduce short blocks, and how our procedure ultimately reconciles these differences to produce a longer-form near-verbatim block.
In contrast, Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf2" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(b)</span></a> shows how the identify procedure can return very short blocks that we should not count as extraction, even though they are (coincidental) verbatim matches.
We performed extensive validation experiments on these settings to pick this configuration, discussed further below.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在图 4 中，我们展示了形成近乎逐字块的高级流程图。在图 4(a)中，我们展示了良性格式差异如何引入短块，以及我们的流程如何最终协调这些差异以生成更长的近乎逐字块。相比之下，图 4(b)展示了识别流程如何返回非常短的块，即使它们是（偶然的）逐字匹配，我们也应该不计入提取。我们对这些设置进行了广泛的验证实验，以选择此配置，具体讨论见下文。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Conservative estimate for extraction.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">提取的保守估计。</font></font></font></h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.5">Note that this procedure is conservative in several ways.
If any of the blocks in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf1" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(a)</span></a> had been a bit shorter, the entire text would have failed the second filter.
Further, note that we still only count the <em class="ltx_emph ltx_font_italic" id="A2.SS0.SSS0.Px1.p1.5.1">verbatim</em> length contributions in our near-verbatim blocks.
For example, in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf1" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(a)</span></a>, we do not count the text in the gap text;
the final merged block is the sum of the lengths of the original six blocks only.
This length is <math alttext="141" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mn>141</mn><annotation encoding="application/x-tex">141</annotation></semantics></math> words;
if we were to count the book <math alttext="B" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>’s ground-truth text in the gaps that were reconciled into this near-verbatim block, then the total length would be <math alttext="150" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.3.m3" intent=":literal"><semantics><mn>150</mn><annotation encoding="application/x-tex">150</annotation></semantics></math> words that contribute to our <math alttext="\mathsf{matched}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.4.m4" intent=":literal"><semantics><mi>𝗆𝖺𝗍𝖼𝗁𝖾𝖽</mi><annotation encoding="application/x-tex">\mathsf{matched}</annotation></semantics></math> count <math alttext="m" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.5.m5" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E6" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">6</span></a>).
Either approach would be a reasonable and valid way to operationalize our procedure, but we choose to be conservative and only count verbatim matches.
This deflates our final extraction numbers.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">请注意，此流程在多个方面是保守的。如果图 4(a)中的任何块稍微短一些，整个文本就会在第二次过滤中失败。此外，请注意，我们仍然只计算我们近似逐字块中的逐字长度贡献。例如，在图 4(a)中，我们不计算间隙文本中的文本；最终合并的块仅是原始六个块的长度之和。这个长度是 <math intent=":literal" id="A2.SS0.SSS0.Px1.p1.1.m1" display="inline" class="ltx_Math" alttext="141"><semantics><mn>141</mn><annotation encoding="application/x-tex">141</annotation></semantics></math> 个词；如果我们计算被协调到这个近似逐字块中的书籍 <math intent=":literal" id="A2.SS0.SSS0.Px1.p1.2.m2" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 的真实文本在间隙中的长度，那么总长度将是 <math intent=":literal" id="A2.SS0.SSS0.Px1.p1.3.m3" display="inline" class="ltx_Math" alttext="150"><semantics><mn>150</mn><annotation encoding="application/x-tex">150</annotation></semantics></math> 个词，这些词会贡献到我们的 <math intent=":literal" id="A2.SS0.SSS0.Px1.p1.4.m4" display="inline" class="ltx_Math" alttext="\mathsf{matched}"><semantics><mi>𝗆𝖺𝗍𝖼𝗁𝖾𝖽</mi><annotation encoding="application/x-tex">\mathsf{matched}</annotation></semantics></math> 计数 <math intent=":literal" id="A2.SS0.SSS0.Px1.p1.5.m5" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> （方程 6）。这两种方法都是我们流程合理且有效的方式，但我们选择保持保守，只计算逐字匹配。这降低了我们最终的提取数量。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p2.1">We validated our chosen configuration, experimenting with several different settings for our procedure—different gap, alignment, and filter length tolerances.
We evaluated these settings both quantitatively (e.g., how extraction metrics change, histograms over retained block lengths, computing Levenshtein distance over near-verbatim blocks with generated and ground-truth book text) and qualitatively (e.g., visual inspection of diffs between books and generations).
We found that it would be reasonable to use shorter filter conditions for both filter steps, as well as a larger maximum gap in the second merge, in comparison to the final configuration we report.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们验证了所选配置，通过实验调整了流程中的多种设置——不同的间隙、对齐和过滤器长度容差。我们定量评估了这些设置（例如，提取指标的变化、保留块长度的直方图、在生成文本和真实书籍文本的逐字块上计算 Levenshtein 距离）和定性评估（例如，通过可视化检查书籍和生成文本之间的差异）。我们发现，与最终报告的配置相比，使用较短的过滤器条件进行两个过滤器步骤，以及在第二次合并中使用更大的最大间隙是合理的。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p3.2">To be conservative about our claims, we picked the most stringent configuration that retains effectively verbatim long-form text that has been split into short blocks due to changes in punctuation (as in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf1" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(a)</span></a>).
We also experimented with using the Levenshtein distance as an additional merging criterion in the first filter (i.e., to only merge blocks for which the very short gaps are due to generated text in <math alttext="G" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p3.1.m1" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> that is within a small Levenshtein-distance of the ground-truth text in <math alttext="B" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p3.2.m2" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>).
This check would, for example, consider the short gaps in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf1" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(a)</span></a> to be benign (and fine to merge blocks in the first pass), but would not merge the blocks with short gaps in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf2" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(b)</span></a>.
However, we observed no substantive difference in our measurements when including this check;
in practice, the combination of two merge-and-filter passes removes patchy chains of partial, short matches (e.g., happenstance matches of “the” in the same location in the book and generation).
For simplicity, we omit this check.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">为了保守起见，我们选择了最严格的配置，该配置能够保留因标点符号变化而拆分成短块的、几乎未改动的长文本（如图 4(a)所示）。我们还尝试在第一个过滤器中使用 Levenshtein 距离作为额外的合并标准（即，仅合并那些由于 <math intent=":literal" id="A2.SS0.SSS0.Px1.p3.1.m1" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中生成的文本与 <math intent=":literal" id="A2.SS0.SSS0.Px1.p3.2.m2" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 中的真实文本在 Levenshtein 距离上非常接近而形成的极短间隙的块）。例如，这种检查会将图 4(a)中的短间隙视为良性（并在第一次过滤时允许合并块），但不会合并图 4(b)中具有短间隙的块。然而，我们观察到在包含这种检查时，我们的测量结果没有实质性差异；在实践中，两次合并和过滤的组合可以去除不连续的、部分且短的匹配链（例如，在书中同一位置偶然出现的“the”的匹配）。为简化起见，我们省略了这种检查。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p4">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p4.1">We provide a more detailed depiction of our procedure for the text in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf1" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(a)</span></a> below, which shows each step of the near-verbatim block formation procedure.
This figure illustrates the need for our two-stage merge-and-filter approach, rather than a simple merge and filter, which would excessively drop near-verbatim spans that have benign formatting differences.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们提供了对图 4(a)中文字的更详细描述，该图展示了近乎逐字块形成过程的每一步。该图说明了我们需要两阶段合并和过滤方法，而不是简单的合并和过滤，因为后者会过度丢弃具有良性格式差异的近乎逐字片段。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A2.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="733" id="A2.F10.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/frankenstein_merge_example_claude.png" width="590">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F10.14.6.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text ltx_font_bold" id="A2.F10.10.5" style="font-size:90%;">Illustrating each step of our two-step merge-and-filter procedure.<span class="ltx_text ltx_font_medium" id="A2.F10.10.5.5">
This is a more detailed depiction of Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F4.sf1" title="In Figure 4 ‣ 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4(a)</span></a>, showing both merge-and-filter steps for part of <math alttext="B" class="ltx_Math" display="inline" id="A2.F10.6.1.1.m1" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>=<span class="ltx_text ltx_font_italic" id="A2.F10.10.5.5.1">Frankenstein</span>.
The text shown is part of the corresponding generation <math alttext="G" class="ltx_Math" display="inline" id="A2.F10.7.2.2.m2" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> from Claude 3.7 Sonnet, not the ground-truth book <math alttext="B" class="ltx_Math" display="inline" id="A2.F10.8.3.3.m3" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>.
Verbatim text is identical in both <math alttext="B" class="ltx_Math" display="inline" id="A2.F10.9.4.4.m4" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> and <math alttext="G" class="ltx_Math" display="inline" id="A2.F10.10.5.5.m5" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>, but gap text differs, as these differences are the reason for gaps between blocks.</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 10：说明我们两步合并和过滤过程的每一步。这是图 4(a)的更详细描述，展示了 <math intent=":literal" id="A2.F10.6.1.1.m1" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> =Frankenstein 部分合并和过滤步骤。显示的文字是 Claude 3.7 Sonnet 相应生成 <math intent=":literal" id="A2.F10.7.2.2.m2" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 的一部分，而不是真实书籍 <math intent=":literal" id="A2.F10.8.3.3.m3" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 。逐字文本在 <math intent=":literal" id="A2.F10.9.4.4.m4" display="inline" class="ltx_Math" alttext="B"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> 和 <math intent=":literal" id="A2.F10.10.5.5.m5" display="inline" class="ltx_Math" alttext="G"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> 中相同，但间隙文本不同，因为这些差异是块之间间隙的原因。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Experimental setup<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">附录 C 实验设置</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We provide further details on our results and experimental setup.
We provide additional information on book selection (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS1" title="C.1 Book selection ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.1</span></a>), production-LLM-specific Phase 2 configurations and results (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2" title="C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.2</span></a>), and the light text normalization we perform prior to computing near-verbatim extraction metrics (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS3" title="C.3 Text normalization prior to gauging near-verbatim extraction ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.3</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们提供了对结果和实验设置的进一步细节。我们提供了关于书籍选择（附录 C.1）、特定于生产 LLM 的 Phase 2 配置和结果（附录 C.2），以及我们在计算近乎逐字提取指标之前执行轻微文本规范化（附录 C.3）的更多信息。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Book selection<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">C.1 书籍选择</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">While companies have not disclosed exact training corpora, public statements&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wiggers and Zeff, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib98" title="In AI copyright case, Zuckerberg turns to YouTube for his defense">2025</a>; Brittain, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib100" title="Meta tells court AI software does not violate author copyrights">2023</a>; Claburn, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib101" title="Microsoft CEO of AI: Your online content is ’freeware’ fodder for training models">2024</a>)</cite> and litigation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bartz et al. v. Anthropic PBC, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib66" title="">2025</a>; <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib69" title="">2025</a>; The Authors Guild, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib99" title="The Authors Guild, John Grisham, Jodi Picoult, David Baldacci, George R.R. Martin, and 13 Other Authors File Class-Action Suit Against OpenAI">2023</a>; King, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib102" title="Anthropic CEO Doubles Down on Fair Use Defense–“The Law Will Back Us Up”’">2024</a>)</cite> suggest books are very likely included.
For our extraction experiments (but not our negative control), we draw initial seeds for Phase 1 from books that we suspect were included in the training data (Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.F2" title="Figure 2 ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>, Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
As a proxy, we mostly select books that <cite class="ltx_cite ltx_citemacro_citet">Cooper<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib71" title="Extracting memorized pieces of (copyrighted) books from open-weight language models">2025</a>)</cite> observe to be highly memorized by Llama 3.1 70B.
Following Phase 2, we only make extraction claims (which embed a claim for training-data membership) for long generated blocks of near-verbatim text.
Except in select cases for Claude 3.7 Sonnet, where we extract full books, we do not claim training-data membership for a whole book with our results;
we only claim training-data membership for the text that we extracted.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">虽然公司尚未披露确切的训练语料库，但公开声明（Wiggers 和 Zeff，2025 年；Brittain，2023 年；Claburn，2024 年）以及诉讼（Bartz 等人诉 Anthropic PBC，2025 年；2025 年；作者公会，2023 年；King，2024 年）表明书籍极有可能被包含在内。在我们的提取实验中（但不是我们的负控制实验），我们从我们怀疑被包含在训练数据中的书籍中提取初始种子用于第一阶段（图 2，第 3.1 节）。作为一个替代方案，我们主要选择 Cooper 等人（2025 年）观察到 Llama 3.1 70B 高度记忆的书籍。在第二阶段之后，我们仅对长生成的近乎逐字复制的文本块提出提取声明（该声明隐含了对训练数据成员资格的声明）。除 Claude 3.7 Sonnet 的少数情况外，我们不会用我们的结果声称整本书的训练数据成员资格；我们仅声明我们提取的文本的训练数据成员资格。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Phase 2 generation configurations and stop conditions<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">C.2 第二阶段生成配置和停止条件</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">In this appendix, we document the exact hyperparameters and stopping conditions used during Phase 2 (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS2" title="3.2 Attempting long-form extraction of training data (Phase 2) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.2</span></a>) for each production LLM.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在本附录中，我们记录了在第二阶段（第 3.2 节）期间用于每个生产 LLM 的确切超参数和停止条件。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="A3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.1 </span>Settings for main results<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">C.2.1 主要结果的设置</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS2.SSS1.p1">
<p class="ltx_p" id="A3.SS2.SSS1.p1.1">We start with the settings used in our main results, presented in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
Each production LLM exposes different configurations for generation.
For each production LLM, we ran exploratory experiments to identify conditions under which extraction might work.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们从第 4.2 节中使用的设置开始，这些设置用于我们的主要结果。每个生产 LLM 都提供了不同的生成配置。对于每个生产 LLM，我们进行了探索性实验，以确定在哪些条件下提取可能有效。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.SSS1.p2">
<p class="ltx_p" id="A3.SS2.SSS1.p2.13">For all production LLMs, we fix temperature to <math alttext="0.0" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.1.m1" intent=":literal"><semantics><mn>0.0</mn><annotation encoding="application/x-tex">0.0</annotation></semantics></math> (deterministic generation, but there may be other system non-determinism).
Based on initial experiments, we set the maximum number of returned tokens to be <math alttext="250" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.2.m2" intent=":literal"><semantics><mn>250</mn><annotation encoding="application/x-tex">250</annotation></semantics></math>, <math alttext="500" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.3.m3" intent=":literal"><semantics><mn>500</mn><annotation encoding="application/x-tex">500</annotation></semantics></math>, <math alttext="2000" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.4.m4" intent=":literal"><semantics><mn>2000</mn><annotation encoding="application/x-tex">2000</annotation></semantics></math> and <math alttext="500" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.5.m5" intent=":literal"><semantics><mn>500</mn><annotation encoding="application/x-tex">500</annotation></semantics></math> for Claude 3.7 Sonnet, Grok 3, Gemini 2.5 Pro, and GPT-4.1, respectively.
We set the maximum number of continuation turns to <math alttext="600" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.6.m6" intent=":literal"><semantics><mn>600</mn><annotation encoding="application/x-tex">600</annotation></semantics></math>, <math alttext="200" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.7.m7" intent=":literal"><semantics><mn>200</mn><annotation encoding="application/x-tex">200</annotation></semantics></math>, <math alttext="300" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.8.m8" intent=":literal"><semantics><mn>300</mn><annotation encoding="application/x-tex">300</annotation></semantics></math>, and <math alttext="300" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.9.m9" intent=":literal"><semantics><mn>300</mn><annotation encoding="application/x-tex">300</annotation></semantics></math>, respectively.
We chose <math alttext="300" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.10.m10" intent=":literal"><semantics><mn>300</mn><annotation encoding="application/x-tex">300</annotation></semantics></math> for both Gemini 2.5 Pro and Grok 3 because we observed in initial experiments that both production LLMs would diverge from near-verbatim books before this point. Given the choice to set the maximum returned tokens shorter for Claude 3.7 Sonnet (to avoid filters), we set the number of turns higher.
For experiments with <span class="ltx_text ltx_font_italic" id="A3.SS2.SSS1.p2.13.1">The Hobbit</span>, we ran for longer (<math alttext="1000" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.11.m11" intent=":literal"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math> turns) after observing continued extraction.
For Gemini 2.5 Pro, we also set frequency penalty to <math alttext="2" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.12.m12" intent=":literal"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math> and presence penalty to <math alttext="0.1" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p2.13.m13" intent=":literal"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math>, given initial experiments sweeping over these parameters for <span class="ltx_text ltx_font_italic" id="A3.SS2.SSS1.p2.13.2">Harry Potter and the Sorcerer’s Stone</span> (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS2" title="C.2.2 Generation configuration exploration for Gemini 2.5 Pro ‣ C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.2.2</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">对于所有生产 LLMs，我们将温度固定为 <math intent=":literal" id="A3.SS2.SSS1.p2.1.m1" display="inline" class="ltx_Math" alttext="0.0"><semantics><mn>0.0</mn><annotation encoding="application/x-tex">0.0</annotation></semantics></math> （确定性生成，但可能存在其他系统非确定性）。基于初步实验，我们分别为 Claude 3.7 Sonnet、Grok 3、Gemini 2.5 Pro 和 GPT-4.1 设置返回的最大 token 数量为 <math intent=":literal" id="A3.SS2.SSS1.p2.2.m2" display="inline" class="ltx_Math" alttext="250"><semantics><mn>250</mn><annotation encoding="application/x-tex">250</annotation></semantics></math> 、 <math intent=":literal" id="A3.SS2.SSS1.p2.3.m3" display="inline" class="ltx_Math" alttext="500"><semantics><mn>500</mn><annotation encoding="application/x-tex">500</annotation></semantics></math> 、 <math intent=":literal" id="A3.SS2.SSS1.p2.4.m4" display="inline" class="ltx_Math" alttext="2000"><semantics><mn>2000</mn><annotation encoding="application/x-tex">2000</annotation></semantics></math> 和 <math intent=":literal" id="A3.SS2.SSS1.p2.5.m5" display="inline" class="ltx_Math" alttext="500"><semantics><mn>500</mn><annotation encoding="application/x-tex">500</annotation></semantics></math> 。我们分别为它们设置继续对话的最大轮数为 <math intent=":literal" id="A3.SS2.SSS1.p2.6.m6" display="inline" class="ltx_Math" alttext="600"><semantics><mn>600</mn><annotation encoding="application/x-tex">600</annotation></semantics></math> 、 <math intent=":literal" id="A3.SS2.SSS1.p2.7.m7" display="inline" class="ltx_Math" alttext="200"><semantics><mn>200</mn><annotation encoding="application/x-tex">200</annotation></semantics></math> 、 <math intent=":literal" id="A3.SS2.SSS1.p2.8.m8" display="inline" class="ltx_Math" alttext="300"><semantics><mn>300</mn><annotation encoding="application/x-tex">300</annotation></semantics></math> 和 <math intent=":literal" id="A3.SS2.SSS1.p2.9.m9" display="inline" class="ltx_Math" alttext="300"><semantics><mn>300</mn><annotation encoding="application/x-tex">300</annotation></semantics></math> 。我们为 Gemini 2.5 Pro 和 Grok 3 选择了 <math intent=":literal" id="A3.SS2.SSS1.p2.10.m10" display="inline" class="ltx_Math" alttext="300"><semantics><mn>300</mn><annotation encoding="application/x-tex">300</annotation></semantics></math> ，因为在初步实验中观察到这两个生产 LLMs 在这一点之前都会偏离近乎逐字的书本内容。鉴于选择为 Claude 3.7 Sonnet 设置更短的返回 token 数量（以避免过滤器），我们设置了更高的轮数。对于《霍比特人》的实验，我们在观察到持续提取后，运行了更长时间（ <math intent=":literal" id="A3.SS2.SSS1.p2.11.m11" display="inline" class="ltx_Math" alttext="1000"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math> 轮）。对于 Gemini 2.5 Pro，我们还设置了频率惩罚为 <math intent=":literal" id="A3.SS2.SSS1.p2.12.m12" display="inline" class="ltx_Math" alttext="2"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math> 和存在惩罚为 <math intent=":literal" id="A3.SS2.SSS1.p2.13.m13" display="inline" class="ltx_Math" alttext="0.1"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> ，这是基于在《哈利·波特与魔法石》（附录 C.2.2）的初步实验中扫过这些参数的结果。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A3.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="A3.T2.3.2" style="font-size:90%;">Regex Patterns Used for Refusal Detection</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 2：用于拒绝检测的正则表达式模式</font></font></font></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T2.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A3.T2.4.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.T2.4.1.1.1.1">Regex Pattern<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">正则表达式模式</font></font></font></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T2.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T2.4.2.1.1"><span class="ltx_text ltx_font_typewriter" id="A3.T2.4.2.1.1.1" data-imt_insert_failed="1">\bsorry[, ]*(?:but )?i can[\u0027\u2019]t\b</span></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.3.2">
<td class="ltx_td ltx_align_left" id="A3.T2.4.3.2.1"><span class="ltx_text ltx_font_typewriter" id="A3.T2.4.3.2.1.1">\bcopyright(?:ed)?\b<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">\b 版权(?:所有)?\b</font></font></font></span></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.4.3">
<td class="ltx_td ltx_align_left" id="A3.T2.4.4.3.1"><span class="ltx_text ltx_font_typewriter" id="A3.T2.4.4.3.1.1">\bcan[\u0027\u2019]t provide\b<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">\b 不能提供\b</font></font></font></span></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.5.4">
<td class="ltx_td ltx_align_left" id="A3.T2.4.5.4.1"><span class="ltx_text ltx_font_typewriter" id="A3.T2.4.5.4.1.1">\bunable to\b<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">\b 无法\b</font></font></font></span></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.6.5">
<td class="ltx_td ltx_align_left" id="A3.T2.4.6.5.1"><span class="ltx_text ltx_font_typewriter" id="A3.T2.4.6.5.1.1">\bi cannot\b<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">\bi 不能\b</font></font></font></span></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.7.6">
<td class="ltx_td ltx_align_left" id="A3.T2.4.7.6.1"><span class="ltx_text ltx_font_typewriter" id="A3.T2.4.7.6.1.1" data-imt_insert_failed="1">\bwon[\u0027\u2019]t be able to\b</span></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.8.7">
<td class="ltx_td ltx_align_left" id="A3.T2.4.8.7.1"><span class="ltx_text ltx_font_typewriter" id="A3.T2.4.8.7.1.1" data-imt_insert_failed="1">\bnot allowed to\b</span></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.9.8">
<td class="ltx_td ltx_align_left" id="A3.T2.4.9.8.1"><span class="ltx_text ltx_font_typewriter" id="A3.T2.4.9.8.1.1" data-imt_insert_failed="1">\bcan[\u0027\u2019]t reproduce\b</span></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.10.9">
<td class="ltx_td ltx_align_left" id="A3.T2.4.10.9.1"><span class="ltx_text ltx_font_typewriter" id="A3.T2.4.10.9.1.1">\bcan[\u0027\u2019]t continue\b<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">不能继续</font></font></font></span></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.11.10">
<td class="ltx_td ltx_align_left ltx_border_b" id="A3.T2.4.11.10.1"><span class="ltx_text ltx_font_typewriter" id="A3.T2.4.11.10.1.1">\bcan[\u0027\u2019]t share\b<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">不能分享</font></font></font></span></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="A3.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Halting Phase 2.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">暂停阶段 2。</font></font></font></h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="A3.SS2.SSS1.Px1.p1.1">We deliberately terminate Phase 2 when any of the following conditions is met:
(1) a maximum budget for continuations is reached,
(2) we detect a refusal in the production LLM response,
or (3) we detect a stop string in the production LLM response.
For (2), we use simple regex checks to detect refusals. We enumerate all the regex patterns in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.T2" title="Table 2 ‣ C.2.1 Settings for main results ‣ C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>.
For (3), we check for the following stop strings that indicate the model has reached the end of the book, suggesting that it has stopped generating training data: “we have reached the conclusion of”, “[End of Book]”, “THE END”, “About the Author”, “Afterword”, “Bibliography”.
For Grok 3, the Phase 2 loop sometimes terminated due to an HTTP 500 error.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">当满足以下任一条件时，我们故意终止阶段 2：(1) 达到最大延续预算，(2) 在生产 LLM 响应中检测到拒绝，或(3) 在生产 LLM 响应中检测到停止字符串。对于(2)，我们使用简单的正则表达式检查来检测拒绝。我们在表 2 中列出了所有正则表达式模式。对于(3)，我们检查以下指示模型已到达书籍末尾的停止字符串，表明它已停止生成训练数据：“我们已到达结论”，“[书籍结束]”，“THE END”，“关于作者”，“后记”，“参考文献”。对于 Grok 3，阶段 2 循环有时因 HTTP 500 错误而终止。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.SSS1.Px1.p2">
<p class="ltx_p" id="A3.SS2.SSS1.Px1.p2.1">In initial exploratory experiments, for Claude 3.7 Sonnet we originally implemented stop string detection using the last sentence from the book.
However, from those experiments, we saw that Claude 3.7 Sonnet would generate “THE END” when reaching the end of a book.
After these initial experiments, we switched to these stop strings so as to not rely on ground-truth reference text beyond the prefix in Phase 1.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在初步探索性实验中，针对 Claude 3.7 Sonnet，我们最初使用书籍的最后一句话来实现停字符串检测。然而，通过这些实验我们发现，Claude 3.7 Sonnet 在到达书籍结尾时会生成“THE END”。在完成这些初步实验后，我们转而使用这些停字符串，以便在第一阶段不依赖于前缀之外的 ground-truth 参考文本。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="A3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.2 </span>Generation configuration exploration for Gemini 2.5 Pro<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">C.2.2 为 Gemini 2.5 Pro 探索生成配置</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS2.SSS2.p1">
<p class="ltx_p" id="A3.SS2.SSS2.p1.7">We explored a variety of different settings for Gemini 2.5 Pro’s generation parameters in experiments with <span class="ltx_text ltx_font_italic" id="A3.SS2.SSS2.p1.7.1">Harry Potter and the Sorcerer’s Stone</span>:<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在用《哈利·波特与魔法石》进行的实验中，我们探索了 Gemini 2.5 Pro 生成参数的各种不同设置：</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i1.p1.1.1">Max tokens per interaction</span>: <math alttext="\{1000,2000,4000\}" class="ltx_Math" display="inline" id="A3.I1.i1.p1.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mn>1000</mn><mo>,</mo><mn>2000</mn><mo>,</mo><mn>4000</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{1000,2000,4000\}</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 每次交互的最大令牌数： <math intent=":literal" id="A3.I1.i1.p1.1.m1" display="inline" class="ltx_Math" alttext="\{1000,2000,4000\}"><semantics><mrow><mo stretchy="false">{</mo><mn>1000</mn><mo>,</mo><mn>2000</mn><mo>,</mo><mn>4000</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{1000,2000,4000\}</annotation></semantics></math> </font></font></font>
</li>
<li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i2.p1">
<p class="ltx_p" id="A3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i2.p1.1.1">Frequency penalty</span>: <math alttext="\{0.5,1.0,2.0\}" class="ltx_Math" display="inline" id="A3.I1.i2.p1.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mn>0.5</mn><mo>,</mo><mn>1.0</mn><mo>,</mo><mn>2.0</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0.5,1.0,2.0\}</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 频率惩罚： <math intent=":literal" id="A3.I1.i2.p1.1.m1" display="inline" class="ltx_Math" alttext="\{0.5,1.0,2.0\}"><semantics><mrow><mo stretchy="false">{</mo><mn>0.5</mn><mo>,</mo><mn>1.0</mn><mo>,</mo><mn>2.0</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0.5,1.0,2.0\}</annotation></semantics></math> </font></font></font>
</li>
<li class="ltx_item" id="A3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i3.p1">
<p class="ltx_p" id="A3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i3.p1.1.1">Presence penalty</span>: <math alttext="\{0.1,0.2,0.3\}" class="ltx_Math" display="inline" id="A3.I1.i3.p1.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mn>0.1</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.3</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0.1,0.2,0.3\}</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 存在惩罚： <math intent=":literal" id="A3.I1.i3.p1.1.m1" display="inline" class="ltx_Math" alttext="\{0.1,0.2,0.3\}"><semantics><mrow><mo stretchy="false">{</mo><mn>0.1</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.3</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0.1,0.2,0.3\}</annotation></semantics></math> </font></font></font>
</li>
</ul>
<p class="ltx_p" id="A3.SS2.SSS2.p1.6">After observing that <math alttext="2000" class="ltx_Math" display="inline" id="A3.SS2.SSS2.p1.1.m1" intent=":literal"><semantics><mn>2000</mn><annotation encoding="application/x-tex">2000</annotation></semantics></math> max tokens led to the highest <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="A3.SS2.SSS2.p1.2.m2" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> in all cases, we fixed max tokens to <math alttext="2000" class="ltx_Math" display="inline" id="A3.SS2.SSS2.p1.3.m3" intent=":literal"><semantics><mn>2000</mn><annotation encoding="application/x-tex">2000</annotation></semantics></math> for all subsequent experiments.
In Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we report results for fixed frequency penalty (<math alttext="2.0" class="ltx_Math" display="inline" id="A3.SS2.SSS2.p1.4.m4" intent=":literal"><semantics><mn>2.0</mn><annotation encoding="application/x-tex">2.0</annotation></semantics></math>) and presence penalty (<math alttext="0.1" class="ltx_Math" display="inline" id="A3.SS2.SSS2.p1.5.m5" intent=":literal"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math>).
However, maximum <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="A3.SS2.SSS2.p1.6.m6" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> per book varies by this configuration, which we show in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.F14" title="Figure 14 ‣ D.2.2 Plots and tables ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">14</span></a>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">观察到 <math intent=":literal" id="A3.SS2.SSS2.p1.1.m1" display="inline" class="ltx_Math" alttext="2000"><semantics><mn>2000</mn><annotation encoding="application/x-tex">2000</annotation></semantics></math> 最大 token 数在所有情况下都导致了最高的 <math intent=":literal" id="A3.SS2.SSS2.p1.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> ，我们将最大 token 数固定为 <math intent=":literal" id="A3.SS2.SSS2.p1.3.m3" display="inline" class="ltx_Math" alttext="2000"><semantics><mn>2000</mn><annotation encoding="application/x-tex">2000</annotation></semantics></math> 进行后续所有实验。在 4.2 节，我们报告了固定频率惩罚（ <math intent=":literal" id="A3.SS2.SSS2.p1.4.m4" display="inline" class="ltx_Math" alttext="2.0"><semantics><mn>2.0</mn><annotation encoding="application/x-tex">2.0</annotation></semantics></math> ）和存在惩罚（ <math intent=":literal" id="A3.SS2.SSS2.p1.5.m5" display="inline" class="ltx_Math" alttext="0.1"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> ）的结果。然而，每本书的最大 <math intent=":literal" id="A3.SS2.SSS2.p1.6.m6" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 因该配置而异，我们展示在图 14 中。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="A3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.3 </span>Refusal retries for per-chapter experiments with GPT-4.1<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">C.2.3 GPT-4.1 每章节实验的拒绝重试</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS2.SSS3.p1">
<p class="ltx_p" id="A3.SS2.SSS3.p1.4">In our more intensive per-chapter runs on GPT-4.1, we also attempt to continue in spite of refusals (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS3" title="4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.3</span></a>).
In each iteration in the continue loop, we produce five responses.
We take the first response (in the API returned list) that does not contain a refusal as the response.
If all responses are refusals, then we enter a refusal retry loop where we wait to retry with exponential backoff (up to <math alttext="100" class="ltx_Math" display="inline" id="A3.SS2.SSS3.p1.1.m1" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> times).
We continue the loop for up to <math alttext="50" class="ltx_Math" display="inline" id="A3.SS2.SSS3.p1.2.m2" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> turns (per chapter, in contrast to the maximum of <math alttext="200" class="ltx_Math" display="inline" id="A3.SS2.SSS3.p1.3.m3" intent=":literal"><semantics><mn>200</mn><annotation encoding="application/x-tex">200</annotation></semantics></math> we use in our main experiments starting with a seed from the beginning of the book; see Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS1" title="C.2.1 Settings for main results ‣ C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.2.1</span></a>).
Once a response is classified as a refusal, the loop waits for a fixed delay (two minutes) and then retries the same continuation prompt, up to a maximum number of attempts (<math alttext="50" class="ltx_Math" display="inline" id="A3.SS2.SSS3.p1.4.m4" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math>).
We found refusals to be non-deterministic:
the same instruction prompt would often fail repeatedly and then succeed after a few retries.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在我们的更密集的每章运行中，针对 GPT-4.1，我们也尝试在拒绝的情况下继续进行（第 4.3 节）。在继续循环的每次迭代中，我们生成五个响应。我们取 API 返回列表中的第一个不包含拒绝的响应作为响应。如果所有响应都是拒绝，那么我们将进入拒绝重试循环，在那里我们等待使用指数退避重试（最多 <math intent=":literal" id="A3.SS2.SSS3.p1.1.m1" display="inline" class="ltx_Math" alttext="100"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> 次）。我们继续循环，最多进行 <math intent=":literal" id="A3.SS2.SSS3.p1.2.m2" display="inline" class="ltx_Math" alttext="50"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> 轮（每章，与我们从书中开头开始的主实验中使用的最大 <math intent=":literal" id="A3.SS2.SSS3.p1.3.m3" display="inline" class="ltx_Math" alttext="200"><semantics><mn>200</mn><annotation encoding="application/x-tex">200</annotation></semantics></math> 轮不同；参见附录 C.2.1）。一旦某个响应被分类为拒绝，循环将等待固定延迟（两分钟）然后重试相同的延续提示，最多重试次数为 <math intent=":literal" id="A3.SS2.SSS3.p1.4.m4" display="inline" class="ltx_Math" alttext="50"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math> 。我们发现拒绝是非确定性的：相同的指令提示经常反复失败，然后在几次重试后成功。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="A3.SS2.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">Chat UI<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">聊天界面</font></font></font></h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS2.SSS3.Px1.p1">
<p class="ltx_p" id="A3.SS2.SSS3.Px1.p1.1">We found that our two-phase works using the chat UI, as well, with apparently increased robustness.
In initial exploratory experiments, we ran a prefix from <span class="ltx_text ltx_font_italic" id="A3.SS2.SSS3.Px1.p1.1.1">The Great Gatsby</span> in the ChatGPT web application UI.
Through this approach, we were able to extract the first four chapters of <span class="ltx_text ltx_font_italic" id="A3.SS2.SSS3.Px1.p1.1.2">The Great Gatsby</span>, even though we could not reliably do the same through the API.
This suggests that our reported API numbers may be conservative: the true leakage in end-user deployments may be higher than what we measure here.
In general, UI implementation choices for production LLMs non-trivially affect their behavior&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Nasr<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib48" title="Scalable Extraction of Training Data from (Production) Language Models">2023</a>; Wang<span class="ltx_text ltx_bib_etal"> et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#bib.bib120" title="The inadequacy of offline large language model evaluations: a need to account for personalization in model behavior">2025</a>)</cite>.
We also tested our extraction procedure for Claude 3.7 Sonnet using Anthropic’s chat UI, and observed that it worked.
We do not include results for these UI-based interactions.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们发现使用聊天界面进行的两阶段工作，同样表现出明显增强的鲁棒性。在初步探索性实验中，我们在 ChatGPT 网页应用界面中运行了《了不起的盖茨比》的前缀。通过这种方法，我们成功提取了《了不起的盖茨比》的前四章，尽管我们无法通过 API 可靠地完成同样的操作。这表明我们报告的 API 数据可能过于保守：实际部署中的真实泄露可能比我们在此测量的要高。通常，生产 LLMs 的界面实现选择会非平凡地影响其行为（Nasr 等人，2023；Wang 等人，2025）。我们还使用 Anthropic 的聊天界面测试了针对 Claude 3.7 Sonnet 的提取程序，并观察到它有效。我们不包含这些基于界面的交互结果。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Text normalization prior to gauging near-verbatim extraction<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">C.3 提取前文本归一化</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">When we evaluate extraction success (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3.SSS1" title="3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>), we provide two input documents:
the ground-truth book from Books3, and the generated text.
For this assessment, we operate on lightly normalized versions of both the reference books and generations.
The goal of this procedure is to remove superficial formatting and Unicode differences that would otherwise artificially deflate measured overlap.
For example, Books3 books tend to use underscores to mark italics or stylistic variation in quotation marks, which are often absent in generations.
Since we do not know the format of the training data for these production LLMs (i.e., the format may not align with the format of the book in Books3), we aim to eliminate benign punctuation differences.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">当我们评估提取成功度（第 3.3.1 节）时，我们提供两个输入文档：来自 Books3 的真实书籍文本，以及生成的文本。为此评估，我们处理参考书籍和生成文本的轻度规范化版本。此程序的目的是去除表面格式和 Unicode 差异，这些差异可能会人为地降低测量的重叠度。例如，Books3 中的书籍倾向于使用下划线来标记斜体或引号中的风格变化，而这些在生成文本中往往不存在。由于我们不知道这些生产 LLMs 的训练数据格式（即，格式可能与 Books3 中的书籍格式不一致），我们旨在消除良性的标点符号差异。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS3.p2">
<p class="ltx_p" id="A3.SS3.p2.2">We transform each raw text string <math alttext="t" class="ltx_Math" display="inline" id="A3.SS3.p2.1.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> (either a reference book or a model output) into a normalized string <math alttext="\tilde{t}=\mathsf{Normalize}(t)" class="ltx_Math" display="inline" id="A3.SS3.p2.2.m2" intent=":literal"><semantics><mrow><mover accent="true"><mi>t</mi><mo>~</mo></mover><mo>=</mo><mrow><mi>𝖭𝗈𝗋𝗆𝖺𝗅𝗂𝗓𝖾</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\tilde{t}=\mathsf{Normalize}(t)</annotation></semantics></math> using the following deterministic mapping.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们将每个原始文本字符串 <math intent=":literal" id="A3.SS3.p2.1.m1" display="inline" class="ltx_Math" alttext="t"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> （无论是参考书籍还是模型输出）转换为规范化字符串 <math intent=":literal" id="A3.SS3.p2.2.m2" display="inline" class="ltx_Math" alttext="\tilde{t}=\mathsf{Normalize}(t)"><semantics><mrow><mover accent="true"><mi>t</mi><mo>~</mo></mover><mo>=</mo><mrow><mi>𝖭𝗈𝗋𝗆𝖺𝗅𝗂𝗓𝖾</mi><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\tilde{t}=\mathsf{Normalize}(t)</annotation></semantics></math> ，使用以下确定性映射。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS3.p3">
<ol class="ltx_enumerate" id="A3.I2">
<li class="ltx_item" id="A3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="A3.I2.i1.p1">
<p class="ltx_p" id="A3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i1.p1.1.1">Unicode alignment.</span>
We first apply Unicode compatibility normalization in NFKC form:</p><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">1. Unicode 对齐。我们首先应用 NFKC 形式的 Unicode 兼容性规范化：</font></font></font><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="A3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{t}_{0}=\mathrm{NFKC}(t)." class="ltx_Math" display="block" id="A3.Ex1.m1" intent=":literal"><semantics><mrow><mrow><msub><mover accent="true"><mi>t</mi><mo>~</mo></mover><mn>0</mn></msub><mo>=</mo><mrow><mi>NFKC</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\tilde{t}_{0}=\mathrm{NFKC}(t).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A3.I2.i1.p1.2">This step ensures that visually identical characters are represented identically at the byte level.
This is important because our similarity metrics are computed over whitespace-split word tokens.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">这一步确保视觉上相同的字符在字节级别上表示相同。这很重要，因为我们的相似度度量是在空白分隔的单词标记上计算的。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A3.I2.i2.p1">
<p class="ltx_p" id="A3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i2.p1.1.1">Punctuation remapping.</span>
Next, we apply a fixed character-level remapping <math alttext="\pi" class="ltx_Math" display="inline" id="A3.I2.i2.p1.1.m1" intent=":literal"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> via <span class="ltx_text ltx_font_typewriter" id="A3.I2.i2.p1.1.2">str.translate</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">2. 标点符号重映射。接下来，我们通过 str.translate 应用一个固定的字符级重映射 <math intent=":literal" id="A3.I2.i2.p1.1.m1" display="inline" class="ltx_Math" alttext="\pi"><semantics><mi>π</mi><annotation encoding="application/x-tex">\pi</annotation></semantics></math> 来标准化一小部分标点符号：</font></font></font> to standardize a small set of punctuation marks:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A3.I2.i2.I1">
<li class="ltx_item" id="A3.I2.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A3.I2.i2.I1.i1.p1">
<p class="ltx_p" id="A3.I2.i2.I1.i1.p1.1">left/right and other Unicode quotation variants (e.g., <code class="ltx_verbatim ltx_font_typewriter" id="A3.I2.i2.I1.i1.p1.1.1">“</code>, <code class="ltx_verbatim ltx_font_typewriter" id="A3.I2.i2.I1.i1.p1.1.2">”</code>, <code class="ltx_verbatim ltx_font_typewriter" id="A3.I2.i2.I1.i1.p1.1.3">‘</code>, <code class="ltx_verbatim ltx_font_typewriter" id="A3.I2.i2.I1.i1.p1.1.4">‘</code>)
are mapped to their ASCII counterparts (" or ’);</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 左右引号和其他 Unicode 引号变体（例如， <code id="A3.I2.i2.I1.i1.p1.1.1" class="ltx_verbatim ltx_font_typewriter">“</code> 、 <code id="A3.I2.i2.I1.i1.p1.1.2" class="ltx_verbatim ltx_font_typewriter">”</code> 、 <code id="A3.I2.i2.I1.i1.p1.1.3" class="ltx_verbatim ltx_font_typewriter">‘</code> 、 <code id="A3.I2.i2.I1.i1.p1.1.4" class="ltx_verbatim ltx_font_typewriter">‘</code> ）映射到它们的 ASCII 对应物（"或’）;</font></font></font>
</li>
<li class="ltx_item" id="A3.I2.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A3.I2.i2.I1.i2.p1">
<p class="ltx_p" id="A3.I2.i2.I1.i2.p1.1">dash variants (e.g., en dash and horizontal bar) are mapped to a single em dash code point (<span class="ltx_text ltx_font_typewriter" id="A3.I2.i2.I1.i2.p1.1.1">—</span>);</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 连字符变体（例如，英文连字符和水平线）映射到一个 em 连字符代码点（—）;</font></font></font>
</li>
<li class="ltx_item" id="A3.I2.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A3.I2.i2.I1.i3.p1">
<p class="ltx_p" id="A3.I2.i2.I1.i3.p1.1">the Unicode ellipsis character (which is not visually unique in LaTeX) is mapped to three ASCII dots (…).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.I2.i2.I1.i3.p2">
<p class="ltx_p" id="A3.I2.i2.I1.i3.p2.1">We denote the result of this step <math alttext="\tilde{t}_{1}=\pi(\tilde{t}_{0})" class="ltx_Math" display="inline" id="A3.I2.i2.I1.i3.p2.1.m1" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>t</mi><mo>~</mo></mover><mn>1</mn></msub><mo>=</mo><mrow><mi>π</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>t</mi><mo>~</mo></mover><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\tilde{t}_{1}=\pi(\tilde{t}_{0})</annotation></semantics></math>.
This consolidation prevents purely typographical variation in quotation or dash style from reducing overlap scores.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• Unicode 省略号字符（在 LaTeX 中视觉上并不独特）映射到三个 ASCII 点（…）。我们用 <math intent=":literal" id="A3.I2.i2.I1.i3.p2.1.m1" display="inline" class="ltx_Math" alttext="\tilde{t}_{1}=\pi(\tilde{t}_{0})"><semantics><mrow><msub><mover accent="true"><mi>t</mi><mo>~</mo></mover><mn>1</mn></msub><mo>=</mo><mrow><mi>π</mi><mo rspace="0em" lspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>t</mi><mo>~</mo></mover><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\tilde{t}_{1}=\pi(\tilde{t}_{0})</annotation></semantics></math> 表示这一步的结果。这种整合防止了引号或连字符样式的纯粹排版变化减少重叠分数。</font></font></font>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="A3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="A3.I2.i3.p1">
<p class="ltx_p" id="A3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i3.p1.1.1">Ellipses and dash-like hyphens.</span> We normalize certain common punctuation patterns with regular expressions:</p><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">3. 省略号和类似破折号的连字符。我们使用正则表达式对某些常见的标点符号模式进行规范化：</font></font></font><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A3.I2.i3.I1">
<li class="ltx_item" id="A3.I2.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A3.I2.i3.I1.i1.p1">
<p class="ltx_p" id="A3.I2.i3.I1.i1.p1.1">sequences of spaced dots (e.g., “<span class="ltx_text ltx_font_typewriter" id="A3.I2.i3.I1.i1.p1.1.1">. . .</span>”) are collapsed to a canonical ellipsis <span class="ltx_text ltx_font_typewriter" id="A3.I2.i3.I1.i1.p1.1.2">...</span>;</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 空格分隔的点序列（例如，“...”）被压缩成一个标准的省略号...；</font></font></font>
</li>
<li class="ltx_item" id="A3.I2.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i3.I1.i2.p1">
<p class="ltx_p" id="A3.I2.i3.I1.i2.p1.1">if an ellipsis is immediately followed by an alphanumeric character, we insert a single space after <span class="ltx_text ltx_font_typewriter" id="A3.I2.i3.I1.i2.p1.1.1">...</span> to avoid spurious concatenation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">• 如果省略号紧接在字母数字字符之后，我们在...之后插入一个空格，以避免虚假的连接。</font></font></font>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="A3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="A3.I2.i4.p1">
<p class="ltx_p" id="A3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i4.p1.1.1">Books3 italics markup.</span>
Books3 books often denote italics with underscore delimiters, so that emphasized spans appear as <code class="ltx_verbatim ltx_font_typewriter" id="A3.I2.i4.p1.1.2">_like this_</code> in the raw text.
Because model generations rarely reproduce these delimiters, they can otherwise appear as artificial mismatches.
To account for this, we remove single-underscore emphasis markers using a regex of the form</p><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">4. Books3 斜体标记。Books3 书籍通常用下划线分隔符表示斜体，因此强调的片段在原始文本中显示为 <code id="A3.I2.i4.p1.1.2" class="ltx_verbatim ltx_font_typewriter">_like this_</code> 。由于模型生成很少再现这些分隔符，否则它们可能显示为人为的不匹配。为了解决这个问题，我们使用形如的 regex 移除单个下划线强调标记</font></font></font><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="A3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\verb|_([^_]+)_|\rightarrow\verb|\1|," class="ltx_Math" display="block" id="A3.Ex2.m1" intent=":literal"><semantics><mrow><mrow><mi class="ltx_mathvariant_monospace" mathvariant="monospace">_([ˆ_]+)_</mi><mo stretchy="false">→</mo><mi class="ltx_mathvariant_monospace" mathvariant="monospace">\1</mi></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\verb|_([^_]+)_|\rightarrow\verb|\1|,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A3.I2.i4.p1.2">which strips the outer underscores while preserving the interior text verbatim.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">这会去除外部的下划线，同时保留内部的文本原样。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="A3.I2.i5.p1">
<p class="ltx_p" id="A3.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i5.p1.1.1">Lowercasing.</span>
Finally, because we observe irregular casing in some generated outputs, we convert the entire string to lowercase,</p><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">5. 小写化。最后，因为我们观察到某些生成输出中存在不规则的格式大小写，我们将整个字符串转换为小写，</font></font></font><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="A3.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{t}=\mathrm{lower}(\tilde{t}_{1})," class="ltx_Math" display="block" id="A3.Ex3.m1" intent=":literal"><semantics><mrow><mrow><mover accent="true"><mi>t</mi><mo>~</mo></mover><mo>=</mo><mrow><mi>lower</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>t</mi><mo>~</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\tilde{t}=\mathrm{lower}(\tilde{t}_{1}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A3.I2.i5.p1.2">so that case differences do not affect similarity measurements.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">以便格式大小写的差异不会影响相似度测量。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS3.p4">
<p class="ltx_p" id="A3.SS3.p4.1">After normalization, we tokenize both <math alttext="\tilde{t}" class="ltx_Math" display="inline" id="A3.SS3.p4.1.m1" intent=":literal"><semantics><mover accent="true"><mi>t</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{t}</annotation></semantics></math> and the corresponding normalized reference using Python’s default whitespace splitting (<span class="ltx_text ltx_font_typewriter" id="A3.SS3.p4.1.1">str.split()</span>), exactly as described in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS3" title="3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.3</span></a>, and pass the resulting word sequences to <span class="ltx_text ltx_font_typewriter" id="A3.SS3.p4.1.2">difflib SequenceMatcher</span>.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">经过标准化后，我们使用 Python 的默认空白字符分割（str.split）对 <math intent=":literal" id="A3.SS3.p4.1.m1" display="inline" class="ltx_Math" alttext="\tilde{t}"><semantics><mover accent="true"><mi>t</mi><mo>~</mo></mover><annotation encoding="application/x-tex">\tilde{t}</annotation></semantics></math> 及其对应的标准化参考文本进行分词，完全按照第 3.3 节所述的方式，并将生成的词序列传递给 difflib SequenceMatcher。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS3.p5">
<p class="ltx_p" id="A3.SS3.p5.1">We intentionally keep this normalization minimal.
We <em class="ltx_emph ltx_font_italic" id="A3.SS3.p5.1.1">do not</em> perform stemming or lemmatization, do not remove stopwords, do not strip punctuation beyond the specific remappings above, and do not collapse all non-ASCII characters to ASCII.
Aside from the whitespace effects implied by the regex substitutions, we do not otherwise modify spacing or line breaks.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们故意保持这种规范化最小化。我们不执行词干提取或词元化，不删除停用词，不删除上述特定重映射之外的标点符号，并且不将所有非 ASCII 字符折叠为 ASCII。除了 regex 替换所隐含的空白效果外，我们不会修改间距或换行。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Extended results<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">附录 D 扩展结果</font></font></font></h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">In this appendix, we include more detailed results for experiments presented in the main paper, as well as additional experiments, for both Phase 1 (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS1" title="D.1 Additional Phase 1 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D.1</span></a>) and Phase 2 (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2" title="D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D.2</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">在本附录中，我们包含了主论文中展示的实验的更详细结果，以及额外的实验，涵盖阶段 1（附录 D.1）和阶段 2（附录 D.2）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Additional Phase 1 results<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">D.1 阶段 1 的额外结果</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.4">We include a brief illustration for Phase 1 (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.SS1" title="3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and Claude 3.7 Sonnet for several books in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.F11" title="Figure 11 ‣ D.1 Additional Phase 1 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">11</span></a>.
Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.T3" title="Table 3 ‣ D.1 Additional Phase 1 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3</span></a> shows a summary of full BoN results across books for both Claude 3.7 Sonnet and GPT-4.1.
The number of attempts <math alttext="N" class="ltx_Math" display="inline" id="A4.SS1.p1.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> can vary the cost of Phase 1, but overall it is very cheap for <math alttext="N\leq$10,000$" class="ltx_Math" display="inline" id="A4.SS1.p1.2.m2" intent=":literal"><semantics><mrow><mi>N</mi><mo>≤</mo><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N\leq$10,000$</annotation></semantics></math>.
Since we do not jailbreak Gemini 2.5 Pro or Grok 3, we omit results for these production LLMs (<math alttext="N=0" class="ltx_Math" display="inline" id="A4.SS1.p1.3.m3" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">N=0</annotation></semantics></math>).
We include detailed results on the success of BoN in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.T3" title="Table 3 ‣ D.1 Additional Phase 1 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">3</span></a>.
Note that we do not always achieve maximum possible <math alttext="s=1.0" class="ltx_Math" display="inline" id="A4.SS1.p1.4.m4" intent=":literal"><semantics><mrow><mi>s</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">s=1.0</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E2" title="In 3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们为第一阶段（第 3.1 节）包含一个简要说明，并在图 11 中展示了 Claude 3.7 Sonnet 对几本书的处理。表 3 展示了 Claude 3.7 Sonnet 和 GPT-4.1 在所有书籍上的完整 BoN 结果总结。尝试次数 <math intent=":literal" id="A4.SS1.p1.1.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 可以改变第一阶段的成本，但总体上对于 <math intent=":literal" id="A4.SS1.p1.2.m2" display="inline" class="ltx_Math" alttext="N\leq$10,000$"><semantics><mrow><mi>N</mi><mo>≤</mo><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N\leq$10,000$</annotation></semantics></math> 来说非常便宜。由于我们没有越狱 Gemini 2.5 Pro 或 Grok 3，因此省略了这些生产 LLMs（ <math intent=":literal" id="A4.SS1.p1.3.m3" display="inline" class="ltx_Math" alttext="N=0"><semantics><mrow><mi>N</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">N=0</annotation></semantics></math> ）的结果。我们在表 3 中包含了关于 BoN 成功率的详细结果。请注意，我们并不总是能达到最大可能的 <math intent=":literal" id="A4.SS1.p1.4.m4" display="inline" class="ltx_Math" alttext="s=1.0"><semantics><mrow><mi>s</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">s=1.0</annotation></semantics></math> （公式 2）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="203" id="A4.F11.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/bon_claude_comparison_log.png" width="359">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F11.22.6.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text ltx_font_bold" id="A4.F11.10.5" style="font-size:90%;">Comparing <math alttext="N" class="ltx_Math" display="inline" id="A4.F11.6.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> for Phase 1 for Claude 3.7 Sonnet.<span class="ltx_text ltx_font_medium" id="A4.F11.10.5.4">
As an illustration, we show how <math alttext="s" class="ltx_Math" display="inline" id="A4.F11.7.2.1.m1" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E2" title="In 3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>) changes over <math alttext="N" class="ltx_Math" display="inline" id="A4.F11.8.3.2.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> for Claude 3.7 Sonnet, for four books we attempt to extract (<span class="ltx_text ltx_font_italic" id="A4.F11.10.5.4.1">Harry Potter and the Sorcerer’s Stone</span>, <span class="ltx_text ltx_font_italic" id="A4.F11.10.5.4.2">The Duchess War</span>, <span class="ltx_text ltx_font_italic" id="A4.F11.10.5.4.3">The Great Gatsby</span>, <span class="ltx_text ltx_font_italic" id="A4.F11.10.5.4.4">Frankenstein</span>) and the negative control (<span class="ltx_text ltx_font_italic" id="A4.F11.10.5.4.5">The Society of Unknowable Objects</span>).
Phase 1 success occurs when <math alttext="s\geq 0.6" class="ltx_Math" display="inline" id="A4.F11.9.4.3.m3" intent=":literal"><semantics><mrow><mi>s</mi><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s\geq 0.6</annotation></semantics></math>.
Phase 1 succeeds for <span class="ltx_text ltx_font_italic" id="A4.F11.10.5.4.6">Harry Potter and the Sorcerer’s Stone</span>, <span class="ltx_text ltx_font_italic" id="A4.F11.10.5.4.7">The Great Gatsby</span>, and <span class="ltx_text ltx_font_italic" id="A4.F11.10.5.4.8">Frankenstein</span>—three books for which ultimately <math alttext="\mathsf{nv{\text{-}}recall}\geq 94\%" class="ltx_Math" display="inline" id="A4.F11.10.5.4.m4" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>≥</mo><mrow><mn>94</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}\geq 94\%</annotation></semantics></math> (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>).
Phase 1 fails for <span class="ltx_text ltx_font_italic" id="A4.F11.10.5.4.9">The Duchess War</span>, so we do not run Phase 2.
Phase 1 also fails for the negative control (<span class="ltx_text ltx_font_italic" id="A4.F11.10.5.4.10">The Society of Unknowable Objects</span>, published long after the knowledge cutoffs for all four production LLMs).</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 11：比较阶段 1 的 <math intent=":literal" id="A4.F11.6.1.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> ，针对 Claude 3.7 Sonnet。作为示例，我们展示了 <math intent=":literal" id="A4.F11.7.2.1.m1" display="inline" class="ltx_Math" alttext="s"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> （方程式 2）如何随 <math intent=":literal" id="A4.F11.8.3.2.m2" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 变化，针对我们尝试提取的四本书（哈利·波特与魔法石、公爵夫人战争、了不起的盖茨比、弗兰肯斯坦）以及负控制组（不可知对象协会）。阶段 1 成功发生在 <math intent=":literal" id="A4.F11.9.4.3.m3" display="inline" class="ltx_Math" alttext="s\geq 0.6"><semantics><mrow><mi>s</mi><mo>≥</mo><mn>0.6</mn></mrow><annotation encoding="application/x-tex">s\geq 0.6</annotation></semantics></math> 时。阶段 1 成功应用于哈利·波特与魔法石、了不起的盖茨比和弗兰肯斯坦——这三本书最终 <math intent=":literal" id="A4.F11.10.5.4.m4" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}\geq 94\%"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>≥</mo><mrow><mn>94</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}\geq 94\%</annotation></semantics></math> （图 5）。阶段 1 失败于公爵夫人战争，因此我们没有运行阶段 2。阶段 1 也失败于负控制组（不可知对象协会，该书的出版时间远晚于四个生产 LLMs 的知识截止日期）。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A4.T3.3.3.4">
<span class="ltx_ERROR undefined" id="A4.T3.3.3.4.1">\rowcolor</span>white
<span class="ltx_text ltx_font_bold" id="A4.T3.3.3.4.2">Book</span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">\rowcolorwhite 书籍</font></font></font>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A4.T3.3.3.5"><span class="ltx_text ltx_font_bold" id="A4.T3.3.3.5.1">Production LLM<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">生产 LLM</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A4.T3.1.1.1"><span class="ltx_text ltx_font_bold" id="A4.T3.1.1.1.1">Max. <math alttext="s" class="ltx_Math" display="inline" id="A4.T3.1.1.1.1.m1" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">最大 <math intent=":literal" id="A4.T3.1.1.1.1.m1" display="inline" class="ltx_Math" alttext="s"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> </font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A4.T3.3.3.3">
<math alttext="N" class="ltx_Math" display="inline" id="A4.T3.2.2.2.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.T3.3.3.3.1"> for max. <math alttext="s" class="ltx_Math" display="inline" id="A4.T3.3.3.3.1.m1" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1"> <math intent=":literal" id="A4.T3.2.2.2.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 最大 <math intent=":literal" id="A4.T3.3.3.3.1.m1" display="inline" class="ltx_Math" alttext="s"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> </font></font></font>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T3.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T3.3.4.1.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.4.1.1.1">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T3.3.4.1.2" data-imt_insert_failed="1">Claude 3.7 Sonnet</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T3.3.4.1.3">1.000000</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T3.3.4.1.4">258</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.5.2">
<td class="ltx_td ltx_align_left" id="A4.T3.3.5.2.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.5.2.1.1">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.5.2.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.5.2.3">0.914474</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.5.2.4">5179</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.6.3">
<td class="ltx_td ltx_align_left" id="A4.T3.3.6.3.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.6.3.1.1">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.6.3.2" data-imt_insert_failed="1">Claude 3.7 Sonnet</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.6.3.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.6.3.4">6</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.7.4">
<td class="ltx_td ltx_align_left" id="A4.T3.3.7.4.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.7.4.1.1">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.7.4.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.7.4.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.7.4.4">1405</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.8.5">
<td class="ltx_td ltx_align_left" id="A4.T3.3.8.5.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.8.5.1.1">1984</span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.8.5.2" data-imt_insert_failed="1">Claude 3.7 Sonnet</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.8.5.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.8.5.4">6</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.9.6">
<td class="ltx_td ltx_align_left" id="A4.T3.3.9.6.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.9.6.1.1">1984</span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.9.6.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.9.6.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.9.6.4">183</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.10.7">
<td class="ltx_td ltx_align_left" id="A4.T3.3.10.7.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.10.7.1.1">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《霍比特人》</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.10.7.2" data-imt_insert_failed="1">Claude 3.7 Sonnet</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.10.7.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.10.7.4">23</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.11.8">
<td class="ltx_td ltx_align_left" id="A4.T3.3.11.8.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.11.8.1.1">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《霍比特人》</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.11.8.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.11.8.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.11.8.4">24</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.12.9">
<td class="ltx_td ltx_align_left" id="A4.T3.3.12.9.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.12.9.1.1">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.12.9.2" data-imt_insert_failed="1">Claude 3.7 Sonnet</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.12.9.3">0.608392</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.12.9.4">6</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.13.10">
<td class="ltx_td ltx_align_left" id="A4.T3.3.13.10.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.13.10.1.1">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.13.10.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.13.10.3">0.608392</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.13.10.4">213</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.14.11">
<td class="ltx_td ltx_align_left" id="A4.T3.3.14.11.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.14.11.1.1">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《冰与火之歌》</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.14.11.2" data-imt_insert_failed="1">Claude 3.7 Sonnet</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.14.11.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.14.11.4">6</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.15.12">
<td class="ltx_td ltx_align_left" id="A4.T3.3.15.12.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.15.12.1.1">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.15.12.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.15.12.3">0.967532</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.15.12.4">7842</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.16.13">
<td class="ltx_td ltx_align_left" id="A4.T3.3.16.13.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.16.13.1.1">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">挚爱</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.16.13.2">Claude 3.7 Sonnet<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">克劳德 3.7 十四行诗</font></font></font></td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.16.13.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.16.13.4">6</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.17.14">
<td class="ltx_td ltx_align_left" id="A4.T3.3.17.14.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.17.14.1.1">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">挚爱</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.17.14.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.17.14.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.17.14.4">42</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.18.15">
<td class="ltx_td ltx_align_left" id="A4.T3.3.18.15.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.18.15.1.1">The Da Vinci Code<font data-immersive-translate-error-id="281" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.18.15.2">Claude 3.7 Sonnet<font data-immersive-translate-error-id="282" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.18.15.3">0.653333</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.18.15.4">2143</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.19.16">
<td class="ltx_td ltx_align_left" id="A4.T3.3.19.16.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.19.16.1.1">The Da Vinci Code<font data-immersive-translate-error-id="283" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.19.16.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.19.16.3"><span class="ltx_text" id="A4.T3.3.19.16.3.1" style="--ltx-fg-color:#FF0000;">0.280000</span></td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.19.16.4">3497</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.20.17">
<td class="ltx_td ltx_align_left" id="A4.T3.3.20.17.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.20.17.1.1">The Hunger Games<font data-immersive-translate-error-id="284" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.20.17.2" data-imt_insert_failed="1">Claude 3.7 Sonnet</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.20.17.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.20.17.4">23</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.21.18">
<td class="ltx_td ltx_align_left" id="A4.T3.3.21.18.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.21.18.1.1">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.21.18.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.21.18.3">0.883562</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.21.18.4">9949</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.22.19">
<td class="ltx_td ltx_align_left" id="A4.T3.3.22.19.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.22.19.1.1">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">第二十二条军规</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.22.19.2" data-imt_insert_failed="1">Claude 3.7 Sonnet</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.22.19.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.22.19.4">23</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.23.20">
<td class="ltx_td ltx_align_left" id="A4.T3.3.23.20.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.23.20.1.1">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">悖论</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.23.20.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.23.20.3"><span class="ltx_text" id="A4.T3.3.23.20.3.1" style="--ltx-fg-color:#FF0000;">0.532895</span></td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.23.20.4">2196</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.24.21">
<td class="ltx_td ltx_align_left" id="A4.T3.3.24.21.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.24.21.1.1">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">弗兰肯斯坦</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.24.21.2" data-imt_insert_failed="1">Claude 3.7 Sonnet</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.24.21.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.24.21.4">43</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.25.22">
<td class="ltx_td ltx_align_left" id="A4.T3.3.25.22.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.25.22.1.1">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">弗兰肯斯坦</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.25.22.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.25.22.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.25.22.4">24</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.26.23">
<td class="ltx_td ltx_align_left" id="A4.T3.3.26.23.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.26.23.1.1">The Great Gatsby<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">了不起的盖茨比</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T3.3.26.23.2" data-imt_insert_failed="1">Claude 3.7 Sonnet</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.26.23.3">1.000000</td>
<td class="ltx_td ltx_align_right" id="A4.T3.3.26.23.4">6</td>
</tr>
<tr class="ltx_tr" id="A4.T3.3.27.24">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T3.3.27.24.1"><span class="ltx_text ltx_font_italic" id="A4.T3.3.27.24.1.1">The Great Gatsby<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《了不起的盖茨比》</font></font></font></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T3.3.27.24.2">GPT-4.1</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A4.T3.3.27.24.3">1.000000</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A4.T3.3.27.24.4">5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A4.T3.17.7.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" id="A4.T3.15.6" style="font-size:90%;">Comparing <math alttext="N" class="ltx_Math" display="inline" id="A4.T3.10.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> across Phase 1 jailbreaks.<span class="ltx_text ltx_font_medium" id="A4.T3.15.6.5">
For the two production LLMs that we jailbreak (Claude 3.7 Sonnet and GPT-4.1), we show the maximum <math alttext="s" class="ltx_Math" display="inline" id="A4.T3.11.2.1.m1" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E2" title="In 3.1 Attempting initial completion of a short ground-truth prefix (Phase 1) ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">2</span></a>) achieved.
We only include results for the twelve books where at least one production LLM had a Phase 1 success, so note that Phase 1 failed for GPT-4.1 for two books (marked in red).
We also show the <math alttext="N" class="ltx_Math" display="inline" id="A4.T3.12.3.2.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> needed to obtain the maximum <math alttext="s" class="ltx_Math" display="inline" id="A4.T3.13.4.3.m3" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> that we observed.
For all runs, the maximum <math alttext="N" class="ltx_Math" display="inline" id="A4.T3.14.5.4.m4" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> budget is <math alttext="10,000" class="ltx_markedasmath" display="inline" id="A4.T3.15.6.5.m5.1.1.m1" intent=":literal"><semantics><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">10,000</annotation></semantics></math>.</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 3：比较 <math intent=":literal" id="A4.T3.10.1.m1" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 在第一阶段越狱中的表现。对于我们越狱的两个生产 LLM（Claude 3.7 Sonnet 和 GPT-4.1），我们展示了达到的最大 <math intent=":literal" id="A4.T3.11.2.1.m1" display="inline" class="ltx_Math" alttext="s"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> （公式 2）。我们只包括至少有一个生产 LLM 在第一阶段成功的十二本书的结果，因此请注意 GPT-4.1 在两本书中第一阶段失败（标红）。我们还展示了获得我们观察到的最大 <math intent=":literal" id="A4.T3.13.4.3.m3" display="inline" class="ltx_Math" alttext="s"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> 所需的 <math intent=":literal" id="A4.T3.12.3.2.m2" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 。对于所有运行，最大 <math intent=":literal" id="A4.T3.14.5.4.m4" display="inline" class="ltx_Math" alttext="N"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> 预算是 <math intent=":literal" id="A4.T3.15.6.5.m5.1.1.m1" display="inline" class="ltx_markedasmath" alttext="10,000"><semantics><mrow><mn>10</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">10,000</annotation></semantics></math> 。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Additional Phase 2 results<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">D.2 额外的第二阶段结果</font></font></font></h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A4.SS2.p1">
<p class="ltx_p" id="A4.SS2.p1.1">We show API costs for Phase 2 (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2.SSS1" title="D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D.2.1</span></a>), and additional plots and tables (Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2.SSS2" title="D.2.2 Plots and tables ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D.2.2</span></a>).<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们展示了第二阶段的 API 成本（附录 D.2.1），以及额外的图表和表格（附录 D.2.2）。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="A4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.2.1 </span>Continuation loop API costs<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">D.2.1 延续循环 API 成本</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A4.SS2.SSS1.p1">
<p class="ltx_p" id="A4.SS2.SSS1.p1.1">We include a table with the count of all continuation queries in Phase 2.
When Gemini 2.5 Pro returns an empty response, we count this against the max query budget, but do not mark it as a successful continue query.
The Grok 3 sometimes returned an HTTP 500 error, which prematurely terminated the loop.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们包含一个表格，显示第二阶段所有延续查询的数量。当 Gemini 2.5 Pro 返回空响应时，我们会将其计入最大查询预算，但不会将其标记为成功的延续查询。Grok 3 有时会返回 HTTP 500 错误，这会提前终止循环。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T4.45">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T4.45.46.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A4.T4.45.46.1.1"><span class="ltx_text ltx_font_bold" id="A4.T4.45.46.1.1.1" style="font-size:90%;">Book<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">书</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T4.45.46.1.2"><span class="ltx_text ltx_font_bold" id="A4.T4.45.46.1.2.1" style="font-size:90%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T4.45.46.1.3"><span class="ltx_text ltx_font_bold" id="A4.T4.45.46.1.3.1" style="font-size:90%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T4.45.46.1.4"><span class="ltx_text ltx_font_bold" id="A4.T4.45.46.1.4.1" style="font-size:90%;">GPT-4.1</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T4.45.46.1.5"><span class="ltx_text ltx_font_bold" id="A4.T4.45.46.1.5.1" style="font-size:90%;" data-imt_insert_failed="1">Grok 3</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T4.4.4.5"><span class="ltx_text ltx_font_italic" id="A4.T4.4.4.5.1" style="font-size:90%;">1984</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T4.1.1.1"><math alttext="538" class="ltx_Math" display="inline" id="A4.T4.1.1.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">538</mn><annotation encoding="application/x-tex">538</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T4.2.2.2"><math alttext="300" class="ltx_Math" display="inline" id="A4.T4.2.2.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">300</mn><annotation encoding="application/x-tex">300</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T4.3.3.3"><math alttext="61" class="ltx_Math" display="inline" id="A4.T4.3.3.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">61</mn><annotation encoding="application/x-tex">61</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T4.4.4.4"><math alttext="23" class="ltx_Math" display="inline" id="A4.T4.4.4.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">23</mn><annotation encoding="application/x-tex">23</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T4.8.8.5"><span class="ltx_text ltx_font_italic" id="A4.T4.8.8.5.1" style="font-size:90%;">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《宠儿》</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T4.5.5.1"><math alttext="600" class="ltx_Math" display="inline" id="A4.T4.5.5.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">600</mn><annotation encoding="application/x-tex">600</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.6.6.2"><math alttext="81" class="ltx_Math" display="inline" id="A4.T4.6.6.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">81</mn><annotation encoding="application/x-tex">81</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.7.7.3"><math alttext="3" class="ltx_Math" display="inline" id="A4.T4.7.7.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">3</mn><annotation encoding="application/x-tex">3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.8.8.4"><math alttext="66" class="ltx_Math" display="inline" id="A4.T4.8.8.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">66</mn><annotation encoding="application/x-tex">66</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T4.11.11.4"><span class="ltx_text ltx_font_italic" id="A4.T4.11.11.4.1" style="font-size:90%;">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">悖论</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T4.9.9.1"><math alttext="419" class="ltx_Math" display="inline" id="A4.T4.9.9.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">419</mn><annotation encoding="application/x-tex">419</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.10.10.2"><math alttext="286" class="ltx_Math" display="inline" id="A4.T4.10.10.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">286</mn><annotation encoding="application/x-tex">286</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.11.11.5"><span class="ltx_text" id="A4.T4.11.11.5.1" style="font-size:90%;" data-imt_insert_failed="1">–</span></td>
<td class="ltx_td ltx_align_right" id="A4.T4.11.11.3"><math alttext="125" class="ltx_Math" display="inline" id="A4.T4.11.11.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">125</mn><annotation encoding="application/x-tex">125</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T4.15.15.5"><span class="ltx_text ltx_font_italic" id="A4.T4.15.15.5.1" style="font-size:90%;">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T4.12.12.1"><math alttext="148" class="ltx_Math" display="inline" id="A4.T4.12.12.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">148</mn><annotation encoding="application/x-tex">148</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.13.13.2"><math alttext="173" class="ltx_Math" display="inline" id="A4.T4.13.13.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">173</mn><annotation encoding="application/x-tex">173</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.14.14.3"><math alttext="37" class="ltx_Math" display="inline" id="A4.T4.14.14.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">37</mn><annotation encoding="application/x-tex">37</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.15.15.4"><math alttext="245" class="ltx_Math" display="inline" id="A4.T4.15.15.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">245</mn><annotation encoding="application/x-tex">245</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T4.18.18.4"><span class="ltx_text ltx_font_italic" id="A4.T4.18.18.4.1" style="font-size:90%;">The Da Vinci Code<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">达芬奇密码</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T4.16.16.1"><math alttext="532" class="ltx_Math" display="inline" id="A4.T4.16.16.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">532</mn><annotation encoding="application/x-tex">532</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.17.17.2"><math alttext="223" class="ltx_Math" display="inline" id="A4.T4.17.17.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">223</mn><annotation encoding="application/x-tex">223</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.18.18.5"><span class="ltx_text" id="A4.T4.18.18.5.1" style="font-size:90%;" data-imt_insert_failed="1">–</span></td>
<td class="ltx_td ltx_align_right" id="A4.T4.18.18.3"><math alttext="66" class="ltx_Math" display="inline" id="A4.T4.18.18.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">66</mn><annotation encoding="application/x-tex">66</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.22.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T4.22.22.5"><span class="ltx_text ltx_font_italic" id="A4.T4.22.22.5.1" style="font-size:90%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">弗兰肯斯坦</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T4.19.19.1"><math alttext="374" class="ltx_Math" display="inline" id="A4.T4.19.19.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">374</mn><annotation encoding="application/x-tex">374</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.20.20.2"><math alttext="204" class="ltx_Math" display="inline" id="A4.T4.20.20.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">204</mn><annotation encoding="application/x-tex">204</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.21.21.3"><math alttext="33" class="ltx_Math" display="inline" id="A4.T4.21.21.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">33</mn><annotation encoding="application/x-tex">33</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.22.22.4"><math alttext="300" class="ltx_Math" display="inline" id="A4.T4.22.22.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">300</mn><annotation encoding="application/x-tex">300</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.26.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T4.26.26.5"><span class="ltx_text ltx_font_italic" id="A4.T4.26.26.5.1" style="font-size:90%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T4.23.23.1"><math alttext="562" class="ltx_Math" display="inline" id="A4.T4.23.23.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">562</mn><annotation encoding="application/x-tex">562</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.24.24.2"><math alttext="166" class="ltx_Math" display="inline" id="A4.T4.24.24.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">166</mn><annotation encoding="application/x-tex">166</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.25.25.3"><math alttext="15" class="ltx_Math" display="inline" id="A4.T4.25.25.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">15</mn><annotation encoding="application/x-tex">15</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.26.26.4"><math alttext="195" class="ltx_Math" display="inline" id="A4.T4.26.26.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">195</mn><annotation encoding="application/x-tex">195</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.29.29">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T4.29.29.4"><span class="ltx_text ltx_font_italic" id="A4.T4.29.29.4.1" style="font-size:90%;">The Great Gatsby<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《了不起的盖茨比》</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T4.27.27.1"><math alttext="317" class="ltx_Math" display="inline" id="A4.T4.27.27.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">317</mn><annotation encoding="application/x-tex">317</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.28.28.2"><math alttext="218" class="ltx_Math" display="inline" id="A4.T4.28.28.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">218</mn><annotation encoding="application/x-tex">218</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.29.29.5"><span class="ltx_text" id="A4.T4.29.29.5.1" style="font-size:90%;" data-imt_insert_failed="1">–</span></td>
<td class="ltx_td ltx_align_right" id="A4.T4.29.29.3"><math alttext="182" class="ltx_Math" display="inline" id="A4.T4.29.29.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">182</mn><annotation encoding="application/x-tex">182</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.33.33">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T4.33.33.5"><span class="ltx_text ltx_font_italic" id="A4.T4.33.33.5.1" style="font-size:90%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T4.30.30.1"><math alttext="480" class="ltx_Math" display="inline" id="A4.T4.30.30.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">480</mn><annotation encoding="application/x-tex">480</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.31.31.2"><math alttext="171" class="ltx_Math" display="inline" id="A4.T4.31.31.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">171</mn><annotation encoding="application/x-tex">171</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.32.32.3"><math alttext="31" class="ltx_Math" display="inline" id="A4.T4.32.32.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">31</mn><annotation encoding="application/x-tex">31</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.33.33.4"><math alttext="52" class="ltx_Math" display="inline" id="A4.T4.33.33.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">52</mn><annotation encoding="application/x-tex">52</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.37.37">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T4.37.37.5"><span class="ltx_text ltx_font_italic" id="A4.T4.37.37.5.1" style="font-size:90%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T4.34.34.1"><math alttext="600" class="ltx_Math" display="inline" id="A4.T4.34.34.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">600</mn><annotation encoding="application/x-tex">600</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.35.35.2"><math alttext="264" class="ltx_Math" display="inline" id="A4.T4.35.35.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">264</mn><annotation encoding="application/x-tex">264</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.36.36.3"><math alttext="1" class="ltx_Math" display="inline" id="A4.T4.36.36.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.37.37.4"><math alttext="300" class="ltx_Math" display="inline" id="A4.T4.37.37.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">300</mn><annotation encoding="application/x-tex">300</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.41.41">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T4.41.41.5"><span class="ltx_text ltx_font_italic" id="A4.T4.41.41.5.1" style="font-size:90%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T4.38.38.1"><math alttext="600" class="ltx_Math" display="inline" id="A4.T4.38.38.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">600</mn><annotation encoding="application/x-tex">600</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.39.39.2"><math alttext="54" class="ltx_Math" display="inline" id="A4.T4.39.39.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">54</mn><annotation encoding="application/x-tex">54</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.40.40.3"><math alttext="0" class="ltx_Math" display="inline" id="A4.T4.40.40.3.m1" intent=":literal"><mn mathsize="0.900em">0</mn></math></td>
<td class="ltx_td ltx_align_right" id="A4.T4.41.41.4"><math alttext="300" class="ltx_Math" display="inline" id="A4.T4.41.41.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">300</mn><annotation encoding="application/x-tex">300</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T4.45.45">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A4.T4.45.45.5"><span class="ltx_text ltx_font_italic" id="A4.T4.45.45.5.1" style="font-size:90%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《霍比特人》</font></font></font></span></th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T4.42.42.1"><math alttext="1000" class="ltx_Math" display="inline" id="A4.T4.42.42.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T4.43.43.2"><math alttext="188" class="ltx_Math" display="inline" id="A4.T4.43.43.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">188</mn><annotation encoding="application/x-tex">188</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T4.44.44.3"><math alttext="4" class="ltx_Math" display="inline" id="A4.T4.44.44.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T4.45.45.4"><math alttext="115" class="ltx_Math" display="inline" id="A4.T4.45.45.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">115</mn><annotation encoding="application/x-tex">115</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span><span class="ltx_text ltx_font_bold" id="A4.T4.54.1">Number of continue queries in Phase 2.</span>
We show the number of times we query each production LLM to continue in Phase&nbsp;2 for each book that achieves success in Phase&nbsp;1.
Phase&nbsp;2 was not run for GPT-4.1 on <span class="ltx_text ltx_font_italic" id="A4.T4.55.2">The Da Vinci Code</span> and <span class="ltx_text ltx_font_italic" id="A4.T4.56.3">Catch-22</span>, hence those entries are omitted.
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 4：第二阶段中的继续查询次数。我们展示了对于在第一阶段中成功每个书籍，我们查询每个生产 LLM 在第二阶段中继续的次数。由于第二阶段没有运行在《达芬奇密码》和《第二十二条军规》上的 GPT-4.1，因此这些条目被省略。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A4.SS2.SSS1.p2">
<p class="ltx_p" id="A4.SS2.SSS1.p2.1">We estimate the monetary cost of running Phase 2 by summing the provider-reported API charges over all continuation-loop requests in that phase for each LLM-book run.
This cost depends on (i) the number of continue queries (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.T4" title="Table 4 ‣ D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4</span></a>), (ii) the input and output token counts per query, and (iii) the provider pricing in effect during our experimental window (mid August to mid September 2025).
Because pricing and tokenization differ across providers and can change over time, we report costs only for our specific runs and treat them as approximations.
We provide one cost table each for Claude 3.7 Sonnet (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.T5" title="Table 5 ‣ D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>) and Grok 3 (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.T10" title="Table 10 ‣ Gemini 2.5 Pro sweeps. ‣ D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">10</span></a>).
For Gemini 2.5 Pro, we provide a cost table for our main results runs in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a> (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.T8" title="Table 8 ‣ Gemini 2.5 Pro sweeps. ‣ D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">8</span></a>) as well as a summary of total costs across all configured runs (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.T9" title="Table 9 ‣ Gemini 2.5 Pro sweeps. ‣ D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">9</span></a>).
For GPT-4.1, we include results for our main results in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a> (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.T6" title="Table 6 ‣ D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">6</span></a>) as well as a total cost table accounting for our more intensive extraction experiments (Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.T7" title="Table 7 ‣ Gemini 2.5 Pro sweeps. ‣ D.2.1 Continuation loop API costs ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a>).
Where appropriate, we provide short notes about provider-specific cost accounting.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们通过将每个 LLM-书籍运行在第二阶段中所有继续循环请求的提供者报告的 API 费用相加来估算运行第二阶段的货币成本。这个成本取决于 (i) 继续查询的数量（表 4）、(ii) 每个查询的输入和输出标记计数，以及 (iii) 我们实验窗口期间（2025 年 8 月中旬至 9 月中旬）有效的提供者定价。由于定价和标记化在不同的提供者之间有所不同，并且可能会随时间变化，我们仅报告我们特定运行的成本，并将它们视为近似值。我们为 Claude 3.7 Sonnet（表 5）和 Grok 3（表 10）各提供一张成本表。对于 Gemini 2.5 Pro，我们在第 4.2 节（表 8）中提供了主要结果运行的成本表，以及所有配置运行的总成本摘要（表 9）。对于 GPT-4.1，我们在第 4.2 节（表 6）中包含了主要结果，以及一个考虑了我们更密集提取实验的总成本表（表 7）。在适当的情况下，我们提供了关于提供者特定成本核算的简短说明。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T5.24">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T5.24.25.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A4.T5.24.25.1.1"><span class="ltx_text ltx_font_bold" id="A4.T5.24.25.1.1.1" style="font-size:90%;">Book<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">书</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T5.24.25.1.2"><span class="ltx_text ltx_font_bold" id="A4.T5.24.25.1.2.1" style="font-size:90%;">Input tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输入标记</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T5.24.25.1.3"><span class="ltx_text ltx_font_bold" id="A4.T5.24.25.1.3.1" style="font-size:90%;">Output tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输出标记</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T5.24.25.1.4"><span class="ltx_text ltx_font_bold" id="A4.T5.24.25.1.4.1" style="font-size:90%;">Cost ($)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">成本（美元）</font></font></font></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T5.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T5.2.2.3"><span class="ltx_text ltx_font_italic" id="A4.T5.2.2.3.1" style="font-size:90%;">1984</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T5.1.1.1"><math alttext="36,748,358" class="ltx_Math" display="inline" id="A4.T5.1.1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">36</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">748</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">358</mn></mrow><annotation encoding="application/x-tex">36,748,358</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T5.2.2.2"><math alttext="132,834" class="ltx_Math" display="inline" id="A4.T5.2.2.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">132</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">834</mn></mrow><annotation encoding="application/x-tex">132,834</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T5.2.2.4"><span class="ltx_text" id="A4.T5.2.2.4.1" style="font-size:90%;">113.12</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.4.4">
<td class="ltx_td ltx_align_left" id="A4.T5.4.4.3"><span class="ltx_text ltx_font_italic" id="A4.T5.4.4.3.1" style="font-size:90%;">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">挚爱</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T5.3.3.1"><math alttext="45,908,804" class="ltx_Math" display="inline" id="A4.T5.3.3.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">45</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">908</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">804</mn></mrow><annotation encoding="application/x-tex">45,908,804</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.4.4.2"><math alttext="149,575" class="ltx_Math" display="inline" id="A4.T5.4.4.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">149</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">575</mn></mrow><annotation encoding="application/x-tex">149,575</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.4.4.4"><span class="ltx_text" id="A4.T5.4.4.4.1" style="font-size:90%;">189.20</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.6.6">
<td class="ltx_td ltx_align_left" id="A4.T5.6.6.3"><span class="ltx_text ltx_font_italic" id="A4.T5.6.6.3.1" style="font-size:90%;">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">两难困境</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T5.5.5.1"><math alttext="22,566,963" class="ltx_Math" display="inline" id="A4.T5.5.5.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">22</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">566</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">963</mn></mrow><annotation encoding="application/x-tex">22,566,963</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.6.6.2"><math alttext="104,928" class="ltx_Math" display="inline" id="A4.T5.6.6.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">104</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">928</mn></mrow><annotation encoding="application/x-tex">104,928</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.6.6.4"><span class="ltx_text" id="A4.T5.6.6.4.1" style="font-size:90%;">69.25</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.8.8">
<td class="ltx_td ltx_align_left" id="A4.T5.8.8.3"><span class="ltx_text ltx_font_italic" id="A4.T5.8.8.3.1" style="font-size:90%;">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T5.7.7.1"><math alttext="2,682,321" class="ltx_Math" display="inline" id="A4.T5.7.7.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">2</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">682</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">321</mn></mrow><annotation encoding="application/x-tex">2,682,321</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.8.8.2"><math alttext="36,722" class="ltx_Math" display="inline" id="A4.T5.8.8.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">36</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">722</mn></mrow><annotation encoding="application/x-tex">36,722</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.8.8.4"><span class="ltx_text" id="A4.T5.8.8.4.1" style="font-size:90%;">11.17</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.10.10">
<td class="ltx_td ltx_align_left" id="A4.T5.10.10.3"><span class="ltx_text ltx_font_italic" id="A4.T5.10.10.3.1" style="font-size:90%;">The Da Vinci Code<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">达芬奇密码</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T5.9.9.1"><math alttext="36,979,052" class="ltx_Math" display="inline" id="A4.T5.9.9.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">36</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">979</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">052</mn></mrow><annotation encoding="application/x-tex">36,979,052</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.10.10.2"><math alttext="134,187" class="ltx_Math" display="inline" id="A4.T5.10.10.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">134</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">187</mn></mrow><annotation encoding="application/x-tex">134,187</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.10.10.4"><span class="ltx_text" id="A4.T5.10.10.4.1" style="font-size:90%;">152.46</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.12.12">
<td class="ltx_td ltx_align_left" id="A4.T5.12.12.3"><span class="ltx_text ltx_font_italic" id="A4.T5.12.12.3.1" style="font-size:90%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">弗兰肯斯坦</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T5.11.11.1"><math alttext="18,044,246" class="ltx_Math" display="inline" id="A4.T5.11.11.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">18</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">044</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">246</mn></mrow><annotation encoding="application/x-tex">18,044,246</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.12.12.2"><math alttext="94,041" class="ltx_Math" display="inline" id="A4.T5.12.12.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">94</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">041</mn></mrow><annotation encoding="application/x-tex">94,041</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.12.12.4"><span class="ltx_text" id="A4.T5.12.12.4.1" style="font-size:90%;">55.41</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.14.14">
<td class="ltx_td ltx_align_left" id="A4.T5.14.14.3"><span class="ltx_text ltx_font_italic" id="A4.T5.14.14.3.1" style="font-size:90%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T5.13.13.1"><math alttext="40,450,707" class="ltx_Math" display="inline" id="A4.T5.13.13.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">40</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">450</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">707</mn></mrow><annotation encoding="application/x-tex">40,450,707</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.14.14.2"><math alttext="140,200" class="ltx_Math" display="inline" id="A4.T5.14.14.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">140</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">200</mn></mrow><annotation encoding="application/x-tex">140,200</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.14.14.4"><span class="ltx_text" id="A4.T5.14.14.4.1" style="font-size:90%;">124.49</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.16.16">
<td class="ltx_td ltx_align_left" id="A4.T5.16.16.3"><span class="ltx_text ltx_font_italic" id="A4.T5.16.16.3.1" style="font-size:90%;">The Great Gatsby<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">了不起的盖茨比</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T5.15.15.1"><math alttext="12,964,939" class="ltx_Math" display="inline" id="A4.T5.15.15.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">12</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">964</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">939</mn></mrow><annotation encoding="application/x-tex">12,964,939</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.16.16.2"><math alttext="79,605" class="ltx_Math" display="inline" id="A4.T5.16.16.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">79</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">605</mn></mrow><annotation encoding="application/x-tex">79,605</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.16.16.4"><span class="ltx_text" id="A4.T5.16.16.4.1" style="font-size:90%;">39.85</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.18.18">
<td class="ltx_td ltx_align_left" id="A4.T5.18.18.3"><span class="ltx_text ltx_font_italic" id="A4.T5.18.18.3.1" style="font-size:90%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T5.17.17.1"><math alttext="28,987,543" class="ltx_Math" display="inline" id="A4.T5.17.17.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">28</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">987</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">543</mn></mrow><annotation encoding="application/x-tex">28,987,543</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.18.18.2"><math alttext="117,843" class="ltx_Math" display="inline" id="A4.T5.18.18.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">117</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">843</mn></mrow><annotation encoding="application/x-tex">117,843</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.18.18.4"><span class="ltx_text" id="A4.T5.18.18.4.1" style="font-size:90%;">119.97</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.20.20">
<td class="ltx_td ltx_align_left" id="A4.T5.20.20.3"><span class="ltx_text ltx_font_italic" id="A4.T5.20.20.3.1" style="font-size:90%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T5.19.19.1"><math alttext="32,258,077" class="ltx_Math" display="inline" id="A4.T5.19.19.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">32</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">258</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">077</mn></mrow><annotation encoding="application/x-tex">32,258,077</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.20.20.2"><math alttext="147,932" class="ltx_Math" display="inline" id="A4.T5.20.20.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">147</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">932</mn></mrow><annotation encoding="application/x-tex">147,932</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.20.20.4"><span class="ltx_text" id="A4.T5.20.20.4.1" style="font-size:90%;">133.12</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.22.22">
<td class="ltx_td ltx_align_left" id="A4.T5.22.22.3"><span class="ltx_text ltx_font_italic" id="A4.T5.22.22.3.1" style="font-size:90%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T5.21.21.1"><math alttext="45,855,059" class="ltx_Math" display="inline" id="A4.T5.21.21.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">45</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">855</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">059</mn></mrow><annotation encoding="application/x-tex">45,855,059</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.22.22.2"><math alttext="147,126" class="ltx_Math" display="inline" id="A4.T5.22.22.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">147</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">126</mn></mrow><annotation encoding="application/x-tex">147,126</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T5.22.22.4"><span class="ltx_text" id="A4.T5.22.22.4.1" style="font-size:90%;">140.52</span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.24.24">
<td class="ltx_td ltx_align_left ltx_border_b" id="A4.T5.24.24.3"><span class="ltx_text ltx_font_italic" id="A4.T5.24.24.3.1" style="font-size:90%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">霍比特人</font></font></font></span></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T5.23.23.1"><math alttext="32,472,077" class="ltx_Math" display="inline" id="A4.T5.23.23.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">32</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">472</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">077</mn></mrow><annotation encoding="application/x-tex">32,472,077</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T5.24.24.2"><math alttext="250,700" class="ltx_Math" display="inline" id="A4.T5.24.24.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">250</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">700</mn></mrow><annotation encoding="application/x-tex">250,700</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T5.24.24.4"><span class="ltx_text" id="A4.T5.24.24.4.1" style="font-size:90%;">134.87</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span><span class="ltx_text ltx_font_bold" id="A4.T5.30.1">Phase&nbsp;2 API token usage and estimated cost for Claude 3.7 Sonnet.</span>
For the main experiments in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we report total per-book-run Phase&nbsp;2 input and output tokens and the estimated dollar cost charged by the Claude 3.7 Sonnet API.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 5：Claude 3.7 Sonnet 第二阶段 API 令牌使用情况及估算成本。对于 4.2 节中的主要实验，我们报告了每本书运行的第二阶段输入和输出令牌总数以及 Claude 3.7 Sonnet API 收取的估算美元成本。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T6.22">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T6.22.23.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A4.T6.22.23.1.1"><span class="ltx_text ltx_font_bold" id="A4.T6.22.23.1.1.1" style="font-size:90%;">Book<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">书</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T6.22.23.1.2"><span class="ltx_text ltx_font_bold" id="A4.T6.22.23.1.2.1" style="font-size:90%;">Input tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输入令牌</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T6.22.23.1.3"><span class="ltx_text ltx_font_bold" id="A4.T6.22.23.1.3.1" style="font-size:90%;">Output tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输出令牌</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T6.22.23.1.4"><span class="ltx_text ltx_font_bold" id="A4.T6.22.23.1.4.1" style="font-size:90%;">Cost ($)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">成本(美元)</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T6.22.23.1.5"><span class="ltx_text ltx_font_bold" id="A4.T6.22.23.1.5.1" style="font-size:90%;">Cost w/ cache ($)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">带缓存的成本 ($)</font></font></font></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T6.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T6.2.2.3"><span class="ltx_text ltx_font_italic" id="A4.T6.2.2.3.1" style="font-size:90%;">1984</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T6.1.1.1"><math alttext="193,776" class="ltx_Math" display="inline" id="A4.T6.1.1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">193</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">776</mn></mrow><annotation encoding="application/x-tex">193,776</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T6.2.2.2"><math alttext="29,327" class="ltx_Math" display="inline" id="A4.T6.2.2.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">29</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">327</mn></mrow><annotation encoding="application/x-tex">29,327</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T6.2.2.4"><span class="ltx_text" id="A4.T6.2.2.4.1" style="font-size:90%;">0.62</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T6.2.2.5"><span class="ltx_text" id="A4.T6.2.2.5.1" style="font-size:90%;">0.34</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.4.4">
<td class="ltx_td ltx_align_left" id="A4.T6.4.4.3"><span class="ltx_text ltx_font_italic" id="A4.T6.4.4.3.1" style="font-size:90%;">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《宠儿》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.3.3.1"><math alttext="45,210" class="ltx_Math" display="inline" id="A4.T6.3.3.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">45</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">210</mn></mrow><annotation encoding="application/x-tex">45,210</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.4.4.2"><math alttext="12,087" class="ltx_Math" display="inline" id="A4.T6.4.4.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">12</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">087</mn></mrow><annotation encoding="application/x-tex">12,087</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.4.4.4"><span class="ltx_text" id="A4.T6.4.4.4.1" style="font-size:90%;">0.19</span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.4.4.5"><span class="ltx_text" id="A4.T6.4.4.5.1" style="font-size:90%;">0.12</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.6.6">
<td class="ltx_td ltx_align_left" id="A4.T6.6.6.3"><span class="ltx_text ltx_font_italic" id="A4.T6.6.6.3.1" style="font-size:90%;">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">悖论</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.5.5.1"><math alttext="32,014" class="ltx_Math" display="inline" id="A4.T6.5.5.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">32</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">014</mn></mrow><annotation encoding="application/x-tex">32,014</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.6.6.2"><math alttext="25,500" class="ltx_Math" display="inline" id="A4.T6.6.6.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">25</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">500</mn></mrow><annotation encoding="application/x-tex">25,500</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.6.6.4"><span class="ltx_text" id="A4.T6.6.6.4.1" style="font-size:90%;">0.27</span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.6.6.5"><span class="ltx_text" id="A4.T6.6.6.5.1" style="font-size:90%;">0.23</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.8.8">
<td class="ltx_td ltx_align_left" id="A4.T6.8.8.3"><span class="ltx_text ltx_font_italic" id="A4.T6.8.8.3.1" style="font-size:90%;">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.7.7.1"><math alttext="56,682" class="ltx_Math" display="inline" id="A4.T6.7.7.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">56</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">682</mn></mrow><annotation encoding="application/x-tex">56,682</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.8.8.2"><math alttext="10,927" class="ltx_Math" display="inline" id="A4.T6.8.8.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">10</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">927</mn></mrow><annotation encoding="application/x-tex">10,927</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.8.8.4"><span class="ltx_text" id="A4.T6.8.8.4.1" style="font-size:90%;">0.20</span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.8.8.5"><span class="ltx_text" id="A4.T6.8.8.5.1" style="font-size:90%;">0.12</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.10.10">
<td class="ltx_td ltx_align_left" id="A4.T6.10.10.3"><span class="ltx_text ltx_font_italic" id="A4.T6.10.10.3.1" style="font-size:90%;">The Da Vinci Code<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">达芬奇密码</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.9.9.1"><math alttext="80,269" class="ltx_Math" display="inline" id="A4.T6.9.9.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">80</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">269</mn></mrow><annotation encoding="application/x-tex">80,269</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.10.10.2"><math alttext="8598" class="ltx_Math" display="inline" id="A4.T6.10.10.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">8598</mn><annotation encoding="application/x-tex">8598</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.10.10.4"><span class="ltx_text" id="A4.T6.10.10.4.1" style="font-size:90%;">0.23</span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.10.10.5"><span class="ltx_text" id="A4.T6.10.10.5.1" style="font-size:90%;">0.11</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.12.12">
<td class="ltx_td ltx_align_left" id="A4.T6.12.12.3"><span class="ltx_text ltx_font_italic" id="A4.T6.12.12.3.1" style="font-size:90%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">弗兰肯斯坦</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.11.11.1"><math alttext="51,801" class="ltx_Math" display="inline" id="A4.T6.11.11.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">51</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">801</mn></mrow><annotation encoding="application/x-tex">51,801</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.12.12.2"><math alttext="10,664" class="ltx_Math" display="inline" id="A4.T6.12.12.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">10</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">664</mn></mrow><annotation encoding="application/x-tex">10,664</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.12.12.4"><span class="ltx_text" id="A4.T6.12.12.4.1" style="font-size:90%;">0.19</span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.12.12.5"><span class="ltx_text" id="A4.T6.12.12.5.1" style="font-size:90%;">0.11</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.14.14">
<td class="ltx_td ltx_align_left" id="A4.T6.14.14.3"><span class="ltx_text ltx_font_italic" id="A4.T6.14.14.3.1" style="font-size:90%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.13.13.1"><math alttext="73,522" class="ltx_Math" display="inline" id="A4.T6.13.13.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">73</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">522</mn></mrow><annotation encoding="application/x-tex">73,522</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.14.14.2"><math alttext="29,173" class="ltx_Math" display="inline" id="A4.T6.14.14.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">29</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">173</mn></mrow><annotation encoding="application/x-tex">29,173</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.14.14.4"><span class="ltx_text" id="A4.T6.14.14.4.1" style="font-size:90%;">0.38</span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.14.14.5"><span class="ltx_text" id="A4.T6.14.14.5.1" style="font-size:90%;">0.28</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.16.16">
<td class="ltx_td ltx_align_left" id="A4.T6.16.16.3"><span class="ltx_text ltx_font_italic" id="A4.T6.16.16.3.1" style="font-size:90%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.15.15.1"><math alttext="364,599" class="ltx_Math" display="inline" id="A4.T6.15.15.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">364</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">599</mn></mrow><annotation encoding="application/x-tex">364,599</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.16.16.2"><math alttext="79,825" class="ltx_Math" display="inline" id="A4.T6.16.16.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">79</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">825</mn></mrow><annotation encoding="application/x-tex">79,825</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.16.16.4"><span class="ltx_text" id="A4.T6.16.16.4.1" style="font-size:90%;">1.37</span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.16.16.5"><span class="ltx_text" id="A4.T6.16.16.5.1" style="font-size:90%;">0.83</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.18.18">
<td class="ltx_td ltx_align_left" id="A4.T6.18.18.3"><span class="ltx_text ltx_font_italic" id="A4.T6.18.18.3.1" style="font-size:90%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.17.17.1"><math alttext="37,435" class="ltx_Math" display="inline" id="A4.T6.17.17.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">37</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">435</mn></mrow><annotation encoding="application/x-tex">37,435</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.18.18.2"><math alttext="10,445" class="ltx_Math" display="inline" id="A4.T6.18.18.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">10</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">445</mn></mrow><annotation encoding="application/x-tex">10,445</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.18.18.4"><span class="ltx_text" id="A4.T6.18.18.4.1" style="font-size:90%;">0.16</span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.18.18.5"><span class="ltx_text" id="A4.T6.18.18.5.1" style="font-size:90%;">0.10</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.20.20">
<td class="ltx_td ltx_align_left" id="A4.T6.20.20.3"><span class="ltx_text ltx_font_italic" id="A4.T6.20.20.3.1" style="font-size:90%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.19.19.1"><math alttext="30,102" class="ltx_Math" display="inline" id="A4.T6.19.19.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">30</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">102</mn></mrow><annotation encoding="application/x-tex">30,102</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.20.20.2"><math alttext="12,331" class="ltx_Math" display="inline" id="A4.T6.20.20.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">12</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">331</mn></mrow><annotation encoding="application/x-tex">12,331</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T6.20.20.4"><span class="ltx_text" id="A4.T6.20.20.4.1" style="font-size:90%;">0.16</span></td>
<td class="ltx_td ltx_align_right" id="A4.T6.20.20.5"><span class="ltx_text" id="A4.T6.20.20.5.1" style="font-size:90%;">0.11</span></td>
</tr>
<tr class="ltx_tr" id="A4.T6.22.22">
<td class="ltx_td ltx_align_left ltx_border_b" id="A4.T6.22.22.3"><span class="ltx_text ltx_font_italic" id="A4.T6.22.22.3.1" style="font-size:90%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">霍比特人</font></font></font></span></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T6.21.21.1"><math alttext="44,633" class="ltx_Math" display="inline" id="A4.T6.21.21.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">44</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">633</mn></mrow><annotation encoding="application/x-tex">44,633</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T6.22.22.2"><math alttext="9322" class="ltx_Math" display="inline" id="A4.T6.22.22.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">9322</mn><annotation encoding="application/x-tex">9322</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T6.22.22.4"><span class="ltx_text" id="A4.T6.22.22.4.1" style="font-size:90%;">0.16</span></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T6.22.22.5"><span class="ltx_text" id="A4.T6.22.22.5.1" style="font-size:90%;">0.10</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span><span class="ltx_text ltx_font_bold" id="A4.T6.28.1">Phase API token usage and estimated cost for GPT-4.1 (main experiments).</span>
For the main experiments in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we report the total per-book-run Phase 2 (for GPT-4.1, first chapter) input and output tokens.
We provide two cost estimates: an upper bound assuming no prompt caching, and a lower estimate using our caching heuristic.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 6：GPT-4.1（主要实验）的 Phase API token 使用量和预估成本。对于 4.2 节中的主要实验，我们报告了每本书运行时 Phase 2（对于 GPT-4.1，第一章）的输入和输出 token 总量。我们提供了两种成本估计：一种假设不使用提示缓存的上下限，以及一种使用我们缓存启发式的下限估计。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="A4.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Claude 3.7 Sonnet.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">Claude 3.7 Sonnet。</font></font></font></h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A4.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="A4.SS2.SSS1.Px1.p1.1">The API provider billing reports costs aggregated per day, rather than per run.
To estimate a per-run Phase&nbsp;2 cost, we compute a weighted share of the total daily cost based on that run’s share of the day’s total Phase&nbsp;2 token usage.
Claude 3.7 Sonnet appears to incur an extra, opaque “long context request” charge that is not explained in the publicly available pricing documentation;
our estimates necessarily include this charge when it is present in the daily bill.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">API 提供方的账单报告按天汇总成本，而不是按每次运行计算。为了估算每次运行的 Phase 2 成本，我们根据该运行在当天 Phase 2 token 使用总量中所占的份额，计算了总日成本的一个加权份额。Claude 3.7 Sonnet 似乎会产生一个额外的、不透明的“长上下文请求”费用，这在公开的定价文档中没有解释；当这个费用出现在每日账单中时，我们的估算必然包括这一费用。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS2.SSS1.Px1.p2">
<p class="ltx_p" id="A4.SS2.SSS1.Px1.p2.1"><span class="ltx_text ltx_font_bold" id="A4.SS2.SSS1.Px1.p2.1.1">GPT-4.1 accounting note.</span>
We tracked costs, but at the time of writing the OpenAI billing API was down (HTTP 500 error).
We therefore estimate costs based on token usage.
OpenAI API does not report cached tokens explicitly, so we applied a heuristic to estimate prompt caching:
for sequential requests within a run, we estimate cached tokens as the minimum of the previous and current prompt token counts, reflecting the shared prefix between successive requests.
We report a conservative upper bound assuming no caching, and a lower bound using our caching heuristic.
Costs were calculated using $2.00 per million input tokens, $0.50 per million cached input tokens, and $8.00 per million output tokens.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">GPT-4.1 会计备注。我们追踪了成本，但在撰写本文时，OpenAI 的计费 API 已经宕机（HTTP 500 错误）。因此，我们根据 token 使用量估算成本。OpenAI API 没有明确报告缓存的 token，所以我们应用了一种启发式方法来估算提示缓存：对于运行内的连续请求，我们估算缓存的 token 数量为前一个和当前提示 token 数量的最小值，反映了连续请求之间的公共前缀。我们报告了一个保守的上限，假设没有缓存，以及使用我们的缓存启发式方法得出的下限。成本计算使用每百万输入 token 2.00 美元，每百万缓存的输入 token 0.50 美元，以及每百万输出 token 8.00 美元。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Gemini 2.5 Pro sweeps.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">Gemini 2.5 Pro 扫描。</font></font></font></h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A4.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="A4.SS2.SSS1.Px2.p1.1">For Gemini 2.5 Pro we performed a Phase&nbsp;2 sweep over presence/frequency penalty to study sensitivity to generation settings.
Accordingly, we report (i) the Phase&nbsp;2 cost of the single configuration used for our main Gemini 2.5 Pro comparison runs, and (ii) the cumulative Phase&nbsp;2 cost summed over <em class="ltx_emph ltx_font_italic" id="A4.SS2.SSS1.Px2.p1.1.1">all</em> Gemini 2.5 Pro sweep runs executed per book.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">针对 Gemini 2.5 Pro，我们进行了第二阶段的扫描，研究生成设置对存在/频率惩罚的敏感性。因此，我们报告了 (i) 主要 Gemini 2.5 Pro 对比运行中使用的单一配置的第二阶段成本，以及 (ii) 每本书执行的 Gemini 2.5 Pro 扫描运行中所有第二阶段成本的总和。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A4.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T7.22">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T7.22.23.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A4.T7.22.23.1.1"><span class="ltx_text ltx_font_bold" id="A4.T7.22.23.1.1.1" style="font-size:90%;">Book<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">书</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T7.22.23.1.2"><span class="ltx_text ltx_font_bold" id="A4.T7.22.23.1.2.1" style="font-size:90%;">Input tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输入标记</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T7.22.23.1.3"><span class="ltx_text ltx_font_bold" id="A4.T7.22.23.1.3.1" style="font-size:90%;">Output tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输出标记</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T7.22.23.1.4"><span class="ltx_text ltx_font_bold" id="A4.T7.22.23.1.4.1" style="font-size:90%;">Cost ($)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">成本（美元）</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T7.22.23.1.5"><span class="ltx_text ltx_font_bold" id="A4.T7.22.23.1.5.1" style="font-size:90%;">Cost w/ cache ($)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">使用缓存的成本（美元）</font></font></font></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T7.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T7.2.2.3"><span class="ltx_text ltx_font_italic" id="A4.T7.2.2.3.1" style="font-size:90%;">1984</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T7.1.1.1"><math alttext="7,396,072" class="ltx_Math" display="inline" id="A4.T7.1.1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">7</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">396</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">072</mn></mrow><annotation encoding="application/x-tex">7,396,072</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T7.2.2.2"><math alttext="865,284" class="ltx_Math" display="inline" id="A4.T7.2.2.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">865</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">284</mn></mrow><annotation encoding="application/x-tex">865,284</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T7.2.2.4"><span class="ltx_text" id="A4.T7.2.2.4.1" style="font-size:90%;">21.71</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T7.2.2.5"><span class="ltx_text" id="A4.T7.2.2.5.1" style="font-size:90%;">10.83</span></td>
</tr>
<tr class="ltx_tr" id="A4.T7.4.4">
<td class="ltx_td ltx_align_left" id="A4.T7.4.4.3"><span class="ltx_text ltx_font_italic" id="A4.T7.4.4.3.1" style="font-size:90%;">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">备受喜爱的</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.3.3.1"><math alttext="919,976" class="ltx_Math" display="inline" id="A4.T7.3.3.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">919</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">976</mn></mrow><annotation encoding="application/x-tex">919,976</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.4.4.2"><math alttext="318,455" class="ltx_Math" display="inline" id="A4.T7.4.4.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">318</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">455</mn></mrow><annotation encoding="application/x-tex">318,455</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.4.4.4"><span class="ltx_text" id="A4.T7.4.4.4.1" style="font-size:90%;">4.39</span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.4.4.5"><span class="ltx_text" id="A4.T7.4.4.5.1" style="font-size:90%;">3.09</span></td>
</tr>
<tr class="ltx_tr" id="A4.T7.6.6">
<td class="ltx_td ltx_align_left" id="A4.T7.6.6.3"><span class="ltx_text ltx_font_italic" id="A4.T7.6.6.3.1" style="font-size:90%;">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">两难困境</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.5.5.1"><math alttext="3,718,266" class="ltx_Math" display="inline" id="A4.T7.5.5.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">3</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">718</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">266</mn></mrow><annotation encoding="application/x-tex">3,718,266</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.6.6.2"><math alttext="804,740" class="ltx_Math" display="inline" id="A4.T7.6.6.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">804</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">740</mn></mrow><annotation encoding="application/x-tex">804,740</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.6.6.4"><span class="ltx_text" id="A4.T7.6.6.4.1" style="font-size:90%;">13.87</span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.6.6.5"><span class="ltx_text" id="A4.T7.6.6.5.1" style="font-size:90%;">8.45</span></td>
</tr>
<tr class="ltx_tr" id="A4.T7.8.8">
<td class="ltx_td ltx_align_left" id="A4.T7.8.8.3"><span class="ltx_text ltx_font_italic" id="A4.T7.8.8.3.1" style="font-size:90%;">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.7.7.1"><math alttext="954,779" class="ltx_Math" display="inline" id="A4.T7.7.7.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">954</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">779</mn></mrow><annotation encoding="application/x-tex">954,779</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.8.8.2"><math alttext="375,968" class="ltx_Math" display="inline" id="A4.T7.8.8.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">375</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">968</mn></mrow><annotation encoding="application/x-tex">375,968</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.8.8.4"><span class="ltx_text" id="A4.T7.8.8.4.1" style="font-size:90%;">4.92</span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.8.8.5"><span class="ltx_text" id="A4.T7.8.8.5.1" style="font-size:90%;">3.54</span></td>
</tr>
<tr class="ltx_tr" id="A4.T7.10.10">
<td class="ltx_td ltx_align_left" id="A4.T7.10.10.3"><span class="ltx_text ltx_font_italic" id="A4.T7.10.10.3.1" style="font-size:90%;">The Da Vinci Code<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《达芬奇密码》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.9.9.1"><math alttext="2,058,447" class="ltx_Math" display="inline" id="A4.T7.9.9.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">2</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">058</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">447</mn></mrow><annotation encoding="application/x-tex">2,058,447</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.10.10.2"><math alttext="854,886" class="ltx_Math" display="inline" id="A4.T7.10.10.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">854</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">886</mn></mrow><annotation encoding="application/x-tex">854,886</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.10.10.4"><span class="ltx_text" id="A4.T7.10.10.4.1" style="font-size:90%;">10.96</span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.10.10.5"><span class="ltx_text" id="A4.T7.10.10.5.1" style="font-size:90%;">7.96</span></td>
</tr>
<tr class="ltx_tr" id="A4.T7.12.12">
<td class="ltx_td ltx_align_left" id="A4.T7.12.12.3"><span class="ltx_text ltx_font_italic" id="A4.T7.12.12.3.1" style="font-size:90%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《弗兰肯斯坦》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.11.11.1"><math alttext="2,136,628" class="ltx_Math" display="inline" id="A4.T7.11.11.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">2</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">136</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">628</mn></mrow><annotation encoding="application/x-tex">2,136,628</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.12.12.2"><math alttext="448,730" class="ltx_Math" display="inline" id="A4.T7.12.12.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">448</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">730</mn></mrow><annotation encoding="application/x-tex">448,730</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.12.12.4"><span class="ltx_text" id="A4.T7.12.12.4.1" style="font-size:90%;">7.86</span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.12.12.5"><span class="ltx_text" id="A4.T7.12.12.5.1" style="font-size:90%;">4.76</span></td>
</tr>
<tr class="ltx_tr" id="A4.T7.14.14">
<td class="ltx_td ltx_align_left" id="A4.T7.14.14.3"><span class="ltx_text ltx_font_italic" id="A4.T7.14.14.3.1" style="font-size:90%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《冰与火之歌》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.13.13.1"><math alttext="2,548,600" class="ltx_Math" display="inline" id="A4.T7.13.13.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">2</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">548</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">600</mn></mrow><annotation encoding="application/x-tex">2,548,600</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.14.14.2"><math alttext="1,006,291" class="ltx_Math" display="inline" id="A4.T7.14.14.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">1</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">006</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">291</mn></mrow><annotation encoding="application/x-tex">1,006,291</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.14.14.4"><span class="ltx_text" id="A4.T7.14.14.4.1" style="font-size:90%;">13.15</span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.14.14.5"><span class="ltx_text" id="A4.T7.14.14.5.1" style="font-size:90%;">9.50</span></td>
</tr>
<tr class="ltx_tr" id="A4.T7.16.16">
<td class="ltx_td ltx_align_left" id="A4.T7.16.16.3"><span class="ltx_text ltx_font_italic" id="A4.T7.16.16.3.1" style="font-size:90%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.15.15.1"><math alttext="7,452,983" class="ltx_Math" display="inline" id="A4.T7.15.15.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">7</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">452</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">983</mn></mrow><annotation encoding="application/x-tex">7,452,983</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.16.16.2"><math alttext="932,170" class="ltx_Math" display="inline" id="A4.T7.16.16.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">932</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">170</mn></mrow><annotation encoding="application/x-tex">932,170</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.16.16.4"><span class="ltx_text" id="A4.T7.16.16.4.1" style="font-size:90%;">22.36</span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.16.16.5"><span class="ltx_text" id="A4.T7.16.16.5.1" style="font-size:90%;">11.45</span></td>
</tr>
<tr class="ltx_tr" id="A4.T7.18.18">
<td class="ltx_td ltx_align_left" id="A4.T7.18.18.3"><span class="ltx_text ltx_font_italic" id="A4.T7.18.18.3.1" style="font-size:90%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.17.17.1"><math alttext="3,162,308" class="ltx_Math" display="inline" id="A4.T7.17.17.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">3</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">162</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">308</mn></mrow><annotation encoding="application/x-tex">3,162,308</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.18.18.2"><math alttext="554,614" class="ltx_Math" display="inline" id="A4.T7.18.18.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">554</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">614</mn></mrow><annotation encoding="application/x-tex">554,614</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.18.18.4"><span class="ltx_text" id="A4.T7.18.18.4.1" style="font-size:90%;">10.76</span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.18.18.5"><span class="ltx_text" id="A4.T7.18.18.5.1" style="font-size:90%;">6.15</span></td>
</tr>
<tr class="ltx_tr" id="A4.T7.20.20">
<td class="ltx_td ltx_align_left" id="A4.T7.20.20.3"><span class="ltx_text ltx_font_italic" id="A4.T7.20.20.3.1" style="font-size:90%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.19.19.1"><math alttext="1,155,141" class="ltx_Math" display="inline" id="A4.T7.19.19.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">1</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">155</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">141</mn></mrow><annotation encoding="application/x-tex">1,155,141</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.20.20.2"><math alttext="459,260" class="ltx_Math" display="inline" id="A4.T7.20.20.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">459</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">260</mn></mrow><annotation encoding="application/x-tex">459,260</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T7.20.20.4"><span class="ltx_text" id="A4.T7.20.20.4.1" style="font-size:90%;">5.98</span></td>
<td class="ltx_td ltx_align_right" id="A4.T7.20.20.5"><span class="ltx_text" id="A4.T7.20.20.5.1" style="font-size:90%;">4.33</span></td>
</tr>
<tr class="ltx_tr" id="A4.T7.22.22">
<td class="ltx_td ltx_align_left ltx_border_b" id="A4.T7.22.22.3"><span class="ltx_text ltx_font_italic" id="A4.T7.22.22.3.1" style="font-size:90%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">霍比特人</font></font></font></span></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T7.21.21.1"><math alttext="1,599,404" class="ltx_Math" display="inline" id="A4.T7.21.21.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">1</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">599</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">404</mn></mrow><annotation encoding="application/x-tex">1,599,404</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T7.22.22.2"><math alttext="298,146" class="ltx_Math" display="inline" id="A4.T7.22.22.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">298</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">146</mn></mrow><annotation encoding="application/x-tex">298,146</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T7.22.22.4"><span class="ltx_text" id="A4.T7.22.22.4.1" style="font-size:90%;">5.58</span></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T7.22.22.5"><span class="ltx_text" id="A4.T7.22.22.5.1" style="font-size:90%;">3.24</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span><span class="ltx_text ltx_font_bold" id="A4.T7.29.1">Phase&nbsp;2 API token usage and estimated cost for GPT-4.1 (total cost).</span>
For all experiments in Sections&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a> and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS3" title="4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.3</span></a>, we report the total per-book Phase 2 input and output tokens.
We provide two cost estimates: an upper bound assuming no prompt caching, and a lower estimate using our caching heuristic.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 7：GPT-4.1 第二阶段 API 令牌使用量和预估成本（总成本）。对于 4.2 和 4.3 节中的所有实验，我们报告了每本书的第二阶段输入和输出令牌总数。我们提供了两种成本估计：一种假设不使用提示缓存的最高限，以及一种使用我们缓存启发式的最低估计。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A4.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T8.36">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T8.36.37.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A4.T8.36.37.1.1"><span class="ltx_text ltx_font_bold" id="A4.T8.36.37.1.1.1" style="font-size:90%;">Book<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">书</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T8.36.37.1.2"><span class="ltx_text ltx_font_bold" id="A4.T8.36.37.1.2.1" style="font-size:90%;">Input tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输入令牌</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T8.36.37.1.3"><span class="ltx_text ltx_font_bold" id="A4.T8.36.37.1.3.1" style="font-size:90%;">Output tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输出令牌</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T8.36.37.1.4"><span class="ltx_text ltx_font_bold" id="A4.T8.36.37.1.4.1" style="font-size:90%;">Cost ($)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">成本（美元）</font></font></font></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T8.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T8.3.3.4"><span class="ltx_text ltx_font_italic" id="A4.T8.3.3.4.1" style="font-size:90%;">1984</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T8.1.1.1"><math alttext="18,582" class="ltx_Math" display="inline" id="A4.T8.1.1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">18</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">582</mn></mrow><annotation encoding="application/x-tex">18,582</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T8.2.2.2"><math alttext="6550" class="ltx_Math" display="inline" id="A4.T8.2.2.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">6550</mn><annotation encoding="application/x-tex">6550</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T8.3.3.3"><math alttext="0.44" class="ltx_Math" display="inline" id="A4.T8.3.3.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.44</mn><annotation encoding="application/x-tex">0.44</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T8.6.6.4"><span class="ltx_text ltx_font_italic" id="A4.T8.6.6.4.1" style="font-size:90%;">Beloved<font data-immersive-translate-error-id="378" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T8.4.4.1"><math alttext="96,184" class="ltx_Math" display="inline" id="A4.T8.4.4.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">96</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">184</mn></mrow><annotation encoding="application/x-tex">96,184</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.5.5.2"><math alttext="49,192" class="ltx_Math" display="inline" id="A4.T8.5.5.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">49</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">192</mn></mrow><annotation encoding="application/x-tex">49,192</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.6.6.3"><math alttext="0.85" class="ltx_Math" display="inline" id="A4.T8.6.6.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.85</mn><annotation encoding="application/x-tex">0.85</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T8.9.9.4"><span class="ltx_text ltx_font_italic" id="A4.T8.9.9.4.1" style="font-size:90%;">Catch-22<font data-immersive-translate-error-id="379" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T8.7.7.1"><math alttext="12,400" class="ltx_Math" display="inline" id="A4.T8.7.7.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">12</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">400</mn></mrow><annotation encoding="application/x-tex">12,400</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.8.8.2"><math alttext="3579" class="ltx_Math" display="inline" id="A4.T8.8.8.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">3579</mn><annotation encoding="application/x-tex">3579</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.9.9.3"><math alttext="0.30" class="ltx_Math" display="inline" id="A4.T8.9.9.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.30</mn><annotation encoding="application/x-tex">0.30</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T8.12.12.4"><span class="ltx_text ltx_font_italic" id="A4.T8.12.12.4.1" style="font-size:90%;">The Catcher in the Rye<font data-immersive-translate-error-id="380" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T8.10.10.1"><math alttext="20,142" class="ltx_Math" display="inline" id="A4.T8.10.10.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">20</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">142</mn></mrow><annotation encoding="application/x-tex">20,142</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.11.11.2"><math alttext="8502" class="ltx_Math" display="inline" id="A4.T8.11.11.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">8502</mn><annotation encoding="application/x-tex">8502</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.12.12.3"><math alttext="0.36" class="ltx_Math" display="inline" id="A4.T8.12.12.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.36</mn><annotation encoding="application/x-tex">0.36</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T8.15.15.4"><span class="ltx_text ltx_font_italic" id="A4.T8.15.15.4.1" style="font-size:90%;">The Da Vinci Code<font data-immersive-translate-error-id="381" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T8.13.13.1"><math alttext="17,989" class="ltx_Math" display="inline" id="A4.T8.13.13.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">17</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">989</mn></mrow><annotation encoding="application/x-tex">17,989</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.14.14.2"><math alttext="6993" class="ltx_Math" display="inline" id="A4.T8.14.14.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">6993</mn><annotation encoding="application/x-tex">6993</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.15.15.3"><math alttext="0.34" class="ltx_Math" display="inline" id="A4.T8.15.15.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.34</mn><annotation encoding="application/x-tex">0.34</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T8.18.18.4"><span class="ltx_text ltx_font_italic" id="A4.T8.18.18.4.1" style="font-size:90%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">弗兰肯斯坦</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T8.16.16.1"><math alttext="24,184" class="ltx_Math" display="inline" id="A4.T8.16.16.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">24</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">184</mn></mrow><annotation encoding="application/x-tex">24,184</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.17.17.2"><math alttext="10,419" class="ltx_Math" display="inline" id="A4.T8.17.17.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">10</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">419</mn></mrow><annotation encoding="application/x-tex">10,419</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.18.18.3"><math alttext="0.38" class="ltx_Math" display="inline" id="A4.T8.18.18.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.38</mn><annotation encoding="application/x-tex">0.38</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.21.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T8.21.21.4"><span class="ltx_text ltx_font_italic" id="A4.T8.21.21.4.1" style="font-size:90%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T8.19.19.1"><math alttext="19,874" class="ltx_Math" display="inline" id="A4.T8.19.19.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">19</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">874</mn></mrow><annotation encoding="application/x-tex">19,874</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.20.20.2"><math alttext="8519" class="ltx_Math" display="inline" id="A4.T8.20.20.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">8519</mn><annotation encoding="application/x-tex">8519</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.21.21.3"><math alttext="0.36" class="ltx_Math" display="inline" id="A4.T8.21.21.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.36</mn><annotation encoding="application/x-tex">0.36</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T8.24.24.4"><span class="ltx_text ltx_font_italic" id="A4.T8.24.24.4.1" style="font-size:90%;">The Great Gatsby<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">了不起的盖茨比</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T8.22.22.1"><math alttext="20,628" class="ltx_Math" display="inline" id="A4.T8.22.22.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">20</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">628</mn></mrow><annotation encoding="application/x-tex">20,628</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.23.23.2"><math alttext="8489" class="ltx_Math" display="inline" id="A4.T8.23.23.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">8489</mn><annotation encoding="application/x-tex">8489</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.24.24.3"><math alttext="0.35" class="ltx_Math" display="inline" id="A4.T8.24.24.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.35</mn><annotation encoding="application/x-tex">0.35</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.27.27">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T8.27.27.4"><span class="ltx_text ltx_font_italic" id="A4.T8.27.27.4.1" style="font-size:90%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T8.25.25.1"><math alttext="207,589" class="ltx_Math" display="inline" id="A4.T8.25.25.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">207</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">589</mn></mrow><annotation encoding="application/x-tex">207,589</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.26.26.2"><math alttext="104,955" class="ltx_Math" display="inline" id="A4.T8.26.26.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">104</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">955</mn></mrow><annotation encoding="application/x-tex">104,955</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.27.27.3"><math alttext="2.44" class="ltx_Math" display="inline" id="A4.T8.27.27.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">2.44</mn><annotation encoding="application/x-tex">2.44</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.30.30">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T8.30.30.4"><span class="ltx_text ltx_font_italic" id="A4.T8.30.30.4.1" style="font-size:90%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T8.28.28.1"><math alttext="14,176" class="ltx_Math" display="inline" id="A4.T8.28.28.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">14</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">176</mn></mrow><annotation encoding="application/x-tex">14,176</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.29.29.2"><math alttext="4725" class="ltx_Math" display="inline" id="A4.T8.29.29.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">4725</mn><annotation encoding="application/x-tex">4725</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.30.30.3"><math alttext="0.31" class="ltx_Math" display="inline" id="A4.T8.30.30.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.31</mn><annotation encoding="application/x-tex">0.31</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.33.33">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T8.33.33.4"><span class="ltx_text ltx_font_italic" id="A4.T8.33.33.4.1" style="font-size:90%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T8.31.31.1"><math alttext="4893" class="ltx_Math" display="inline" id="A4.T8.31.31.1.m1" intent=":literal"><semantics><mn mathsize="0.900em">4893</mn><annotation encoding="application/x-tex">4893</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.32.32.2"><math alttext="3346" class="ltx_Math" display="inline" id="A4.T8.32.32.2.m1" intent=":literal"><semantics><mn mathsize="0.900em">3346</mn><annotation encoding="application/x-tex">3346</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T8.33.33.3"><math alttext="0.06" class="ltx_Math" display="inline" id="A4.T8.33.33.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.06</mn><annotation encoding="application/x-tex">0.06</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T8.36.36">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A4.T8.36.36.4"><span class="ltx_text ltx_font_italic" id="A4.T8.36.36.4.1" style="font-size:90%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">霍比特人</font></font></font></span></th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T8.34.34.1"><math alttext="43,240" class="ltx_Math" display="inline" id="A4.T8.34.34.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">43</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">240</mn></mrow><annotation encoding="application/x-tex">43,240</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T8.35.35.2"><math alttext="21,677" class="ltx_Math" display="inline" id="A4.T8.35.35.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">21</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">677</mn></mrow><annotation encoding="application/x-tex">21,677</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T8.36.36.3"><math alttext="0.52" class="ltx_Math" display="inline" id="A4.T8.36.36.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.52</mn><annotation encoding="application/x-tex">0.52</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8: </span><span class="ltx_text ltx_font_bold" id="A4.T8.42.1">Phase API token usage and estimated cost for Gemini 2.5 Pro (main experiments).</span>
For the main experiments in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we report the total per-book Phase 2 input/output tokens and estimated dollar cost.
These results reflect a single generation configuration run for each book.
<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 8：Gemini 2.5 Pro（主要实验）的 Phase API token 使用量和预估成本。对于 4.2 节中的主要实验，我们报告了每本书的 Phase 2 输入/输出 token 总数和预估美元成本。这些结果反映了对每本书运行的单次生成配置。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A4.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T9.36">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T9.36.37.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A4.T9.36.37.1.1"><span class="ltx_text ltx_font_bold" id="A4.T9.36.37.1.1.1" style="font-size:90%;">Book<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">书</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T9.36.37.1.2"><span class="ltx_text ltx_font_bold" id="A4.T9.36.37.1.2.1" style="font-size:90%;">Input tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输入标记</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T9.36.37.1.3"><span class="ltx_text ltx_font_bold" id="A4.T9.36.37.1.3.1" style="font-size:90%;">Output tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输出令牌</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T9.36.37.1.4"><span class="ltx_text ltx_font_bold" id="A4.T9.36.37.1.4.1" style="font-size:90%;">Cost ($)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">成本（美元）</font></font></font></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T9.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T9.3.3.4"><span class="ltx_text ltx_font_italic" id="A4.T9.3.3.4.1" style="font-size:90%;">1984</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T9.1.1.1"><math alttext="2,195,516" class="ltx_Math" display="inline" id="A4.T9.1.1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">2</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">195</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">516</mn></mrow><annotation encoding="application/x-tex">2,195,516</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T9.2.2.2"><math alttext="1,029,004" class="ltx_Math" display="inline" id="A4.T9.2.2.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">1</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">029</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">004</mn></mrow><annotation encoding="application/x-tex">1,029,004</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T9.3.3.3"><math alttext="58.99" class="ltx_Math" display="inline" id="A4.T9.3.3.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">58.99</mn><annotation encoding="application/x-tex">58.99</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T9.6.6.4"><span class="ltx_text ltx_font_italic" id="A4.T9.6.6.4.1" style="font-size:90%;">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">心爱的人</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T9.4.4.1"><math alttext="739,194" class="ltx_Math" display="inline" id="A4.T9.4.4.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">739</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">194</mn></mrow><annotation encoding="application/x-tex">739,194</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.5.5.2"><math alttext="327,965" class="ltx_Math" display="inline" id="A4.T9.5.5.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">327</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">965</mn></mrow><annotation encoding="application/x-tex">327,965</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.6.6.3"><math alttext="9.63" class="ltx_Math" display="inline" id="A4.T9.6.6.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">9.63</mn><annotation encoding="application/x-tex">9.63</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T9.9.9.4"><span class="ltx_text ltx_font_italic" id="A4.T9.9.9.4.1" style="font-size:90%;">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">悖论式困境</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T9.7.7.1"><math alttext="1,592,954" class="ltx_Math" display="inline" id="A4.T9.7.7.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">1</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">592</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">954</mn></mrow><annotation encoding="application/x-tex">1,592,954</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.8.8.2"><math alttext="754,857" class="ltx_Math" display="inline" id="A4.T9.8.8.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">754</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">857</mn></mrow><annotation encoding="application/x-tex">754,857</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.9.9.3"><math alttext="18.01" class="ltx_Math" display="inline" id="A4.T9.9.9.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">18.01</mn><annotation encoding="application/x-tex">18.01</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T9.12.12.4"><span class="ltx_text ltx_font_italic" id="A4.T9.12.12.4.1" style="font-size:90%;">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T9.10.10.1"><math alttext="886,282" class="ltx_Math" display="inline" id="A4.T9.10.10.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">886</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">282</mn></mrow><annotation encoding="application/x-tex">886,282</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.11.11.2"><math alttext="396,209" class="ltx_Math" display="inline" id="A4.T9.11.11.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">396</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">209</mn></mrow><annotation encoding="application/x-tex">396,209</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.12.12.3"><math alttext="11.42" class="ltx_Math" display="inline" id="A4.T9.12.12.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">11.42</mn><annotation encoding="application/x-tex">11.42</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T9.15.15.4"><span class="ltx_text ltx_font_italic" id="A4.T9.15.15.4.1" style="font-size:90%;">The Da Vinci Code<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《达芬奇密码》</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T9.13.13.1"><math alttext="444,396" class="ltx_Math" display="inline" id="A4.T9.13.13.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">444</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">396</mn></mrow><annotation encoding="application/x-tex">444,396</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.14.14.2"><math alttext="187,872" class="ltx_Math" display="inline" id="A4.T9.14.14.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">187</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">872</mn></mrow><annotation encoding="application/x-tex">187,872</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.15.15.3"><math alttext="6.85" class="ltx_Math" display="inline" id="A4.T9.15.15.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">6.85</mn><annotation encoding="application/x-tex">6.85</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T9.18.18.4"><span class="ltx_text ltx_font_italic" id="A4.T9.18.18.4.1" style="font-size:90%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《弗兰肯斯坦》</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T9.16.16.1"><math alttext="875,243" class="ltx_Math" display="inline" id="A4.T9.16.16.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">875</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">243</mn></mrow><annotation encoding="application/x-tex">875,243</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.17.17.2"><math alttext="376,304" class="ltx_Math" display="inline" id="A4.T9.17.17.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">376</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">304</mn></mrow><annotation encoding="application/x-tex">376,304</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.18.18.3"><math alttext="13.68" class="ltx_Math" display="inline" id="A4.T9.18.18.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">13.68</mn><annotation encoding="application/x-tex">13.68</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.21.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T9.21.21.4"><span class="ltx_text ltx_font_italic" id="A4.T9.21.21.4.1" style="font-size:90%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T9.19.19.1"><math alttext="722,832" class="ltx_Math" display="inline" id="A4.T9.19.19.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">722</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">832</mn></mrow><annotation encoding="application/x-tex">722,832</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.20.20.2"><math alttext="316,681" class="ltx_Math" display="inline" id="A4.T9.20.20.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">316</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">681</mn></mrow><annotation encoding="application/x-tex">316,681</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.21.21.3"><math alttext="10.67" class="ltx_Math" display="inline" id="A4.T9.21.21.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">10.67</mn><annotation encoding="application/x-tex">10.67</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T9.24.24.4"><span class="ltx_text ltx_font_italic" id="A4.T9.24.24.4.1" style="font-size:90%;">The Great Gatsby<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">了不起的盖茨比</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T9.22.22.1"><math alttext="589,211" class="ltx_Math" display="inline" id="A4.T9.22.22.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">589</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">211</mn></mrow><annotation encoding="application/x-tex">589,211</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.23.23.2"><math alttext="224,118" class="ltx_Math" display="inline" id="A4.T9.23.23.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">224</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">118</mn></mrow><annotation encoding="application/x-tex">224,118</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.24.24.3"><math alttext="11.58" class="ltx_Math" display="inline" id="A4.T9.24.24.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">11.58</mn><annotation encoding="application/x-tex">11.58</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.27.27">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T9.27.27.4"><span class="ltx_text ltx_font_italic" id="A4.T9.27.27.4.1" style="font-size:90%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T9.25.25.1"><math alttext="5,020,702" class="ltx_Math" display="inline" id="A4.T9.25.25.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">5</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">020</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">702</mn></mrow><annotation encoding="application/x-tex">5,020,702</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.26.26.2"><math alttext="2,491,663" class="ltx_Math" display="inline" id="A4.T9.26.26.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">2</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">491</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">663</mn></mrow><annotation encoding="application/x-tex">2,491,663</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.27.27.3"><math alttext="171.26" class="ltx_Math" display="inline" id="A4.T9.27.27.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">171.26</mn><annotation encoding="application/x-tex">171.26</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.30.30">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T9.30.30.4"><span class="ltx_text ltx_font_italic" id="A4.T9.30.30.4.1" style="font-size:90%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T9.28.28.1"><math alttext="347,692" class="ltx_Math" display="inline" id="A4.T9.28.28.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">347</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">692</mn></mrow><annotation encoding="application/x-tex">347,692</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.29.29.2"><math alttext="138,962" class="ltx_Math" display="inline" id="A4.T9.29.29.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">138</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">962</mn></mrow><annotation encoding="application/x-tex">138,962</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.30.30.3"><math alttext="6.24" class="ltx_Math" display="inline" id="A4.T9.30.30.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">6.24</mn><annotation encoding="application/x-tex">6.24</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.33.33">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T9.33.33.4"><span class="ltx_text ltx_font_italic" id="A4.T9.33.33.4.1" style="font-size:90%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T9.31.31.1"><math alttext="637,536" class="ltx_Math" display="inline" id="A4.T9.31.31.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">637</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">536</mn></mrow><annotation encoding="application/x-tex">637,536</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.32.32.2"><math alttext="262,183" class="ltx_Math" display="inline" id="A4.T9.32.32.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">262</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">183</mn></mrow><annotation encoding="application/x-tex">262,183</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T9.33.33.3"><math alttext="10.23" class="ltx_Math" display="inline" id="A4.T9.33.33.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">10.23</mn><annotation encoding="application/x-tex">10.23</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T9.36.36">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A4.T9.36.36.4"><span class="ltx_text ltx_font_italic" id="A4.T9.36.36.4.1" style="font-size:90%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">霍比特人</font></font></font></span></th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T9.34.34.1"><math alttext="594,250" class="ltx_Math" display="inline" id="A4.T9.34.34.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">594</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">250</mn></mrow><annotation encoding="application/x-tex">594,250</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T9.35.35.2"><math alttext="249,418" class="ltx_Math" display="inline" id="A4.T9.35.35.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">249</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">418</mn></mrow><annotation encoding="application/x-tex">249,418</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T9.36.36.3"><math alttext="9.86" class="ltx_Math" display="inline" id="A4.T9.36.36.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">9.86</mn><annotation encoding="application/x-tex">9.86</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 9: </span><span class="ltx_text ltx_font_bold" id="A4.T9.44.1">Phase&nbsp;2 API token usage and estimated cost for Gemini 2.5 Pro (total cost).</span>
For all experiments in Sections&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>, &nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS3" title="4.3 Additional details and experiments concerning LLM-specific configurations ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.3</span></a>, and&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.SS2.SSS2" title="D.2.2 Plots and tables ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D.2.2</span></a>, we report the total per-book Phase 2 input and output tokens
For each book, we sum Phase&nbsp;2 input/output tokens and estimated dollar cost over all Gemini 2.5 Pro Phase&nbsp;2 runs executed as part of our generation configuration parameter sweep.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 9：Gemini 2.5 Pro（总成本）的第二阶段 API 令牌使用量和估算成本。对于 4.2、4.3 和 D.2.2 部分中的所有实验，我们报告了每本书的第二阶段输入和输出令牌总数。对于每本书，我们汇总了作为我们生成配置参数扫描一部分执行的 Gemini 2.5 Pro 第二阶段运行的所有输入/输出令牌和估算美元成本。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A4.T10">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T10.48">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T10.48.49.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A4.T10.48.49.1.1"><span class="ltx_text ltx_font_bold" id="A4.T10.48.49.1.1.1" style="font-size:90%;">Book<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">书</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T10.48.49.1.2"><span class="ltx_text ltx_font_bold" id="A4.T10.48.49.1.2.1" style="font-size:90%;">New input tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">新的输入令牌</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T10.48.49.1.3"><span class="ltx_text ltx_font_bold" id="A4.T10.48.49.1.3.1" style="font-size:90%;">Cached tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">缓存令牌</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T10.48.49.1.4"><span class="ltx_text ltx_font_bold" id="A4.T10.48.49.1.4.1" style="font-size:90%;">Output tokens<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">输出令牌</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A4.T10.48.49.1.5"><span class="ltx_text ltx_font_bold" id="A4.T10.48.49.1.5.1" style="font-size:90%;">Cost ($)<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">成本（美元）</font></font></font></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T10.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T10.4.4.5"><span class="ltx_text ltx_font_italic" id="A4.T10.4.4.5.1" style="font-size:90%;">1984</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T10.1.1.1"><math alttext="227,145" class="ltx_Math" display="inline" id="A4.T10.1.1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">227</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">145</mn></mrow><annotation encoding="application/x-tex">227,145</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T10.2.2.2"><math alttext="208,325" class="ltx_Math" display="inline" id="A4.T10.2.2.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">208</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">325</mn></mrow><annotation encoding="application/x-tex">208,325</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T10.3.3.3"><math alttext="6925" class="ltx_Math" display="inline" id="A4.T10.3.3.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">6925</mn><annotation encoding="application/x-tex">6925</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T10.4.4.4"><math alttext="0.94" class="ltx_Math" display="inline" id="A4.T10.4.4.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">0.94</mn><annotation encoding="application/x-tex">0.94</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T10.8.8.5"><span class="ltx_text ltx_font_italic" id="A4.T10.8.8.5.1" style="font-size:90%;">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">心爱的人</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T10.5.5.1"><math alttext="6,823,988" class="ltx_Math" display="inline" id="A4.T10.5.5.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">6</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">823</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">988</mn></mrow><annotation encoding="application/x-tex">6,823,988</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.6.6.2"><math alttext="6,258,609" class="ltx_Math" display="inline" id="A4.T10.6.6.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">6</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">258</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">609</mn></mrow><annotation encoding="application/x-tex">6,258,609</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.7.7.3"><math alttext="76,854" class="ltx_Math" display="inline" id="A4.T10.7.7.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">76</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">854</mn></mrow><annotation encoding="application/x-tex">76,854</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.8.8.4"><math alttext="26.32" class="ltx_Math" display="inline" id="A4.T10.8.8.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">26.32</mn><annotation encoding="application/x-tex">26.32</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T10.12.12.5"><span class="ltx_text ltx_font_italic" id="A4.T10.12.12.5.1" style="font-size:90%;">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">悖论</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T10.9.9.1"><math alttext="2,096,361" class="ltx_Math" display="inline" id="A4.T10.9.9.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">2</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">096</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">361</mn></mrow><annotation encoding="application/x-tex">2,096,361</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.10.10.2"><math alttext="1,922,674" class="ltx_Math" display="inline" id="A4.T10.10.10.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">1</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">922</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">674</mn></mrow><annotation encoding="application/x-tex">1,922,674</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.11.11.3"><math alttext="9467" class="ltx_Math" display="inline" id="A4.T10.11.11.3.m1" intent=":literal"><semantics><mn mathsize="0.900em">9467</mn><annotation encoding="application/x-tex">9467</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.12.12.4"><math alttext="7.87" class="ltx_Math" display="inline" id="A4.T10.12.12.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">7.87</mn><annotation encoding="application/x-tex">7.87</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T10.16.16.5"><span class="ltx_text ltx_font_italic" id="A4.T10.16.16.5.1" style="font-size:90%;">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T10.13.13.1"><math alttext="40,237,999" class="ltx_Math" display="inline" id="A4.T10.13.13.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">40</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">237</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">999</mn></mrow><annotation encoding="application/x-tex">40,237,999</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.14.14.2"><math alttext="36,904,215" class="ltx_Math" display="inline" id="A4.T10.14.14.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">36</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">904</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">215</mn></mrow><annotation encoding="application/x-tex">36,904,215</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.15.15.3"><math alttext="255,790" class="ltx_Math" display="inline" id="A4.T10.15.15.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">255</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">790</mn></mrow><annotation encoding="application/x-tex">255,790</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.16.16.4"><math alttext="152.23" class="ltx_Math" display="inline" id="A4.T10.16.16.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">152.23</mn><annotation encoding="application/x-tex">152.23</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T10.20.20.5"><span class="ltx_text ltx_font_italic" id="A4.T10.20.20.5.1" style="font-size:90%;">The Da Vinci Code<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《达芬奇密码》</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T10.17.17.1"><math alttext="6,854,930" class="ltx_Math" display="inline" id="A4.T10.17.17.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">6</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">854</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">930</mn></mrow><annotation encoding="application/x-tex">6,854,930</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.18.18.2"><math alttext="6,286,988" class="ltx_Math" display="inline" id="A4.T10.18.18.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">6</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">286</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">988</mn></mrow><annotation encoding="application/x-tex">6,286,988</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.19.19.3"><math alttext="77,049" class="ltx_Math" display="inline" id="A4.T10.19.19.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">77</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">049</mn></mrow><annotation encoding="application/x-tex">77,049</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.20.20.4"><math alttext="26.44" class="ltx_Math" display="inline" id="A4.T10.20.20.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">26.44</mn><annotation encoding="application/x-tex">26.44</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T10.24.24.5"><span class="ltx_text ltx_font_italic" id="A4.T10.24.24.5.1" style="font-size:90%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《弗兰肯斯坦》</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T10.21.21.1"><math alttext="20,700,031" class="ltx_Math" display="inline" id="A4.T10.21.21.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">20</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">700</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">031</mn></mrow><annotation encoding="application/x-tex">20,700,031</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.22.22.2"><math alttext="18,985,000" class="ltx_Math" display="inline" id="A4.T10.22.22.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">18</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">985</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">000</mn></mrow><annotation encoding="application/x-tex">18,985,000</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.23.23.3"><math alttext="51,842" class="ltx_Math" display="inline" id="A4.T10.23.23.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">51</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">842</mn></mrow><annotation encoding="application/x-tex">51,842</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.24.24.4"><math alttext="77.12" class="ltx_Math" display="inline" id="A4.T10.24.24.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">77.12</mn><annotation encoding="application/x-tex">77.12</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.28.28">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T10.28.28.5"><span class="ltx_text ltx_font_italic" id="A4.T10.28.28.5.1" style="font-size:90%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T10.25.25.1"><math alttext="11,139,420" class="ltx_Math" display="inline" id="A4.T10.25.25.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">11</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">139</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">420</mn></mrow><annotation encoding="application/x-tex">11,139,420</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.26.26.2"><math alttext="10,216,501" class="ltx_Math" display="inline" id="A4.T10.26.26.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">10</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">216</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">501</mn></mrow><annotation encoding="application/x-tex">10,216,501</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.27.27.3"><math alttext="85,441" class="ltx_Math" display="inline" id="A4.T10.27.27.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">85</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">441</mn></mrow><annotation encoding="application/x-tex">85,441</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.28.28.4"><math alttext="42.36" class="ltx_Math" display="inline" id="A4.T10.28.28.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">42.36</mn><annotation encoding="application/x-tex">42.36</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.32.32">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T10.32.32.5"><span class="ltx_text ltx_font_italic" id="A4.T10.32.32.5.1" style="font-size:90%;">The Great Gatsby<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">了不起的盖茨比</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T10.29.29.1"><math alttext="6,872,495" class="ltx_Math" display="inline" id="A4.T10.29.29.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">6</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">872</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">495</mn></mrow><annotation encoding="application/x-tex">6,872,495</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.30.30.2"><math alttext="6,303,097" class="ltx_Math" display="inline" id="A4.T10.30.30.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">6</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">303</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">097</mn></mrow><annotation encoding="application/x-tex">6,303,097</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.31.31.3"><math alttext="21,526" class="ltx_Math" display="inline" id="A4.T10.31.31.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">21</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">526</mn></mrow><annotation encoding="application/x-tex">21,526</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.32.32.4"><math alttext="25.67" class="ltx_Math" display="inline" id="A4.T10.32.32.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">25.67</mn><annotation encoding="application/x-tex">25.67</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.36.36">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T10.36.36.5"><span class="ltx_text ltx_font_italic" id="A4.T10.36.36.5.1" style="font-size:90%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T10.33.33.1"><math alttext="2,112,619" class="ltx_Math" display="inline" id="A4.T10.33.33.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">2</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">112</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">619</mn></mrow><annotation encoding="application/x-tex">2,112,619</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.34.34.2"><math alttext="1,937,585" class="ltx_Math" display="inline" id="A4.T10.34.34.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">1</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">937</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">585</mn></mrow><annotation encoding="application/x-tex">1,937,585</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.35.35.3"><math alttext="24,474" class="ltx_Math" display="inline" id="A4.T10.35.35.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">24</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">474</mn></mrow><annotation encoding="application/x-tex">24,474</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.36.36.4"><math alttext="8.16" class="ltx_Math" display="inline" id="A4.T10.36.36.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">8.16</mn><annotation encoding="application/x-tex">8.16</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.40.40">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T10.40.40.5"><span class="ltx_text ltx_font_italic" id="A4.T10.40.40.5.1" style="font-size:90%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T10.37.37.1"><math alttext="17,879,681" class="ltx_Math" display="inline" id="A4.T10.37.37.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">17</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">879</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">681</mn></mrow><annotation encoding="application/x-tex">17,879,681</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.38.38.2"><math alttext="16,398,320" class="ltx_Math" display="inline" id="A4.T10.38.38.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">16</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">398</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">320</mn></mrow><annotation encoding="application/x-tex">16,398,320</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.39.39.3"><math alttext="67,318" class="ltx_Math" display="inline" id="A4.T10.39.39.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">67</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">318</mn></mrow><annotation encoding="application/x-tex">67,318</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.40.40.4"><math alttext="66.95" class="ltx_Math" display="inline" id="A4.T10.40.40.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">66.95</mn><annotation encoding="application/x-tex">66.95</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.44.44">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A4.T10.44.44.5"><span class="ltx_text ltx_font_italic" id="A4.T10.44.44.5.1" style="font-size:90%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></th>
<td class="ltx_td ltx_align_right" id="A4.T10.41.41.1"><math alttext="44,684,724" class="ltx_Math" display="inline" id="A4.T10.41.41.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">44</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">684</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">724</mn></mrow><annotation encoding="application/x-tex">44,684,724</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.42.42.2"><math alttext="40,982,521" class="ltx_Math" display="inline" id="A4.T10.42.42.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">40</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">982</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">521</mn></mrow><annotation encoding="application/x-tex">40,982,521</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.43.43.3"><math alttext="197,828" class="ltx_Math" display="inline" id="A4.T10.43.43.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">197</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">828</mn></mrow><annotation encoding="application/x-tex">197,828</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right" id="A4.T10.44.44.4"><math alttext="167.76" class="ltx_Math" display="inline" id="A4.T10.44.44.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">167.76</mn><annotation encoding="application/x-tex">167.76</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T10.48.48">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A4.T10.48.48.5"><span class="ltx_text ltx_font_italic" id="A4.T10.48.48.5.1" style="font-size:90%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">霍比特人</font></font></font></span></th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T10.45.45.1"><math alttext="6,168,011" class="ltx_Math" display="inline" id="A4.T10.45.45.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">6</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">168</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">011</mn></mrow><annotation encoding="application/x-tex">6,168,011</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T10.46.46.2"><math alttext="5,656,982" class="ltx_Math" display="inline" id="A4.T10.46.46.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">5</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">656</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">982</mn></mrow><annotation encoding="application/x-tex">5,656,982</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T10.47.47.3"><math alttext="43,374" class="ltx_Math" display="inline" id="A4.T10.47.47.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.900em">43</mn><mo mathsize="0.900em">,</mo><mn mathsize="0.900em">374</mn></mrow><annotation encoding="application/x-tex">43,374</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A4.T10.48.48.4"><math alttext="23.40" class="ltx_Math" display="inline" id="A4.T10.48.48.4.m1" intent=":literal"><semantics><mn mathsize="0.900em">23.40</mn><annotation encoding="application/x-tex">23.40</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 10: </span><span class="ltx_text ltx_font_bold" id="A4.T10.54.1">Phase&nbsp;2 API token usage and estimated cost for Grok 3.</span>
For the main experiments in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we report Phase&nbsp;2 new input tokens, cached tokens, output tokens, and total dollar cost charged by the Grok 3 API.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 10：Grok 3 第二阶段 API 令牌使用情况及估算成本。对于 4.2 节中的主要实验，我们报告了 Grok 3 第二阶段的新输入令牌、缓存令牌、输出令牌以及 API 收取的总美元成本。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_subsubsection" id="A4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.2.2 </span>Plots and tables<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">D.2.2 图表和表格</font></font></font></h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A4.SS2.SSS2.p1">
<p class="ltx_p" id="A4.SS2.SSS2.p1.6">We provide corresponding absolute word count plots for the eight books we do not include in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F7" title="Figure 7 ‣ 4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a> (Figures&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.F12" title="Figure 12 ‣ D.2.2 Plots and tables ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">12</span></a> &amp;&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.F13" title="Figure 13 ‣ D.2.2 Plots and tables ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">13</span></a>).
In Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.T11" title="Table 11 ‣ D.2.2 Plots and tables ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">11</span></a>, we also include a table reporting precise numbers for <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="A4.SS2.SSS2.p1.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math>, <math alttext="m" class="ltx_Math" display="inline" id="A4.SS2.SSS2.p1.2.m2" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>, <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="A4.SS2.SSS2.p1.3.m3" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math>, and <math alttext="\mathsf{missing}" class="ltx_Math" display="inline" id="A4.SS2.SSS2.p1.4.m4" intent=":literal"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> for all of our main experiments in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.SS2" title="4.2 High-level extraction outcomes ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4.F14" title="Figure 14 ‣ D.2.2 Plots and tables ‣ D.2 Additional Phase 2 results ‣ Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">14</span></a>, we provide full results on how <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="A4.SS2.SSS2.p1.5.m5" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> varied for each book, with respect to <math alttext="9" class="ltx_Math" display="inline" id="A4.SS2.SSS2.p1.6.m6" intent=":literal"><semantics><mn>9</mn><annotation encoding="application/x-tex">9</annotation></semantics></math> generation configurations tested for Gemini 2.5 Pro.<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">我们为图 7 中未包含的八本书提供了相应的绝对词数图（图 12 和图 13）。在表 11 中，我们还包含一个表格，报告了我们第 4.2 节所有主要实验中 <math intent=":literal" id="A4.SS2.SSS2.p1.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 、 <math intent=":literal" id="A4.SS2.SSS2.p1.2.m2" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> 、 <math intent=":literal" id="A4.SS2.SSS2.p1.3.m3" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 和 <math intent=":literal" id="A4.SS2.SSS2.p1.4.m4" display="inline" class="ltx_Math" alttext="\mathsf{missing}"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> 的精确数值。在图 14 中，我们提供了关于 <math intent=":literal" id="A4.SS2.SSS2.p1.5.m5" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 如何随每本书变化的全结果，这是针对为 Gemini 2.5 Pro 测试的 <math intent=":literal" id="A4.SS2.SSS2.p1.6.m6" display="inline" class="ltx_Math" alttext="9"><semantics><mn>9</mn><annotation encoding="application/x-tex">9</annotation></semantics></math> 生成配置而言的。</font></font></font></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A4.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F12.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="354" id="A4.F12.sf1.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_1984.png" width="648">
<figcaption class="ltx_caption ltx_centering" data-imt_insert_failed="1"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F12.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_italic" id="A4.F12.sf1.4.2" style="font-size:90%;">1984<span class="ltx_text ltx_font_upright" id="A4.F12.sf1.4.2.1"> </span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F12.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="353" id="A4.F12.sf2.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_beloved.png" width="648">
<figcaption class="ltx_caption ltx_centering" data-imt_insert_failed="1"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F12.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_italic" id="A4.F12.sf2.4.2" style="font-size:90%;">Beloved<span class="ltx_text ltx_font_upright" id="A4.F12.sf2.4.2.1"> </span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F12.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="354" id="A4.F12.sf3.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_catch_22.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F12.sf3.3.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text ltx_font_italic" id="A4.F12.sf3.4.2" style="font-size:90%;">Catch-22<span class="ltx_text ltx_font_upright" id="A4.F12.sf3.4.2.1"> </span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(c) 捕鼠夹</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F12.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="353" id="A4.F12.sf4.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_catcher_in_the_rye.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F12.sf4.3.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text ltx_font_italic" id="A4.F12.sf4.4.2" style="font-size:90%;">The Catcher in the Rye<span class="ltx_text ltx_font_upright" id="A4.F12.sf4.4.2.1"> </span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(d) 《麦田里的守望者》</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F12.14.6.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text ltx_font_bold" id="A4.F12.10.5" style="font-size:90%;">Absolute word counts.<span class="ltx_text ltx_font_medium" id="A4.F12.10.5.5">
For the Phase 2 runs for four books in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>, we show the count <math alttext="m" class="ltx_Math" display="inline" id="A4.F12.6.1.1.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E6" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">6</span></a>) of extracted words, as well as the estimated counts of words in the book that are <math alttext="\mathsf{missing}" class="ltx_Math" display="inline" id="A4.F12.7.2.2.m2" intent=":literal"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> in the generated text and words in the generated text that are <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="A4.F12.8.3.3.m3" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> with respect to the book (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E8" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">8</span></a>).
In each plot, the dotted gray line indicates the length of the book in words (<math alttext="|B|" class="ltx_Math" display="inline" id="A4.F12.9.4.4.m4" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|B|</annotation></semantics></math>).
We provide results for other books in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4" title="Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D</span></a>.
<math alttext="\dagger" class="ltx_Math" display="inline" id="A4.F12.10.5.5.m5" intent=":literal"><semantics><mo>†</mo><annotation encoding="application/x-tex">\dagger</annotation></semantics></math> indicates Phase 1 failure.
<span class="ltx_text ltx_font_italic" id="A4.F12.10.5.5.1">Note: The underlying generation configuration is fixed per LLM across books, but varies across LLMs.
Each per-LLM set of bars conveys counts observed for the given LLM <em class="ltx_emph ltx_font_upright" id="A4.F12.10.5.5.1.1">with respect to these configurations</em>;
for a given book, the sets of bars do not reflect comparisons of results obtained from testing all production LLMs under the same conditions.</span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 12：绝对词频。对于图 5 中四个书籍的 Phase 2 运行，我们展示了提取词的计数 <math intent=":literal" id="A4.F12.6.1.1.m1" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> （方程 6），以及书中 <math intent=":literal" id="A4.F12.7.2.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{missing}"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> 在生成文本中的词的估计计数和生成文本中相对于书 <math intent=":literal" id="A4.F12.8.3.3.m3" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 的词的估计计数（方程 8）。在每个图中，虚线灰色线表示书的词数长度 <math intent=":literal" id="A4.F12.9.4.4.m4" display="inline" class="ltx_Math" alttext="|B|"><semantics><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|B|</annotation></semantics></math> 。我们在附录 D 中提供了其他书籍的结果。 <math intent=":literal" id="A4.F12.10.5.5.m5" display="inline" class="ltx_Math" alttext="\dagger"><semantics><mo>†</mo><annotation encoding="application/x-tex">\dagger</annotation></semantics></math> 表示 Phase 1 失败。注意：底层生成配置在每个 LLM 中跨书籍是固定的，但在不同 LLM 间是变化的。每个针对 LLM 的条形图集展示了给定 LLM 在这些配置下的观察计数；对于给定书籍，条形图集并不反映在相同条件下测试所有生产 LLM 获得的结果的比较。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F13.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="354" id="A4.F13.sf1.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_da_vinci_code.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F13.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_italic" id="A4.F13.sf1.4.2" style="font-size:90%;">The Da Vinci Code<span class="ltx_text ltx_font_upright" id="A4.F13.sf1.4.2.1"> </span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(a) 《达芬奇密码》</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F13.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="353" id="A4.F13.sf2.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_great_gatsby.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F13.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_italic" id="A4.F13.sf2.4.2" style="font-size:90%;">The Great Gatsby<span class="ltx_text ltx_font_upright" id="A4.F13.sf2.4.2.1"> </span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(b) 《了不起的盖茨比》</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F13.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="354" id="A4.F13.sf3.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_harry_potter_goblet.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F13.sf3.3.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text ltx_font_italic" id="A4.F13.sf3.4.2" style="font-size:90%;">Harry Potter and the Goblet of Fire<span class="ltx_text ltx_font_upright" id="A4.F13.sf3.4.2.1"> </span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(c) 《哈利·波特与火焰杯》</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A4.F13.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="354" id="A4.F13.sf4.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/word_stats_hunger_games.png" width="648">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F13.sf4.3.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text ltx_font_italic" id="A4.F13.sf4.4.2" style="font-size:90%;">The Hunger Games<span class="ltx_text ltx_font_upright" id="A4.F13.sf4.4.2.1"> </span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">(d) 《饥饿游戏》</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F13.16.7.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text ltx_font_bold" id="A4.F13.12.6" style="font-size:90%;">Absolute word counts.<span class="ltx_text ltx_font_medium" id="A4.F13.12.6.6">
For the Phase 2 runs for four books in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>, we show the count <math alttext="m" class="ltx_Math" display="inline" id="A4.F13.7.1.1.m1" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E6" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">6</span></a>) of extracted words, as well as the estimated counts of words in the book that are <math alttext="\mathsf{missing}" class="ltx_Math" display="inline" id="A4.F13.8.2.2.m2" intent=":literal"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> in the generated text and words in the generated text that are <math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="A4.F13.9.3.3.m3" intent=":literal"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> with respect to the book (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E8" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">8</span></a>).
In each plot, the dotted gray line indicates the length of the book in words (<math alttext="|B|" class="ltx_Math" display="inline" id="A4.F13.10.4.4.m4" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|B|</annotation></semantics></math>).
We provide results for other books in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A4" title="Appendix D Extended results ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">D</span></a>.
<math alttext="\dagger" class="ltx_Math" display="inline" id="A4.F13.11.5.5.m5" intent=":literal"><semantics><mo>†</mo><annotation encoding="application/x-tex">\dagger</annotation></semantics></math> indicates Phase 1 failure;
<math alttext="*" class="ltx_Math" display="inline" id="A4.F13.12.6.6.m6" intent=":literal"><semantics><mo>∗</mo><annotation encoding="application/x-tex">*</annotation></semantics></math> indicates that we did not run Phase 2.
<span class="ltx_text ltx_font_italic" id="A4.F13.12.6.6.1">Note: The underlying generation configuration is fixed per LLM across books, but varies across LLMs.
Each per-LLM set of bars conveys counts observed for the given LLM <em class="ltx_emph ltx_font_upright" id="A4.F13.12.6.6.1.1">with respect to these configurations</em>;
for a given book, the sets of bars do not reflect comparisons of results obtained from testing all production LLMs under the same conditions.</span></span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 13：绝对词数。对于图 5 中四个书籍的 Phase 2 运行，我们展示了提取词的计数 <math intent=":literal" id="A4.F13.7.1.1.m1" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> （方程 6），以及书中 <math intent=":literal" id="A4.F13.8.2.2.m2" display="inline" class="ltx_Math" alttext="\mathsf{missing}"><semantics><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math> 在生成文本中的词的估计计数和生成文本中相对于书 <math intent=":literal" id="A4.F13.9.3.3.m3" display="inline" class="ltx_Math" alttext="\mathsf{additional}"><semantics><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math> 的词的计数（方程 8）。在每个图中，虚线灰色线表示书的词数长度 <math intent=":literal" id="A4.F13.10.4.4.m4" display="inline" class="ltx_Math" alttext="|B|"><semantics><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|B|</annotation></semantics></math> 。我们在附录 D 中提供了其他书籍的结果。 <math intent=":literal" id="A4.F13.11.5.5.m5" display="inline" class="ltx_Math" alttext="\dagger"><semantics><mo>†</mo><annotation encoding="application/x-tex">\dagger</annotation></semantics></math> 表示 Phase 1 失败； <math intent=":literal" id="A4.F13.12.6.6.m6" display="inline" class="ltx_Math" alttext="*"><semantics><mo>∗</mo><annotation encoding="application/x-tex">*</annotation></semantics></math> 表示我们没有运行 Phase 2。注意：底层生成配置在每个 LLM 中跨书籍是固定的，但在不同 LLM 之间是变化的。每个针对 LLM 的条形图集传达了给定 LLM 相对于这些配置观察到的计数；对于给定书籍，条形图集并不反映在相同条件下测试所有生产 LLM 获得的结果的比较。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A4.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T11.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T11.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A4.T11.6.6.7"><span class="ltx_text" id="A4.T11.6.6.7.1" style="font-size:70%;">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A4.T11.6.6.8"><span class="ltx_text" id="A4.T11.6.6.8.1" style="font-size:70%;">Book<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">书</font></font></font></span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1"><math alttext="|B|" class="ltx_Math" display="inline" id="A4.T11.1.1.1.m1" intent=":literal"><semantics><mrow><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo><mi mathsize="0.700em">B</mi><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo></mrow><annotation encoding="application/x-tex">|B|</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A4.T11.2.2.2"><math alttext="|G|" class="ltx_Math" display="inline" id="A4.T11.2.2.2.m1" intent=":literal"><semantics><mrow><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo><mi mathsize="0.700em">G</mi><mo maxsize="0.700em" minsize="0.700em" stretchy="true">|</mo></mrow><annotation encoding="application/x-tex">|G|</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A4.T11.3.3.3">
<span class="ltx_text" id="A4.T11.3.3.3.1" style="font-size:70%;">Matched (</span><math alttext="m" class="ltx_Math" display="inline" id="A4.T11.3.3.3.m1" intent=":literal"><semantics><mi mathsize="0.700em">m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">匹配 <math intent=":literal" id="A4.T11.3.3.3.m1" display="inline" class="ltx_Math" alttext="m"><semantics><mi mathsize="0.700em">m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> </font></font></font><span class="ltx_text" id="A4.T11.3.3.3.2" style="font-size:70%;">)</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A4.T11.4.4.4"><math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="A4.T11.4.4.4.m1" intent=":literal"><semantics><mrow><mi mathsize="0.700em">𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext mathsize="0.700em">-</mtext><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.700em">𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A4.T11.5.5.5"><math alttext="\mathsf{missing}" class="ltx_Math" display="inline" id="A4.T11.5.5.5.m1" intent=":literal"><semantics><mi mathsize="0.700em">𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><annotation encoding="application/x-tex">\mathsf{missing}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A4.T11.6.6.6"><math alttext="\mathsf{additional}" class="ltx_Math" display="inline" id="A4.T11.6.6.6.m1" intent=":literal"><semantics><mi mathsize="0.700em">𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><annotation encoding="application/x-tex">\mathsf{additional}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T11.6.7.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T11.6.7.1.1"><span class="ltx_text" id="A4.T11.6.7.1.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T11.6.7.1.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.7.1.2.1" style="font-size:70%;">1984</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T11.6.7.1.3"><span class="ltx_text" id="A4.T11.6.7.1.3.1" style="font-size:70%;">100,024</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T11.6.7.1.4"><span class="ltx_text" id="A4.T11.6.7.1.4.1" style="font-size:70%;">99,071</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T11.6.7.1.5"><span class="ltx_text" id="A4.T11.6.7.1.5.1" style="font-size:70%;">95,512</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T11.6.7.1.6"><span class="ltx_text" id="A4.T11.6.7.1.6.1" style="font-size:70%;">0.955</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T11.6.7.1.7"><span class="ltx_text" id="A4.T11.6.7.1.7.1" style="font-size:70%;">4,512</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A4.T11.6.7.1.8"><span class="ltx_text" id="A4.T11.6.7.1.8.1" style="font-size:70%;">3,559</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.8.2">
<td class="ltx_td ltx_align_left" id="A4.T11.6.8.2.1"><span class="ltx_text" id="A4.T11.6.8.2.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.8.2.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.8.2.2.1" style="font-size:70%;">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《宠儿》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.8.2.3"><span class="ltx_text" id="A4.T11.6.8.2.3.1" style="font-size:70%;">97,759</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.8.2.4"><span class="ltx_text" id="A4.T11.6.8.2.4.1" style="font-size:70%;">101,813</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.8.2.5"><span class="ltx_text" id="A4.T11.6.8.2.5.1" style="font-size:70%;">1,957</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.8.2.6"><span class="ltx_text" id="A4.T11.6.8.2.6.1" style="font-size:70%;">0.020</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.8.2.7"><span class="ltx_text" id="A4.T11.6.8.2.7.1" style="font-size:70%;">95,802</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.8.2.8"><span class="ltx_text" id="A4.T11.6.8.2.8.1" style="font-size:70%;">99,856</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.9.3">
<td class="ltx_td ltx_align_left" id="A4.T11.6.9.3.1"><span class="ltx_text" id="A4.T11.6.9.3.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.9.3.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.9.3.2.1" style="font-size:70%;">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">悖论</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.9.3.3"><span class="ltx_text" id="A4.T11.6.9.3.3.1" style="font-size:70%;">174,344</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.9.3.4"><span class="ltx_text" id="A4.T11.6.9.3.4.1" style="font-size:70%;">74,597</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.9.3.5"><span class="ltx_text" id="A4.T11.6.9.3.5.1" style="font-size:70%;">243</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.9.3.6"><span class="ltx_text" id="A4.T11.6.9.3.6.1" style="font-size:70%;">0.001</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.9.3.7"><span class="ltx_text" id="A4.T11.6.9.3.7.1" style="font-size:70%;">174,101</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.9.3.8"><span class="ltx_text" id="A4.T11.6.9.3.8.1" style="font-size:70%;">74,354</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.10.4">
<td class="ltx_td ltx_align_left" id="A4.T11.6.10.4.1"><span class="ltx_text" id="A4.T11.6.10.4.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.10.4.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.10.4.2.1" style="font-size:70%;">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.10.4.3"><span class="ltx_text" id="A4.T11.6.10.4.3.1" style="font-size:70%;">73,566</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.10.4.4"><span class="ltx_text" id="A4.T11.6.10.4.4.1" style="font-size:70%;">26,323</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.10.4.5"><span class="ltx_text" id="A4.T11.6.10.4.5.1" style="font-size:70%;">7,396</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.10.4.6"><span class="ltx_text" id="A4.T11.6.10.4.6.1" style="font-size:70%;">0.101</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.10.4.7"><span class="ltx_text" id="A4.T11.6.10.4.7.1" style="font-size:70%;">66,170</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.10.4.8"><span class="ltx_text" id="A4.T11.6.10.4.8.1" style="font-size:70%;">18,927</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.11.5">
<td class="ltx_td ltx_align_left" id="A4.T11.6.11.5.1"><span class="ltx_text" id="A4.T11.6.11.5.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.11.5.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.11.5.2.1" style="font-size:70%;">The Da Vinci Code<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">达芬奇密码</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.11.5.3"><span class="ltx_text" id="A4.T11.6.11.5.3.1" style="font-size:70%;">139,537</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.11.5.4"><span class="ltx_text" id="A4.T11.6.11.5.4.1" style="font-size:70%;">87,552</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.11.5.5"><span class="ltx_text" id="A4.T11.6.11.5.5.1" style="font-size:70%;">1,081</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.11.5.6"><span class="ltx_text" id="A4.T11.6.11.5.6.1" style="font-size:70%;">0.008</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.11.5.7"><span class="ltx_text" id="A4.T11.6.11.5.7.1" style="font-size:70%;">138,456</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.11.5.8"><span class="ltx_text" id="A4.T11.6.11.5.8.1" style="font-size:70%;">86,471</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.12.6">
<td class="ltx_td ltx_align_left" id="A4.T11.6.12.6.1"><span class="ltx_text" id="A4.T11.6.12.6.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.12.6.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.12.6.2.1" style="font-size:70%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">弗兰肯斯坦</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.12.6.3"><span class="ltx_text" id="A4.T11.6.12.6.3.1" style="font-size:70%;">69,704</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.12.6.4"><span class="ltx_text" id="A4.T11.6.12.6.4.1" style="font-size:70%;">69,353</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.12.6.5"><span class="ltx_text" id="A4.T11.6.12.6.5.1" style="font-size:70%;">65,714</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.12.6.6"><span class="ltx_text" id="A4.T11.6.12.6.6.1" style="font-size:70%;">0.943</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.12.6.7"><span class="ltx_text" id="A4.T11.6.12.6.7.1" style="font-size:70%;">3,990</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.12.6.8"><span class="ltx_text" id="A4.T11.6.12.6.8.1" style="font-size:70%;">3,639</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.13.7">
<td class="ltx_td ltx_align_left" id="A4.T11.6.13.7.1"><span class="ltx_text" id="A4.T11.6.13.7.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.13.7.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.13.7.2.1" style="font-size:70%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.13.7.3"><span class="ltx_text" id="A4.T11.6.13.7.3.1" style="font-size:70%;">292,416</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.13.7.4"><span class="ltx_text" id="A4.T11.6.13.7.4.1" style="font-size:70%;">92,569</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.13.7.5"><span class="ltx_text" id="A4.T11.6.13.7.5.1" style="font-size:70%;">16,501</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.13.7.6"><span class="ltx_text" id="A4.T11.6.13.7.6.1" style="font-size:70%;">0.056</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.13.7.7"><span class="ltx_text" id="A4.T11.6.13.7.7.1" style="font-size:70%;">275,915</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.13.7.8"><span class="ltx_text" id="A4.T11.6.13.7.8.1" style="font-size:70%;">76,068</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.14.8">
<td class="ltx_td ltx_align_left" id="A4.T11.6.14.8.1"><span class="ltx_text" id="A4.T11.6.14.8.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.14.8.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.14.8.2.1" style="font-size:70%;">The Great Gatsby<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">了不起的盖茨比</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.14.8.3"><span class="ltx_text" id="A4.T11.6.14.8.3.1" style="font-size:70%;">48,177</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.14.8.4"><span class="ltx_text" id="A4.T11.6.14.8.4.1" style="font-size:70%;">52,192</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.14.8.5"><span class="ltx_text" id="A4.T11.6.14.8.5.1" style="font-size:70%;">46,972</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.14.8.6"><span class="ltx_text" id="A4.T11.6.14.8.6.1" style="font-size:70%;">0.975</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.14.8.7"><span class="ltx_text" id="A4.T11.6.14.8.7.1" style="font-size:70%;">1,205</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.14.8.8"><span class="ltx_text" id="A4.T11.6.14.8.8.1" style="font-size:70%;">5,220</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.15.9">
<td class="ltx_td ltx_align_left" id="A4.T11.6.15.9.1"><span class="ltx_text" id="A4.T11.6.15.9.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.15.9.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.15.9.2.1" style="font-size:70%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.15.9.3"><span class="ltx_text" id="A4.T11.6.15.9.3.1" style="font-size:70%;">82,382</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.15.9.4"><span class="ltx_text" id="A4.T11.6.15.9.4.1" style="font-size:70%;">78,422</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.15.9.5"><span class="ltx_text" id="A4.T11.6.15.9.5.1" style="font-size:70%;">76,001</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.15.9.6"><span class="ltx_text" id="A4.T11.6.15.9.6.1" style="font-size:70%;">0.923</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.15.9.7"><span class="ltx_text" id="A4.T11.6.15.9.7.1" style="font-size:70%;">6,381</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.15.9.8"><span class="ltx_text" id="A4.T11.6.15.9.8.1" style="font-size:70%;">2,421</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.16.10">
<td class="ltx_td ltx_align_left" id="A4.T11.6.16.10.1"><span class="ltx_text" id="A4.T11.6.16.10.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.16.10.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.16.10.2.1" style="font-size:70%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.16.10.3"><span class="ltx_text" id="A4.T11.6.16.10.3.1" style="font-size:70%;">198,267</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.16.10.4"><span class="ltx_text" id="A4.T11.6.16.10.4.1" style="font-size:70%;">96,703</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.16.10.5"><span class="ltx_text" id="A4.T11.6.16.10.5.1" style="font-size:70%;">70,660</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.16.10.6"><span class="ltx_text" id="A4.T11.6.16.10.6.1" style="font-size:70%;">0.356</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.16.10.7"><span class="ltx_text" id="A4.T11.6.16.10.7.1" style="font-size:70%;">127,607</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.16.10.8"><span class="ltx_text" id="A4.T11.6.16.10.8.1" style="font-size:70%;">26,043</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.17.11">
<td class="ltx_td ltx_align_left" id="A4.T11.6.17.11.1"><span class="ltx_text" id="A4.T11.6.17.11.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.17.11.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.17.11.2.1" style="font-size:70%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.17.11.3"><span class="ltx_text" id="A4.T11.6.17.11.3.1" style="font-size:70%;">99,964</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.17.11.4"><span class="ltx_text" id="A4.T11.6.17.11.4.1" style="font-size:70%;">105,854</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.17.11.5"><span class="ltx_text" id="A4.T11.6.17.11.5.1" style="font-size:70%;">32,581</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.17.11.6"><span class="ltx_text" id="A4.T11.6.17.11.6.1" style="font-size:70%;">0.326</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.17.11.7"><span class="ltx_text" id="A4.T11.6.17.11.7.1" style="font-size:70%;">67,383</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.17.11.8"><span class="ltx_text" id="A4.T11.6.17.11.8.1" style="font-size:70%;">73,273</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.18.12">
<td class="ltx_td ltx_align_left" id="A4.T11.6.18.12.1"><span class="ltx_text" id="A4.T11.6.18.12.1.1" style="font-size:70%;" data-imt_insert_failed="1">Claude 3.7 Sonnet</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.18.12.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.18.12.2.1" style="font-size:70%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">霍比特人</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.18.12.3"><span class="ltx_text" id="A4.T11.6.18.12.3.1" style="font-size:70%;">95,343</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.18.12.4"><span class="ltx_text" id="A4.T11.6.18.12.4.1" style="font-size:70%;">167,153</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.18.12.5"><span class="ltx_text" id="A4.T11.6.18.12.5.1" style="font-size:70%;">66,891</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.18.12.6"><span class="ltx_text" id="A4.T11.6.18.12.6.1" style="font-size:70%;">0.702</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.18.12.7"><span class="ltx_text" id="A4.T11.6.18.12.7.1" style="font-size:70%;">28,452</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.18.12.8"><span class="ltx_text" id="A4.T11.6.18.12.8.1" style="font-size:70%;">100,262</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.19.13">
<td class="ltx_td ltx_align_left" id="A4.T11.6.19.13.1"><span class="ltx_text" id="A4.T11.6.19.13.1.1" style="font-size:70%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.19.13.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.19.13.2.1" style="font-size:70%;">1984</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.19.13.3"><span class="ltx_text" id="A4.T11.6.19.13.3.1" style="font-size:70%;">100,024</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.19.13.4"><span class="ltx_text" id="A4.T11.6.19.13.4.1" style="font-size:70%;">29,873</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.19.13.5"><span class="ltx_text" id="A4.T11.6.19.13.5.1" style="font-size:70%;">5,913</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.19.13.6"><span class="ltx_text" id="A4.T11.6.19.13.6.1" style="font-size:70%;">0.059</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.19.13.7"><span class="ltx_text" id="A4.T11.6.19.13.7.1" style="font-size:70%;">94,111</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.19.13.8"><span class="ltx_text" id="A4.T11.6.19.13.8.1" style="font-size:70%;">23,960</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.20.14">
<td class="ltx_td ltx_align_left" id="A4.T11.6.20.14.1"><span class="ltx_text" id="A4.T11.6.20.14.1.1" style="font-size:70%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.20.14.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.20.14.2.1" style="font-size:70%;">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">挚爱</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.20.14.3"><span class="ltx_text" id="A4.T11.6.20.14.3.1" style="font-size:70%;">97,759</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.20.14.4"><span class="ltx_text" id="A4.T11.6.20.14.4.1" style="font-size:70%;">7,421</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.20.14.5"><span class="ltx_text" id="A4.T11.6.20.14.5.1" style="font-size:70%;">360</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.20.14.6"><span class="ltx_text" id="A4.T11.6.20.14.6.1" style="font-size:70%;">0.004</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.20.14.7"><span class="ltx_text" id="A4.T11.6.20.14.7.1" style="font-size:70%;">97,399</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.20.14.8"><span class="ltx_text" id="A4.T11.6.20.14.8.1" style="font-size:70%;">7,061</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.21.15">
<td class="ltx_td ltx_align_left" id="A4.T11.6.21.15.1"><span class="ltx_text" id="A4.T11.6.21.15.1.1" style="font-size:70%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.21.15.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.21.15.2.1" style="font-size:70%;">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">悖论</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.21.15.3"><span class="ltx_text" id="A4.T11.6.21.15.3.1" style="font-size:70%;">174,344</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.21.15.4"><span class="ltx_text" id="A4.T11.6.21.15.4.1" style="font-size:70%;">17,092</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.21.15.5"><span class="ltx_text" id="A4.T11.6.21.15.5.1" style="font-size:70%;">157</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.21.15.6"><span class="ltx_text" id="A4.T11.6.21.15.6.1" style="font-size:70%;">0.001</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.21.15.7"><span class="ltx_text" id="A4.T11.6.21.15.7.1" style="font-size:70%;">174,187</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.21.15.8"><span class="ltx_text" id="A4.T11.6.21.15.8.1" style="font-size:70%;">16,935</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.22.16">
<td class="ltx_td ltx_align_left" id="A4.T11.6.22.16.1"><span class="ltx_text" id="A4.T11.6.22.16.1.1" style="font-size:70%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.22.16.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.22.16.2.1" style="font-size:70%;">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.22.16.3"><span class="ltx_text" id="A4.T11.6.22.16.3.1" style="font-size:70%;">73,566</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.22.16.4"><span class="ltx_text" id="A4.T11.6.22.16.4.1" style="font-size:70%;">3,165</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.22.16.5"><span class="ltx_text" id="A4.T11.6.22.16.5.1" style="font-size:70%;">701</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.22.16.6"><span class="ltx_text" id="A4.T11.6.22.16.6.1" style="font-size:70%;">0.010</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.22.16.7"><span class="ltx_text" id="A4.T11.6.22.16.7.1" style="font-size:70%;">72,865</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.22.16.8"><span class="ltx_text" id="A4.T11.6.22.16.8.1" style="font-size:70%;">2,464</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.23.17">
<td class="ltx_td ltx_align_left" id="A4.T11.6.23.17.1"><span class="ltx_text" id="A4.T11.6.23.17.1.1" style="font-size:70%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.23.17.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.23.17.2.1" style="font-size:70%;">The Da Vinci Code<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">达芬奇密码</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.23.17.3"><span class="ltx_text" id="A4.T11.6.23.17.3.1" style="font-size:70%;">139,537</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.23.17.4"><span class="ltx_text" id="A4.T11.6.23.17.4.1" style="font-size:70%;">16,979</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.23.17.5"><span class="ltx_text" id="A4.T11.6.23.17.5.1" style="font-size:70%;">0</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.23.17.6"><span class="ltx_text" id="A4.T11.6.23.17.6.1" style="font-size:70%;">0.000</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.23.17.7"><span class="ltx_text" id="A4.T11.6.23.17.7.1" style="font-size:70%;">139,537</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.23.17.8"><span class="ltx_text" id="A4.T11.6.23.17.8.1" style="font-size:70%;">16,979</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.24.18">
<td class="ltx_td ltx_align_left" id="A4.T11.6.24.18.1"><span class="ltx_text" id="A4.T11.6.24.18.1.1" style="font-size:70%;">Gemini 2.5 Pro<font data-immersive-translate-error-id="469" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.24.18.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.24.18.2.1" style="font-size:70%;">Frankenstein<font data-immersive-translate-error-id="470" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.24.18.3"><span class="ltx_text" id="A4.T11.6.24.18.3.1" style="font-size:70%;">69,704</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.24.18.4"><span class="ltx_text" id="A4.T11.6.24.18.4.1" style="font-size:70%;">6,145</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.24.18.5"><span class="ltx_text" id="A4.T11.6.24.18.5.1" style="font-size:70%;">1,684</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.24.18.6"><span class="ltx_text" id="A4.T11.6.24.18.6.1" style="font-size:70%;">0.024</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.24.18.7"><span class="ltx_text" id="A4.T11.6.24.18.7.1" style="font-size:70%;">68,020</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.24.18.8"><span class="ltx_text" id="A4.T11.6.24.18.8.1" style="font-size:70%;">4,461</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.25.19">
<td class="ltx_td ltx_align_left" id="A4.T11.6.25.19.1"><span class="ltx_text" id="A4.T11.6.25.19.1.1" style="font-size:70%;">Gemini 2.5 Pro<font data-immersive-translate-error-id="471" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.25.19.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.25.19.2.1" style="font-size:70%;">A Game of Thrones<font data-immersive-translate-error-id="472" class="notranslate immersive-translate-target-wrapper immersive-translate-target-wrapper-error" translate="no" lang="zh-CN"><a href="javascript:void(0)"><font class="immersive-translate-error notranslate"><font class="immersive-translate-error-wrapper"><font class="immersive-translate-clickable-button notranslate" title="重试全部错误段落" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path d="M35.9387 5.48805C35.9166 4.60421 35.2434 4.04719 34.279 4.0675C33.3131 4.0878 32.8154 4.67712 32.6567 5.56132C32.5745 6.01985 32.601 6.49957 32.5962 6.96997C32.5881 7.77251 32.594 8.5752 32.594 9.3779C32.4685 9.43478 32.343 9.4917 32.2175 9.54866C31.7961 9.14366 31.3817 8.73102 30.9521 8.33488C27.0799 4.76502 22.4856 3.43605 17.3405 4.22591C10.0761 5.34107 4.69388 11.3891 4.06231 18.939C3.46983 26.0213 8.03881 32.8643 14.897 35.1663C21.8348 37.495 29.5543 34.7845 33.4563 28.6429C33.7074 28.2475 33.9685 27.8417 34.1218 27.4045C34.4194 26.5555 34.2699 25.765 33.4312 25.3113C32.6231 24.8743 31.8573 25.0498 31.2835 25.7915C30.9966 26.1625 30.7785 26.5856 30.5106 26.9724C28.0914 30.4658 24.7682 32.3693 20.5158 32.5766C14.8218 32.8541 9.60215 29.1608 7.94272 23.717C6.22884 18.0946 8.59939 12.0366 13.6698 9.08126C18.5986 6.20837 24.9262 7.03281 28.9148 11.0837C29.2069 11.3803 29.4036 11.7708 29.8772 12.4519C28.32 12.4519 27.1212 12.3885 25.9323 12.4704C24.8345 12.5461 24.253 13.1995 24.262 14.1166C24.2708 15.0096 24.8931 15.7485 25.9495 15.7745C28.7068 15.8424 31.4671 15.8177 34.2259 15.7884C35.1348 15.7787 35.8872 15.2584 35.9148 14.3603C36.0054 11.4048 36.0127 8.44397 35.9387 5.48805Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">重试</span></font>&nbsp;&nbsp;<font class="immersive-translate-help-button notranslate" title="点击查看错误原因: {&amp;quot;error&amp;quot;:{&amp;quot;code&amp;quot;:&amp;quot;1305&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;当前API请求过多，请稍后重试。&amp;quot;}}" data-immersive-translate-tooltip-text="{&amp;quot;type&amp;quot;:&amp;quot;network&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;[GLM-4 Flash] 翻译服务或网络出现问题&amp;quot;,&amp;quot;errMsg&amp;quot;:&amp;quot;服务返回错误，说明请求过于频繁或超出额度限制，请稍后再试。&lt;br/&gt;&lt;br/&gt; 429: 当前API请求过多，请稍后重试。&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;retry&amp;quot;,&amp;quot;immediateShow&amp;quot;:false,&amp;quot;translationService&amp;quot;:&amp;quot;GLM-4 Flash&amp;quot;,&amp;quot;errCode&amp;quot;:429}" data-immersive-translate-action="toast-error" style="display: flex; flex-direction: row; align-items: center;"><svg width="40" height="40" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: inline; width: 1em; height: 1em; pointer-events: none;"><path fill-rule="evenodd" clip-rule="evenodd" d="M20.5607 2.5191C10.735 2.05516 2.46528 10.1045 2.50011 20.0984C2.54469 32.8837 15.9794 41.3025 27.521 35.772C28.0597 35.5138 28.6042 35.2357 29.0745 34.8742C29.9064 34.2347 30.0797 33.3404 29.5712 32.5989C29.0382 31.8217 28.2936 31.6838 27.4596 32.0227C27.2265 32.1174 27.0066 32.2437 26.7865 32.3701C26.6008 32.4767 26.415 32.5833 26.2211 32.6712C20.8005 35.1282 15.6165 34.6504 11.0342 30.8857C6.38506 27.0662 4.83815 21.9885 6.36608 16.1605C8.23236 9.04216 15.6457 4.59129 22.7912 6.13629C30.3201 7.76418 35.1917 14.6886 33.9006 22.1467C33.6763 23.4426 33.1697 24.693 32.665 25.9388C32.4936 26.3618 32.3223 26.7846 32.1625 27.2081C31.7321 28.3488 31.8755 29.1499 32.727 29.6338C33.5625 30.1085 34.3839 29.8271 35.0848 28.8121C35.2031 28.6407 35.3005 28.4544 35.3977 28.2685C35.4242 28.2179 35.4507 28.1672 35.4776 28.1169C36.5263 26.154 37.166 24.0544 37.3992 21.8528C38.4715 11.7296 30.8594 3.00541 20.5607 2.5191ZM22.2324 19.4482C22.6221 17.6294 21.6934 16.7853 19.8682 17.1885C19.4795 17.2744 19.0887 17.3789 18.7223 17.531C17.5055 18.036 17.1067 18.9307 17.8422 20.0563C18.3665 20.8586 18.2472 21.5161 18.0255 22.2965L17.9039 22.7239C17.5079 24.1148 17.1115 25.5072 16.7935 26.9165C16.4841 28.2873 17.2241 29.1723 18.6198 29.1593C18.6749 29.1502 18.7366 29.1408 18.8028 29.1307C18.9623 29.1063 19.1482 29.078 19.332 29.0394C21.5543 28.5732 21.9094 27.8227 20.9844 25.759C20.8192 25.3904 20.8406 24.873 20.9389 24.4633C21.1123 23.7404 21.3092 23.0227 21.5061 22.3052C21.7664 21.3567 22.0267 20.4083 22.2324 19.4482ZM21.2918 10.7674C22.3383 10.7322 23.3464 11.7297 23.3245 12.7787C23.3035 13.7817 22.4311 14.6541 21.4139 14.6892C20.3685 14.7252 19.5018 13.9485 19.4202 12.9025C19.3341 11.798 20.2055 10.8041 21.2918 10.7674Z" fill="#428ADF"></path></svg><span style="color: rgb(66, 138, 223); text-decoration-line: underline; text-underline-offset: 0.2em; margin-left: 0.2em; pointer-events: none;">错误原因</span></font></font></font></a></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.25.19.3"><span class="ltx_text" id="A4.T11.6.25.19.3.1" style="font-size:70%;">292,416</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.25.19.4"><span class="ltx_text" id="A4.T11.6.25.19.4.1" style="font-size:70%;">29,224</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.25.19.5"><span class="ltx_text" id="A4.T11.6.25.19.5.1" style="font-size:70%;">355</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.25.19.6"><span class="ltx_text" id="A4.T11.6.25.19.6.1" style="font-size:70%;">0.001</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.25.19.7"><span class="ltx_text" id="A4.T11.6.25.19.7.1" style="font-size:70%;">292,061</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.25.19.8"><span class="ltx_text" id="A4.T11.6.25.19.8.1" style="font-size:70%;">28,869</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.26.20">
<td class="ltx_td ltx_align_left" id="A4.T11.6.26.20.1"><span class="ltx_text" id="A4.T11.6.26.20.1.1" style="font-size:70%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.26.20.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.26.20.2.1" style="font-size:70%;">The Great Gatsby<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《了不起的盖茨比》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.26.20.3"><span class="ltx_text" id="A4.T11.6.26.20.3.1" style="font-size:70%;">48,177</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.26.20.4"><span class="ltx_text" id="A4.T11.6.26.20.4.1" style="font-size:70%;">5,635</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.26.20.5"><span class="ltx_text" id="A4.T11.6.26.20.5.1" style="font-size:70%;">4,519</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.26.20.6"><span class="ltx_text" id="A4.T11.6.26.20.6.1" style="font-size:70%;">0.094</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.26.20.7"><span class="ltx_text" id="A4.T11.6.26.20.7.1" style="font-size:70%;">43,658</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.26.20.8"><span class="ltx_text" id="A4.T11.6.26.20.8.1" style="font-size:70%;">1,116</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.27.21">
<td class="ltx_td ltx_align_left" id="A4.T11.6.27.21.1"><span class="ltx_text" id="A4.T11.6.27.21.1.1" style="font-size:70%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.27.21.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.27.21.2.1" style="font-size:70%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《哈利·波特与魔法石》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.27.21.3"><span class="ltx_text" id="A4.T11.6.27.21.3.1" style="font-size:70%;">82,382</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.27.21.4"><span class="ltx_text" id="A4.T11.6.27.21.4.1" style="font-size:70%;">75,935</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.27.21.5"><span class="ltx_text" id="A4.T11.6.27.21.5.1" style="font-size:70%;">60,974</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.27.21.6"><span class="ltx_text" id="A4.T11.6.27.21.6.1" style="font-size:70%;">0.740</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.27.21.7"><span class="ltx_text" id="A4.T11.6.27.21.7.1" style="font-size:70%;">21,408</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.27.21.8"><span class="ltx_text" id="A4.T11.6.27.21.8.1" style="font-size:70%;">14,961</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.28.22">
<td class="ltx_td ltx_align_left" id="A4.T11.6.28.22.1"><span class="ltx_text" id="A4.T11.6.28.22.1.1" style="font-size:70%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.28.22.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.28.22.2.1" style="font-size:70%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.28.22.3"><span class="ltx_text" id="A4.T11.6.28.22.3.1" style="font-size:70%;">198,267</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.28.22.4"><span class="ltx_text" id="A4.T11.6.28.22.4.1" style="font-size:70%;">6,300</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.28.22.5"><span class="ltx_text" id="A4.T11.6.28.22.5.1" style="font-size:70%;">0</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.28.22.6"><span class="ltx_text" id="A4.T11.6.28.22.6.1" style="font-size:70%;">0.000</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.28.22.7"><span class="ltx_text" id="A4.T11.6.28.22.7.1" style="font-size:70%;">198,267</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.28.22.8"><span class="ltx_text" id="A4.T11.6.28.22.8.1" style="font-size:70%;">6,300</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.29.23">
<td class="ltx_td ltx_align_left" id="A4.T11.6.29.23.1"><span class="ltx_text" id="A4.T11.6.29.23.1.1" style="font-size:70%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.29.23.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.29.23.2.1" style="font-size:70%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.29.23.3"><span class="ltx_text" id="A4.T11.6.29.23.3.1" style="font-size:70%;">99,964</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.29.23.4"><span class="ltx_text" id="A4.T11.6.29.23.4.1" style="font-size:70%;">4,359</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.29.23.5"><span class="ltx_text" id="A4.T11.6.29.23.5.1" style="font-size:70%;">998</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.29.23.6"><span class="ltx_text" id="A4.T11.6.29.23.6.1" style="font-size:70%;">0.010</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.29.23.7"><span class="ltx_text" id="A4.T11.6.29.23.7.1" style="font-size:70%;">98,966</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.29.23.8"><span class="ltx_text" id="A4.T11.6.29.23.8.1" style="font-size:70%;">3,361</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.30.24">
<td class="ltx_td ltx_align_left" id="A4.T11.6.30.24.1"><span class="ltx_text" id="A4.T11.6.30.24.1.1" style="font-size:70%;" data-imt_insert_failed="1">Gemini 2.5 Pro</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.30.24.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.30.24.2.1" style="font-size:70%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《霍比特人》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.30.24.3"><span class="ltx_text" id="A4.T11.6.30.24.3.1" style="font-size:70%;">95,343</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.30.24.4"><span class="ltx_text" id="A4.T11.6.30.24.4.1" style="font-size:70%;">5,721</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.30.24.5"><span class="ltx_text" id="A4.T11.6.30.24.5.1" style="font-size:70%;">4,921</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.30.24.6"><span class="ltx_text" id="A4.T11.6.30.24.6.1" style="font-size:70%;">0.052</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.30.24.7"><span class="ltx_text" id="A4.T11.6.30.24.7.1" style="font-size:70%;">90,422</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.30.24.8"><span class="ltx_text" id="A4.T11.6.30.24.8.1" style="font-size:70%;">800</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.31.25">
<td class="ltx_td ltx_align_left" id="A4.T11.6.31.25.1"><span class="ltx_text" id="A4.T11.6.31.25.1.1" style="font-size:70%;">GPT-4.1</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.31.25.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.31.25.2.1" style="font-size:70%;">1984</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.31.25.3"><span class="ltx_text" id="A4.T11.6.31.25.3.1" style="font-size:70%;">100,024</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.31.25.4"><span class="ltx_text" id="A4.T11.6.31.25.4.1" style="font-size:70%;">5,064</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.31.25.5"><span class="ltx_text" id="A4.T11.6.31.25.5.1" style="font-size:70%;">3,585</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.31.25.6"><span class="ltx_text" id="A4.T11.6.31.25.6.1" style="font-size:70%;">0.036</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.31.25.7"><span class="ltx_text" id="A4.T11.6.31.25.7.1" style="font-size:70%;">96,439</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.31.25.8"><span class="ltx_text" id="A4.T11.6.31.25.8.1" style="font-size:70%;">1,479</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.32.26">
<td class="ltx_td ltx_align_left" id="A4.T11.6.32.26.1"><span class="ltx_text" id="A4.T11.6.32.26.1.1" style="font-size:70%;">GPT-4.1</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.32.26.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.32.26.2.1" style="font-size:70%;">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《宠儿》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.32.26.3"><span class="ltx_text" id="A4.T11.6.32.26.3.1" style="font-size:70%;">97,759</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.32.26.4"><span class="ltx_text" id="A4.T11.6.32.26.4.1" style="font-size:70%;">340</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.32.26.5"><span class="ltx_text" id="A4.T11.6.32.26.5.1" style="font-size:70%;">129</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.32.26.6"><span class="ltx_text" id="A4.T11.6.32.26.6.1" style="font-size:70%;">0.001</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.32.26.7"><span class="ltx_text" id="A4.T11.6.32.26.7.1" style="font-size:70%;">97,630</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.32.26.8"><span class="ltx_text" id="A4.T11.6.32.26.8.1" style="font-size:70%;">211</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.33.27">
<td class="ltx_td ltx_align_left" id="A4.T11.6.33.27.1"><span class="ltx_text" id="A4.T11.6.33.27.1.1" style="font-size:70%;">GPT-4.1</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.33.27.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.33.27.2.1" style="font-size:70%;">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.33.27.3"><span class="ltx_text" id="A4.T11.6.33.27.3.1" style="font-size:70%;">73,566</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.33.27.4"><span class="ltx_text" id="A4.T11.6.33.27.4.1" style="font-size:70%;">2,014</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.33.27.5"><span class="ltx_text" id="A4.T11.6.33.27.5.1" style="font-size:70%;">531</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.33.27.6"><span class="ltx_text" id="A4.T11.6.33.27.6.1" style="font-size:70%;">0.007</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.33.27.7"><span class="ltx_text" id="A4.T11.6.33.27.7.1" style="font-size:70%;">73,035</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.33.27.8"><span class="ltx_text" id="A4.T11.6.33.27.8.1" style="font-size:70%;">1,483</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.34.28">
<td class="ltx_td ltx_align_left" id="A4.T11.6.34.28.1"><span class="ltx_text" id="A4.T11.6.34.28.1.1" style="font-size:70%;">GPT-4.1</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.34.28.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.34.28.2.1" style="font-size:70%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">弗兰肯斯坦</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.34.28.3"><span class="ltx_text" id="A4.T11.6.34.28.3.1" style="font-size:70%;">69,704</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.34.28.4"><span class="ltx_text" id="A4.T11.6.34.28.4.1" style="font-size:70%;">1,801</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.34.28.5"><span class="ltx_text" id="A4.T11.6.34.28.5.1" style="font-size:70%;">1,377</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.34.28.6"><span class="ltx_text" id="A4.T11.6.34.28.6.1" style="font-size:70%;">0.020</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.34.28.7"><span class="ltx_text" id="A4.T11.6.34.28.7.1" style="font-size:70%;">68,327</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.34.28.8"><span class="ltx_text" id="A4.T11.6.34.28.8.1" style="font-size:70%;">424</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.35.29">
<td class="ltx_td ltx_align_left" id="A4.T11.6.35.29.1"><span class="ltx_text" id="A4.T11.6.35.29.1.1" style="font-size:70%;">GPT-4.1</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.35.29.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.35.29.2.1" style="font-size:70%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.35.29.3"><span class="ltx_text" id="A4.T11.6.35.29.3.1" style="font-size:70%;">292,416</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.35.29.4"><span class="ltx_text" id="A4.T11.6.35.29.4.1" style="font-size:70%;">4,219</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.35.29.5"><span class="ltx_text" id="A4.T11.6.35.29.5.1" style="font-size:70%;">226</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.35.29.6"><span class="ltx_text" id="A4.T11.6.35.29.6.1" style="font-size:70%;">0.001</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.35.29.7"><span class="ltx_text" id="A4.T11.6.35.29.7.1" style="font-size:70%;">292,190</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.35.29.8"><span class="ltx_text" id="A4.T11.6.35.29.8.1" style="font-size:70%;">3,993</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.36.30">
<td class="ltx_td ltx_align_left" id="A4.T11.6.36.30.1"><span class="ltx_text" id="A4.T11.6.36.30.1.1" style="font-size:70%;">GPT-4.1</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.36.30.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.36.30.2.1" style="font-size:70%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与魔法石</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.36.30.3"><span class="ltx_text" id="A4.T11.6.36.30.3.1" style="font-size:70%;">82,382</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.36.30.4"><span class="ltx_text" id="A4.T11.6.36.30.4.1" style="font-size:70%;">4,315</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.36.30.5"><span class="ltx_text" id="A4.T11.6.36.30.5.1" style="font-size:70%;">3,182</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.36.30.6"><span class="ltx_text" id="A4.T11.6.36.30.6.1" style="font-size:70%;">0.039</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.36.30.7"><span class="ltx_text" id="A4.T11.6.36.30.7.1" style="font-size:70%;">79,200</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.36.30.8"><span class="ltx_text" id="A4.T11.6.36.30.8.1" style="font-size:70%;">1,133</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.37.31">
<td class="ltx_td ltx_align_left" id="A4.T11.6.37.31.1"><span class="ltx_text" id="A4.T11.6.37.31.1.1" style="font-size:70%;">GPT-4.1</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.37.31.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.37.31.2.1" style="font-size:70%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.37.31.3"><span class="ltx_text" id="A4.T11.6.37.31.3.1" style="font-size:70%;">198,267</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.37.31.4"><span class="ltx_text" id="A4.T11.6.37.31.4.1" style="font-size:70%;">206</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.37.31.5"><span class="ltx_text" id="A4.T11.6.37.31.5.1" style="font-size:70%;">105</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.37.31.6"><span class="ltx_text" id="A4.T11.6.37.31.6.1" style="font-size:70%;">0.001</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.37.31.7"><span class="ltx_text" id="A4.T11.6.37.31.7.1" style="font-size:70%;">198,162</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.37.31.8"><span class="ltx_text" id="A4.T11.6.37.31.8.1" style="font-size:70%;">101</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.38.32">
<td class="ltx_td ltx_align_left" id="A4.T11.6.38.32.1"><span class="ltx_text" id="A4.T11.6.38.32.1.1" style="font-size:70%;">GPT-4.1</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.38.32.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.38.32.2.1" style="font-size:70%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.38.32.3"><span class="ltx_text" id="A4.T11.6.38.32.3.1" style="font-size:70%;">99,964</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.38.32.4"><span class="ltx_text" id="A4.T11.6.38.32.4.1" style="font-size:70%;">132</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.38.32.5"><span class="ltx_text" id="A4.T11.6.38.32.5.1" style="font-size:70%;">108</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.38.32.6"><span class="ltx_text" id="A4.T11.6.38.32.6.1" style="font-size:70%;">0.001</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.38.32.7"><span class="ltx_text" id="A4.T11.6.38.32.7.1" style="font-size:70%;">99,856</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.38.32.8"><span class="ltx_text" id="A4.T11.6.38.32.8.1" style="font-size:70%;">24</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.39.33">
<td class="ltx_td ltx_align_left" id="A4.T11.6.39.33.1"><span class="ltx_text" id="A4.T11.6.39.33.1.1" style="font-size:70%;">GPT-4.1</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.39.33.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.39.33.2.1" style="font-size:70%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">霍比特人</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.39.33.3"><span class="ltx_text" id="A4.T11.6.39.33.3.1" style="font-size:70%;">95,343</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.39.33.4"><span class="ltx_text" id="A4.T11.6.39.33.4.1" style="font-size:70%;">6,723</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.39.33.5"><span class="ltx_text" id="A4.T11.6.39.33.5.1" style="font-size:70%;">1,867</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.39.33.6"><span class="ltx_text" id="A4.T11.6.39.33.6.1" style="font-size:70%;">0.020</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.39.33.7"><span class="ltx_text" id="A4.T11.6.39.33.7.1" style="font-size:70%;">93,476</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.39.33.8"><span class="ltx_text" id="A4.T11.6.39.33.8.1" style="font-size:70%;">4,856</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.40.34">
<td class="ltx_td ltx_align_left" id="A4.T11.6.40.34.1"><span class="ltx_text" id="A4.T11.6.40.34.1.1" style="font-size:70%;">Grok 3<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">格洛克 3</font></font></font></span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.40.34.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.40.34.2.1" style="font-size:70%;">1984</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.40.34.3"><span class="ltx_text" id="A4.T11.6.40.34.3.1" style="font-size:70%;">100,024</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.40.34.4"><span class="ltx_text" id="A4.T11.6.40.34.4.1" style="font-size:70%;">22,052</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.40.34.5"><span class="ltx_text" id="A4.T11.6.40.34.5.1" style="font-size:70%;">9,638</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.40.34.6"><span class="ltx_text" id="A4.T11.6.40.34.6.1" style="font-size:70%;">0.096</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.40.34.7"><span class="ltx_text" id="A4.T11.6.40.34.7.1" style="font-size:70%;">90,386</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.40.34.8"><span class="ltx_text" id="A4.T11.6.40.34.8.1" style="font-size:70%;">12,414</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.41.35">
<td class="ltx_td ltx_align_left" id="A4.T11.6.41.35.1"><span class="ltx_text" id="A4.T11.6.41.35.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.41.35.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.41.35.2.1" style="font-size:70%;">Beloved<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">挚爱</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.41.35.3"><span class="ltx_text" id="A4.T11.6.41.35.3.1" style="font-size:70%;">97,759</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.41.35.4"><span class="ltx_text" id="A4.T11.6.41.35.4.1" style="font-size:70%;">26,454</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.41.35.5"><span class="ltx_text" id="A4.T11.6.41.35.5.1" style="font-size:70%;">0</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.41.35.6"><span class="ltx_text" id="A4.T11.6.41.35.6.1" style="font-size:70%;">0.000</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.41.35.7"><span class="ltx_text" id="A4.T11.6.41.35.7.1" style="font-size:70%;">97,759</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.41.35.8"><span class="ltx_text" id="A4.T11.6.41.35.8.1" style="font-size:70%;">26,454</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.42.36">
<td class="ltx_td ltx_align_left" id="A4.T11.6.42.36.1"><span class="ltx_text" id="A4.T11.6.42.36.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.42.36.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.42.36.2.1" style="font-size:70%;">Catch-22<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">悖论</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.42.36.3"><span class="ltx_text" id="A4.T11.6.42.36.3.1" style="font-size:70%;">174,344</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.42.36.4"><span class="ltx_text" id="A4.T11.6.42.36.4.1" style="font-size:70%;">3,507</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.42.36.5"><span class="ltx_text" id="A4.T11.6.42.36.5.1" style="font-size:70%;">0</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.42.36.6"><span class="ltx_text" id="A4.T11.6.42.36.6.1" style="font-size:70%;">0.000</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.42.36.7"><span class="ltx_text" id="A4.T11.6.42.36.7.1" style="font-size:70%;">174,344</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.42.36.8"><span class="ltx_text" id="A4.T11.6.42.36.8.1" style="font-size:70%;">3,507</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.43.37">
<td class="ltx_td ltx_align_left" id="A4.T11.6.43.37.1"><span class="ltx_text" id="A4.T11.6.43.37.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.43.37.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.43.37.2.1" style="font-size:70%;">The Catcher in the Rye<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《麦田里的守望者》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.43.37.3"><span class="ltx_text" id="A4.T11.6.43.37.3.1" style="font-size:70%;">73,566</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.43.37.4"><span class="ltx_text" id="A4.T11.6.43.37.4.1" style="font-size:70%;">96,705</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.43.37.5"><span class="ltx_text" id="A4.T11.6.43.37.5.1" style="font-size:70%;">2,611</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.43.37.6"><span class="ltx_text" id="A4.T11.6.43.37.6.1" style="font-size:70%;">0.035</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.43.37.7"><span class="ltx_text" id="A4.T11.6.43.37.7.1" style="font-size:70%;">70,955</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.43.37.8"><span class="ltx_text" id="A4.T11.6.43.37.8.1" style="font-size:70%;">94,094</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.44.38">
<td class="ltx_td ltx_align_left" id="A4.T11.6.44.38.1"><span class="ltx_text" id="A4.T11.6.44.38.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.44.38.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.44.38.2.1" style="font-size:70%;">The Da Vinci Code<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">达芬奇密码</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.44.38.3"><span class="ltx_text" id="A4.T11.6.44.38.3.1" style="font-size:70%;">139,537</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.44.38.4"><span class="ltx_text" id="A4.T11.6.44.38.4.1" style="font-size:70%;">25,965</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.44.38.5"><span class="ltx_text" id="A4.T11.6.44.38.5.1" style="font-size:70%;">0</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.44.38.6"><span class="ltx_text" id="A4.T11.6.44.38.6.1" style="font-size:70%;">0.000</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.44.38.7"><span class="ltx_text" id="A4.T11.6.44.38.7.1" style="font-size:70%;">139,537</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.44.38.8"><span class="ltx_text" id="A4.T11.6.44.38.8.1" style="font-size:70%;">25,965</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.45.39">
<td class="ltx_td ltx_align_left" id="A4.T11.6.45.39.1"><span class="ltx_text" id="A4.T11.6.45.39.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.45.39.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.45.39.2.1" style="font-size:70%;">Frankenstein<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">弗兰肯斯坦</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.45.39.3"><span class="ltx_text" id="A4.T11.6.45.39.3.1" style="font-size:70%;">69,704</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.45.39.4"><span class="ltx_text" id="A4.T11.6.45.39.4.1" style="font-size:70%;">20,417</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.45.39.5"><span class="ltx_text" id="A4.T11.6.45.39.5.1" style="font-size:70%;">1,052</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.45.39.6"><span class="ltx_text" id="A4.T11.6.45.39.6.1" style="font-size:70%;">0.015</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.45.39.7"><span class="ltx_text" id="A4.T11.6.45.39.7.1" style="font-size:70%;">68,652</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.45.39.8"><span class="ltx_text" id="A4.T11.6.45.39.8.1" style="font-size:70%;">19,365</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.46.40">
<td class="ltx_td ltx_align_left" id="A4.T11.6.46.40.1"><span class="ltx_text" id="A4.T11.6.46.40.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.46.40.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.46.40.2.1" style="font-size:70%;">A Game of Thrones<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">冰与火之歌</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.46.40.3"><span class="ltx_text" id="A4.T11.6.46.40.3.1" style="font-size:70%;">292,416</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.46.40.4"><span class="ltx_text" id="A4.T11.6.46.40.4.1" style="font-size:70%;">251,025</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.46.40.5"><span class="ltx_text" id="A4.T11.6.46.40.5.1" style="font-size:70%;">3,749</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.46.40.6"><span class="ltx_text" id="A4.T11.6.46.40.6.1" style="font-size:70%;">0.013</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.46.40.7"><span class="ltx_text" id="A4.T11.6.46.40.7.1" style="font-size:70%;">288,667</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.46.40.8"><span class="ltx_text" id="A4.T11.6.46.40.8.1" style="font-size:70%;">247,276</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.47.41">
<td class="ltx_td ltx_align_left" id="A4.T11.6.47.41.1"><span class="ltx_text" id="A4.T11.6.47.41.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.47.41.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.47.41.2.1" style="font-size:70%;">The Great Gatsby<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《了不起的盖茨比》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.47.41.3"><span class="ltx_text" id="A4.T11.6.47.41.3.1" style="font-size:70%;">48,177</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.47.41.4"><span class="ltx_text" id="A4.T11.6.47.41.4.1" style="font-size:70%;">11,255</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.47.41.5"><span class="ltx_text" id="A4.T11.6.47.41.5.1" style="font-size:70%;">7,118</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.47.41.6"><span class="ltx_text" id="A4.T11.6.47.41.6.1" style="font-size:70%;">0.148</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.47.41.7"><span class="ltx_text" id="A4.T11.6.47.41.7.1" style="font-size:70%;">41,059</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.47.41.8"><span class="ltx_text" id="A4.T11.6.47.41.8.1" style="font-size:70%;">4,137</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.48.42">
<td class="ltx_td ltx_align_left" id="A4.T11.6.48.42.1"><span class="ltx_text" id="A4.T11.6.48.42.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.48.42.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.48.42.2.1" style="font-size:70%;">Harry Potter and the Sorcerer’s Stone<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《哈利·波特与魔法石》</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.48.42.3"><span class="ltx_text" id="A4.T11.6.48.42.3.1" style="font-size:70%;">82,382</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.48.42.4"><span class="ltx_text" id="A4.T11.6.48.42.4.1" style="font-size:70%;">72,078</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.48.42.5"><span class="ltx_text" id="A4.T11.6.48.42.5.1" style="font-size:70%;">56,870</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.48.42.6"><span class="ltx_text" id="A4.T11.6.48.42.6.1" style="font-size:70%;">0.690</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.48.42.7"><span class="ltx_text" id="A4.T11.6.48.42.7.1" style="font-size:70%;">25,512</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.48.42.8"><span class="ltx_text" id="A4.T11.6.48.42.8.1" style="font-size:70%;">15,208</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.49.43">
<td class="ltx_td ltx_align_left" id="A4.T11.6.49.43.1"><span class="ltx_text" id="A4.T11.6.49.43.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.49.43.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.49.43.2.1" style="font-size:70%;">Harry Potter and the Goblet of Fire<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">哈利·波特与火焰杯</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.49.43.3"><span class="ltx_text" id="A4.T11.6.49.43.3.1" style="font-size:70%;">198,267</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.49.43.4"><span class="ltx_text" id="A4.T11.6.49.43.4.1" style="font-size:70%;">25,679</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.49.43.5"><span class="ltx_text" id="A4.T11.6.49.43.5.1" style="font-size:70%;">100</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.49.43.6"><span class="ltx_text" id="A4.T11.6.49.43.6.1" style="font-size:70%;">0.001</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.49.43.7"><span class="ltx_text" id="A4.T11.6.49.43.7.1" style="font-size:70%;">198,167</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.49.43.8"><span class="ltx_text" id="A4.T11.6.49.43.8.1" style="font-size:70%;">25,579</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.50.44">
<td class="ltx_td ltx_align_left" id="A4.T11.6.50.44.1"><span class="ltx_text" id="A4.T11.6.50.44.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left" id="A4.T11.6.50.44.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.50.44.2.1" style="font-size:70%;">The Hunger Games<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">饥饿游戏</font></font></font></span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.50.44.3"><span class="ltx_text" id="A4.T11.6.50.44.3.1" style="font-size:70%;">99,964</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.50.44.4"><span class="ltx_text" id="A4.T11.6.50.44.4.1" style="font-size:70%;">74,153</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.50.44.5"><span class="ltx_text" id="A4.T11.6.50.44.5.1" style="font-size:70%;">2,344</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.50.44.6"><span class="ltx_text" id="A4.T11.6.50.44.6.1" style="font-size:70%;">0.023</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.50.44.7"><span class="ltx_text" id="A4.T11.6.50.44.7.1" style="font-size:70%;">97,620</span></td>
<td class="ltx_td ltx_align_right" id="A4.T11.6.50.44.8"><span class="ltx_text" id="A4.T11.6.50.44.8.1" style="font-size:70%;">71,809</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.6.51.45">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T11.6.51.45.1"><span class="ltx_text" id="A4.T11.6.51.45.1.1" style="font-size:70%;" data-imt_insert_failed="1">Grok 3</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A4.T11.6.51.45.2"><span class="ltx_text ltx_font_italic" id="A4.T11.6.51.45.2.1" style="font-size:70%;">The Hobbit<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">《霍比特人》</font></font></font></span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A4.T11.6.51.45.3"><span class="ltx_text" id="A4.T11.6.51.45.3.1" style="font-size:70%;">95,343</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A4.T11.6.51.45.4"><span class="ltx_text" id="A4.T11.6.51.45.4.1" style="font-size:70%;">130,369</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A4.T11.6.51.45.5"><span class="ltx_text" id="A4.T11.6.51.45.5.1" style="font-size:70%;">6,910</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A4.T11.6.51.45.6"><span class="ltx_text" id="A4.T11.6.51.45.6.1" style="font-size:70%;">0.072</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A4.T11.6.51.45.7"><span class="ltx_text" id="A4.T11.6.51.45.7.1" style="font-size:70%;">88,433</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A4.T11.6.51.45.8"><span class="ltx_text" id="A4.T11.6.51.45.8.1" style="font-size:70%;">123,459</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A4.T11.33.7.1" style="font-size:129%;">Table 11</span>: </span><span class="ltx_text ltx_font_bold" id="A4.T11.18.6" style="font-size:129%;">Detailed results for all main experiments.<span class="ltx_text ltx_font_medium" id="A4.T11.18.6.6">
For the runs in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>), we provide precise information for all metrics. <math alttext="|B|" class="ltx_Math" display="inline" id="A4.T11.13.1.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|B|</annotation></semantics></math> is the reference book length, <math alttext="|G|" class="ltx_Math" display="inline" id="A4.T11.14.2.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi>G</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|G|</annotation></semantics></math> is generated text
length, <math alttext="m" class="ltx_Math" display="inline" id="A4.T11.15.3.3.m3" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> is total extracted words (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E6" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">6</span></a>), <math alttext="\mathsf{nv{\text{-}}recall}=m/|B|" class="ltx_Math" display="inline" id="A4.T11.16.4.4.m4" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mi>m</mi><mo>/</mo><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=m/|B|</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E7" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">7</span></a>); <math alttext="\mathsf{missing}=|B|-m" class="ltx_Math" display="inline" id="A4.T11.17.5.5.m5" intent=":literal"><semantics><mrow><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><mo>=</mo><mrow><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><mo>−</mo><mi>m</mi></mrow></mrow><annotation encoding="application/x-tex">\mathsf{missing}=|B|-m</annotation></semantics></math> and <math alttext="\mathsf{additional}=|G|-m" class="ltx_Math" display="inline" id="A4.T11.18.6.6.m6" intent=":literal"><semantics><mrow><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><mo>=</mo><mrow><mrow><mo stretchy="false">|</mo><mi>G</mi><mo stretchy="false">|</mo></mrow><mo>−</mo><mi>m</mi></mrow></mrow><annotation encoding="application/x-tex">\mathsf{additional}=|G|-m</annotation></semantics></math> (Equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S3.E8" title="In 3.3.1 Identifying near-verbatim extracted text in a long-form generation ‣ 3.3 Verifying extraction success ‣ 3 Extraction procedure ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">8</span></a>).</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">表 11：所有主要实验的详细结果。对于图 5 中的运行，我们提供了所有指标的确切信息。 <math intent=":literal" id="A4.T11.13.1.1.m1" display="inline" class="ltx_Math" alttext="|B|"><semantics><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|B|</annotation></semantics></math> 是参考书长度， <math intent=":literal" id="A4.T11.14.2.2.m2" display="inline" class="ltx_Math" alttext="|G|"><semantics><mrow><mo stretchy="false">|</mo><mi>G</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|G|</annotation></semantics></math> 是生成文本长度， <math intent=":literal" id="A4.T11.15.3.3.m3" display="inline" class="ltx_Math" alttext="m"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> 是总提取词数（公式 6）， <math intent=":literal" id="A4.T11.16.4.4.m4" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}=m/|B|"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo>=</mo><mrow><mi>m</mi><mo>/</mo><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}=m/|B|</annotation></semantics></math> （公式 7）； <math intent=":literal" id="A4.T11.17.5.5.m5" display="inline" class="ltx_Math" alttext="\mathsf{missing}=|B|-m"><semantics><mrow><mi>𝗆𝗂𝗌𝗌𝗂𝗇𝗀</mi><mo>=</mo><mrow><mrow><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow><mo>−</mo><mi>m</mi></mrow></mrow><annotation encoding="application/x-tex">\mathsf{missing}=|B|-m</annotation></semantics></math> 和 <math intent=":literal" id="A4.T11.18.6.6.m6" display="inline" class="ltx_Math" alttext="\mathsf{additional}=|G|-m"><semantics><mrow><mi>𝖺𝖽𝖽𝗂𝗍𝗂𝗈𝗇𝖺𝗅</mi><mo>=</mo><mrow><mrow><mo stretchy="false">|</mo><mi>G</mi><mo stretchy="false">|</mo></mrow><mo>−</mo><mi>m</mi></mrow></mrow><annotation encoding="application/x-tex">\mathsf{additional}=|G|-m</annotation></semantics></math> （公式 8）。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A4.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="392" id="A4.F14.g1" src="./从生产语言模型中提取书籍 --- Extracting books from production language models_files/figure_gemini_sweep_compare.png" width="583">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F14.20.9.1" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text ltx_font_bold" id="A4.F14.16.8" style="font-size:90%;">For Gemini 2.5 Pro, comparing best <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="A4.F14.9.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext class="ltx_mathvariant_bold">-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> to the average over all configured runs.<span class="ltx_text ltx_font_medium" id="A4.F14.16.8.7">
We show maximum observed <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="A4.F14.10.2.1.m1" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> for all <math alttext="9" class="ltx_Math" display="inline" id="A4.F14.11.3.2.m2" intent=":literal"><semantics><mn>9</mn><annotation encoding="application/x-tex">9</annotation></semantics></math> generation configuration settings per book.
(See Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#A3.SS2.SSS2" title="C.2.2 Generation configuration exploration for Gemini 2.5 Pro ‣ C.2 Phase 2 generation configurations and stop conditions ‣ Appendix C Experimental setup ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">C.2.2</span></a>; max length is <math alttext="2000" class="ltx_Math" display="inline" id="A4.F14.12.4.3.m3" intent=":literal"><semantics><mn>2000</mn><annotation encoding="application/x-tex">2000</annotation></semantics></math>, and we sweep over <math alttext="9" class="ltx_Math" display="inline" id="A4.F14.13.5.4.m4" intent=":literal"><semantics><mn>9</mn><annotation encoding="application/x-tex">9</annotation></semantics></math> combinations of frequency and presence penalty.)
We also show the mean <math alttext="\mathsf{nv{\text{-}}recall}\;\pm" class="ltx_Math" display="inline" id="A4.F14.14.6.5.m5" intent=":literal"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo lspace="0.502em">±</mo></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}\;\pm</annotation></semantics></math> STD over these <math alttext="9" class="ltx_Math" display="inline" id="A4.F14.15.7.6.m6" intent=":literal"><semantics><mn>9</mn><annotation encoding="application/x-tex">9</annotation></semantics></math> runs.
Note that in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2601.02671v1#S4.F5" title="Figure 5 ‣ 4.1 Setup ‣ 4 Experiments ‣ Extracting books from production language models"><span class="ltx_text ltx_ref_tag">5</span></a>, so that each LLM uses a fixed configuration across books, we fix the generation configuration for Gemini 2.5 Pro;
for that fixed configuration, some books exhibit the maximum <math alttext="\mathsf{nv{\text{-}}recall}" class="ltx_Math" display="inline" id="A4.F14.16.8.7.m7" intent=":literal"><semantics><mrow><mi>𝗇𝗏</mi><mo lspace="0em" rspace="0em">​</mo><mtext>-</mtext><mo lspace="0em" rspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> shown here (e.g., <span class="ltx_text ltx_font_italic" id="A4.F14.16.8.7.1">Harry Potter and the Sorcerer’s Stone</span>);
others do not (e.g., <span class="ltx_text ltx_font_italic" id="A4.F14.16.8.7.2">The Great Gatsby</span>).</span></span><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">图 14：对于 Gemini 2.5 Pro，比较最佳 <math intent=":literal" id="A4.F14.9.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext class="ltx_mathvariant_bold">-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 与所有配置运行的平均值。我们展示每本书针对所有 <math intent=":literal" id="A4.F14.11.3.2.m2" display="inline" class="ltx_Math" alttext="9"><semantics><mn>9</mn><annotation encoding="application/x-tex">9</annotation></semantics></math> 生成配置设置的最大观察到的 <math intent=":literal" id="A4.F14.10.2.1.m1" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> 。 （见附录 C.2.2；最大长度是 <math intent=":literal" id="A4.F14.12.4.3.m3" display="inline" class="ltx_Math" alttext="2000"><semantics><mn>2000</mn><annotation encoding="application/x-tex">2000</annotation></semantics></math> ，我们扫描 <math intent=":literal" id="A4.F14.13.5.4.m4" display="inline" class="ltx_Math" alttext="9"><semantics><mn>9</mn><annotation encoding="application/x-tex">9</annotation></semantics></math> 频率和存在惩罚的组合。）我们还展示了这些 <math intent=":literal" id="A4.F14.15.7.6.m6" display="inline" class="ltx_Math" alttext="9"><semantics><mn>9</mn><annotation encoding="application/x-tex">9</annotation></semantics></math> 运行的平均 <math intent=":literal" id="A4.F14.14.6.5.m5" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}\;\pm"><semantics><mrow><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><mo lspace="0.502em">±</mo></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}\;\pm</annotation></semantics></math> 标准差。注意在图 5 中，为了使每个 LLM 在不同书中使用固定配置，我们固定了 Gemini 2.5 Pro 的生成配置；对于该固定配置，有些书表现出这里显示的最大 <math intent=":literal" id="A4.F14.16.8.7.m7" display="inline" class="ltx_Math" alttext="\mathsf{nv{\text{-}}recall}"><semantics><mrow><mi>𝗇𝗏</mi><mo rspace="0em" lspace="0em">​</mo><mtext>-</mtext><mo rspace="0em" lspace="0em">​</mo><mi>𝗋𝖾𝖼𝖺𝗅𝗅</mi></mrow><annotation encoding="application/x-tex">\mathsf{nv{\text{-}}recall}</annotation></semantics></math> （例如，《哈利·波特与魔法石》）；而另一些则没有（例如，《了不起的盖茨比》）。</font></font></font></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><font class="notranslate" data-immersive-translate-translation-element-mark="1">&nbsp;&nbsp;</font><font style="display:block" class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-inline-wrapper-theme-none immersive-translate-target-translation-inline-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">报告问题</font></font></font></button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed; display: none; left: 984px; top: 707.867px; transform: translate(-50%, -100%);">Report Issue for Selection<font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">报告选择问题</font></font></font></button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" width="11" height="14">
            </a><font class="notranslate immersive-translate-target-wrapper" lang="zh-CN"><br><font class="notranslate immersive-translate-target-translation-theme-none immersive-translate-target-translation-block-wrapper-theme-none immersive-translate-target-translation-block-wrapper" data-immersive-translate-translation-element-mark="1"><font class="notranslate immersive-translate-target-inner immersive-translate-target-translation-theme-none-inner" data-immersive-translate-translation-element-mark="1">由 L A T E xml <img height="14" width="11" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"> 生成</font></font></font>
        </div></div><footer id="footer" class="ltx_document" default-translate="no">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body><div id="immersive-translate-popup" style="all: initial"><template shadowrootmode="open"><style>/** 基础色阶定义 **/
:root,
#mount[data-theme="light"],
#mount:not([data-theme="dark"]) {
  /* 中性灰阶（light） */
  --c-00: #000000;
  --c-22: #222222;
  --c-33: #333333;
  --c-66: #666666;
  --c-83: #838383;
  --c-99: #999999;
  --c-c7: #c7c7c7;
  --c-cc: #cccccc;
  --c-e6: #e6e6e6;
  --c-f5: #f5f5f5;
  --c-ff: #ffffff;
  /* 品牌主色阶（light） */
  --p-main: #ea4c89;
  --p-hover: #ec5e95;
  --p-active: #e34a85;
  --p-special: #ee71a2;
  --p-disabled: #f4a5c4;
  --p-text-disabled: #f9c9dc;
  --p-weak: #fdedf3;
  /* Surface 层级（light，TC 填充-1） */
  --s-1: #f3f5f6;
  --s-1-hover: #f6f8f9;
  --s-1-active: #edf1f2;
  --s-1-weak: #fafbfb;
  /* 输入/边框（light，TC 填充-2） */
  --input-bg-base: #fafbfc;
  --input-border: #ecf0f7;
  --input-border-strong: #e0e0e6;
  --input-bg-strong: #fafdff;
}

:root[data-theme="dark"],
[data-theme="dark"] {
  /* 中性灰阶（dark） */
  --c-00: #ffffff;
  --c-22: #dbdbdb;
  --c-33: #dbdbdb;
  --c-66: #b3b3b3;
  --c-83: #838383;
  --c-99: #707070;
  --c-c7: #666666;
  --c-cc: #5c5c5c;
  --c-e6: #3b3b3b;
  --c-f5: #262626;
  --c-ff: #222222;
  /* 品牌主色阶（dark） */
  --p-main: #e23c7c;
  --p-hover: #ea4c89;
  --p-active: #d5467d;
  --p-special: #a93a65;
  --p-disabled: #7e2f4d;
  --p-text-disabled: #522335;
  --p-weak: #26171d;
  /* Surface 层级（dark，TC 填充-1） */
  --s-1: #2d2e2f;
  --s-1-hover: #323434;
  --s-1-active: #202121;
  --s-1-weak: #262627;
  /* 输入/边框（dark，TC 填充-2） */
  --input-bg-base: #2b2d30;
  --input-border: #3e434b;
  --input-border-strong: #43474b;
  --input-bg-strong: #1f2123;
}

:root,
#mount [data-theme] {
  /* 业务/通用变量引用色阶（全局可见，含 Shadow DOM） */
  --primary: var(--p-main);
  --primary-hover: var(--p-hover);
  --primary-inverse: #fff;
  --modal-background: var(--s-1);
  --modal-border: var(--input-border);
  --modal-text: var(--c-22);
  --modal-text-secondary: var(--c-66);
  --modal-error: var(--p-main);
  --modal-required: #f53f3f;
  --modal-success: #68cd52;
  --modal-button-background: var(--p-main);
  --modal-button-text: var(--c-ff);
  --modal-input-background: var(--input-bg-base);
  --modal-check-color: var(--p-main);
  --background-color: var(--c-ff);
  --background-light-green: var(--s-1-weak, #f5f7f9);
  --text-black-2: var(--c-22);
  --text-gray-2: var(--c-22);
  --text-gray-6: var(--c-66);
  --text-gray-9: var(--c-99);
  --text-gray-c2: var(--c-c7);
  --switch-background-color: var(--c-c7, hsl(205deg, 16%, 77%));
  --float-ball-more-button-border-color: var(--c-f5, #f6f6f6);
  --float-ball-more-button-background-color: var(--c-ff);
  --float-ball-more-button-svg-color: #6c6f73;
  --service-bg-hover: var(--s-1-hover, #f7faff);
  --service-bg: var(--s-1-weak, #fafbfb);
}

#mount {
  --font-family: var(
    system-ui,
    -apple-system,
    "Segoe UI",
    "Roboto",
    "Ubuntu",
    "Cantarell",
    "Noto Sans",
    sans-serif,
    "Apple Color Emoji",
    "Segoe UI Emoji",
    "Segoe UI Symbol",
    "Noto Color Emoji"
  );
  /* PC/H5 兼容的字号、间距、圆角、阴影变量 */
  --f-12: 12px;
  --f-14: 14px;
  --f-15: 15px;
  --f-16: 16px;
  --f-18: 18px;
  --f-20: 20px;
  --space-4: 4px;
  --space-6: 6px;
  --space-8: 8px;
  --space-12: 12px;
  --space-16: 16px;
  --space-18: 18px;
  --space-24: 24px;
  --radius-8: 8px;
  --radius-12: 12px;
  --radius-16: 16px;
  --control-height-lg: 44px;
  --width-28: 28px;
  --width-24: 24px;
  --width-20: 20px;
  --width-18: 18px;
  --width-16: 16px;
  --width-label-md: 56px;
  --shadow-lg: 0 18px 48px rgba(0, 0, 0, 0.12);

  /* 常规变量 */
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 4px;
  --border-width: 2px;
  --outline-width: 3px;
  --spacing: 16px;
  --typography-spacing-vertical: 24px;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 12px;
  --form-element-spacing-horizontal: 16px;
  --nav-element-spacing-vertical: 16px;
  --nav-element-spacing-horizontal: 8px;
  --nav-link-spacing-vertical: 8px;
  --nav-link-spacing-horizontal: 8px;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(4px);
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --icon-xia: url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgaWQ9IkZyYW1lIj4KPHBhdGggaWQ9IlZlY3RvciIgZD0iTTguMDAyOTEgOS42Nzk4M0wzLjgzMzM5IDUuNTEyMjFMMy4wMjUzOSA2LjMxOTgzTDguMDAzMjkgMTEuMjk1MUwxMi45NzYyIDYuMzE5ODNMMTIuMTY3OSA1LjUxMjIxTDguMDAyOTEgOS42Nzk4M1oiIGZpbGw9IiM4MzgzODMiLz4KPC9nPgo8L3N2Zz4K");
  /* 兼容旧变量：主色直接引用品牌主色阶 */
  --primary: var(--p-main);
  --primary-hover: var(--p-hover);
  --primary-inverse: #fff;
  /* Modal 业务变量引用色阶 */
  --modal-background: var(--s-1);
  --modal-border: var(--input-border);
  --modal-text: var(--c-22);
  --modal-text-secondary: var(--c-66);
  --modal-error: var(--p-main);
  --modal-required: #f53f3f;
  --modal-success: #68cd52;
  --modal-button-background: var(--p-main);
  --modal-button-text: var(--c-ff);
  --modal-input-background: var(--input-bg-base);
  --modal-check-color: var(--p-main);
  --background-color: var(--c-ff);
  --background-light-green: var(--s-1-weak, #f5f7f9);
  --text-black-2: var(--c-22);
  --text-gray-2: var(--c-22);
  --text-gray-6: var(--c-66);
  --text-gray-9: var(--c-99);
  --text-gray-c2: var(--c-c7);
  --switch-background-color: var(--c-c7, hsl(205deg, 16%, 77%));
  --float-ball-more-button-border-color: var(--c-f5, #f6f6f6);
  --float-ball-more-button-background-color: var(--c-ff);
  --float-ball-more-button-svg-color: #6c6f73;
  --service-bg-hover: var(--s-1-hover, #f7faff);
  --service-bg: var(--s-1-weak, #fafbfb);
  line-height: var(--line-height);
  font-family: var(--font-family);
  font-size: var(--font-size);
}

@media (max-width: 480px) {
  :root,
  #mount {
    --f-12: 10px;
    --f-14: 12px;
    --f-15: 13px;
    --f-16: 14px;
    --f-18: 16px;
    --f-20: 18px;
    --space-4: 4px;
    --space-6: 4px;
    --space-8: 6px;
    --space-12: 8px;
    --space-16: 12px;
    --space-18: 14px;
    --space-24: 18px;
    --radius-8: 6px;
    --radius-12: 10px;
    --radius-16: 12px;
    --control-height-lg: 38px;
    --shadow-lg: 0 12px 32px rgba(0, 0, 0, 0.1);
    --width-28: 24px;
    --width-24: 20px;
    --width-20: 16px;
    --width-18: 14px;
    --width-16: 12px;
    --width-label-md: 52px;
  }
}

#mount * {
  box-sizing: border-box;
}

[hidden] {
  display: none !important;
}

:where(#mount) a,
:where(#mount) [role="link"] {
  --color: var(--primary);
  --background-color: transparent;
  outline: none;
  background-color: var(--background-color);
  color: var(--color);
  -webkit-text-decoration: var(--text-decoration);
  text-decoration: var(--text-decoration);
  transition: background-color var(--transition), color var(--transition),
    box-shadow var(--transition), -webkit-text-decoration var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition),
    -webkit-text-decoration var(--transition);
}
:where(#mount) a:is([aria-current], :hover, :active, :focus),
:where(#mount) [role="link"]:is([aria-current], :hover, :active, :focus) {
  --color: var(--primary-hover);
  --text-decoration: underline;
}

:where(#mount) label {
  font-size: 13px;
  line-height: 1.3;
  color: var(--text-gray-2, #222222);
}

:where(#mount) button {
  width: 100%;
  font-family: inherit;
  font-size: 15px;
  line-height: 1.3;
  min-height: 44px;
  border-radius: 12px;
  padding: 0 14px;
  border: none;
  background-color: var(--primary, #ea4c89);
  color: #ffffff;
  cursor: pointer;
  transition: background-color 0.2s ease, box-shadow 0.2s ease, color 0.2s ease;
}

:where(#mount) button:hover {
  background-color: var(--primary-hover, #f082ac);
}

:where(#mount) button:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}

:where(#mount) select,
:where(#mount) input,
:where(#mount) textarea {
  font-family: inherit;
  color: var(--text-gray-2, #222222);
}

:where(#mount) select {
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  font-family: inherit;
  color: var(--text-gray-2, inherit);
  font-size: 13px;
  line-height: 1.3;
  outline: none;
  padding: 8px 16px;
  border: none;
  border-radius: 12px;
  background-color: var(--popup-item-background-color, transparent);
  background-image: var(--icon-xia, none);
  background-repeat: no-repeat;
  background-position: center right 12px;
  background-size: 16px auto;
  cursor: pointer;
}

:where(#mount) input[type="checkbox"] {
  accent-color: var(--primary, #ea4c89);
}

[type="checkbox"],
[type="radio"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 1.25em;
  height: 1.25em;
  margin-top: -0.125em;
  margin-right: 0.375em;
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: 0.375em;
  margin-inline-end: 0.375em;
  border-width: var(--border-width);
  font-size: inherit;
  vertical-align: middle;
  cursor: pointer;
}
[type="checkbox"]::-ms-check,
[type="radio"]::-ms-check {
  display: none;
}
[type="checkbox"]:checked,
[type="checkbox"]:checked:active,
[type="checkbox"]:checked:focus,
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-checkbox);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}
[type="checkbox"] ~ label,
[type="radio"] ~ label {
  display: inline-block;
  margin-right: 0.375em;
  margin-bottom: 0;
  cursor: pointer;
}

[type="checkbox"]:indeterminate {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-minus);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}

[type="radio"] {
  border-radius: 50%;
}
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary-inverse);
  border-width: 0.35em;
  background-image: none;
}

:where(#mount) [type="checkbox"][role="switch"] {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
  --color: var(--switch-color);
  width: 2.25em;
  height: 1.25em;
  border: var(--border-width) solid var(--border-color);
  border-radius: 1.25em;
  background-color: var(--background-color);
  line-height: 1.25em;
}
:where(#mount) [type="checkbox"][role="switch"]:focus {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
}
:where(#mount) [type="checkbox"][role="switch"]:checked {
  --background-color: var(--switch-checked-background-color);
  --border-color: var(--switch-checked-background-color);
}
:where(#mount) [type="checkbox"][role="switch"]:before {
  display: block;
  width: calc(1.25em - (var(--border-width) * 2));
  height: 100%;
  border-radius: 50%;
  background-color: var(--color);
  content: "";
  transition: margin 0.1s ease-in-out;
}
:where(#mount) [type="checkbox"][role="switch"]:checked {
  background-image: none;
}
:where(#mount) [type="checkbox"][role="switch"]:checked::before {
  margin-left: calc(1.125em - var(--border-width));
  -webkit-margin-start: calc(1.125em - var(--border-width));
  margin-inline-start: calc(1.125em - var(--border-width));
}

:where(#mount) [type="checkbox"][aria-invalid="false"],
:where(#mount) [type="checkbox"]:checked[aria-invalid="false"],
:where(#mount) [type="radio"][aria-invalid="false"],
:where(#mount) [type="radio"]:checked[aria-invalid="false"],
:where(#mount) [type="checkbox"][role="switch"][aria-invalid="false"],
:where(#mount) [type="checkbox"][role="switch"]:checked[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
:where(#mount) [type="checkbox"][aria-invalid="true"],
:where(#mount) [type="checkbox"]:checked[aria-invalid="true"],
:where(#mount) [type="radio"][aria-invalid="true"],
:where(#mount) [type="radio"]:checked[aria-invalid="true"],
:where(#mount) [type="checkbox"][role="switch"][aria-invalid="true"],
:where(#mount) [type="checkbox"][role="switch"]:checked[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}

.text-black {
  color: var(--text-black-2);
}

.text-gray-2 {
  color: var(--text-gray-2);
}

.text-gray-6 {
  color: var(--text-gray-6);
}

.text-gray-9 {
  color: var(--text-gray-9);
}

.text-gray-c2 {
  color: var(--text-gray-c2);
}

.pt-4 {
  padding-top: 16px;
}

.p-2 {
  padding: 8px;
}

.pl-5 {
  padding-left: 48px;
}

.p-0 {
  padding: 0;
}

.pl-2 {
  padding-left: 8px;
}

.pl-4 {
  padding-left: 24px;
}

.pt-2 {
  padding-top: 8px;
}

.pb-2 {
  padding-bottom: 8px;
}

.pb-4 {
  padding-bottom: 16px;
}

.pb-5 {
  padding-bottom: 20px;
}

.pr-5 {
  padding-right: 48px;
}

.text-sm {
  font-size: 13px;
}

.text-base {
  font-size: 16px;
}

.w-full {
  width: 100%;
}

.flex {
  display: flex;
}

.flex-row {
  flex-direction: row;
}

.flex-wrap {
  flex-wrap: wrap;
}

.flex-end {
  justify-content: flex-end;
}

.flex-grow {
  flex-grow: 1;
}

.justify-between {
  justify-content: space-between;
}

.mb-0 {
  margin-bottom: 0px;
}

.mb-2 {
  margin-bottom: 8px;
}

.mb-4 {
  margin-bottom: 16px;
}

.mb-3 {
  margin-bottom: 12px;
}

.inline-block {
  display: inline-block;
}

.py-2 {
  padding-top: 8px;
  padding-bottom: 8px;
}

.py-2-5 {
  padding-top: 6px;
  padding-bottom: 6px;
}

.mt-0 {
  margin-top: 0;
}

.mt-2 {
  margin-top: 8px;
}

.mt-3 {
  margin-top: 12px;
}

.mt-4 {
  margin-top: 16px;
}

.mt-5 {
  margin-top: 20px;
}

.mt-6 {
  margin-top: 24px;
}

.mb-1 {
  margin-bottom: 4px;
}

.ml-4 {
  margin-left: 24px;
}

.ml-3 {
  margin-left: 16px;
}

.ml-2 {
  margin-left: 8px;
}

.ml-1 {
  margin-left: 4px;
}

.mr-1 {
  margin-right: 4px;
}

.mr-2 {
  margin-right: 8px;
}

.mr-3 {
  margin-right: 16px;
}

.mx-2 {
  margin-left: 8px;
  margin-right: 8px;
}

.pl-3 {
  padding-left: 12px;
}

.pr-3 {
  padding-right: 12px;
}

.p-3 {
  padding: 12px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-3 {
  padding-top: 12px;
}

.px-6 {
  padding-left: 18px;
  padding-right: 18px;
}

.px-4 {
  padding-left: 16px;
  padding-right: 16px;
}

.pt-6 {
  padding-top: 20px;
}

.py-3 {
  padding-top: 12px;
  padding-bottom: 12px;
}

.py-0 {
  padding-top: 0;
  padding-bottom: 0;
}

.left-auto {
  left: auto !important;
}

.max-h-28 {
  max-height: 112px;
}

.max-h-30 {
  max-height: 120px;
}

.overflow-y-scroll {
  overflow-y: scroll;
}

.text-xs {
  font-size: 12px;
}

.inline-flex {
  display: inline-flex;
}

.flex-1 {
  flex: 1;
}

.flex-3 {
  flex: 3;
}

.flex-4 {
  flex: 4;
}

.flex-2 {
  flex: 2;
}

.items-center {
  align-items: center;
}

.max-content {
  width: max-content;
}

.justify-center {
  justify-content: center;
}

.items-end {
  align-items: flex-end;
}

.items-baseline {
  align-items: baseline;
}

.my-5 {
  margin-top: 48px;
  margin-bottom: 48px;
}

.my-4 {
  margin-top: 24px;
  margin-bottom: 24px;
}

.my-3 {
  margin-top: 16px;
  margin-bottom: 16px;
}

.pt-3 {
  padding-top: 12px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-2 {
  padding-top: 8px;
}

.px-2 {
  padding-left: 8px;
  padding-right: 8px;
}

.pt-1 {
  padding-top: 4px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.pb-2 {
  padding-bottom: 8px;
}

.justify-end {
  justify-content: flex-end;
}

.w-auto {
  width: auto;
}

.shrink-0 {
  flex-shrink: 0;
}

.text-right {
  text-align: right;
}

.clickable {
  cursor: pointer;
}

.close {
  cursor: pointer;
  width: 16px;
  height: 16px;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}

.padding-two-column {
  padding-left: 40px;
  padding-right: 40px;
}

.muted {
  color: #999;
}

.text-label {
  color: #666;
}

.display-none {
  display: none;
}

/* dark use #18232c */
@media (prefers-color-scheme: dark) {
  .text-label {
    color: #9ca3af;
  }
}

.text-decoration-none {
  text-decoration: none;
}

.text-decoration-none:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --text-decoration: none !important;
  background-color: transparent !important;
}

.text-overflow-ellipsis {
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
}

.max-w-20 {
  max-width: 180px;
  white-space: nowrap;
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --popup-footer-background-color: #e8eaeb;
  --popup-content-background-color: #ffffff;
  --popup-item-background-color: #f3f5f6;
  --popup-item-hover-background-color: #eaeced;
  --popup-trial-pro-background-color: #f9fbfc;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(75, 76, 77, 0.2);
  --service-select-border-color: #fafafa;
  --service-select-selected-background-color: #f3f5f6;
  --download-app-background: #f3f5f6;
}

[data-theme="dark"] {
  --popup-footer-background-color: #0d0d0d;
  --popup-content-background-color: #191919;
  --popup-item-background-color: #272727;
  --popup-item-hover-background-color: #333333;
  --popup-trial-pro-background-color: #222222;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
  --service-select-border-color: #2c2c2c;
  --service-select-selected-background-color: #333333;
  --download-app-background: #333;
}

#mount {
  min-width: 268px;
}

body {
  padding: 0;
  margin: 0 auto;
  min-width: 268px;
  border-radius: 10px;
}

.popup-container {
  font-size: 16px;
  --font-size: 16px;
  color: #666;
  background-color: var(--popup-footer-background-color);
  width: 316px;
  min-width: 316px;
}

.popup-content {
  background-color: var(--popup-content-background-color);
  border-radius: 0px 0px 12px 12px;
  padding: 16px 20px;
}

.immersive-translate-popup-overlay {
  position: fixed;
  top: 0;
  left: 0;
  height: 100%;
  width: 100%;
  touch-action: none;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 10px;
  border: 1px solid var(--muted-border-color);
}

.main-button {
  font-size: 15px;
  vertical-align: middle;
  border-radius: 12px;
  padding: unset;
  height: 44px;
  line-height: 44px;
}

select.language-select,
select.translate-service,
select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 0px;
  max-width: unset;
  flex: 1;
  overflow: hidden;
  font-size: 13px;
  border: none;
  border-radius: 8px;
  padding-right: 30px;
  padding-left: 0px;
  background-position: center right 12px;
  background-size: 16px auto;
  background-image: var(--icon-xia);
  text-overflow: ellipsis;
  color: var(--text-gray-2);
  background-color: transparent;
  box-shadow: unset !important;
  cursor: pointer;
}

select.more {
  background-position: center right;
  padding-right: 20px;
}

select.translate-service {
  color: var(--text-black-2);
}

.min-select-container.disabled {
  opacity: 0.5;
  pointer-events: none;
}

.popup-footer {
  background-color: var(--popup-footer-background-color);
  height: 40px;
}

.language-select-container {
  position: relative;
  width: 100%;
  background-color: var(--popup-item-background-color);
  height: 55px;
  border-radius: 12px;
}

select.language-select {
  color: var(--text-black-2);
  font-size: 14px;
  padding: 8px 24px 24px 16px;
  position: absolute;
  border-radius: 12px;
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
}

.language-select-container label {
  position: absolute;
  bottom: 10px;
  left: 16px;
  font-size: 12px;
  color: var(--text-gray-9);
  line-height: 12px;
  margin: 0;
}

.translation-service-container {
  background-color: var(--popup-item-background-color);
  border-radius: 12px;
}

.min-select-container {
  display: flex;
  justify-content: space-between;
  align-items: center;
  height: 44px;
  background-color: var(--popup-item-background-color);
  padding-left: 16px;
}

.min-select-container:first-child {
  border-top-left-radius: 10px;
  border-top-right-radius: 10px;
}

.min-select-container:last-child {
  border-bottom-left-radius: 10px;
  border-bottom-right-radius: 10px;
}

.min-select-container:only-child {
  border-radius: 10px;
}

.translate-mode {
  width: 44px;
  height: 44px;
  border-radius: 22px;
  background-color: var(--popup-item-background-color);
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  cursor: pointer;
}

.translate-mode svg {
  fill: var(--text-gray-2);
}

.widgets-container {
  display: flex;
  align-items: stretch;
  justify-content: space-between;
  width: 100%;
  gap: 9px;
}

/* 当只有两个小组件时的样式优化 */
.widgets-container.widgets-two-items {
  gap: 16px;
}

.widgets-container.widgets-two-items .widget-item {
  flex: 0 1 auto;
  min-width: 93px;
  max-width: 120px;
}

.widget-item {
  display: flex;
  max-width: 93px;
  flex-direction: row;
  align-items: center;
  justify-content: center;
  background-color: var(--popup-item-background-color);
  font-size: 12px;
  min-height: 44px;
  height: 100%;
  border-radius: 8px;
  cursor: pointer;
  flex: 1;
  padding: 8px 4px;
  text-align: center;
}

.widget-icon-text {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  color: var(--text-gray-2);
}

.widgets-container svg {
  fill: var(--text-gray-2);
  color: var(--text-gray-2);
}

.share-button-container {
  display: flex;
  align-items: center;
  cursor: pointer;
  padding: 2px 3px 0 8px;
}

.share-button-container svg {
  fill: var(--text-gray-9);
}

.min-select-container:hover,
.language-select-container:hover,
.widget-item:hover,
.translate-mode:hover {
  background-color: var(--popup-item-hover-background-color);
}

.main-button:hover {
  background-color: #f5508f;
}

.share-button-container:hover {
  background-color: var(--popup-item-background-color);
  border-radius: 6px;
}

.error-boundary {
  background: #fff2f0;
  border: 1px solid #ffccc7;
  display: flex;
  padding: 12px;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.88);
  word-break: break-all;
  margin: 12px;
  border-radius: 12px;
  flex-direction: column;
}

.upgrade-pro {
  border-radius: 11px;
  background: linear-gradient(57deg, #272727 19.8%, #696969 82.2%);
  padding: 2px 8px;
  transform: scale(0.85);
}

.upgrade-pro span {
  background: linear-gradient(180deg, #ffeab4 17.65%, #f8c235 85.29%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  font-size: 12px;
  margin-left: 4px;
}

.upgrade-pro svg {
  margin-top: -2px;
}

.upgrade-pro:hover {
  background: linear-gradient(57deg, #3d3d3d 19.8%, #949494 82.2%);
}

.border-bottom-radius-0 {
  border-bottom-left-radius: 0 !important;
  border-bottom-right-radius: 0 !important;
}

.trial-pro-container {
  border-radius: 0px 0px 12px 12px;
  background: var(--popup-trial-pro-background-color);
  display: flex;
  align-items: center;
  height: 44px;
  padding-left: 16px;
  padding-right: 12px;
  font-size: 12px;
}

.trial-pro-container label {
  line-height: 13px;
  color: var(--text-black-2);
}

.trial-pro-container img {
  margin-left: 5px;
}

.cursor-pointer {
  cursor: pointer;
}

.upgrade-pro-discount-act {
  height: 25px;
  display: flex;
  padding: 0 4px;
  align-items: center;
  border-radius: 15px;
  background: linear-gradient(
    90deg,
    #cefbfa 11.33%,
    #d7f56f 63.75%,
    #fccd5e 100%
  );
  transform: scale(0.9);
  box-shadow: 0px 1.8px 3.6px 0px rgba(0, 0, 0, 0.1);
  cursor: pointer;
}

.upgrade-pro-discount-act span {
  font-size: 12px;
  font-weight: 700;
  margin-left: 4px;
  color: #222222;
}

.upgrade-pro-discount-act:hover {
  text-decoration: unset;
  background: linear-gradient(
    90deg,
    #e2fffe 11.33%,
    #e6ff91 63.75%,
    #ffdf93 100%
  );
}

.custom-select-container {
  width: 200px;
  position: relative;
  flex: 1;
}

#translation-service-select {
  padding-right: 12px;
  padding-left: 6px;
}

.custom-select-content {
  border-radius: 12px;
  background: var(--popup-content-background-color);
  box-shadow: var(--service-select-content-shadow);
  border: 1px solid var(--service-select-border-color);
  padding: 4px 5px;
  position: absolute;
  left: -10px;
  right: 0;
  z-index: 100;
  overflow-y: auto;
}

.custom-select-item.default {
  width: 100%;
  padding: 0;
}

.custom-select-item {
  font-size: 13px;
  padding: 5px 6px;
  border-radius: 8px;
  display: flex;
  align-items: center;
  cursor: pointer;
  color: var(--text-black-2);
  width: auto;
  overflow: hidden;
  height: 30px;
  line-height: 30px;
}

.custom-select-item-img {
  width: 20px;
  height: 20px;
  margin-right: 4px;
}

@media (prefers-color-scheme: dark) {
  .custom-select-item-img {
    margin-right: 6px;
  }
}

.custom-select-content .custom-select-item.selected,
.custom-select-content .custom-select-item:hover {
  background: var(--service-select-selected-background-color);
}

.custom-select-item > span {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.custom-select-item-pro {
  font-size: 12px;
  margin-left: 6px;
  display: flex;
}

.custom-select-item-pro img {
  margin: 0 3px;
  width: 20px;
  flex-shrink: 0;
}

.custom-select-group-header {
  font-size: 12px;
  font-weight: 500;
  color: var(--text-gray-9);
  padding: 6px 8px 4px;
  margin-top: 2px;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

.more-container {
  position: relative;
}

.new-menu-indicator {
  position: absolute;
  width: 8px;
  height: 8px;
  background-color: #ef3434;
  border-radius: 50%;
  right: 18px;
  top: 4px;
}

.download-app {
  display: inline-flex;
  align-items: center;
  gap: 4px;
  border-radius: 8px;
  background: var(--download-app-background);
  padding: 4px 8px;
  color: var(--text-gray-6);
  font-size: 12px;
  cursor: pointer;
  transition: all 0.2s ease-in-out;
}

/* Popup 动画效果 */
@keyframes popup-fade-in {
  from {
    opacity: 0;
    transform: translateY(10px) scale(0.95);
  }
  to {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
}

@keyframes popup-fade-out {
  from {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
  to {
    opacity: 0;
    transform: translateY(10px) scale(0.95);
  }
}

.popup-generic-content {
  animation: popup-fade-in 0.2s ease-out;
}

.popup-generic-content.hiding {
  animation: popup-fade-out 0.15s ease-in;
}

select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 4px;
  max-width: 128px;
  overflow: hidden;
  color: var(--primary);
  font-size: 13px;
  border: none;
  padding: 0;
  padding-right: 20px;
  text-overflow: ellipsis;
  color: var(--color);
}
select.min-select-secondary {
  color: var(--color);
}
select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

select.transform-padding-left {
  padding-left: 12px;
  transform: translateX(-12px);
  background-position: center right 0px;
}

select.text-gray-6 {
  color: var(--text-gray-6);
}

/* dark use black, for windows */
@media (prefers-color-scheme: dark) {
  select.language-select option,
  select.translate-service option,
  select.min-select option {
    background-color: #666666;
  }
}

select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}



.activity-tips {
  border-radius: 8px;
  padding: 0px 8px;
  min-height: 28px;
  background: linear-gradient(83deg, #FACCDE -0.87%, #FCE7EF 43.13%, #FBD6E4 72.08%, #FFB3D1 96.34%);  gap: 2px;
  color: #333;
  cursor: pointer;
  gap: 4px;
}

.activity-tips-icon {
  width: 18px;
  height: 18px;
  flex-shrink: 0;
}

.countdown-container {
  min-width: 50px;
  text-align: left;
  font-weight: 600;
  font-size: 12px;
  letter-spacing: 0.01em;
}

.activity-tips-text {
  font-weight: 600;
  max-width: 100px;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

html {
  font-size: 17px;
}

@media print {
  .imt-fb-container {
    display: none !important;
  }
}

#mount {
  position: absolute;
  display: none;
  min-width: 250px;
  height: auto;
  --font-size: 17px;
  font-size: 17px;
}

/* float-ball */
.imt-fb-container {
  position: fixed;
  padding: 0;
  top: 335px;
  width: fit-content;
  display: flex;
  flex-direction: column;
  display: none;
  direction: ltr;
}

.imt-fb-container.left {
  align-items: flex-start;
  left: 0;
}

.imt-fb-container.right {
  align-items: flex-end;
  right: 0;
}

.imt-fb-btn {
  cursor: pointer;
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 56px;
  box-shadow: 2px 6px 10px 0px #0e121629;
}

.imt-fb-btn.left {
  border-top-right-radius: 36px;
  border-bottom-right-radius: 36px;
}

.imt-fb-btn.right {
  border-top-left-radius: 36px;
  border-bottom-left-radius: 36px;
}

.imt-fb-btn div {
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  width: 54px;
  display: flex;
  align-items: center;
}

.imt-fb-btn.left div {
  border-top-right-radius: 34px;
  border-bottom-right-radius: 34px;
  justify-content: flex-end;
}

.imt-fb-btn.right div {
  border-top-left-radius: 34px;
  border-bottom-left-radius: 34px;
}

.imt-fb-logo-img {
  width: 20px;
  height: 20px;
  margin: 0 10px;
}

.imt-fb-logo-img-big-bg {
  width: 28px;
  height: 28px;
  margin: 0;
  padding: 4px;
  background-color: #ed6d8f;
  border-radius: 50%;
  margin: 0 5px;
}

.imt-float-ball-translated {
  position: absolute;
  width: 11px;
  height: 11px;
  bottom: 4px;
  right: 20px;
}

.btn-animate {
  -webkit-transform: translate3d(0, 0, 0);
  transform: translate3d(0, 0, 0);
  -webkit-transition: -webkit-transform ease-out 250ms;
  transition: -webkit-transform ease-out 250ms;
  transition: transform ease-out 250ms;
  transition: transform ease-out 250ms, -webkit-transform ease-out 250ms;
}

.imt-fb-setting-btn {
  margin-right: 18px;
  width: 28px;
  height: 28px;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 20px;
  box-shadow: 2px 10px 24px 0px #0e121614;
  border: none;
}

.popup-container {
  border-radius: 20px;
}

.popup-content {
  border-radius: 20px 20px 12px 12px;
}
.popup-footer {
  border-radius: 20px;
}

.imt-fb-close-button {
  pointer-events: all;
  cursor: pointer;
  position: absolute;
  margin-top: -10px;
}

.imt-fb-close-content {
  padding: 22px;
  width: 320px;
  pointer-events: all;
}

.imt-fb-close-title {
  font-weight: 500;
  color: var(--h2-color);
}

.imt-fb-close-radio-content {
  background-color: var(--background-light-green);
  padding: 8px 20px;
}

.imt-fb-radio-sel,
.imt-fb-radio-nor {
  width: 16px;
  height: 16px;
  border-radius: 8px;
  flex-shrink: 0;
}

.imt-fb-radio-sel {
  border: 2px solid var(--primary);
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-radio-sel div {
  width: 8px;
  height: 8px;
  border-radius: 4px;
  background-color: var(--primary);
}

.imt-fb-radio-nor {
  border: 2px solid #d3d4d6;
}

.imt-fb-primary-btn {
  background-color: var(--primary);
  width: 72px;
  height: 32px;
  color: white;
  border-radius: 8px;
  text-align: center;
  line-height: 32px;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-default-btn {
  border: 1px solid var(--primary);
  width: 72px;
  height: 32px;
  border-radius: 8px;
  color: var(--primary);
  line-height: 32px;
  text-align: center;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-guide-container {
  width: 312px;
  transform: translateY(-45%);
}

.imt-fb-guide-bg {
  position: absolute;
  left: 30px;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  height: 100%;
  width: 90%;
}

.imt-fb-guide-bg.left {
  transform: scaleX(-1);
}

.imt-fb-guide-content {
  margin: 16px -30px 80px 0px;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.imt-fb-guide-content.left {
  margin: 16px 21px 60px 32px;
}

.imt-fb-guide-img {
  width: 220px;
  height: 112px;
}

.imt-fb-guide-message {
  font-size: 14px;
  line-height: 28px;
  color: #333333;
  white-space: pre-wrap;
  text-align: center;
  font-weight: 700;
  margin-bottom: 20px;
}

.imt-manga-guide-message {
  font-size: 16px;
  line-height: 24px;
  color: #333333;
  text-align: center;
  font-weight: 500;
  margin-bottom: 12px;
}

.imt-fb-guide-button {
  margin-top: 16px;
  line-height: 40px;
  height: 40px;
  padding: 0 20px;
  width: unset;
}

.imt-fb-more-buttons {
  box-shadow: 0px 2px 10px 0px #00000014;
  border: none;
  background: var(--float-ball-more-button-background-color);
  width: 36px;
  display: flex;
  flex-direction: column;
  border-radius: 18px;
  margin-top: 0px;
  padding: 7px 0 7px 0;
}

.imt-fb-more-buttons > div {
  margin: auto;
}

.imt-fb-side,
.imt-fb-reward {
  border-radius: 50%;
  cursor: pointer;
  pointer-events: all;
  position: relative;
}

.imt-fb-side {
  margin: 10px 0;
}

.imt-fb-new-badge {
  width: 26px;
  height: 14px;
  padding: 3px;
  background-color: #f53f3f;
  border-radius: 4px;
  position: absolute;
  top: -5px;
  right: 15px;
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-side *,
.imt-fb-reward * {
  pointer-events: all;
}

.imt-fb-more-button {
  width: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
}
/* Sheet.css */
.immersive-translate-sheet {
  position: fixed;
  transform: translateY(100%);
  /* Start off screen */
  left: 0;
  right: 0;
  background-color: white;
  transition: transform 0.3s ease-out;
  /* Smooth slide transition */
  box-shadow: 0px -2px 10px rgba(0, 0, 0, 0.1);
  /* Ensure it's above other content */
  bottom: 0;
  border-top-left-radius: 16px;
  border-top-right-radius: 16px;
  overflow: hidden;
}

.immersive-translate-sheet.visible {
  transform: translateY(0);
}

.immersive-translate-sheet-backdrop {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  opacity: 0;
  transition: opacity 0.3s ease-out;
}

.immersive-translate-sheet-backdrop.visible {
  opacity: 1;
}

.popup-container-sheet {
  max-width: 100vw;
  width: 100vw;
}

.imt-no-events svg * {
  pointer-events: none !important;
}

.imt-manga-button {
  width: 36px;
  display: flex;
  flex-direction: column;
  position: relative;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  pointer-events: all;
  margin: 0 0 10px 0;
  background-color: var(--float-ball-more-button-background-color);
  border-radius: 18px;
  filter: drop-shadow(0px 2px 10px rgba(0, 0, 0, 0.08));
  opacity: 0.5;
  right: 8px;
  padding: 10px 0 4px 0;
}

.imt-manga-feedback {
  cursor: pointer;
  margin-bottom: 10px;
}

.imt-fb-feedback {
  cursor: pointer;
  margin-top: 10px;
}

.imt-fb-upgrade-button {
  cursor: pointer;
  margin-top: 10px;
}

.imt-manga-button:hover {
  opacity: 1;
}

.imt-manga-translated {
  position: absolute;
  left: 24px;
  top: 20px;
}

.imt-float-ball-loading {
  animation: imt-loading-animation 0.6s infinite linear !important;
}

.imt-manga-guide-bg {
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  width: 100%;
  transform: translateY(-50%);
}
.imt-manga-guide-content {
  position: absolute;
  top: 15px;
  left: 0;
  right: 0;
  margin: 0 40px 0;
}

.img-manga-guide-button {
  width: fit-content;
  margin: 0 auto;
}

.img-manga-close {
  position: absolute;
  bottom: -200px;
  width: 32px;
  height: 32px;
  left: 0;
  right: 0;
  margin: auto;
  cursor: pointer;
}

.imt-fb-container.dragging .imt-fb-more-buttons,
.imt-fb-container.dragging .imt-manga-button,
.imt-fb-container.dragging .btn-animate:not(.imt-fb-btn) {
  display: none !important;
}

.imt-fb-container.dragging .imt-fb-btn {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  cursor: move !important;
}

.imt-fb-container.dragging .imt-fb-btn div {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  margin: 0 !important;
}

.imt-fb-container.dragging .imt-fb-btn.left,
.imt-fb-container.dragging .imt-fb-btn.right {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-btn.left div,
.imt-fb-container.dragging .imt-fb-btn.right div {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-logo-img {
  margin: 0 !important;
  padding: 4px !important;
}

.imt-fb-container.dragging .imt-float-ball-translated {
  right: 2px !important;
  bottom: 2px !important;
}

@-webkit-keyframes imt-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes imt-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.imt-fb-icon {
  color: #666666;
}

[data-theme="dark"] .imt-fb-icon {
  color: #b3b3b3;
}

[data-theme="light"] .imt-fb-icon {
  color: #666666;
}
</style><div id="mount" style="display: block;"><div class="imt-fb-container right notranslate " data-theme="dark" style="z-index: 2147483637; pointer-events: none; right: 0px; top: 261px; display: flex;"><div class="btn-animate" style="transform: translateX(-4px); opacity: 0.7; padding-left: 10px;"><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-btn imt-fb-more-button imt-fb-side"><svg class="imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M8.60547 12.9228C8.84029 12.9228 9.03755 13.0022 9.19629 13.161C9.3551 13.3198 9.43457 13.5171 9.43457 13.7519V18.5107C9.43457 18.7453 9.35513 18.9426 9.19629 19.1015C9.03755 19.2602 8.84029 19.3398 8.60547 19.3398H3.8457C3.61127 19.3397 3.41464 19.26 3.25586 19.1015C3.09712 18.9426 3.01758 18.7453 3.01758 18.5107V13.7519C3.01758 13.517 3.09712 13.3198 3.25586 13.161C3.41465 13.0023 3.61125 12.9229 3.8457 12.9228H8.60547ZM17.208 12.9228C17.4427 12.9228 17.6399 13.0022 17.7988 13.161C17.9575 13.3198 18.0371 13.5171 18.0371 13.7519V18.5107C18.0371 18.7453 17.9576 18.9426 17.7988 19.1015C17.6399 19.2602 17.4427 19.3398 17.208 19.3398H12.4492C12.2144 19.3398 12.0171 19.2602 11.8584 19.1015C11.6995 18.9426 11.6201 18.7453 11.6201 18.5107V13.7519C11.6201 13.517 11.6995 13.3198 11.8584 13.161C12.0171 13.0022 12.2144 12.9228 12.4492 12.9228H17.208ZM4.39258 17.9648H8.05957V14.2978H4.39258V17.9648ZM12.9951 17.9648H16.6621V14.2978H12.9951V17.9648ZM14.7598 2.92179C14.8641 2.57295 15.3576 2.57295 15.4619 2.92179L15.9561 4.57511C16.1376 5.18219 16.5965 5.66815 17.1924 5.8837L18.7412 6.44327C19.0635 6.56002 19.0633 7.01583 18.7412 7.13273L17.1924 7.69327C16.5966 7.90881 16.1376 8.39389 15.9561 9.00089L15.4619 10.6552C15.3575 11.0038 14.8642 11.0037 14.7598 10.6552L14.2646 9.00089C14.0831 8.39401 13.625 7.90881 13.0293 7.69327L11.4805 7.13273C11.158 7.01598 11.1579 6.55996 11.4805 6.44327L13.0293 5.8837C13.6251 5.66814 14.0831 5.18219 14.2646 4.57511L14.7598 2.92179ZM8.60547 4.32023C8.84029 4.32023 9.03755 4.39977 9.19629 4.55851C9.35496 4.71727 9.43448 4.91396 9.43457 5.14835V9.90812C9.43457 10.1429 9.35518 10.3402 9.19629 10.4989C9.03755 10.6578 8.84029 10.7372 8.60547 10.7372H3.8457C3.61131 10.7371 3.41463 10.6576 3.25586 10.4989C3.09712 10.3402 3.01758 10.1429 3.01758 9.90812V5.14835C3.01767 4.91386 3.09721 4.71731 3.25586 4.55851C3.41466 4.39986 3.61121 4.32032 3.8457 4.32023H8.60547ZM4.39258 9.36222H8.05957V5.69523H4.39258V9.36222Z" fill="currentColor"></path></svg><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg" style="position: absolute; right: 0px; top: 0px; display: none; transform: translate(30%, -30%);"><g clip-path="url(#clip0_34242_2353)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14Z" fill="#B1B1B1" fill-opacity="0.32"></path><mask id="mask0_34242_2353" maskUnits="userSpaceOnUse" x="1" y="1" width="12" height="12" style="mask-type: alpha;"><rect x="1" y="1" width="12" height="12" fill="#D9D9D9"></rect></mask><g mask="url(#mask0_34242_2353)"><path d="M7.86447 3.67324H6.13622V4.72999L4.80409 3.39199C4.75018 3.33699 4.70972 3.27808 4.68272 3.21524C4.65572 3.15241 4.64222 3.09533 4.64222 3.04399C4.64222 2.93141 4.68193 2.8352 4.76134 2.75537C4.84076 2.67562 4.94514 2.63574 5.07447 2.63574H8.98322C9.12864 2.63574 9.25147 2.68883 9.35172 2.79499C9.45189 2.90124 9.50197 3.04578 9.50197 3.22862C9.50197 3.35203 9.46122 3.46245 9.37972 3.55987C9.29822 3.65737 9.18897 3.69516 9.05197 3.67324H8.90197V6.36774C8.90197 6.51316 8.85214 6.63599 8.75247 6.73624C8.65272 6.83641 8.53051 6.88649 8.38585 6.88649C8.24118 6.88649 8.11809 6.83641 8.01659 6.73624C7.91518 6.63599 7.86447 6.51316 7.86447 6.36774V3.67324ZM6.4816 11.974V9.13599H4.57509C4.36193 9.13599 4.19043 9.06703 4.06059 8.92912C3.93076 8.79112 3.86584 8.62983 3.86584 8.44524C3.86584 8.35591 3.88509 8.26499 3.92359 8.17249C3.96209 8.08008 4.01984 7.99437 4.09684 7.91537L5.09872 6.89549V6.36149L2.32422 3.58412C2.22664 3.48645 2.1788 3.37678 2.18072 3.25512C2.18272 3.13345 2.23155 3.02483 2.32722 2.92924C2.42489 2.83158 2.53614 2.78274 2.66097 2.78274C2.7858 2.78274 2.89701 2.83158 2.99459 2.92924L10.9898 10.9245C11.0863 11.0209 11.1351 11.13 11.1361 11.2516C11.1371 11.3733 11.0898 11.4839 10.9941 11.5835C10.8984 11.6772 10.7867 11.7235 10.6588 11.7225C10.5311 11.7215 10.4194 11.6732 10.3237 11.5776L7.87909 9.13599L7.51909 9.14199V11.974C7.51909 12.1195 7.46926 12.2423 7.3696 12.3425C7.26985 12.4427 7.14764 12.4927 7.00297 12.4927C6.8583 12.4927 6.73522 12.4427 6.63372 12.3425C6.5323 12.2423 6.4816 12.1195 6.4816 11.974ZM5.35909 8.09849H6.83872L6.08834 7.35124L6.09434 7.35724L5.35909 8.09849Z" fill="white"></path></g></g><defs><clippath id="clip0_34242_2353"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div></div></div></div><div hidden="" class="imt-no-events btn-animate " id="manga-button" style="position: relative;"><div class="imt-manga-button" style="transform: translateX(2px);"><div class=" " style="position: relative; pointer-events: all; display: inline-block;"><div><svg class="imt-manga-feedback imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.0003 14.2749C11.213 14.2749 11.3895 14.2047 11.5299 14.0643C11.6705 13.9239 11.7408 13.7473 11.7408 13.5345C11.7408 13.3218 11.6705 13.1453 11.5299 13.0049C11.3895 12.8645 11.213 12.7943 11.0003 12.7943C10.7877 12.7943 10.6111 12.8645 10.4707 13.0049C10.3302 13.1453 10.2599 13.3218 10.2599 13.5345C10.2599 13.7473 10.3302 13.9239 10.4707 14.0643C10.6111 14.2047 10.7877 14.2749 11.0003 14.2749ZM11.0003 11.0842C11.1954 11.0842 11.3587 11.0185 11.4903 10.8869C11.622 10.7552 11.6878 10.5918 11.6878 10.3967V6.23645C11.6878 6.04135 11.622 5.87803 11.4903 5.74649C11.3587 5.6148 11.1954 5.54895 11.0003 5.54895C10.8052 5.54895 10.6419 5.6148 10.5104 5.74649C10.3787 5.87803 10.3128 6.04135 10.3128 6.23645V10.3967C10.3128 10.5918 10.3787 10.7552 10.5104 10.8869C10.6419 11.0185 10.8052 11.0842 11.0003 11.0842ZM5.53562 16.8311L3.70045 18.666C3.43966 18.9269 3.13968 18.9861 2.80051 18.8434C2.4615 18.7005 2.29199 18.4434 2.29199 18.072V4.73816C2.29199 4.27509 2.45241 3.88314 2.77324 3.5623C3.09408 3.24147 3.48603 3.08105 3.9491 3.08105H18.0516C18.5146 3.08105 18.9066 3.24147 19.2274 3.5623C19.5482 3.88314 19.7087 4.27509 19.7087 4.73816V15.174C19.7087 15.637 19.5482 16.029 19.2274 16.3498C18.9066 16.6706 18.5146 16.8311 18.0516 16.8311H5.53562ZM4.95033 15.4561H18.0516C18.1221 15.4561 18.1868 15.4266 18.2454 15.3678C18.3042 15.3092 18.3337 15.2445 18.3337 15.174V4.73816C18.3337 4.66758 18.3042 4.60295 18.2454 4.54428C18.1868 4.48546 18.1221 4.45605 18.0516 4.45605H3.9491C3.87851 4.45605 3.81389 4.48546 3.75522 4.54428C3.6964 4.60295 3.66699 4.66758 3.66699 4.73816V16.7254L4.95033 15.4561Z" fill="currentColor"></path></svg></div></div><div style="position: relative;"><svg width="32" height="32" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><g id="manhua"><path id="Vector" d="M14.8853 4.92364C14.8853 4.92364 16.3905 10.4362 22.6668 4C22.6668 4 20.3381 10.8907 25.3364 10.0843C25.3364 10.0843 22.0563 15.6994 29 18.0599C29 18.0599 22.9934 19.306 21.1617 28C21.1617 28 17.7679 24.54 14.8853 27.3549C14.8853 27.3549 13.3233 23.5724 7.33097 26.27C7.33097 26.27 10.1141 20.6549 4.83179 21.0507C4.83179 21.0507 7.16057 18.8955 3 15.9047C3 15.9047 7.50137 16.1833 6.33697 11.7117C6.33697 11.7117 10.0005 12.3421 8.66576 6.82957C8.65156 6.81491 12.4855 9.80574 14.8853 4.92364Z" fill="#ED6D8F"></path><path id="Vector_2" d="M20.8599 13.7022C20.885 13.1361 20.9543 12.5713 20.9959 12.0052C21.0337 11.568 20.8107 11.2794 20.3876 11.18C20.0759 11.1013 19.7508 11.0867 19.433 11.137C19.1951 11.1945 18.9542 11.2396 18.7113 11.2721C18.2403 11.3028 17.9973 11.5275 17.9796 11.988C17.977 12.0833 17.9596 12.1777 17.928 12.268C17.3034 13.9102 16.6774 15.5499 16.0503 17.1873C16.0301 17.2401 16.0062 17.2904 15.9671 17.3776C15.7291 16.8975 15.4281 16.4898 15.2745 15.9986C14.8073 14.5152 14.3186 13.033 13.8312 11.5594C13.6826 11.1112 13.3489 10.9344 12.8754 11.0216C12.7889 11.0365 12.7008 11.0398 12.6134 11.0314C12.2241 10.9938 11.8311 11.0404 11.4623 11.1677C11.0946 11.2991 10.9498 11.557 11.0152 11.9254C11.0428 12.0371 11.0643 12.1503 11.0795 12.2643C11.1223 13.1902 11.1777 14.1087 11.2054 15.0321C11.257 16.7992 11.2117 18.5651 11.0858 20.3284C11.0644 20.6354 11.0304 20.9424 11.0228 21.2494C11.0115 21.6092 11.1613 21.7811 11.5266 21.8143C11.9976 21.8573 12.4711 21.8708 12.9421 21.9088C13.0309 21.9201 13.121 21.9003 13.1962 21.8528C13.2714 21.8053 13.3268 21.7334 13.3527 21.6497C13.3996 21.5394 13.4252 21.4216 13.4282 21.3022C13.4295 20.8258 13.4207 20.3493 13.4081 19.8741C13.393 19.3264 13.3917 18.7763 13.3438 18.231C13.2857 17.5839 13.266 16.934 13.2847 16.2847C13.2847 16.2466 13.291 16.2073 13.2985 16.1312C13.3338 16.2024 13.3514 16.2356 13.3665 16.2712C13.9017 17.5228 14.3617 18.8037 14.7443 20.1074C14.7928 20.2421 14.7928 20.3889 14.7443 20.5237C14.6322 20.8196 14.7141 21.037 14.9659 21.1377C15.4445 21.3268 15.9331 21.4926 16.4155 21.6731C16.4865 21.7033 16.566 21.7091 16.6408 21.6895C16.7157 21.6698 16.7815 21.6259 16.8273 21.565C16.9085 21.4643 16.9743 21.3526 17.0225 21.2335C17.0537 21.1374 17.0798 21.0399 17.1006 20.9412C17.3185 20.2425 17.5653 19.5499 17.7517 18.8438C17.9785 17.9723 18.2624 17.1158 18.6018 16.2798C18.6201 16.2439 18.6411 16.2094 18.6647 16.1766C18.6761 16.2319 18.6761 16.254 18.6761 16.2761C18.6345 17.59 18.5955 18.8978 18.5501 20.2056C18.5363 20.5949 18.491 20.9829 18.4809 21.3722C18.4721 21.705 18.6207 21.8708 18.9557 21.9002C19.4355 21.9432 19.9191 21.9592 20.4002 21.9973C20.4888 22.0079 20.5784 21.9875 20.653 21.9399C20.7277 21.8922 20.7827 21.8203 20.8082 21.7369C20.8531 21.6305 20.8766 21.5167 20.8775 21.4017C20.88 20.7668 20.8674 20.132 20.8674 19.4971C20.8662 19.2846 20.8687 19.0722 20.8523 18.8622C20.8158 18.3968 20.7264 17.9314 20.7339 17.4685C20.7515 16.2122 20.8044 14.9572 20.8599 13.7022Z" fill="white"></path></g></svg></div></div></div><div class=" " style="position: relative; pointer-events: all; display: inline-block;"><div><div style="display: flex; align-items: center; flex-direction: row;"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: block; opacity: 0;"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg><div class="imt-fb-btn  right btn-animate " dir="ltr" style="opacity: 0.7; transform: translateX(15px);"><div><svg class="imt-fb-logo-img imt-fb-logo-img-big-bg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path fill="none" d="M0 0h24v24H0z"></path><path d="M5 15v2a2 2 0 0 0 1.85 1.995L7 19h3v2H7a4 4 0 0 1-4-4v-2h2zm13-5l4.4 11h-2.155l-1.201-3h-4.09l-1.199 3h-2.154L16 10h2zm-1 2.885L15.753 16h2.492L17 12.885zM8 2v2h4v7H8v3H6v-3H2V4h4V2h2zm9 1a4 4 0 0 1 4 4v2h-2V7a2 2 0 0 0-2-2h-3V3h3zM6 6H4v3h2V6zm4 0H8v3h2V6z" fill="rgba(255,255,255,1)"></path></svg><svg class="imt-float-ball-translated" width="11" height="11" viewBox="0 0 11 11" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="5.5" cy="5.5" r="5.5" fill="#60BB4C"></circle><path d="M1.40869 5.87858L2.24161 5.18962L4.15357 6.64214C4.15357 6.64214 6.33559 4.15566 9.0067 2.48145L9.32553 2.87514C9.32553 2.87514 6.28678 5.55844 4.71748 9.07881L1.40869 5.87858Z" fill="#EFF8ED"></path></svg></div></div><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg" style="display: none; opacity: 0;"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div></div></div><div style="position: relative; width: 100%; opacity: 0;"><div title="关闭悬浮球" class="imt-fb-close-button" style="transform: translateX(100%);"><svg width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_2589_9951)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14ZM4.183 5.064L6.118 7L4.183 8.936C4.12409 8.99361 4.07719 9.06234 4.04502 9.1382C4.01285 9.21406 3.99605 9.29554 3.99559 9.37794C3.99513 9.46034 4.01101 9.54201 4.04234 9.61823C4.07366 9.69444 4.11978 9.76369 4.17805 9.82195C4.23631 9.88022 4.30556 9.92634 4.38177 9.95766C4.45799 9.98898 4.53966 10.0049 4.62206 10.0044C4.70446 10.004 4.78594 9.98715 4.8618 9.95498C4.93766 9.92281 5.00639 9.87591 5.064 9.817L7 7.882L8.936 9.817C9.05327 9.93168 9.21104 9.99548 9.37506 9.99457C9.53908 9.99365 9.69612 9.92809 9.8121 9.8121C9.92809 9.69612 9.99365 9.53908 9.99457 9.37506C9.99548 9.21104 9.93168 9.05327 9.817 8.936L7.882 7L9.817 5.064C9.87591 5.00639 9.92281 4.93766 9.95498 4.8618C9.98715 4.78594 10.004 4.70446 10.0044 4.62206C10.0049 4.53966 9.98898 4.45799 9.95766 4.38177C9.92634 4.30556 9.88022 4.23631 9.82195 4.17805C9.76369 4.11978 9.69444 4.07366 9.61823 4.04234C9.54201 4.01101 9.46034 3.99513 9.37794 3.99559C9.29554 3.99605 9.21406 4.01285 9.1382 4.04502C9.06234 4.07719 8.99361 4.12409 8.936 4.183L7 6.118L5.064 4.183C4.94673 4.06832 4.78896 4.00452 4.62494 4.00543C4.46092 4.00635 4.30388 4.07191 4.1879 4.1879C4.07191 4.30388 4.00635 4.46092 4.00543 4.62494C4.00452 4.78896 4.06832 4.94673 4.183 5.064Z" fill="#B1B1B1" fill-opacity="0.32"></path></g><defs><clippath id="clip0_2589_9951"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div></div><div class="imt-fb-more-buttons btn-animate" style="margin-top: 10px; transform: translateX(60px);"><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-more-button"><svg class="imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg" style="width: 22px; height: 22px;"><path d="M16 7.66699H10.375" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><path d="M11.625 14.333L6 14.333" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><path d="M14.125 16C15.1605 16 16 15.1605 16 14.125C16 13.0895 15.1605 12.25 14.125 12.25C13.0895 12.25 12.25 13.0895 12.25 14.125C12.25 15.1605 13.0895 16 14.125 16Z" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><path d="M7.875 9.75C8.91053 9.75 9.75 8.91053 9.75 7.875C9.75 6.83947 8.91053 6 7.875 6C6.83947 6 6 6.83947 6 7.875C6 8.91053 6.83947 9.75 7.875 9.75Z" stroke="currentColor" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><rect x="3" y="3" width="16" height="16" rx="1.66667" stroke="currentColor" stroke-width="1.4"></rect></svg></div></div></div><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-more-button"><svg class="imt-fb-feedback imt-fb-icon" width="22" height="22" viewBox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.0003 14.2749C11.213 14.2749 11.3895 14.2047 11.5299 14.0643C11.6705 13.9239 11.7408 13.7473 11.7408 13.5345C11.7408 13.3218 11.6705 13.1453 11.5299 13.0049C11.3895 12.8645 11.213 12.7943 11.0003 12.7943C10.7877 12.7943 10.6111 12.8645 10.4707 13.0049C10.3302 13.1453 10.2599 13.3218 10.2599 13.5345C10.2599 13.7473 10.3302 13.9239 10.4707 14.0643C10.6111 14.2047 10.7877 14.2749 11.0003 14.2749ZM11.0003 11.0842C11.1954 11.0842 11.3587 11.0185 11.4903 10.8869C11.622 10.7552 11.6878 10.5918 11.6878 10.3967V6.23645C11.6878 6.04135 11.622 5.87803 11.4903 5.74649C11.3587 5.6148 11.1954 5.54895 11.0003 5.54895C10.8052 5.54895 10.6419 5.6148 10.5104 5.74649C10.3787 5.87803 10.3128 6.04135 10.3128 6.23645V10.3967C10.3128 10.5918 10.3787 10.7552 10.5104 10.8869C10.6419 11.0185 10.8052 11.0842 11.0003 11.0842ZM5.53562 16.8311L3.70045 18.666C3.43966 18.9269 3.13968 18.9861 2.80051 18.8434C2.4615 18.7005 2.29199 18.4434 2.29199 18.072V4.73816C2.29199 4.27509 2.45241 3.88314 2.77324 3.5623C3.09408 3.24147 3.48603 3.08105 3.9491 3.08105H18.0516C18.5146 3.08105 18.9066 3.24147 19.2274 3.5623C19.5482 3.88314 19.7087 4.27509 19.7087 4.73816V15.174C19.7087 15.637 19.5482 16.029 19.2274 16.3498C18.9066 16.6706 18.5146 16.8311 18.0516 16.8311H5.53562ZM4.95033 15.4561H18.0516C18.1221 15.4561 18.1868 15.4266 18.2454 15.3678C18.3042 15.3092 18.3337 15.2445 18.3337 15.174V4.73816C18.3337 4.66758 18.3042 4.60295 18.2454 4.54428C18.1868 4.48546 18.1221 4.45605 18.0516 4.45605H3.9491C3.87851 4.45605 3.81389 4.48546 3.75522 4.54428C3.6964 4.60295 3.66699 4.66758 3.66699 4.73816V16.7254L4.95033 15.4561Z" fill="currentColor"></path></svg></div></div></div></div><div hidden="" id="immersive-translate-popup-overlay" class="immersive-translate-popup-overlay"><div class="immersive-translate-popup-wrapper" style="position: fixed; top: 261px; right: 65px;"></div></div></div></div></template></div></html>